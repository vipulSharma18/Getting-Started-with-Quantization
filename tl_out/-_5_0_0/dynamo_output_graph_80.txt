class GraphModule(torch.nn.Module):
    def forward(self, L_input_ids_: "i64[1, 1][1, 1]cuda:0", L_self_modules_model_modules_embed_tokens_parameters_weight_: "f16[128256, 4096][4096, 1]cuda:0", L_cache_position_: "i64[1][1]cuda:0", L_position_ids_: "i64[1, 1][1, 1]cuda:0", L_attention_mask_: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0", L_self_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_0_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_0_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_1_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_1_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_1_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_1_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_1_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_2_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_2_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_2_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_2_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_2_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_3_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_3_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_3_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_3_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_3_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_4_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_4_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_4_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_4_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_4_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_5_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_5_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_5_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_5_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_5_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_6_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_6_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_6_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_6_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_6_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_7_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_7_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_7_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_7_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_7_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_8_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_8_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_8_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_8_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_8_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_9_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_9_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_9_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_9_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_9_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_10_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_10_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_10_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_10_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_10_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_11_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_11_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_11_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_11_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_11_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_12_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_12_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_12_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_12_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_12_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_13_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_13_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_13_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_13_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_13_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_14_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_14_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_14_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_14_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_14_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_15_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_15_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_15_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_15_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_15_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_16_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_16_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_16_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_16_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_16_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_17_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_17_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_17_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_17_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_17_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_18_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_18_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_18_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_18_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_18_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_19_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_19_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_19_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_19_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_19_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_20_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_20_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_20_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_20_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_20_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_21_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_21_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_21_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_21_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_21_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_22_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_22_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_22_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_22_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_22_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_23_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_23_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_23_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_23_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_23_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_24_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_24_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_24_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_24_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_24_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_25_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_25_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_25_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_25_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_25_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_26_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_26_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_26_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_26_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_26_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_27_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_27_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_27_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_27_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_27_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_28_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_28_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_28_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_28_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_28_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_29_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_29_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_29_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_29_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_29_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_30_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_30_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_30_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_30_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_30_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_layers_modules_31_modules_input_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]cuda:0", L_past_key_values_layers_31_keys: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_past_key_values_layers_31_values: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", L_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_31_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_31_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]cuda:0", L_self_modules_model_modules_norm_parameters_weight_: "f16[4096][1]cuda:0", L_self_modules_lm_head_parameters_weight_: "f16[128256, 4096][4096, 1]cuda:0"):
        l_input_ids_ = L_input_ids_
        l_self_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_embed_tokens_parameters_weight_
        l_cache_position_ = L_cache_position_
        l_position_ids_ = L_position_ids_
        l_attention_mask_ = L_attention_mask_
        l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_0_keys = L_past_key_values_layers_0_keys
        l_past_key_values_layers_0_values = L_past_key_values_layers_0_values
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_1_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_1_keys = L_past_key_values_layers_1_keys
        l_past_key_values_layers_1_values = L_past_key_values_layers_1_values
        l_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_1_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_1_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_1_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_1_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_2_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_2_keys = L_past_key_values_layers_2_keys
        l_past_key_values_layers_2_values = L_past_key_values_layers_2_values
        l_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_2_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_2_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_2_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_2_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_3_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_3_keys = L_past_key_values_layers_3_keys
        l_past_key_values_layers_3_values = L_past_key_values_layers_3_values
        l_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_3_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_3_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_3_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_3_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_4_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_4_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_4_keys = L_past_key_values_layers_4_keys
        l_past_key_values_layers_4_values = L_past_key_values_layers_4_values
        l_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_4_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_4_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_4_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_4_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_5_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_5_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_5_keys = L_past_key_values_layers_5_keys
        l_past_key_values_layers_5_values = L_past_key_values_layers_5_values
        l_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_5_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_5_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_5_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_5_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_6_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_6_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_6_keys = L_past_key_values_layers_6_keys
        l_past_key_values_layers_6_values = L_past_key_values_layers_6_values
        l_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_6_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_6_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_6_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_6_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_7_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_7_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_7_keys = L_past_key_values_layers_7_keys
        l_past_key_values_layers_7_values = L_past_key_values_layers_7_values
        l_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_7_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_7_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_7_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_7_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_8_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_8_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_8_keys = L_past_key_values_layers_8_keys
        l_past_key_values_layers_8_values = L_past_key_values_layers_8_values
        l_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_8_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_8_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_8_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_8_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_9_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_9_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_9_keys = L_past_key_values_layers_9_keys
        l_past_key_values_layers_9_values = L_past_key_values_layers_9_values
        l_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_9_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_9_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_9_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_9_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_10_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_10_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_10_keys = L_past_key_values_layers_10_keys
        l_past_key_values_layers_10_values = L_past_key_values_layers_10_values
        l_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_10_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_10_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_10_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_10_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_11_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_11_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_11_keys = L_past_key_values_layers_11_keys
        l_past_key_values_layers_11_values = L_past_key_values_layers_11_values
        l_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_11_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_11_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_11_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_11_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_12_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_12_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_12_keys = L_past_key_values_layers_12_keys
        l_past_key_values_layers_12_values = L_past_key_values_layers_12_values
        l_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_12_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_12_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_12_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_12_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_13_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_13_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_13_keys = L_past_key_values_layers_13_keys
        l_past_key_values_layers_13_values = L_past_key_values_layers_13_values
        l_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_13_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_13_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_13_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_13_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_14_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_14_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_14_keys = L_past_key_values_layers_14_keys
        l_past_key_values_layers_14_values = L_past_key_values_layers_14_values
        l_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_14_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_14_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_14_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_14_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_15_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_15_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_15_keys = L_past_key_values_layers_15_keys
        l_past_key_values_layers_15_values = L_past_key_values_layers_15_values
        l_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_15_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_15_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_15_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_15_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_16_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_16_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_16_keys = L_past_key_values_layers_16_keys
        l_past_key_values_layers_16_values = L_past_key_values_layers_16_values
        l_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_16_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_16_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_16_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_16_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_17_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_17_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_17_keys = L_past_key_values_layers_17_keys
        l_past_key_values_layers_17_values = L_past_key_values_layers_17_values
        l_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_17_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_17_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_17_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_17_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_18_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_18_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_18_keys = L_past_key_values_layers_18_keys
        l_past_key_values_layers_18_values = L_past_key_values_layers_18_values
        l_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_18_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_18_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_18_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_18_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_19_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_19_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_19_keys = L_past_key_values_layers_19_keys
        l_past_key_values_layers_19_values = L_past_key_values_layers_19_values
        l_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_19_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_19_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_19_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_19_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_20_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_20_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_20_keys = L_past_key_values_layers_20_keys
        l_past_key_values_layers_20_values = L_past_key_values_layers_20_values
        l_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_20_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_20_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_20_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_20_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_21_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_21_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_21_keys = L_past_key_values_layers_21_keys
        l_past_key_values_layers_21_values = L_past_key_values_layers_21_values
        l_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_21_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_21_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_21_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_21_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_22_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_22_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_22_keys = L_past_key_values_layers_22_keys
        l_past_key_values_layers_22_values = L_past_key_values_layers_22_values
        l_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_22_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_22_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_22_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_22_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_23_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_23_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_23_keys = L_past_key_values_layers_23_keys
        l_past_key_values_layers_23_values = L_past_key_values_layers_23_values
        l_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_23_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_23_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_23_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_23_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_24_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_24_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_24_keys = L_past_key_values_layers_24_keys
        l_past_key_values_layers_24_values = L_past_key_values_layers_24_values
        l_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_24_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_24_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_24_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_24_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_25_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_25_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_25_keys = L_past_key_values_layers_25_keys
        l_past_key_values_layers_25_values = L_past_key_values_layers_25_values
        l_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_25_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_25_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_25_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_25_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_26_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_26_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_26_keys = L_past_key_values_layers_26_keys
        l_past_key_values_layers_26_values = L_past_key_values_layers_26_values
        l_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_26_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_26_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_26_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_26_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_27_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_27_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_27_keys = L_past_key_values_layers_27_keys
        l_past_key_values_layers_27_values = L_past_key_values_layers_27_values
        l_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_27_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_27_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_27_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_27_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_28_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_28_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_28_keys = L_past_key_values_layers_28_keys
        l_past_key_values_layers_28_values = L_past_key_values_layers_28_values
        l_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_28_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_28_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_28_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_28_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_29_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_29_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_29_keys = L_past_key_values_layers_29_keys
        l_past_key_values_layers_29_values = L_past_key_values_layers_29_values
        l_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_29_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_29_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_29_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_29_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_30_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_30_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_30_keys = L_past_key_values_layers_30_keys
        l_past_key_values_layers_30_values = L_past_key_values_layers_30_values
        l_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_30_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_30_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_30_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_30_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_31_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_31_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_v_proj_parameters_weight_
        l_past_key_values_layers_31_keys = L_past_key_values_layers_31_keys
        l_past_key_values_layers_31_values = L_past_key_values_layers_31_values
        l_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_31_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_31_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_31_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_31_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_model_modules_norm_parameters_weight_ = L_self_modules_model_modules_norm_parameters_weight_
        l_self_modules_lm_head_parameters_weight_ = L_self_modules_lm_head_parameters_weight_
        
         # File: /app/low_bit_inference/model.py:346 in forward, code: inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)
        inputs_embeds: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_input_ids_, l_self_modules_model_modules_embed_tokens_parameters_weight_, 128004, None, 2.0, False, False);  l_input_ids_ = l_self_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:65 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /app/low_bit_inference/model.py:66 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 1][1, 1, 1]cuda:0" = l_position_ids_[(slice(None, None, None), None, slice(None, None, None))];  l_position_ids_ = None
        position_ids_expanded: "f32[1, 1, 1][1, 1, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /app/low_bit_inference/model.py:70 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 1][1, 1, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 1, 64][64, 1, 1]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /app/low_bit_inference/model.py:71 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 1, 128][128, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /app/low_bit_inference/model.py:72 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 1, 128][128, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 1, 128][128, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /app/low_bit_inference/model.py:73 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 1, 128][128, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 1, 128][128, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /app/low_bit_inference/model.py:75 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "f16[1, 1, 128][128, 128, 1]cuda:0" = cos_1.to(dtype = torch.float16);  cos_1 = None
        sin_2: "f16[1, 1, 128][128, 128, 1]cuda:0" = sin_1.to(dtype = torch.float16);  sin_1 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx = torch.autograd.function.FunctionCtx();  function_ctx = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = inputs_embeds.contiguous()
        contiguous_1: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X: "f16[1, 4096][4096, 1]cuda:0" = contiguous.view(-1, 4096);  contiguous = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 326, constant_args_idx = 325, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y, 'X_ptr': X, 'W_ptr': contiguous_1, 'RSTD_ptr': RSTD});  X = contiguous_1 = RSTD = triton_kernel_wrapper_mutation = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_1: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y.view(1, 1, 4096);  Y = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_1, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_2: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear.view((1, 1, -1, 128));  linear = None
        query_states: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_1, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_3: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_1.view((1, 1, -1, 128));  linear_1 = None
        key_states: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_3.transpose(1, 2);  view_3 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_1, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_1 = l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_4: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_2.view((1, 1, -1, 128));  linear_2 = None
        value_states: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_4.transpose(1, 2);  view_4 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_2: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2;  x2 = None
        cat_1: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_3: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_4: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_5: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy_: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_0_keys.index_copy_(2, l_cache_position_, k_embed);  k_embed = index_copy_ = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_0_values.index_copy_(2, l_cache_position_, value_states);  value_states = index_copy__1 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_6: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_0_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_0_keys = None
        hidden_states: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_6.expand(1, 8, 4, 2048, 128);  getitem_6 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states.reshape(1, 32, 2048, 128);  hidden_states = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_7: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_0_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_0_values = None
        hidden_states_1: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_7.expand(1, 8, 4, 2048, 128);  getitem_7 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_1.reshape(1, 32, 2048, 128);  hidden_states_1 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed.contiguous();  q_embed = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_1: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key.contiguous();  key = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_1: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value.contiguous();  value = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = causal_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = causal_mask = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_4: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output.transpose(1, 2);  attn_output = None
        attn_output_1: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_4.contiguous();  transpose_4 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_2: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_1.reshape(1, 1, -1);  attn_output_1 = None
        attn_output_2: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_2.contiguous();  reshape_2 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_3: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_2: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = inputs_embeds + attn_output_3;  inputs_embeds = attn_output_3 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_1 = torch.autograd.function.FunctionCtx();  function_ctx_1 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_7: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_2.contiguous()
        contiguous_8: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_1: "f16[1, 4096][4096, 1]cuda:0" = contiguous_7.view(-1, 4096);  contiguous_7 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_2: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_1: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_1 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 327, constant_args_idx = 326, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_2, 'X_ptr': X_1, 'W_ptr': contiguous_8, 'RSTD_ptr': RSTD_1});  X_1 = contiguous_8 = RSTD_1 = triton_kernel_wrapper_mutation_1 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_3: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_2.view(1, 1, 4096);  Y_2 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_4: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_3, l_self_modules_model_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
        linear_5: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_3, l_self_modules_model_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_3 = l_self_modules_model_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_6: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu * linear_5;  silu = linear_5 = None
        down_proj: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_6, l_self_modules_model_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_6 = l_self_modules_model_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_3: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_2 + down_proj;  hidden_states_2 = down_proj = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_2 = torch.autograd.function.FunctionCtx();  function_ctx_2 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_9: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_3.contiguous()
        contiguous_10: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_1_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_2: "f16[1, 4096][4096, 1]cuda:0" = contiguous_9.view(-1, 4096);  contiguous_9 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_4: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_2: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_2 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 328, constant_args_idx = 327, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_4, 'X_ptr': X_2, 'W_ptr': contiguous_10, 'RSTD_ptr': RSTD_2});  X_2 = contiguous_10 = RSTD_2 = triton_kernel_wrapper_mutation_2 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_5: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_4.view(1, 1, 4096);  Y_4 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_7: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_5, l_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_9: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_7.view((1, 1, -1, 128));  linear_7 = None
        query_states_1: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_9.transpose(1, 2);  view_9 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_8: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_5, l_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_10: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_8.view((1, 1, -1, 128));  linear_8 = None
        key_states_1: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_10.transpose(1, 2);  view_10 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_9: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_5, l_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_5 = l_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_11: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_9.view((1, 1, -1, 128));  linear_9 = None
        value_states_1: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_11.transpose(1, 2);  view_11 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_4: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_4: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_7: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_1 * cos_4
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_2: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_1[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_2: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_1[(Ellipsis, slice(64, None, None))];  query_states_1 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_2: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_2;  x2_2 = None
        cat_3: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_2, x1_2), dim = -1);  neg_2 = x1_2 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_8: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_3 * sin_4;  cat_3 = None
        q_embed_1: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_7 + mul_8;  mul_7 = mul_8 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_9: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_1 * cos_4;  cos_4 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_3: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_1[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_3: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_1[(Ellipsis, slice(64, None, None))];  key_states_1 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_3: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_3;  x2_3 = None
        cat_4: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_3, x1_3), dim = -1);  neg_3 = x1_3 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_10: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_4 * sin_4;  cat_4 = sin_4 = None
        k_embed_1: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_9 + mul_10;  mul_9 = mul_10 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__2: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_1_keys.index_copy_(2, l_cache_position_, k_embed_1);  k_embed_1 = index_copy__2 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__3: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_1_values.index_copy_(2, l_cache_position_, value_states_1);  value_states_1 = index_copy__3 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_13: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_1_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_1_keys = None
        hidden_states_4: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_13.expand(1, 8, 4, 2048, 128);  getitem_13 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_2: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_4.reshape(1, 32, 2048, 128);  hidden_states_4 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_14: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_1_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_1_values = None
        hidden_states_5: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_14.expand(1, 8, 4, 2048, 128);  getitem_14 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_2: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_5.reshape(1, 32, 2048, 128);  hidden_states_5 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_1: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_1: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_1.contiguous();  q_embed_1 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_3: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_2.contiguous();  key_2 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_3: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_2.contiguous();  value_2 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_4: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_1, key_3, value_3, attn_mask = causal_mask_1, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_1 = key_3 = value_3 = causal_mask_1 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_8: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_4.transpose(1, 2);  attn_output_4 = None
        attn_output_5: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_8.contiguous();  transpose_8 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_5: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_5.reshape(1, 1, -1);  attn_output_5 = None
        attn_output_6: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_5.contiguous();  reshape_5 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_7: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_6, l_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_6 = l_self_modules_model_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_6: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_3 + attn_output_7;  hidden_states_3 = attn_output_7 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_3 = torch.autograd.function.FunctionCtx();  function_ctx_3 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_16: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_6.contiguous()
        contiguous_17: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_3: "f16[1, 4096][4096, 1]cuda:0" = contiguous_16.view(-1, 4096);  contiguous_16 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_6: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_3: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_3 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 329, constant_args_idx = 328, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_6, 'X_ptr': X_3, 'W_ptr': contiguous_17, 'RSTD_ptr': RSTD_3});  X_3 = contiguous_17 = RSTD_3 = triton_kernel_wrapper_mutation_3 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_7: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_6.view(1, 1, 4096);  Y_6 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_11: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_7, l_self_modules_model_modules_layers_modules_1_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_1_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_1: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_11, inplace = False);  linear_11 = None
        linear_12: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_7, l_self_modules_model_modules_layers_modules_1_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_7 = l_self_modules_model_modules_layers_modules_1_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_11: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_1 * linear_12;  silu_1 = linear_12 = None
        down_proj_1: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_11, l_self_modules_model_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_11 = l_self_modules_model_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_7: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_6 + down_proj_1;  hidden_states_6 = down_proj_1 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_4 = torch.autograd.function.FunctionCtx();  function_ctx_4 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_18: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_7.contiguous()
        contiguous_19: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_2_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_4: "f16[1, 4096][4096, 1]cuda:0" = contiguous_18.view(-1, 4096);  contiguous_18 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_8: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_4: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_4 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 330, constant_args_idx = 329, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_8, 'X_ptr': X_4, 'W_ptr': contiguous_19, 'RSTD_ptr': RSTD_4});  X_4 = contiguous_19 = RSTD_4 = triton_kernel_wrapper_mutation_4 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_9: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_8.view(1, 1, 4096);  Y_8 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_14: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_9, l_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_16: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_14.view((1, 1, -1, 128));  linear_14 = None
        query_states_2: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_16.transpose(1, 2);  view_16 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_15: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_9, l_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_17: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_15.view((1, 1, -1, 128));  linear_15 = None
        key_states_2: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_17.transpose(1, 2);  view_17 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_16: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_9, l_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_9 = l_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_18: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_16.view((1, 1, -1, 128));  linear_16 = None
        value_states_2: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_18.transpose(1, 2);  view_18 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_5: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_5: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_12: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_2 * cos_5
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_4: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_2[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_4: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_2[(Ellipsis, slice(64, None, None))];  query_states_2 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_4: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_4;  x2_4 = None
        cat_5: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_4, x1_4), dim = -1);  neg_4 = x1_4 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_13: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_5 * sin_5;  cat_5 = None
        q_embed_2: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_12 + mul_13;  mul_12 = mul_13 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_14: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_2 * cos_5;  cos_5 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_5: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_2[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_5: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_2[(Ellipsis, slice(64, None, None))];  key_states_2 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_5: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_5;  x2_5 = None
        cat_6: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_5, x1_5), dim = -1);  neg_5 = x1_5 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_15: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_6 * sin_5;  cat_6 = sin_5 = None
        k_embed_2: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_14 + mul_15;  mul_14 = mul_15 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__4: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_2_keys.index_copy_(2, l_cache_position_, k_embed_2);  k_embed_2 = index_copy__4 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__5: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_2_values.index_copy_(2, l_cache_position_, value_states_2);  value_states_2 = index_copy__5 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_20: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_2_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_2_keys = None
        hidden_states_8: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_20.expand(1, 8, 4, 2048, 128);  getitem_20 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_4: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_8.reshape(1, 32, 2048, 128);  hidden_states_8 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_21: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_2_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_2_values = None
        hidden_states_9: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_21.expand(1, 8, 4, 2048, 128);  getitem_21 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_4: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_9.reshape(1, 32, 2048, 128);  hidden_states_9 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_2: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_2: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_2.contiguous();  q_embed_2 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_5: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_4.contiguous();  key_4 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_5: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_4.contiguous();  value_4 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_8: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_2, key_5, value_5, attn_mask = causal_mask_2, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_2 = key_5 = value_5 = causal_mask_2 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_12: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_8.transpose(1, 2);  attn_output_8 = None
        attn_output_9: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_12.contiguous();  transpose_12 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_8: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_9.reshape(1, 1, -1);  attn_output_9 = None
        attn_output_10: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_8.contiguous();  reshape_8 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_11: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_10, l_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_10 = l_self_modules_model_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_10: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_7 + attn_output_11;  hidden_states_7 = attn_output_11 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_5 = torch.autograd.function.FunctionCtx();  function_ctx_5 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_25: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_10.contiguous()
        contiguous_26: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_5: "f16[1, 4096][4096, 1]cuda:0" = contiguous_25.view(-1, 4096);  contiguous_25 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_10: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_5: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_5 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 331, constant_args_idx = 330, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_10, 'X_ptr': X_5, 'W_ptr': contiguous_26, 'RSTD_ptr': RSTD_5});  X_5 = contiguous_26 = RSTD_5 = triton_kernel_wrapper_mutation_5 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_11: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_10.view(1, 1, 4096);  Y_10 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_18: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_11, l_self_modules_model_modules_layers_modules_2_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_2_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_2: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_18, inplace = False);  linear_18 = None
        linear_19: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_11, l_self_modules_model_modules_layers_modules_2_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_11 = l_self_modules_model_modules_layers_modules_2_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_16: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_2 * linear_19;  silu_2 = linear_19 = None
        down_proj_2: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_16, l_self_modules_model_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_16 = l_self_modules_model_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_11: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_10 + down_proj_2;  hidden_states_10 = down_proj_2 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_6 = torch.autograd.function.FunctionCtx();  function_ctx_6 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_27: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_11.contiguous()
        contiguous_28: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_3_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_6: "f16[1, 4096][4096, 1]cuda:0" = contiguous_27.view(-1, 4096);  contiguous_27 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_12: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_6: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_6 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 332, constant_args_idx = 331, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_12, 'X_ptr': X_6, 'W_ptr': contiguous_28, 'RSTD_ptr': RSTD_6});  X_6 = contiguous_28 = RSTD_6 = triton_kernel_wrapper_mutation_6 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_13: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_12.view(1, 1, 4096);  Y_12 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_21: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_13, l_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_23: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_21.view((1, 1, -1, 128));  linear_21 = None
        query_states_3: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_23.transpose(1, 2);  view_23 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_22: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_13, l_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_24: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_22.view((1, 1, -1, 128));  linear_22 = None
        key_states_3: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_24.transpose(1, 2);  view_24 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_23: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_13, l_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_13 = l_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_25: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_23.view((1, 1, -1, 128));  linear_23 = None
        value_states_3: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_25.transpose(1, 2);  view_25 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_6: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_6: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_17: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_3 * cos_6
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_6: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_3[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_6: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_3[(Ellipsis, slice(64, None, None))];  query_states_3 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_6: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_6;  x2_6 = None
        cat_7: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_6, x1_6), dim = -1);  neg_6 = x1_6 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_18: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_7 * sin_6;  cat_7 = None
        q_embed_3: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_17 + mul_18;  mul_17 = mul_18 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_19: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_3 * cos_6;  cos_6 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_7: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_3[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_7: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_3[(Ellipsis, slice(64, None, None))];  key_states_3 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_7: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_7;  x2_7 = None
        cat_8: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_7, x1_7), dim = -1);  neg_7 = x1_7 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_20: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_8 * sin_6;  cat_8 = sin_6 = None
        k_embed_3: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_19 + mul_20;  mul_19 = mul_20 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__6: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_3_keys.index_copy_(2, l_cache_position_, k_embed_3);  k_embed_3 = index_copy__6 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__7: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_3_values.index_copy_(2, l_cache_position_, value_states_3);  value_states_3 = index_copy__7 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_27: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_3_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_3_keys = None
        hidden_states_12: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_27.expand(1, 8, 4, 2048, 128);  getitem_27 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_6: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_12.reshape(1, 32, 2048, 128);  hidden_states_12 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_28: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_3_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_3_values = None
        hidden_states_13: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_28.expand(1, 8, 4, 2048, 128);  getitem_28 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_6: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_13.reshape(1, 32, 2048, 128);  hidden_states_13 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_3: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_3: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_3.contiguous();  q_embed_3 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_7: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_6.contiguous();  key_6 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_7: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_6.contiguous();  value_6 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_12: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_3, key_7, value_7, attn_mask = causal_mask_3, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_3 = key_7 = value_7 = causal_mask_3 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_16: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_12.transpose(1, 2);  attn_output_12 = None
        attn_output_13: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_16.contiguous();  transpose_16 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_11: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_13.reshape(1, 1, -1);  attn_output_13 = None
        attn_output_14: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_11.contiguous();  reshape_11 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_15: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_14, l_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_14 = l_self_modules_model_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_14: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_11 + attn_output_15;  hidden_states_11 = attn_output_15 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_7 = torch.autograd.function.FunctionCtx();  function_ctx_7 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_34: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_14.contiguous()
        contiguous_35: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_7: "f16[1, 4096][4096, 1]cuda:0" = contiguous_34.view(-1, 4096);  contiguous_34 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_14: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_7: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_7 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 333, constant_args_idx = 332, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_14, 'X_ptr': X_7, 'W_ptr': contiguous_35, 'RSTD_ptr': RSTD_7});  X_7 = contiguous_35 = RSTD_7 = triton_kernel_wrapper_mutation_7 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_15: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_14.view(1, 1, 4096);  Y_14 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_25: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_15, l_self_modules_model_modules_layers_modules_3_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_3_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_3: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_25, inplace = False);  linear_25 = None
        linear_26: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_15, l_self_modules_model_modules_layers_modules_3_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_15 = l_self_modules_model_modules_layers_modules_3_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_21: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_3 * linear_26;  silu_3 = linear_26 = None
        down_proj_3: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_21, l_self_modules_model_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_21 = l_self_modules_model_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_15: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_14 + down_proj_3;  hidden_states_14 = down_proj_3 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_8 = torch.autograd.function.FunctionCtx();  function_ctx_8 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_36: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_15.contiguous()
        contiguous_37: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_4_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_4_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_8: "f16[1, 4096][4096, 1]cuda:0" = contiguous_36.view(-1, 4096);  contiguous_36 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_16: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_8: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_8 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 334, constant_args_idx = 333, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_16, 'X_ptr': X_8, 'W_ptr': contiguous_37, 'RSTD_ptr': RSTD_8});  X_8 = contiguous_37 = RSTD_8 = triton_kernel_wrapper_mutation_8 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_17: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_16.view(1, 1, 4096);  Y_16 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_28: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_17, l_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_30: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_28.view((1, 1, -1, 128));  linear_28 = None
        query_states_4: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_30.transpose(1, 2);  view_30 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_29: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_17, l_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_31: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_29.view((1, 1, -1, 128));  linear_29 = None
        key_states_4: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_31.transpose(1, 2);  view_31 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_30: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_17, l_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_17 = l_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_32: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_30.view((1, 1, -1, 128));  linear_30 = None
        value_states_4: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_32.transpose(1, 2);  view_32 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_7: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_7: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_22: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_4 * cos_7
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_8: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_4[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_8: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_4[(Ellipsis, slice(64, None, None))];  query_states_4 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_8: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_8;  x2_8 = None
        cat_9: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_8, x1_8), dim = -1);  neg_8 = x1_8 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_23: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_9 * sin_7;  cat_9 = None
        q_embed_4: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_22 + mul_23;  mul_22 = mul_23 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_24: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_4 * cos_7;  cos_7 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_9: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_4[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_9: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_4[(Ellipsis, slice(64, None, None))];  key_states_4 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_9: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_9;  x2_9 = None
        cat_10: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_9, x1_9), dim = -1);  neg_9 = x1_9 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_25: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_10 * sin_7;  cat_10 = sin_7 = None
        k_embed_4: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_24 + mul_25;  mul_24 = mul_25 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__8: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_4_keys.index_copy_(2, l_cache_position_, k_embed_4);  k_embed_4 = index_copy__8 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__9: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_4_values.index_copy_(2, l_cache_position_, value_states_4);  value_states_4 = index_copy__9 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_34: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_4_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_4_keys = None
        hidden_states_16: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_34.expand(1, 8, 4, 2048, 128);  getitem_34 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_8: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_16.reshape(1, 32, 2048, 128);  hidden_states_16 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_35: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_4_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_4_values = None
        hidden_states_17: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_35.expand(1, 8, 4, 2048, 128);  getitem_35 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_8: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_17.reshape(1, 32, 2048, 128);  hidden_states_17 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_4: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_4: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_4.contiguous();  q_embed_4 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_9: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_8.contiguous();  key_8 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_9: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_8.contiguous();  value_8 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_16: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_4, key_9, value_9, attn_mask = causal_mask_4, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_4 = key_9 = value_9 = causal_mask_4 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_20: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_16.transpose(1, 2);  attn_output_16 = None
        attn_output_17: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_20.contiguous();  transpose_20 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_14: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_17.reshape(1, 1, -1);  attn_output_17 = None
        attn_output_18: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_14.contiguous();  reshape_14 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_19: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_18, l_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_18 = l_self_modules_model_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_18: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_15 + attn_output_19;  hidden_states_15 = attn_output_19 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_9 = torch.autograd.function.FunctionCtx();  function_ctx_9 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_43: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_18.contiguous()
        contiguous_44: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_9: "f16[1, 4096][4096, 1]cuda:0" = contiguous_43.view(-1, 4096);  contiguous_43 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_18: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_9: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_9 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 335, constant_args_idx = 334, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_18, 'X_ptr': X_9, 'W_ptr': contiguous_44, 'RSTD_ptr': RSTD_9});  X_9 = contiguous_44 = RSTD_9 = triton_kernel_wrapper_mutation_9 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_19: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_18.view(1, 1, 4096);  Y_18 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_32: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_19, l_self_modules_model_modules_layers_modules_4_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_4_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_4: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_32, inplace = False);  linear_32 = None
        linear_33: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_19, l_self_modules_model_modules_layers_modules_4_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_19 = l_self_modules_model_modules_layers_modules_4_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_26: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_4 * linear_33;  silu_4 = linear_33 = None
        down_proj_4: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_26, l_self_modules_model_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_26 = l_self_modules_model_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_19: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_18 + down_proj_4;  hidden_states_18 = down_proj_4 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_10 = torch.autograd.function.FunctionCtx();  function_ctx_10 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_45: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_19.contiguous()
        contiguous_46: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_5_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_5_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_10: "f16[1, 4096][4096, 1]cuda:0" = contiguous_45.view(-1, 4096);  contiguous_45 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_20: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_10: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_10 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 336, constant_args_idx = 335, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_20, 'X_ptr': X_10, 'W_ptr': contiguous_46, 'RSTD_ptr': RSTD_10});  X_10 = contiguous_46 = RSTD_10 = triton_kernel_wrapper_mutation_10 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_21: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_20.view(1, 1, 4096);  Y_20 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_35: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_21, l_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_37: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_35.view((1, 1, -1, 128));  linear_35 = None
        query_states_5: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_37.transpose(1, 2);  view_37 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_36: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_21, l_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_38: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_36.view((1, 1, -1, 128));  linear_36 = None
        key_states_5: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_38.transpose(1, 2);  view_38 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_37: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_21, l_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_21 = l_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_39: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_37.view((1, 1, -1, 128));  linear_37 = None
        value_states_5: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_39.transpose(1, 2);  view_39 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_8: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_8: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_27: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_5 * cos_8
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_10: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_5[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_10: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_5[(Ellipsis, slice(64, None, None))];  query_states_5 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_10: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_10;  x2_10 = None
        cat_11: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_10, x1_10), dim = -1);  neg_10 = x1_10 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_28: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_11 * sin_8;  cat_11 = None
        q_embed_5: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_27 + mul_28;  mul_27 = mul_28 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_29: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_5 * cos_8;  cos_8 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_11: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_5[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_11: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_5[(Ellipsis, slice(64, None, None))];  key_states_5 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_11: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_11;  x2_11 = None
        cat_12: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_11, x1_11), dim = -1);  neg_11 = x1_11 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_30: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_12 * sin_8;  cat_12 = sin_8 = None
        k_embed_5: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_29 + mul_30;  mul_29 = mul_30 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__10: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_5_keys.index_copy_(2, l_cache_position_, k_embed_5);  k_embed_5 = index_copy__10 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__11: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_5_values.index_copy_(2, l_cache_position_, value_states_5);  value_states_5 = index_copy__11 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_41: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_5_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_5_keys = None
        hidden_states_20: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_41.expand(1, 8, 4, 2048, 128);  getitem_41 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_10: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_20.reshape(1, 32, 2048, 128);  hidden_states_20 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_42: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_5_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_5_values = None
        hidden_states_21: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_42.expand(1, 8, 4, 2048, 128);  getitem_42 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_10: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_21.reshape(1, 32, 2048, 128);  hidden_states_21 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_5: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_5: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_5.contiguous();  q_embed_5 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_11: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_10.contiguous();  key_10 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_11: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_10.contiguous();  value_10 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_20: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_5, key_11, value_11, attn_mask = causal_mask_5, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_5 = key_11 = value_11 = causal_mask_5 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_24: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_20.transpose(1, 2);  attn_output_20 = None
        attn_output_21: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_24.contiguous();  transpose_24 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_17: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_21.reshape(1, 1, -1);  attn_output_21 = None
        attn_output_22: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_17.contiguous();  reshape_17 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_23: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_22, l_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_22 = l_self_modules_model_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_22: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_19 + attn_output_23;  hidden_states_19 = attn_output_23 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_11 = torch.autograd.function.FunctionCtx();  function_ctx_11 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_52: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_22.contiguous()
        contiguous_53: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_11: "f16[1, 4096][4096, 1]cuda:0" = contiguous_52.view(-1, 4096);  contiguous_52 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_22: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_11: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_11 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 337, constant_args_idx = 336, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_22, 'X_ptr': X_11, 'W_ptr': contiguous_53, 'RSTD_ptr': RSTD_11});  X_11 = contiguous_53 = RSTD_11 = triton_kernel_wrapper_mutation_11 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_23: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_22.view(1, 1, 4096);  Y_22 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_39: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_23, l_self_modules_model_modules_layers_modules_5_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_5_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_5: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_39, inplace = False);  linear_39 = None
        linear_40: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_23, l_self_modules_model_modules_layers_modules_5_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_23 = l_self_modules_model_modules_layers_modules_5_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_31: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_5 * linear_40;  silu_5 = linear_40 = None
        down_proj_5: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_31, l_self_modules_model_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_31 = l_self_modules_model_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_23: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_22 + down_proj_5;  hidden_states_22 = down_proj_5 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_12 = torch.autograd.function.FunctionCtx();  function_ctx_12 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_54: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_23.contiguous()
        contiguous_55: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_6_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_6_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_12: "f16[1, 4096][4096, 1]cuda:0" = contiguous_54.view(-1, 4096);  contiguous_54 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_24: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_12: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_12 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 338, constant_args_idx = 337, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_24, 'X_ptr': X_12, 'W_ptr': contiguous_55, 'RSTD_ptr': RSTD_12});  X_12 = contiguous_55 = RSTD_12 = triton_kernel_wrapper_mutation_12 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_25: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_24.view(1, 1, 4096);  Y_24 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_42: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_25, l_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_44: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_42.view((1, 1, -1, 128));  linear_42 = None
        query_states_6: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_44.transpose(1, 2);  view_44 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_43: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_25, l_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_45: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_43.view((1, 1, -1, 128));  linear_43 = None
        key_states_6: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_45.transpose(1, 2);  view_45 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_44: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_25, l_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_25 = l_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_46: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_44.view((1, 1, -1, 128));  linear_44 = None
        value_states_6: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_46.transpose(1, 2);  view_46 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_9: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_9: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_32: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_6 * cos_9
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_12: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_6[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_12: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_6[(Ellipsis, slice(64, None, None))];  query_states_6 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_12: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_12;  x2_12 = None
        cat_13: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_12, x1_12), dim = -1);  neg_12 = x1_12 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_33: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_13 * sin_9;  cat_13 = None
        q_embed_6: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_32 + mul_33;  mul_32 = mul_33 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_34: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_6 * cos_9;  cos_9 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_13: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_6[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_13: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_6[(Ellipsis, slice(64, None, None))];  key_states_6 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_13: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_13;  x2_13 = None
        cat_14: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_13, x1_13), dim = -1);  neg_13 = x1_13 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_35: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_14 * sin_9;  cat_14 = sin_9 = None
        k_embed_6: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_34 + mul_35;  mul_34 = mul_35 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__12: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_6_keys.index_copy_(2, l_cache_position_, k_embed_6);  k_embed_6 = index_copy__12 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__13: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_6_values.index_copy_(2, l_cache_position_, value_states_6);  value_states_6 = index_copy__13 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_48: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_6_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_6_keys = None
        hidden_states_24: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_48.expand(1, 8, 4, 2048, 128);  getitem_48 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_12: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_24.reshape(1, 32, 2048, 128);  hidden_states_24 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_49: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_6_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_6_values = None
        hidden_states_25: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_49.expand(1, 8, 4, 2048, 128);  getitem_49 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_12: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_25.reshape(1, 32, 2048, 128);  hidden_states_25 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_6: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_6: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_6.contiguous();  q_embed_6 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_13: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_12.contiguous();  key_12 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_13: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_12.contiguous();  value_12 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_24: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_6, key_13, value_13, attn_mask = causal_mask_6, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_6 = key_13 = value_13 = causal_mask_6 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_28: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_24.transpose(1, 2);  attn_output_24 = None
        attn_output_25: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_28.contiguous();  transpose_28 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_20: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_25.reshape(1, 1, -1);  attn_output_25 = None
        attn_output_26: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_20.contiguous();  reshape_20 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_27: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_26, l_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_26 = l_self_modules_model_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_26: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_23 + attn_output_27;  hidden_states_23 = attn_output_27 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_13 = torch.autograd.function.FunctionCtx();  function_ctx_13 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_61: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_26.contiguous()
        contiguous_62: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_13: "f16[1, 4096][4096, 1]cuda:0" = contiguous_61.view(-1, 4096);  contiguous_61 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_26: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_13: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_13 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 339, constant_args_idx = 338, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_26, 'X_ptr': X_13, 'W_ptr': contiguous_62, 'RSTD_ptr': RSTD_13});  X_13 = contiguous_62 = RSTD_13 = triton_kernel_wrapper_mutation_13 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_27: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_26.view(1, 1, 4096);  Y_26 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_46: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_27, l_self_modules_model_modules_layers_modules_6_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_6_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_6: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_46, inplace = False);  linear_46 = None
        linear_47: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_27, l_self_modules_model_modules_layers_modules_6_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_27 = l_self_modules_model_modules_layers_modules_6_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_36: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_6 * linear_47;  silu_6 = linear_47 = None
        down_proj_6: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_36, l_self_modules_model_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_36 = l_self_modules_model_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_27: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_26 + down_proj_6;  hidden_states_26 = down_proj_6 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_14 = torch.autograd.function.FunctionCtx();  function_ctx_14 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_63: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_27.contiguous()
        contiguous_64: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_7_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_7_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_14: "f16[1, 4096][4096, 1]cuda:0" = contiguous_63.view(-1, 4096);  contiguous_63 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_28: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_14: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_14 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 340, constant_args_idx = 339, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_28, 'X_ptr': X_14, 'W_ptr': contiguous_64, 'RSTD_ptr': RSTD_14});  X_14 = contiguous_64 = RSTD_14 = triton_kernel_wrapper_mutation_14 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_29: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_28.view(1, 1, 4096);  Y_28 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_49: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_29, l_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_51: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_49.view((1, 1, -1, 128));  linear_49 = None
        query_states_7: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_51.transpose(1, 2);  view_51 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_50: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_29, l_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_52: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_50.view((1, 1, -1, 128));  linear_50 = None
        key_states_7: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_52.transpose(1, 2);  view_52 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_51: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_29, l_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_29 = l_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_53: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_51.view((1, 1, -1, 128));  linear_51 = None
        value_states_7: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_53.transpose(1, 2);  view_53 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_10: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_10: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_37: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_7 * cos_10
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_14: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_7[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_14: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_7[(Ellipsis, slice(64, None, None))];  query_states_7 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_14: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_14;  x2_14 = None
        cat_15: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_14, x1_14), dim = -1);  neg_14 = x1_14 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_38: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_15 * sin_10;  cat_15 = None
        q_embed_7: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_37 + mul_38;  mul_37 = mul_38 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_39: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_7 * cos_10;  cos_10 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_15: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_7[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_15: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_7[(Ellipsis, slice(64, None, None))];  key_states_7 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_15: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_15;  x2_15 = None
        cat_16: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_15, x1_15), dim = -1);  neg_15 = x1_15 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_40: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_16 * sin_10;  cat_16 = sin_10 = None
        k_embed_7: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_39 + mul_40;  mul_39 = mul_40 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__14: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_7_keys.index_copy_(2, l_cache_position_, k_embed_7);  k_embed_7 = index_copy__14 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__15: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_7_values.index_copy_(2, l_cache_position_, value_states_7);  value_states_7 = index_copy__15 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_55: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_7_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_7_keys = None
        hidden_states_28: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_55.expand(1, 8, 4, 2048, 128);  getitem_55 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_14: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_28.reshape(1, 32, 2048, 128);  hidden_states_28 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_56: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_7_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_7_values = None
        hidden_states_29: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_56.expand(1, 8, 4, 2048, 128);  getitem_56 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_14: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_29.reshape(1, 32, 2048, 128);  hidden_states_29 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_7: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_7: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_7.contiguous();  q_embed_7 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_15: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_14.contiguous();  key_14 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_15: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_14.contiguous();  value_14 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_28: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_7, key_15, value_15, attn_mask = causal_mask_7, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_7 = key_15 = value_15 = causal_mask_7 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_32: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_28.transpose(1, 2);  attn_output_28 = None
        attn_output_29: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_32.contiguous();  transpose_32 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_23: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_29.reshape(1, 1, -1);  attn_output_29 = None
        attn_output_30: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_23.contiguous();  reshape_23 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_31: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_30, l_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_30 = l_self_modules_model_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_30: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_27 + attn_output_31;  hidden_states_27 = attn_output_31 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_15 = torch.autograd.function.FunctionCtx();  function_ctx_15 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_70: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_30.contiguous()
        contiguous_71: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_15: "f16[1, 4096][4096, 1]cuda:0" = contiguous_70.view(-1, 4096);  contiguous_70 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_30: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_15: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_15 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 341, constant_args_idx = 340, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_30, 'X_ptr': X_15, 'W_ptr': contiguous_71, 'RSTD_ptr': RSTD_15});  X_15 = contiguous_71 = RSTD_15 = triton_kernel_wrapper_mutation_15 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_31: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_30.view(1, 1, 4096);  Y_30 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_53: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_31, l_self_modules_model_modules_layers_modules_7_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_7_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_7: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_53, inplace = False);  linear_53 = None
        linear_54: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_31, l_self_modules_model_modules_layers_modules_7_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_31 = l_self_modules_model_modules_layers_modules_7_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_41: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_7 * linear_54;  silu_7 = linear_54 = None
        down_proj_7: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_41, l_self_modules_model_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_41 = l_self_modules_model_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_31: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_30 + down_proj_7;  hidden_states_30 = down_proj_7 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_16 = torch.autograd.function.FunctionCtx();  function_ctx_16 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_72: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_31.contiguous()
        contiguous_73: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_8_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_8_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_16: "f16[1, 4096][4096, 1]cuda:0" = contiguous_72.view(-1, 4096);  contiguous_72 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_32: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_16: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_16 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 342, constant_args_idx = 341, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_32, 'X_ptr': X_16, 'W_ptr': contiguous_73, 'RSTD_ptr': RSTD_16});  X_16 = contiguous_73 = RSTD_16 = triton_kernel_wrapper_mutation_16 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_33: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_32.view(1, 1, 4096);  Y_32 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_56: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_33, l_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_58: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_56.view((1, 1, -1, 128));  linear_56 = None
        query_states_8: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_58.transpose(1, 2);  view_58 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_57: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_33, l_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_59: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_57.view((1, 1, -1, 128));  linear_57 = None
        key_states_8: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_59.transpose(1, 2);  view_59 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_58: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_33, l_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_33 = l_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_60: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_58.view((1, 1, -1, 128));  linear_58 = None
        value_states_8: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_60.transpose(1, 2);  view_60 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_11: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_11: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_42: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_8 * cos_11
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_16: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_8[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_16: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_8[(Ellipsis, slice(64, None, None))];  query_states_8 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_16: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_16;  x2_16 = None
        cat_17: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_16, x1_16), dim = -1);  neg_16 = x1_16 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_43: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_17 * sin_11;  cat_17 = None
        q_embed_8: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_42 + mul_43;  mul_42 = mul_43 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_44: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_8 * cos_11;  cos_11 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_17: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_8[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_17: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_8[(Ellipsis, slice(64, None, None))];  key_states_8 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_17: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_17;  x2_17 = None
        cat_18: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_17, x1_17), dim = -1);  neg_17 = x1_17 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_45: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_18 * sin_11;  cat_18 = sin_11 = None
        k_embed_8: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_44 + mul_45;  mul_44 = mul_45 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__16: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_8_keys.index_copy_(2, l_cache_position_, k_embed_8);  k_embed_8 = index_copy__16 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__17: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_8_values.index_copy_(2, l_cache_position_, value_states_8);  value_states_8 = index_copy__17 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_62: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_8_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_8_keys = None
        hidden_states_32: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_62.expand(1, 8, 4, 2048, 128);  getitem_62 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_16: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_32.reshape(1, 32, 2048, 128);  hidden_states_32 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_63: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_8_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_8_values = None
        hidden_states_33: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_63.expand(1, 8, 4, 2048, 128);  getitem_63 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_16: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_33.reshape(1, 32, 2048, 128);  hidden_states_33 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_8: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_8: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_8.contiguous();  q_embed_8 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_17: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_16.contiguous();  key_16 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_17: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_16.contiguous();  value_16 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_32: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_8, key_17, value_17, attn_mask = causal_mask_8, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_8 = key_17 = value_17 = causal_mask_8 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_36: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_32.transpose(1, 2);  attn_output_32 = None
        attn_output_33: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_36.contiguous();  transpose_36 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_26: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_33.reshape(1, 1, -1);  attn_output_33 = None
        attn_output_34: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_26.contiguous();  reshape_26 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_35: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_34, l_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_34 = l_self_modules_model_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_34: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_31 + attn_output_35;  hidden_states_31 = attn_output_35 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_17 = torch.autograd.function.FunctionCtx();  function_ctx_17 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_79: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_34.contiguous()
        contiguous_80: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_17: "f16[1, 4096][4096, 1]cuda:0" = contiguous_79.view(-1, 4096);  contiguous_79 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_34: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_17: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_17 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 343, constant_args_idx = 342, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_34, 'X_ptr': X_17, 'W_ptr': contiguous_80, 'RSTD_ptr': RSTD_17});  X_17 = contiguous_80 = RSTD_17 = triton_kernel_wrapper_mutation_17 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_35: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_34.view(1, 1, 4096);  Y_34 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_60: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_35, l_self_modules_model_modules_layers_modules_8_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_8_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_8: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_60, inplace = False);  linear_60 = None
        linear_61: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_35, l_self_modules_model_modules_layers_modules_8_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_35 = l_self_modules_model_modules_layers_modules_8_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_46: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_8 * linear_61;  silu_8 = linear_61 = None
        down_proj_8: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_46, l_self_modules_model_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_46 = l_self_modules_model_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_35: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_34 + down_proj_8;  hidden_states_34 = down_proj_8 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_18 = torch.autograd.function.FunctionCtx();  function_ctx_18 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_81: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_35.contiguous()
        contiguous_82: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_9_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_9_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_18: "f16[1, 4096][4096, 1]cuda:0" = contiguous_81.view(-1, 4096);  contiguous_81 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_36: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_18: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_18 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 344, constant_args_idx = 343, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_36, 'X_ptr': X_18, 'W_ptr': contiguous_82, 'RSTD_ptr': RSTD_18});  X_18 = contiguous_82 = RSTD_18 = triton_kernel_wrapper_mutation_18 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_37: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_36.view(1, 1, 4096);  Y_36 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_63: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_37, l_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_65: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_63.view((1, 1, -1, 128));  linear_63 = None
        query_states_9: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_65.transpose(1, 2);  view_65 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_64: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_37, l_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_66: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_64.view((1, 1, -1, 128));  linear_64 = None
        key_states_9: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_66.transpose(1, 2);  view_66 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_65: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_37, l_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_37 = l_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_67: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_65.view((1, 1, -1, 128));  linear_65 = None
        value_states_9: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_67.transpose(1, 2);  view_67 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_12: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_12: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_47: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_9 * cos_12
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_18: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_9[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_18: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_9[(Ellipsis, slice(64, None, None))];  query_states_9 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_18: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_18;  x2_18 = None
        cat_19: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_18, x1_18), dim = -1);  neg_18 = x1_18 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_48: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_19 * sin_12;  cat_19 = None
        q_embed_9: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_47 + mul_48;  mul_47 = mul_48 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_49: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_9 * cos_12;  cos_12 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_19: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_9[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_19: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_9[(Ellipsis, slice(64, None, None))];  key_states_9 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_19: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_19;  x2_19 = None
        cat_20: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_19, x1_19), dim = -1);  neg_19 = x1_19 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_50: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_20 * sin_12;  cat_20 = sin_12 = None
        k_embed_9: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_49 + mul_50;  mul_49 = mul_50 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__18: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_9_keys.index_copy_(2, l_cache_position_, k_embed_9);  k_embed_9 = index_copy__18 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__19: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_9_values.index_copy_(2, l_cache_position_, value_states_9);  value_states_9 = index_copy__19 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_69: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_9_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_9_keys = None
        hidden_states_36: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_69.expand(1, 8, 4, 2048, 128);  getitem_69 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_18: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_36.reshape(1, 32, 2048, 128);  hidden_states_36 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_70: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_9_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_9_values = None
        hidden_states_37: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_70.expand(1, 8, 4, 2048, 128);  getitem_70 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_18: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_37.reshape(1, 32, 2048, 128);  hidden_states_37 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_9: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_9: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_9.contiguous();  q_embed_9 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_19: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_18.contiguous();  key_18 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_19: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_18.contiguous();  value_18 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_36: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_9, key_19, value_19, attn_mask = causal_mask_9, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_9 = key_19 = value_19 = causal_mask_9 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_40: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_36.transpose(1, 2);  attn_output_36 = None
        attn_output_37: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_40.contiguous();  transpose_40 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_29: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_37.reshape(1, 1, -1);  attn_output_37 = None
        attn_output_38: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_29.contiguous();  reshape_29 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_39: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_38, l_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_38 = l_self_modules_model_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_38: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_35 + attn_output_39;  hidden_states_35 = attn_output_39 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_19 = torch.autograd.function.FunctionCtx();  function_ctx_19 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_88: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_38.contiguous()
        contiguous_89: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_19: "f16[1, 4096][4096, 1]cuda:0" = contiguous_88.view(-1, 4096);  contiguous_88 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_38: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_19: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_19 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 345, constant_args_idx = 344, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_38, 'X_ptr': X_19, 'W_ptr': contiguous_89, 'RSTD_ptr': RSTD_19});  X_19 = contiguous_89 = RSTD_19 = triton_kernel_wrapper_mutation_19 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_39: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_38.view(1, 1, 4096);  Y_38 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_67: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_39, l_self_modules_model_modules_layers_modules_9_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_9_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_9: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_67, inplace = False);  linear_67 = None
        linear_68: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_39, l_self_modules_model_modules_layers_modules_9_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_39 = l_self_modules_model_modules_layers_modules_9_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_51: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_9 * linear_68;  silu_9 = linear_68 = None
        down_proj_9: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_51, l_self_modules_model_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_51 = l_self_modules_model_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_39: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_38 + down_proj_9;  hidden_states_38 = down_proj_9 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_20 = torch.autograd.function.FunctionCtx();  function_ctx_20 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_90: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_39.contiguous()
        contiguous_91: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_10_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_10_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_20: "f16[1, 4096][4096, 1]cuda:0" = contiguous_90.view(-1, 4096);  contiguous_90 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_40: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_20: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_20 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 346, constant_args_idx = 345, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_40, 'X_ptr': X_20, 'W_ptr': contiguous_91, 'RSTD_ptr': RSTD_20});  X_20 = contiguous_91 = RSTD_20 = triton_kernel_wrapper_mutation_20 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_41: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_40.view(1, 1, 4096);  Y_40 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_70: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_41, l_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_72: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_70.view((1, 1, -1, 128));  linear_70 = None
        query_states_10: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_72.transpose(1, 2);  view_72 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_71: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_41, l_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_73: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_71.view((1, 1, -1, 128));  linear_71 = None
        key_states_10: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_73.transpose(1, 2);  view_73 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_72: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_41, l_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_41 = l_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_74: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_72.view((1, 1, -1, 128));  linear_72 = None
        value_states_10: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_74.transpose(1, 2);  view_74 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_13: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_13: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_52: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_10 * cos_13
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_20: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_10[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_20: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_10[(Ellipsis, slice(64, None, None))];  query_states_10 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_20: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_20;  x2_20 = None
        cat_21: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_20, x1_20), dim = -1);  neg_20 = x1_20 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_53: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_21 * sin_13;  cat_21 = None
        q_embed_10: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_52 + mul_53;  mul_52 = mul_53 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_54: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_10 * cos_13;  cos_13 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_21: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_10[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_21: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_10[(Ellipsis, slice(64, None, None))];  key_states_10 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_21: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_21;  x2_21 = None
        cat_22: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_21, x1_21), dim = -1);  neg_21 = x1_21 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_55: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_22 * sin_13;  cat_22 = sin_13 = None
        k_embed_10: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_54 + mul_55;  mul_54 = mul_55 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__20: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_10_keys.index_copy_(2, l_cache_position_, k_embed_10);  k_embed_10 = index_copy__20 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__21: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_10_values.index_copy_(2, l_cache_position_, value_states_10);  value_states_10 = index_copy__21 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_76: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_10_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_10_keys = None
        hidden_states_40: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_76.expand(1, 8, 4, 2048, 128);  getitem_76 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_20: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_40.reshape(1, 32, 2048, 128);  hidden_states_40 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_77: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_10_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_10_values = None
        hidden_states_41: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_77.expand(1, 8, 4, 2048, 128);  getitem_77 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_20: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_41.reshape(1, 32, 2048, 128);  hidden_states_41 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_10: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_10: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_10.contiguous();  q_embed_10 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_21: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_20.contiguous();  key_20 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_21: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_20.contiguous();  value_20 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_40: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_10, key_21, value_21, attn_mask = causal_mask_10, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_10 = key_21 = value_21 = causal_mask_10 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_44: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_40.transpose(1, 2);  attn_output_40 = None
        attn_output_41: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_44.contiguous();  transpose_44 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_32: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_41.reshape(1, 1, -1);  attn_output_41 = None
        attn_output_42: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_32.contiguous();  reshape_32 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_43: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_42, l_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_42 = l_self_modules_model_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_42: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_39 + attn_output_43;  hidden_states_39 = attn_output_43 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_21 = torch.autograd.function.FunctionCtx();  function_ctx_21 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_97: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_42.contiguous()
        contiguous_98: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_21: "f16[1, 4096][4096, 1]cuda:0" = contiguous_97.view(-1, 4096);  contiguous_97 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_42: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_21: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_21 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 347, constant_args_idx = 346, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_42, 'X_ptr': X_21, 'W_ptr': contiguous_98, 'RSTD_ptr': RSTD_21});  X_21 = contiguous_98 = RSTD_21 = triton_kernel_wrapper_mutation_21 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_43: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_42.view(1, 1, 4096);  Y_42 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_74: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_43, l_self_modules_model_modules_layers_modules_10_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_10_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_10: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_74, inplace = False);  linear_74 = None
        linear_75: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_43, l_self_modules_model_modules_layers_modules_10_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_43 = l_self_modules_model_modules_layers_modules_10_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_56: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_10 * linear_75;  silu_10 = linear_75 = None
        down_proj_10: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_56, l_self_modules_model_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_56 = l_self_modules_model_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_43: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_42 + down_proj_10;  hidden_states_42 = down_proj_10 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_22 = torch.autograd.function.FunctionCtx();  function_ctx_22 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_99: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_43.contiguous()
        contiguous_100: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_11_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_11_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_22: "f16[1, 4096][4096, 1]cuda:0" = contiguous_99.view(-1, 4096);  contiguous_99 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_44: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_22: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_22 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 348, constant_args_idx = 347, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_44, 'X_ptr': X_22, 'W_ptr': contiguous_100, 'RSTD_ptr': RSTD_22});  X_22 = contiguous_100 = RSTD_22 = triton_kernel_wrapper_mutation_22 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_45: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_44.view(1, 1, 4096);  Y_44 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_77: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_45, l_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_79: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_77.view((1, 1, -1, 128));  linear_77 = None
        query_states_11: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_79.transpose(1, 2);  view_79 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_78: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_45, l_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_80: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_78.view((1, 1, -1, 128));  linear_78 = None
        key_states_11: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_80.transpose(1, 2);  view_80 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_79: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_45, l_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_45 = l_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_81: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_79.view((1, 1, -1, 128));  linear_79 = None
        value_states_11: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_81.transpose(1, 2);  view_81 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_14: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_14: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_57: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_11 * cos_14
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_22: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_11[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_22: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_11[(Ellipsis, slice(64, None, None))];  query_states_11 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_22: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_22;  x2_22 = None
        cat_23: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_22, x1_22), dim = -1);  neg_22 = x1_22 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_58: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_23 * sin_14;  cat_23 = None
        q_embed_11: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_57 + mul_58;  mul_57 = mul_58 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_59: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_11 * cos_14;  cos_14 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_23: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_11[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_23: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_11[(Ellipsis, slice(64, None, None))];  key_states_11 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_23: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_23;  x2_23 = None
        cat_24: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_23, x1_23), dim = -1);  neg_23 = x1_23 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_60: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_24 * sin_14;  cat_24 = sin_14 = None
        k_embed_11: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_59 + mul_60;  mul_59 = mul_60 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__22: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_11_keys.index_copy_(2, l_cache_position_, k_embed_11);  k_embed_11 = index_copy__22 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__23: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_11_values.index_copy_(2, l_cache_position_, value_states_11);  value_states_11 = index_copy__23 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_83: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_11_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_11_keys = None
        hidden_states_44: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_83.expand(1, 8, 4, 2048, 128);  getitem_83 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_22: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_44.reshape(1, 32, 2048, 128);  hidden_states_44 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_84: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_11_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_11_values = None
        hidden_states_45: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_84.expand(1, 8, 4, 2048, 128);  getitem_84 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_22: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_45.reshape(1, 32, 2048, 128);  hidden_states_45 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_11: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_11: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_11.contiguous();  q_embed_11 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_23: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_22.contiguous();  key_22 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_23: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_22.contiguous();  value_22 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_44: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_11, key_23, value_23, attn_mask = causal_mask_11, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_11 = key_23 = value_23 = causal_mask_11 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_48: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_44.transpose(1, 2);  attn_output_44 = None
        attn_output_45: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_48.contiguous();  transpose_48 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_35: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_45.reshape(1, 1, -1);  attn_output_45 = None
        attn_output_46: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_35.contiguous();  reshape_35 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_47: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_46, l_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_46 = l_self_modules_model_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_46: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_43 + attn_output_47;  hidden_states_43 = attn_output_47 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_23 = torch.autograd.function.FunctionCtx();  function_ctx_23 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_106: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_46.contiguous()
        contiguous_107: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_23: "f16[1, 4096][4096, 1]cuda:0" = contiguous_106.view(-1, 4096);  contiguous_106 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_46: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_23: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_23 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 349, constant_args_idx = 348, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_46, 'X_ptr': X_23, 'W_ptr': contiguous_107, 'RSTD_ptr': RSTD_23});  X_23 = contiguous_107 = RSTD_23 = triton_kernel_wrapper_mutation_23 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_47: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_46.view(1, 1, 4096);  Y_46 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_81: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_47, l_self_modules_model_modules_layers_modules_11_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_11_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_11: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_81, inplace = False);  linear_81 = None
        linear_82: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_47, l_self_modules_model_modules_layers_modules_11_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_47 = l_self_modules_model_modules_layers_modules_11_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_61: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_11 * linear_82;  silu_11 = linear_82 = None
        down_proj_11: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_61, l_self_modules_model_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_61 = l_self_modules_model_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_47: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_46 + down_proj_11;  hidden_states_46 = down_proj_11 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_24 = torch.autograd.function.FunctionCtx();  function_ctx_24 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_108: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_47.contiguous()
        contiguous_109: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_12_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_12_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_24: "f16[1, 4096][4096, 1]cuda:0" = contiguous_108.view(-1, 4096);  contiguous_108 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_48: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_24: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_24 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 350, constant_args_idx = 349, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_48, 'X_ptr': X_24, 'W_ptr': contiguous_109, 'RSTD_ptr': RSTD_24});  X_24 = contiguous_109 = RSTD_24 = triton_kernel_wrapper_mutation_24 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_49: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_48.view(1, 1, 4096);  Y_48 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_84: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_49, l_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_86: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_84.view((1, 1, -1, 128));  linear_84 = None
        query_states_12: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_86.transpose(1, 2);  view_86 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_85: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_49, l_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_87: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_85.view((1, 1, -1, 128));  linear_85 = None
        key_states_12: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_87.transpose(1, 2);  view_87 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_86: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_49, l_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_49 = l_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_88: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_86.view((1, 1, -1, 128));  linear_86 = None
        value_states_12: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_88.transpose(1, 2);  view_88 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_15: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_15: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_62: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_12 * cos_15
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_24: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_12[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_24: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_12[(Ellipsis, slice(64, None, None))];  query_states_12 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_24: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_24;  x2_24 = None
        cat_25: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_24, x1_24), dim = -1);  neg_24 = x1_24 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_63: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_25 * sin_15;  cat_25 = None
        q_embed_12: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_62 + mul_63;  mul_62 = mul_63 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_64: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_12 * cos_15;  cos_15 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_25: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_12[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_25: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_12[(Ellipsis, slice(64, None, None))];  key_states_12 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_25: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_25;  x2_25 = None
        cat_26: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_25, x1_25), dim = -1);  neg_25 = x1_25 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_65: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_26 * sin_15;  cat_26 = sin_15 = None
        k_embed_12: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_64 + mul_65;  mul_64 = mul_65 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__24: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_12_keys.index_copy_(2, l_cache_position_, k_embed_12);  k_embed_12 = index_copy__24 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__25: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_12_values.index_copy_(2, l_cache_position_, value_states_12);  value_states_12 = index_copy__25 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_90: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_12_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_12_keys = None
        hidden_states_48: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_90.expand(1, 8, 4, 2048, 128);  getitem_90 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_24: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_48.reshape(1, 32, 2048, 128);  hidden_states_48 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_91: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_12_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_12_values = None
        hidden_states_49: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_91.expand(1, 8, 4, 2048, 128);  getitem_91 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_24: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_49.reshape(1, 32, 2048, 128);  hidden_states_49 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_12: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_12: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_12.contiguous();  q_embed_12 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_25: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_24.contiguous();  key_24 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_25: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_24.contiguous();  value_24 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_48: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_12, key_25, value_25, attn_mask = causal_mask_12, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_12 = key_25 = value_25 = causal_mask_12 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_52: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_48.transpose(1, 2);  attn_output_48 = None
        attn_output_49: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_52.contiguous();  transpose_52 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_38: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_49.reshape(1, 1, -1);  attn_output_49 = None
        attn_output_50: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_38.contiguous();  reshape_38 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_51: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_50, l_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_50 = l_self_modules_model_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_50: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_47 + attn_output_51;  hidden_states_47 = attn_output_51 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_25 = torch.autograd.function.FunctionCtx();  function_ctx_25 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_115: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_50.contiguous()
        contiguous_116: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_25: "f16[1, 4096][4096, 1]cuda:0" = contiguous_115.view(-1, 4096);  contiguous_115 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_50: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_25: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_25 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 351, constant_args_idx = 350, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_50, 'X_ptr': X_25, 'W_ptr': contiguous_116, 'RSTD_ptr': RSTD_25});  X_25 = contiguous_116 = RSTD_25 = triton_kernel_wrapper_mutation_25 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_51: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_50.view(1, 1, 4096);  Y_50 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_88: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_51, l_self_modules_model_modules_layers_modules_12_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_12_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_12: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_88, inplace = False);  linear_88 = None
        linear_89: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_51, l_self_modules_model_modules_layers_modules_12_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_51 = l_self_modules_model_modules_layers_modules_12_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_66: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_12 * linear_89;  silu_12 = linear_89 = None
        down_proj_12: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_66, l_self_modules_model_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_66 = l_self_modules_model_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_51: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_50 + down_proj_12;  hidden_states_50 = down_proj_12 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_26 = torch.autograd.function.FunctionCtx();  function_ctx_26 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_117: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_51.contiguous()
        contiguous_118: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_13_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_13_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_26: "f16[1, 4096][4096, 1]cuda:0" = contiguous_117.view(-1, 4096);  contiguous_117 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_52: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_26: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_26 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 352, constant_args_idx = 351, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_52, 'X_ptr': X_26, 'W_ptr': contiguous_118, 'RSTD_ptr': RSTD_26});  X_26 = contiguous_118 = RSTD_26 = triton_kernel_wrapper_mutation_26 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_53: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_52.view(1, 1, 4096);  Y_52 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_91: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_53, l_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_93: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_91.view((1, 1, -1, 128));  linear_91 = None
        query_states_13: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_93.transpose(1, 2);  view_93 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_92: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_53, l_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_94: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_92.view((1, 1, -1, 128));  linear_92 = None
        key_states_13: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_94.transpose(1, 2);  view_94 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_93: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_53, l_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_53 = l_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_95: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_93.view((1, 1, -1, 128));  linear_93 = None
        value_states_13: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_95.transpose(1, 2);  view_95 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_16: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_16: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_67: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_13 * cos_16
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_26: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_13[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_26: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_13[(Ellipsis, slice(64, None, None))];  query_states_13 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_26: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_26;  x2_26 = None
        cat_27: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_26, x1_26), dim = -1);  neg_26 = x1_26 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_68: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_27 * sin_16;  cat_27 = None
        q_embed_13: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_67 + mul_68;  mul_67 = mul_68 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_69: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_13 * cos_16;  cos_16 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_27: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_13[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_27: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_13[(Ellipsis, slice(64, None, None))];  key_states_13 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_27: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_27;  x2_27 = None
        cat_28: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_27, x1_27), dim = -1);  neg_27 = x1_27 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_70: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_28 * sin_16;  cat_28 = sin_16 = None
        k_embed_13: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_69 + mul_70;  mul_69 = mul_70 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__26: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_13_keys.index_copy_(2, l_cache_position_, k_embed_13);  k_embed_13 = index_copy__26 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__27: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_13_values.index_copy_(2, l_cache_position_, value_states_13);  value_states_13 = index_copy__27 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_97: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_13_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_13_keys = None
        hidden_states_52: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_97.expand(1, 8, 4, 2048, 128);  getitem_97 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_26: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_52.reshape(1, 32, 2048, 128);  hidden_states_52 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_98: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_13_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_13_values = None
        hidden_states_53: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_98.expand(1, 8, 4, 2048, 128);  getitem_98 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_26: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_53.reshape(1, 32, 2048, 128);  hidden_states_53 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_13: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_13: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_13.contiguous();  q_embed_13 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_27: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_26.contiguous();  key_26 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_27: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_26.contiguous();  value_26 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_52: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_13, key_27, value_27, attn_mask = causal_mask_13, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_13 = key_27 = value_27 = causal_mask_13 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_56: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_52.transpose(1, 2);  attn_output_52 = None
        attn_output_53: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_56.contiguous();  transpose_56 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_41: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_53.reshape(1, 1, -1);  attn_output_53 = None
        attn_output_54: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_41.contiguous();  reshape_41 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_55: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_54, l_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_54 = l_self_modules_model_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_54: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_51 + attn_output_55;  hidden_states_51 = attn_output_55 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_27 = torch.autograd.function.FunctionCtx();  function_ctx_27 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_124: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_54.contiguous()
        contiguous_125: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_27: "f16[1, 4096][4096, 1]cuda:0" = contiguous_124.view(-1, 4096);  contiguous_124 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_54: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_27: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_27 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 353, constant_args_idx = 352, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_54, 'X_ptr': X_27, 'W_ptr': contiguous_125, 'RSTD_ptr': RSTD_27});  X_27 = contiguous_125 = RSTD_27 = triton_kernel_wrapper_mutation_27 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_55: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_54.view(1, 1, 4096);  Y_54 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_95: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_55, l_self_modules_model_modules_layers_modules_13_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_13_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_13: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_95, inplace = False);  linear_95 = None
        linear_96: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_55, l_self_modules_model_modules_layers_modules_13_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_55 = l_self_modules_model_modules_layers_modules_13_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_71: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_13 * linear_96;  silu_13 = linear_96 = None
        down_proj_13: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_71, l_self_modules_model_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_71 = l_self_modules_model_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_55: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_54 + down_proj_13;  hidden_states_54 = down_proj_13 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_28 = torch.autograd.function.FunctionCtx();  function_ctx_28 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_126: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_55.contiguous()
        contiguous_127: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_14_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_14_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_28: "f16[1, 4096][4096, 1]cuda:0" = contiguous_126.view(-1, 4096);  contiguous_126 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_56: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_28: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_28 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 354, constant_args_idx = 353, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_56, 'X_ptr': X_28, 'W_ptr': contiguous_127, 'RSTD_ptr': RSTD_28});  X_28 = contiguous_127 = RSTD_28 = triton_kernel_wrapper_mutation_28 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_57: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_56.view(1, 1, 4096);  Y_56 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_98: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_57, l_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_100: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_98.view((1, 1, -1, 128));  linear_98 = None
        query_states_14: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_100.transpose(1, 2);  view_100 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_99: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_57, l_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_101: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_99.view((1, 1, -1, 128));  linear_99 = None
        key_states_14: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_101.transpose(1, 2);  view_101 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_100: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_57, l_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_57 = l_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_102: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_100.view((1, 1, -1, 128));  linear_100 = None
        value_states_14: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_102.transpose(1, 2);  view_102 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_17: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_17: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_72: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_14 * cos_17
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_28: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_14[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_28: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_14[(Ellipsis, slice(64, None, None))];  query_states_14 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_28: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_28;  x2_28 = None
        cat_29: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_28, x1_28), dim = -1);  neg_28 = x1_28 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_73: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_29 * sin_17;  cat_29 = None
        q_embed_14: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_72 + mul_73;  mul_72 = mul_73 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_74: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_14 * cos_17;  cos_17 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_29: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_14[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_29: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_14[(Ellipsis, slice(64, None, None))];  key_states_14 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_29: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_29;  x2_29 = None
        cat_30: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_29, x1_29), dim = -1);  neg_29 = x1_29 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_75: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_30 * sin_17;  cat_30 = sin_17 = None
        k_embed_14: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_74 + mul_75;  mul_74 = mul_75 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__28: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_14_keys.index_copy_(2, l_cache_position_, k_embed_14);  k_embed_14 = index_copy__28 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__29: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_14_values.index_copy_(2, l_cache_position_, value_states_14);  value_states_14 = index_copy__29 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_104: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_14_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_14_keys = None
        hidden_states_56: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_104.expand(1, 8, 4, 2048, 128);  getitem_104 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_28: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_56.reshape(1, 32, 2048, 128);  hidden_states_56 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_105: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_14_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_14_values = None
        hidden_states_57: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_105.expand(1, 8, 4, 2048, 128);  getitem_105 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_28: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_57.reshape(1, 32, 2048, 128);  hidden_states_57 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_14: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_14: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_14.contiguous();  q_embed_14 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_29: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_28.contiguous();  key_28 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_29: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_28.contiguous();  value_28 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_56: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_14, key_29, value_29, attn_mask = causal_mask_14, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_14 = key_29 = value_29 = causal_mask_14 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_60: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_56.transpose(1, 2);  attn_output_56 = None
        attn_output_57: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_60.contiguous();  transpose_60 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_44: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_57.reshape(1, 1, -1);  attn_output_57 = None
        attn_output_58: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_44.contiguous();  reshape_44 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_59: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_58, l_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_58 = l_self_modules_model_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_58: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_55 + attn_output_59;  hidden_states_55 = attn_output_59 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_29 = torch.autograd.function.FunctionCtx();  function_ctx_29 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_133: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_58.contiguous()
        contiguous_134: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_29: "f16[1, 4096][4096, 1]cuda:0" = contiguous_133.view(-1, 4096);  contiguous_133 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_58: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_29: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_29 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 355, constant_args_idx = 354, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_58, 'X_ptr': X_29, 'W_ptr': contiguous_134, 'RSTD_ptr': RSTD_29});  X_29 = contiguous_134 = RSTD_29 = triton_kernel_wrapper_mutation_29 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_59: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_58.view(1, 1, 4096);  Y_58 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_102: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_59, l_self_modules_model_modules_layers_modules_14_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_14_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_14: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_102, inplace = False);  linear_102 = None
        linear_103: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_59, l_self_modules_model_modules_layers_modules_14_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_59 = l_self_modules_model_modules_layers_modules_14_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_76: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_14 * linear_103;  silu_14 = linear_103 = None
        down_proj_14: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_76, l_self_modules_model_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_76 = l_self_modules_model_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_59: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_58 + down_proj_14;  hidden_states_58 = down_proj_14 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_30 = torch.autograd.function.FunctionCtx();  function_ctx_30 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_135: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_59.contiguous()
        contiguous_136: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_15_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_15_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_30: "f16[1, 4096][4096, 1]cuda:0" = contiguous_135.view(-1, 4096);  contiguous_135 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_60: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_30: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_30 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 356, constant_args_idx = 355, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_60, 'X_ptr': X_30, 'W_ptr': contiguous_136, 'RSTD_ptr': RSTD_30});  X_30 = contiguous_136 = RSTD_30 = triton_kernel_wrapper_mutation_30 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_61: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_60.view(1, 1, 4096);  Y_60 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_105: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_61, l_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_107: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_105.view((1, 1, -1, 128));  linear_105 = None
        query_states_15: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_107.transpose(1, 2);  view_107 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_106: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_61, l_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_108: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_106.view((1, 1, -1, 128));  linear_106 = None
        key_states_15: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_108.transpose(1, 2);  view_108 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_107: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_61, l_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_61 = l_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_109: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_107.view((1, 1, -1, 128));  linear_107 = None
        value_states_15: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_109.transpose(1, 2);  view_109 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_18: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_18: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_77: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_15 * cos_18
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_30: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_15[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_30: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_15[(Ellipsis, slice(64, None, None))];  query_states_15 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_30: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_30;  x2_30 = None
        cat_31: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_30, x1_30), dim = -1);  neg_30 = x1_30 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_78: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_31 * sin_18;  cat_31 = None
        q_embed_15: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_77 + mul_78;  mul_77 = mul_78 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_79: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_15 * cos_18;  cos_18 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_31: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_15[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_31: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_15[(Ellipsis, slice(64, None, None))];  key_states_15 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_31: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_31;  x2_31 = None
        cat_32: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_31, x1_31), dim = -1);  neg_31 = x1_31 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_80: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_32 * sin_18;  cat_32 = sin_18 = None
        k_embed_15: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_79 + mul_80;  mul_79 = mul_80 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__30: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_15_keys.index_copy_(2, l_cache_position_, k_embed_15);  k_embed_15 = index_copy__30 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__31: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_15_values.index_copy_(2, l_cache_position_, value_states_15);  value_states_15 = index_copy__31 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_111: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_15_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_15_keys = None
        hidden_states_60: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_111.expand(1, 8, 4, 2048, 128);  getitem_111 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_30: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_60.reshape(1, 32, 2048, 128);  hidden_states_60 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_112: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_15_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_15_values = None
        hidden_states_61: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_112.expand(1, 8, 4, 2048, 128);  getitem_112 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_30: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_61.reshape(1, 32, 2048, 128);  hidden_states_61 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_15: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_15: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_15.contiguous();  q_embed_15 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_31: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_30.contiguous();  key_30 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_31: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_30.contiguous();  value_30 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_60: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_15, key_31, value_31, attn_mask = causal_mask_15, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_15 = key_31 = value_31 = causal_mask_15 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_64: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_60.transpose(1, 2);  attn_output_60 = None
        attn_output_61: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_64.contiguous();  transpose_64 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_47: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_61.reshape(1, 1, -1);  attn_output_61 = None
        attn_output_62: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_47.contiguous();  reshape_47 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_63: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_62, l_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_62 = l_self_modules_model_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_62: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_59 + attn_output_63;  hidden_states_59 = attn_output_63 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_31 = torch.autograd.function.FunctionCtx();  function_ctx_31 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_142: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_62.contiguous()
        contiguous_143: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_31: "f16[1, 4096][4096, 1]cuda:0" = contiguous_142.view(-1, 4096);  contiguous_142 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_62: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_31: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_31 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 357, constant_args_idx = 356, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_62, 'X_ptr': X_31, 'W_ptr': contiguous_143, 'RSTD_ptr': RSTD_31});  X_31 = contiguous_143 = RSTD_31 = triton_kernel_wrapper_mutation_31 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_63: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_62.view(1, 1, 4096);  Y_62 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_109: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_63, l_self_modules_model_modules_layers_modules_15_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_15_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_15: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_109, inplace = False);  linear_109 = None
        linear_110: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_63, l_self_modules_model_modules_layers_modules_15_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_63 = l_self_modules_model_modules_layers_modules_15_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_81: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_15 * linear_110;  silu_15 = linear_110 = None
        down_proj_15: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_81, l_self_modules_model_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_81 = l_self_modules_model_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_63: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_62 + down_proj_15;  hidden_states_62 = down_proj_15 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_32 = torch.autograd.function.FunctionCtx();  function_ctx_32 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_144: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_63.contiguous()
        contiguous_145: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_16_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_16_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_32: "f16[1, 4096][4096, 1]cuda:0" = contiguous_144.view(-1, 4096);  contiguous_144 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_64: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_32: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_32 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 358, constant_args_idx = 357, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_64, 'X_ptr': X_32, 'W_ptr': contiguous_145, 'RSTD_ptr': RSTD_32});  X_32 = contiguous_145 = RSTD_32 = triton_kernel_wrapper_mutation_32 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_65: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_64.view(1, 1, 4096);  Y_64 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_112: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_65, l_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_114: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_112.view((1, 1, -1, 128));  linear_112 = None
        query_states_16: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_114.transpose(1, 2);  view_114 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_113: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_65, l_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_115: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_113.view((1, 1, -1, 128));  linear_113 = None
        key_states_16: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_115.transpose(1, 2);  view_115 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_114: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_65, l_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_65 = l_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_116: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_114.view((1, 1, -1, 128));  linear_114 = None
        value_states_16: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_116.transpose(1, 2);  view_116 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_19: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_19: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_82: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_16 * cos_19
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_32: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_16[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_32: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_16[(Ellipsis, slice(64, None, None))];  query_states_16 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_32: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_32;  x2_32 = None
        cat_33: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_32, x1_32), dim = -1);  neg_32 = x1_32 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_83: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_33 * sin_19;  cat_33 = None
        q_embed_16: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_82 + mul_83;  mul_82 = mul_83 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_84: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_16 * cos_19;  cos_19 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_33: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_16[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_33: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_16[(Ellipsis, slice(64, None, None))];  key_states_16 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_33: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_33;  x2_33 = None
        cat_34: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_33, x1_33), dim = -1);  neg_33 = x1_33 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_85: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_34 * sin_19;  cat_34 = sin_19 = None
        k_embed_16: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_84 + mul_85;  mul_84 = mul_85 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__32: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_16_keys.index_copy_(2, l_cache_position_, k_embed_16);  k_embed_16 = index_copy__32 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__33: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_16_values.index_copy_(2, l_cache_position_, value_states_16);  value_states_16 = index_copy__33 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_118: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_16_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_16_keys = None
        hidden_states_64: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_118.expand(1, 8, 4, 2048, 128);  getitem_118 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_32: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_64.reshape(1, 32, 2048, 128);  hidden_states_64 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_119: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_16_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_16_values = None
        hidden_states_65: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_119.expand(1, 8, 4, 2048, 128);  getitem_119 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_32: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_65.reshape(1, 32, 2048, 128);  hidden_states_65 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_16: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_16: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_16.contiguous();  q_embed_16 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_33: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_32.contiguous();  key_32 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_33: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_32.contiguous();  value_32 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_64: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_16, key_33, value_33, attn_mask = causal_mask_16, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_16 = key_33 = value_33 = causal_mask_16 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_68: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_64.transpose(1, 2);  attn_output_64 = None
        attn_output_65: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_68.contiguous();  transpose_68 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_50: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_65.reshape(1, 1, -1);  attn_output_65 = None
        attn_output_66: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_50.contiguous();  reshape_50 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_67: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_66, l_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_66 = l_self_modules_model_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_66: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_63 + attn_output_67;  hidden_states_63 = attn_output_67 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_33 = torch.autograd.function.FunctionCtx();  function_ctx_33 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_151: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_66.contiguous()
        contiguous_152: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_33: "f16[1, 4096][4096, 1]cuda:0" = contiguous_151.view(-1, 4096);  contiguous_151 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_66: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_33: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_33 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 359, constant_args_idx = 358, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_66, 'X_ptr': X_33, 'W_ptr': contiguous_152, 'RSTD_ptr': RSTD_33});  X_33 = contiguous_152 = RSTD_33 = triton_kernel_wrapper_mutation_33 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_67: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_66.view(1, 1, 4096);  Y_66 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_116: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_67, l_self_modules_model_modules_layers_modules_16_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_16_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_16: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_116, inplace = False);  linear_116 = None
        linear_117: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_67, l_self_modules_model_modules_layers_modules_16_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_67 = l_self_modules_model_modules_layers_modules_16_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_86: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_16 * linear_117;  silu_16 = linear_117 = None
        down_proj_16: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_86, l_self_modules_model_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_86 = l_self_modules_model_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_67: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_66 + down_proj_16;  hidden_states_66 = down_proj_16 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_34 = torch.autograd.function.FunctionCtx();  function_ctx_34 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_153: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_67.contiguous()
        contiguous_154: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_17_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_17_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_34: "f16[1, 4096][4096, 1]cuda:0" = contiguous_153.view(-1, 4096);  contiguous_153 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_68: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_34: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_34 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 360, constant_args_idx = 359, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_68, 'X_ptr': X_34, 'W_ptr': contiguous_154, 'RSTD_ptr': RSTD_34});  X_34 = contiguous_154 = RSTD_34 = triton_kernel_wrapper_mutation_34 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_69: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_68.view(1, 1, 4096);  Y_68 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_119: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_69, l_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_121: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_119.view((1, 1, -1, 128));  linear_119 = None
        query_states_17: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_121.transpose(1, 2);  view_121 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_120: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_69, l_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_122: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_120.view((1, 1, -1, 128));  linear_120 = None
        key_states_17: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_122.transpose(1, 2);  view_122 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_121: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_69, l_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_69 = l_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_123: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_121.view((1, 1, -1, 128));  linear_121 = None
        value_states_17: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_123.transpose(1, 2);  view_123 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_20: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_20: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_87: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_17 * cos_20
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_34: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_17[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_34: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_17[(Ellipsis, slice(64, None, None))];  query_states_17 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_34: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_34;  x2_34 = None
        cat_35: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_34, x1_34), dim = -1);  neg_34 = x1_34 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_88: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_35 * sin_20;  cat_35 = None
        q_embed_17: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_87 + mul_88;  mul_87 = mul_88 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_89: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_17 * cos_20;  cos_20 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_35: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_17[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_35: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_17[(Ellipsis, slice(64, None, None))];  key_states_17 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_35: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_35;  x2_35 = None
        cat_36: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_35, x1_35), dim = -1);  neg_35 = x1_35 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_90: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_36 * sin_20;  cat_36 = sin_20 = None
        k_embed_17: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_89 + mul_90;  mul_89 = mul_90 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__34: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_17_keys.index_copy_(2, l_cache_position_, k_embed_17);  k_embed_17 = index_copy__34 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__35: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_17_values.index_copy_(2, l_cache_position_, value_states_17);  value_states_17 = index_copy__35 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_125: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_17_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_17_keys = None
        hidden_states_68: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_125.expand(1, 8, 4, 2048, 128);  getitem_125 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_34: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_68.reshape(1, 32, 2048, 128);  hidden_states_68 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_126: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_17_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_17_values = None
        hidden_states_69: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_126.expand(1, 8, 4, 2048, 128);  getitem_126 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_34: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_69.reshape(1, 32, 2048, 128);  hidden_states_69 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_17: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_17: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_17.contiguous();  q_embed_17 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_35: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_34.contiguous();  key_34 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_35: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_34.contiguous();  value_34 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_68: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_17, key_35, value_35, attn_mask = causal_mask_17, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_17 = key_35 = value_35 = causal_mask_17 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_72: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_68.transpose(1, 2);  attn_output_68 = None
        attn_output_69: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_72.contiguous();  transpose_72 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_53: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_69.reshape(1, 1, -1);  attn_output_69 = None
        attn_output_70: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_53.contiguous();  reshape_53 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_71: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_70, l_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_70 = l_self_modules_model_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_70: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_67 + attn_output_71;  hidden_states_67 = attn_output_71 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_35 = torch.autograd.function.FunctionCtx();  function_ctx_35 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_160: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_70.contiguous()
        contiguous_161: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_35: "f16[1, 4096][4096, 1]cuda:0" = contiguous_160.view(-1, 4096);  contiguous_160 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_70: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_35: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_35 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 361, constant_args_idx = 360, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_70, 'X_ptr': X_35, 'W_ptr': contiguous_161, 'RSTD_ptr': RSTD_35});  X_35 = contiguous_161 = RSTD_35 = triton_kernel_wrapper_mutation_35 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_71: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_70.view(1, 1, 4096);  Y_70 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_123: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_71, l_self_modules_model_modules_layers_modules_17_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_17_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_17: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_123, inplace = False);  linear_123 = None
        linear_124: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_71, l_self_modules_model_modules_layers_modules_17_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_71 = l_self_modules_model_modules_layers_modules_17_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_91: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_17 * linear_124;  silu_17 = linear_124 = None
        down_proj_17: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_91, l_self_modules_model_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_91 = l_self_modules_model_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_71: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_70 + down_proj_17;  hidden_states_70 = down_proj_17 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_36 = torch.autograd.function.FunctionCtx();  function_ctx_36 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_162: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_71.contiguous()
        contiguous_163: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_18_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_18_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_36: "f16[1, 4096][4096, 1]cuda:0" = contiguous_162.view(-1, 4096);  contiguous_162 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_72: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_36: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_36 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 362, constant_args_idx = 361, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_72, 'X_ptr': X_36, 'W_ptr': contiguous_163, 'RSTD_ptr': RSTD_36});  X_36 = contiguous_163 = RSTD_36 = triton_kernel_wrapper_mutation_36 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_73: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_72.view(1, 1, 4096);  Y_72 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_126: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_73, l_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_128: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_126.view((1, 1, -1, 128));  linear_126 = None
        query_states_18: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_128.transpose(1, 2);  view_128 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_127: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_73, l_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_129: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_127.view((1, 1, -1, 128));  linear_127 = None
        key_states_18: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_129.transpose(1, 2);  view_129 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_128: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_73, l_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_73 = l_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_130: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_128.view((1, 1, -1, 128));  linear_128 = None
        value_states_18: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_130.transpose(1, 2);  view_130 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_21: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_21: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_92: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_18 * cos_21
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_36: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_18[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_36: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_18[(Ellipsis, slice(64, None, None))];  query_states_18 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_36: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_36;  x2_36 = None
        cat_37: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_36, x1_36), dim = -1);  neg_36 = x1_36 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_93: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_37 * sin_21;  cat_37 = None
        q_embed_18: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_92 + mul_93;  mul_92 = mul_93 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_94: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_18 * cos_21;  cos_21 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_37: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_18[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_37: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_18[(Ellipsis, slice(64, None, None))];  key_states_18 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_37: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_37;  x2_37 = None
        cat_38: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_37, x1_37), dim = -1);  neg_37 = x1_37 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_95: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_38 * sin_21;  cat_38 = sin_21 = None
        k_embed_18: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_94 + mul_95;  mul_94 = mul_95 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__36: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_18_keys.index_copy_(2, l_cache_position_, k_embed_18);  k_embed_18 = index_copy__36 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__37: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_18_values.index_copy_(2, l_cache_position_, value_states_18);  value_states_18 = index_copy__37 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_132: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_18_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_18_keys = None
        hidden_states_72: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_132.expand(1, 8, 4, 2048, 128);  getitem_132 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_36: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_72.reshape(1, 32, 2048, 128);  hidden_states_72 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_133: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_18_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_18_values = None
        hidden_states_73: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_133.expand(1, 8, 4, 2048, 128);  getitem_133 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_36: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_73.reshape(1, 32, 2048, 128);  hidden_states_73 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_18: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_18: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_18.contiguous();  q_embed_18 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_37: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_36.contiguous();  key_36 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_37: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_36.contiguous();  value_36 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_72: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_18, key_37, value_37, attn_mask = causal_mask_18, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_18 = key_37 = value_37 = causal_mask_18 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_76: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_72.transpose(1, 2);  attn_output_72 = None
        attn_output_73: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_76.contiguous();  transpose_76 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_56: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_73.reshape(1, 1, -1);  attn_output_73 = None
        attn_output_74: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_56.contiguous();  reshape_56 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_75: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_74, l_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_74 = l_self_modules_model_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_74: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_71 + attn_output_75;  hidden_states_71 = attn_output_75 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_37 = torch.autograd.function.FunctionCtx();  function_ctx_37 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_169: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_74.contiguous()
        contiguous_170: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_37: "f16[1, 4096][4096, 1]cuda:0" = contiguous_169.view(-1, 4096);  contiguous_169 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_74: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_37: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_37 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 363, constant_args_idx = 362, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_74, 'X_ptr': X_37, 'W_ptr': contiguous_170, 'RSTD_ptr': RSTD_37});  X_37 = contiguous_170 = RSTD_37 = triton_kernel_wrapper_mutation_37 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_75: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_74.view(1, 1, 4096);  Y_74 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_130: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_75, l_self_modules_model_modules_layers_modules_18_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_18_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_18: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_130, inplace = False);  linear_130 = None
        linear_131: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_75, l_self_modules_model_modules_layers_modules_18_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_75 = l_self_modules_model_modules_layers_modules_18_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_96: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_18 * linear_131;  silu_18 = linear_131 = None
        down_proj_18: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_96, l_self_modules_model_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_96 = l_self_modules_model_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_75: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_74 + down_proj_18;  hidden_states_74 = down_proj_18 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_38 = torch.autograd.function.FunctionCtx();  function_ctx_38 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_171: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_75.contiguous()
        contiguous_172: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_19_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_19_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_38: "f16[1, 4096][4096, 1]cuda:0" = contiguous_171.view(-1, 4096);  contiguous_171 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_76: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_38: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_38 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 364, constant_args_idx = 363, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_76, 'X_ptr': X_38, 'W_ptr': contiguous_172, 'RSTD_ptr': RSTD_38});  X_38 = contiguous_172 = RSTD_38 = triton_kernel_wrapper_mutation_38 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_77: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_76.view(1, 1, 4096);  Y_76 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_133: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_77, l_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_135: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_133.view((1, 1, -1, 128));  linear_133 = None
        query_states_19: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_135.transpose(1, 2);  view_135 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_134: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_77, l_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_136: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_134.view((1, 1, -1, 128));  linear_134 = None
        key_states_19: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_136.transpose(1, 2);  view_136 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_135: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_77, l_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_77 = l_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_137: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_135.view((1, 1, -1, 128));  linear_135 = None
        value_states_19: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_137.transpose(1, 2);  view_137 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_22: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_22: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_97: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_19 * cos_22
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_38: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_19[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_38: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_19[(Ellipsis, slice(64, None, None))];  query_states_19 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_38: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_38;  x2_38 = None
        cat_39: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_38, x1_38), dim = -1);  neg_38 = x1_38 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_98: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_39 * sin_22;  cat_39 = None
        q_embed_19: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_97 + mul_98;  mul_97 = mul_98 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_99: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_19 * cos_22;  cos_22 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_39: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_19[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_39: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_19[(Ellipsis, slice(64, None, None))];  key_states_19 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_39: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_39;  x2_39 = None
        cat_40: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_39, x1_39), dim = -1);  neg_39 = x1_39 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_100: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_40 * sin_22;  cat_40 = sin_22 = None
        k_embed_19: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_99 + mul_100;  mul_99 = mul_100 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__38: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_19_keys.index_copy_(2, l_cache_position_, k_embed_19);  k_embed_19 = index_copy__38 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__39: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_19_values.index_copy_(2, l_cache_position_, value_states_19);  value_states_19 = index_copy__39 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_139: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_19_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_19_keys = None
        hidden_states_76: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_139.expand(1, 8, 4, 2048, 128);  getitem_139 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_38: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_76.reshape(1, 32, 2048, 128);  hidden_states_76 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_140: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_19_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_19_values = None
        hidden_states_77: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_140.expand(1, 8, 4, 2048, 128);  getitem_140 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_38: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_77.reshape(1, 32, 2048, 128);  hidden_states_77 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_19: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_19: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_19.contiguous();  q_embed_19 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_39: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_38.contiguous();  key_38 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_39: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_38.contiguous();  value_38 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_76: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_19, key_39, value_39, attn_mask = causal_mask_19, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_19 = key_39 = value_39 = causal_mask_19 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_80: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_76.transpose(1, 2);  attn_output_76 = None
        attn_output_77: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_80.contiguous();  transpose_80 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_59: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_77.reshape(1, 1, -1);  attn_output_77 = None
        attn_output_78: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_59.contiguous();  reshape_59 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_79: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_78, l_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_78 = l_self_modules_model_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_78: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_75 + attn_output_79;  hidden_states_75 = attn_output_79 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_39 = torch.autograd.function.FunctionCtx();  function_ctx_39 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_178: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_78.contiguous()
        contiguous_179: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_39: "f16[1, 4096][4096, 1]cuda:0" = contiguous_178.view(-1, 4096);  contiguous_178 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_78: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_39: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_39 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 365, constant_args_idx = 364, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_78, 'X_ptr': X_39, 'W_ptr': contiguous_179, 'RSTD_ptr': RSTD_39});  X_39 = contiguous_179 = RSTD_39 = triton_kernel_wrapper_mutation_39 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_79: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_78.view(1, 1, 4096);  Y_78 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_137: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_79, l_self_modules_model_modules_layers_modules_19_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_19_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_19: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_137, inplace = False);  linear_137 = None
        linear_138: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_79, l_self_modules_model_modules_layers_modules_19_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_79 = l_self_modules_model_modules_layers_modules_19_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_101: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_19 * linear_138;  silu_19 = linear_138 = None
        down_proj_19: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_101, l_self_modules_model_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_101 = l_self_modules_model_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_79: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_78 + down_proj_19;  hidden_states_78 = down_proj_19 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_40 = torch.autograd.function.FunctionCtx();  function_ctx_40 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_180: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_79.contiguous()
        contiguous_181: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_20_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_20_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_40: "f16[1, 4096][4096, 1]cuda:0" = contiguous_180.view(-1, 4096);  contiguous_180 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_80: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_40: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_40 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 366, constant_args_idx = 365, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_80, 'X_ptr': X_40, 'W_ptr': contiguous_181, 'RSTD_ptr': RSTD_40});  X_40 = contiguous_181 = RSTD_40 = triton_kernel_wrapper_mutation_40 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_81: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_80.view(1, 1, 4096);  Y_80 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_140: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_81, l_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_142: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_140.view((1, 1, -1, 128));  linear_140 = None
        query_states_20: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_142.transpose(1, 2);  view_142 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_141: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_81, l_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_143: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_141.view((1, 1, -1, 128));  linear_141 = None
        key_states_20: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_143.transpose(1, 2);  view_143 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_142: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_81, l_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_81 = l_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_144: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_142.view((1, 1, -1, 128));  linear_142 = None
        value_states_20: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_144.transpose(1, 2);  view_144 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_23: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_23: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_102: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_20 * cos_23
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_40: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_20[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_40: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_20[(Ellipsis, slice(64, None, None))];  query_states_20 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_40: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_40;  x2_40 = None
        cat_41: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_40, x1_40), dim = -1);  neg_40 = x1_40 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_103: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_41 * sin_23;  cat_41 = None
        q_embed_20: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_102 + mul_103;  mul_102 = mul_103 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_104: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_20 * cos_23;  cos_23 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_41: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_20[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_41: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_20[(Ellipsis, slice(64, None, None))];  key_states_20 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_41: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_41;  x2_41 = None
        cat_42: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_41, x1_41), dim = -1);  neg_41 = x1_41 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_105: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_42 * sin_23;  cat_42 = sin_23 = None
        k_embed_20: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_104 + mul_105;  mul_104 = mul_105 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__40: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_20_keys.index_copy_(2, l_cache_position_, k_embed_20);  k_embed_20 = index_copy__40 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__41: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_20_values.index_copy_(2, l_cache_position_, value_states_20);  value_states_20 = index_copy__41 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_146: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_20_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_20_keys = None
        hidden_states_80: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_146.expand(1, 8, 4, 2048, 128);  getitem_146 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_40: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_80.reshape(1, 32, 2048, 128);  hidden_states_80 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_147: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_20_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_20_values = None
        hidden_states_81: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_147.expand(1, 8, 4, 2048, 128);  getitem_147 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_40: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_81.reshape(1, 32, 2048, 128);  hidden_states_81 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_20: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_20: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_20.contiguous();  q_embed_20 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_41: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_40.contiguous();  key_40 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_41: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_40.contiguous();  value_40 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_80: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_20, key_41, value_41, attn_mask = causal_mask_20, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_20 = key_41 = value_41 = causal_mask_20 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_84: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_80.transpose(1, 2);  attn_output_80 = None
        attn_output_81: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_84.contiguous();  transpose_84 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_62: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_81.reshape(1, 1, -1);  attn_output_81 = None
        attn_output_82: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_62.contiguous();  reshape_62 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_83: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_82, l_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_82 = l_self_modules_model_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_82: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_79 + attn_output_83;  hidden_states_79 = attn_output_83 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_41 = torch.autograd.function.FunctionCtx();  function_ctx_41 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_187: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_82.contiguous()
        contiguous_188: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_41: "f16[1, 4096][4096, 1]cuda:0" = contiguous_187.view(-1, 4096);  contiguous_187 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_82: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_41: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_41 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 367, constant_args_idx = 366, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_82, 'X_ptr': X_41, 'W_ptr': contiguous_188, 'RSTD_ptr': RSTD_41});  X_41 = contiguous_188 = RSTD_41 = triton_kernel_wrapper_mutation_41 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_83: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_82.view(1, 1, 4096);  Y_82 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_144: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_83, l_self_modules_model_modules_layers_modules_20_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_20_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_20: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_144, inplace = False);  linear_144 = None
        linear_145: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_83, l_self_modules_model_modules_layers_modules_20_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_83 = l_self_modules_model_modules_layers_modules_20_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_106: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_20 * linear_145;  silu_20 = linear_145 = None
        down_proj_20: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_106, l_self_modules_model_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_106 = l_self_modules_model_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_83: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_82 + down_proj_20;  hidden_states_82 = down_proj_20 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_42 = torch.autograd.function.FunctionCtx();  function_ctx_42 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_189: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_83.contiguous()
        contiguous_190: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_21_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_21_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_42: "f16[1, 4096][4096, 1]cuda:0" = contiguous_189.view(-1, 4096);  contiguous_189 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_84: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_42: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_42 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 368, constant_args_idx = 367, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_84, 'X_ptr': X_42, 'W_ptr': contiguous_190, 'RSTD_ptr': RSTD_42});  X_42 = contiguous_190 = RSTD_42 = triton_kernel_wrapper_mutation_42 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_85: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_84.view(1, 1, 4096);  Y_84 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_147: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_85, l_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_149: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_147.view((1, 1, -1, 128));  linear_147 = None
        query_states_21: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_149.transpose(1, 2);  view_149 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_148: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_85, l_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_150: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_148.view((1, 1, -1, 128));  linear_148 = None
        key_states_21: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_150.transpose(1, 2);  view_150 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_149: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_85, l_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_85 = l_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_151: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_149.view((1, 1, -1, 128));  linear_149 = None
        value_states_21: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_151.transpose(1, 2);  view_151 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_24: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_24: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_107: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_21 * cos_24
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_42: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_21[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_42: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_21[(Ellipsis, slice(64, None, None))];  query_states_21 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_42: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_42;  x2_42 = None
        cat_43: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_42, x1_42), dim = -1);  neg_42 = x1_42 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_108: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_43 * sin_24;  cat_43 = None
        q_embed_21: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_107 + mul_108;  mul_107 = mul_108 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_109: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_21 * cos_24;  cos_24 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_43: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_21[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_43: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_21[(Ellipsis, slice(64, None, None))];  key_states_21 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_43: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_43;  x2_43 = None
        cat_44: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_43, x1_43), dim = -1);  neg_43 = x1_43 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_110: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_44 * sin_24;  cat_44 = sin_24 = None
        k_embed_21: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_109 + mul_110;  mul_109 = mul_110 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__42: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_21_keys.index_copy_(2, l_cache_position_, k_embed_21);  k_embed_21 = index_copy__42 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__43: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_21_values.index_copy_(2, l_cache_position_, value_states_21);  value_states_21 = index_copy__43 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_153: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_21_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_21_keys = None
        hidden_states_84: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_153.expand(1, 8, 4, 2048, 128);  getitem_153 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_42: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_84.reshape(1, 32, 2048, 128);  hidden_states_84 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_154: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_21_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_21_values = None
        hidden_states_85: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_154.expand(1, 8, 4, 2048, 128);  getitem_154 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_42: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_85.reshape(1, 32, 2048, 128);  hidden_states_85 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_21: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_21: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_21.contiguous();  q_embed_21 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_43: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_42.contiguous();  key_42 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_43: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_42.contiguous();  value_42 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_84: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_21, key_43, value_43, attn_mask = causal_mask_21, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_21 = key_43 = value_43 = causal_mask_21 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_88: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_84.transpose(1, 2);  attn_output_84 = None
        attn_output_85: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_88.contiguous();  transpose_88 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_65: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_85.reshape(1, 1, -1);  attn_output_85 = None
        attn_output_86: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_65.contiguous();  reshape_65 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_87: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_86, l_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_86 = l_self_modules_model_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_86: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_83 + attn_output_87;  hidden_states_83 = attn_output_87 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_43 = torch.autograd.function.FunctionCtx();  function_ctx_43 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_196: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_86.contiguous()
        contiguous_197: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_43: "f16[1, 4096][4096, 1]cuda:0" = contiguous_196.view(-1, 4096);  contiguous_196 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_86: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_43: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_43 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 369, constant_args_idx = 368, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_86, 'X_ptr': X_43, 'W_ptr': contiguous_197, 'RSTD_ptr': RSTD_43});  X_43 = contiguous_197 = RSTD_43 = triton_kernel_wrapper_mutation_43 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_87: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_86.view(1, 1, 4096);  Y_86 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_151: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_87, l_self_modules_model_modules_layers_modules_21_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_21_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_21: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_151, inplace = False);  linear_151 = None
        linear_152: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_87, l_self_modules_model_modules_layers_modules_21_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_87 = l_self_modules_model_modules_layers_modules_21_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_111: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_21 * linear_152;  silu_21 = linear_152 = None
        down_proj_21: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_111, l_self_modules_model_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_111 = l_self_modules_model_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_87: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_86 + down_proj_21;  hidden_states_86 = down_proj_21 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_44 = torch.autograd.function.FunctionCtx();  function_ctx_44 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_198: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_87.contiguous()
        contiguous_199: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_22_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_22_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_44: "f16[1, 4096][4096, 1]cuda:0" = contiguous_198.view(-1, 4096);  contiguous_198 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_88: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_44: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_44 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 370, constant_args_idx = 369, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_88, 'X_ptr': X_44, 'W_ptr': contiguous_199, 'RSTD_ptr': RSTD_44});  X_44 = contiguous_199 = RSTD_44 = triton_kernel_wrapper_mutation_44 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_89: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_88.view(1, 1, 4096);  Y_88 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_154: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_89, l_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_156: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_154.view((1, 1, -1, 128));  linear_154 = None
        query_states_22: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_156.transpose(1, 2);  view_156 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_155: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_89, l_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_157: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_155.view((1, 1, -1, 128));  linear_155 = None
        key_states_22: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_157.transpose(1, 2);  view_157 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_156: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_89, l_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_89 = l_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_158: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_156.view((1, 1, -1, 128));  linear_156 = None
        value_states_22: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_158.transpose(1, 2);  view_158 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_25: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_25: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_112: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_22 * cos_25
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_44: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_22[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_44: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_22[(Ellipsis, slice(64, None, None))];  query_states_22 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_44: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_44;  x2_44 = None
        cat_45: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_44, x1_44), dim = -1);  neg_44 = x1_44 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_113: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_45 * sin_25;  cat_45 = None
        q_embed_22: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_112 + mul_113;  mul_112 = mul_113 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_114: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_22 * cos_25;  cos_25 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_45: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_22[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_45: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_22[(Ellipsis, slice(64, None, None))];  key_states_22 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_45: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_45;  x2_45 = None
        cat_46: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_45, x1_45), dim = -1);  neg_45 = x1_45 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_115: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_46 * sin_25;  cat_46 = sin_25 = None
        k_embed_22: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_114 + mul_115;  mul_114 = mul_115 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__44: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_22_keys.index_copy_(2, l_cache_position_, k_embed_22);  k_embed_22 = index_copy__44 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__45: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_22_values.index_copy_(2, l_cache_position_, value_states_22);  value_states_22 = index_copy__45 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_160: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_22_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_22_keys = None
        hidden_states_88: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_160.expand(1, 8, 4, 2048, 128);  getitem_160 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_44: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_88.reshape(1, 32, 2048, 128);  hidden_states_88 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_161: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_22_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_22_values = None
        hidden_states_89: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_161.expand(1, 8, 4, 2048, 128);  getitem_161 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_44: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_89.reshape(1, 32, 2048, 128);  hidden_states_89 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_22: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_22: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_22.contiguous();  q_embed_22 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_45: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_44.contiguous();  key_44 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_45: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_44.contiguous();  value_44 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_88: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_22, key_45, value_45, attn_mask = causal_mask_22, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_22 = key_45 = value_45 = causal_mask_22 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_92: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_88.transpose(1, 2);  attn_output_88 = None
        attn_output_89: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_92.contiguous();  transpose_92 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_68: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_89.reshape(1, 1, -1);  attn_output_89 = None
        attn_output_90: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_68.contiguous();  reshape_68 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_91: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_90, l_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_90 = l_self_modules_model_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_90: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_87 + attn_output_91;  hidden_states_87 = attn_output_91 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_45 = torch.autograd.function.FunctionCtx();  function_ctx_45 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_205: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_90.contiguous()
        contiguous_206: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_45: "f16[1, 4096][4096, 1]cuda:0" = contiguous_205.view(-1, 4096);  contiguous_205 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_90: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_45: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_45 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 371, constant_args_idx = 370, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_90, 'X_ptr': X_45, 'W_ptr': contiguous_206, 'RSTD_ptr': RSTD_45});  X_45 = contiguous_206 = RSTD_45 = triton_kernel_wrapper_mutation_45 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_91: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_90.view(1, 1, 4096);  Y_90 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_158: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_91, l_self_modules_model_modules_layers_modules_22_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_22_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_22: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_158, inplace = False);  linear_158 = None
        linear_159: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_91, l_self_modules_model_modules_layers_modules_22_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_91 = l_self_modules_model_modules_layers_modules_22_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_116: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_22 * linear_159;  silu_22 = linear_159 = None
        down_proj_22: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_116, l_self_modules_model_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_116 = l_self_modules_model_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_91: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_90 + down_proj_22;  hidden_states_90 = down_proj_22 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_46 = torch.autograd.function.FunctionCtx();  function_ctx_46 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_207: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_91.contiguous()
        contiguous_208: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_23_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_23_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_46: "f16[1, 4096][4096, 1]cuda:0" = contiguous_207.view(-1, 4096);  contiguous_207 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_92: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_46: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_46 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 372, constant_args_idx = 371, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_92, 'X_ptr': X_46, 'W_ptr': contiguous_208, 'RSTD_ptr': RSTD_46});  X_46 = contiguous_208 = RSTD_46 = triton_kernel_wrapper_mutation_46 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_93: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_92.view(1, 1, 4096);  Y_92 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_161: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_93, l_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_163: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_161.view((1, 1, -1, 128));  linear_161 = None
        query_states_23: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_163.transpose(1, 2);  view_163 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_162: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_93, l_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_164: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_162.view((1, 1, -1, 128));  linear_162 = None
        key_states_23: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_164.transpose(1, 2);  view_164 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_163: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_93, l_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_93 = l_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_165: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_163.view((1, 1, -1, 128));  linear_163 = None
        value_states_23: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_165.transpose(1, 2);  view_165 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_26: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_26: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_117: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_23 * cos_26
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_46: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_23[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_46: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_23[(Ellipsis, slice(64, None, None))];  query_states_23 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_46: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_46;  x2_46 = None
        cat_47: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_46, x1_46), dim = -1);  neg_46 = x1_46 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_118: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_47 * sin_26;  cat_47 = None
        q_embed_23: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_117 + mul_118;  mul_117 = mul_118 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_119: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_23 * cos_26;  cos_26 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_47: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_23[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_47: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_23[(Ellipsis, slice(64, None, None))];  key_states_23 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_47: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_47;  x2_47 = None
        cat_48: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_47, x1_47), dim = -1);  neg_47 = x1_47 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_120: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_48 * sin_26;  cat_48 = sin_26 = None
        k_embed_23: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_119 + mul_120;  mul_119 = mul_120 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__46: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_23_keys.index_copy_(2, l_cache_position_, k_embed_23);  k_embed_23 = index_copy__46 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__47: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_23_values.index_copy_(2, l_cache_position_, value_states_23);  value_states_23 = index_copy__47 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_167: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_23_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_23_keys = None
        hidden_states_92: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_167.expand(1, 8, 4, 2048, 128);  getitem_167 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_46: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_92.reshape(1, 32, 2048, 128);  hidden_states_92 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_168: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_23_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_23_values = None
        hidden_states_93: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_168.expand(1, 8, 4, 2048, 128);  getitem_168 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_46: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_93.reshape(1, 32, 2048, 128);  hidden_states_93 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_23: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_23: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_23.contiguous();  q_embed_23 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_47: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_46.contiguous();  key_46 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_47: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_46.contiguous();  value_46 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_92: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_23, key_47, value_47, attn_mask = causal_mask_23, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_23 = key_47 = value_47 = causal_mask_23 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_96: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_92.transpose(1, 2);  attn_output_92 = None
        attn_output_93: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_96.contiguous();  transpose_96 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_71: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_93.reshape(1, 1, -1);  attn_output_93 = None
        attn_output_94: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_71.contiguous();  reshape_71 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_95: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_94, l_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_94 = l_self_modules_model_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_94: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_91 + attn_output_95;  hidden_states_91 = attn_output_95 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_47 = torch.autograd.function.FunctionCtx();  function_ctx_47 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_214: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_94.contiguous()
        contiguous_215: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_47: "f16[1, 4096][4096, 1]cuda:0" = contiguous_214.view(-1, 4096);  contiguous_214 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_94: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_47: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_47 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 373, constant_args_idx = 372, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_94, 'X_ptr': X_47, 'W_ptr': contiguous_215, 'RSTD_ptr': RSTD_47});  X_47 = contiguous_215 = RSTD_47 = triton_kernel_wrapper_mutation_47 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_95: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_94.view(1, 1, 4096);  Y_94 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_165: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_95, l_self_modules_model_modules_layers_modules_23_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_23_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_23: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_165, inplace = False);  linear_165 = None
        linear_166: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_95, l_self_modules_model_modules_layers_modules_23_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_95 = l_self_modules_model_modules_layers_modules_23_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_121: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_23 * linear_166;  silu_23 = linear_166 = None
        down_proj_23: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_121, l_self_modules_model_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_121 = l_self_modules_model_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_95: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_94 + down_proj_23;  hidden_states_94 = down_proj_23 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_48 = torch.autograd.function.FunctionCtx();  function_ctx_48 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_216: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_95.contiguous()
        contiguous_217: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_24_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_24_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_48: "f16[1, 4096][4096, 1]cuda:0" = contiguous_216.view(-1, 4096);  contiguous_216 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_96: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_48: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_48 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 374, constant_args_idx = 373, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_96, 'X_ptr': X_48, 'W_ptr': contiguous_217, 'RSTD_ptr': RSTD_48});  X_48 = contiguous_217 = RSTD_48 = triton_kernel_wrapper_mutation_48 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_97: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_96.view(1, 1, 4096);  Y_96 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_168: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_97, l_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_170: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_168.view((1, 1, -1, 128));  linear_168 = None
        query_states_24: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_170.transpose(1, 2);  view_170 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_169: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_97, l_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_171: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_169.view((1, 1, -1, 128));  linear_169 = None
        key_states_24: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_171.transpose(1, 2);  view_171 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_170: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_97, l_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_97 = l_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_172: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_170.view((1, 1, -1, 128));  linear_170 = None
        value_states_24: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_172.transpose(1, 2);  view_172 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_27: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_27: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_122: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_24 * cos_27
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_48: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_24[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_48: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_24[(Ellipsis, slice(64, None, None))];  query_states_24 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_48: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_48;  x2_48 = None
        cat_49: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_48, x1_48), dim = -1);  neg_48 = x1_48 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_123: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_49 * sin_27;  cat_49 = None
        q_embed_24: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_122 + mul_123;  mul_122 = mul_123 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_124: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_24 * cos_27;  cos_27 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_49: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_24[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_49: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_24[(Ellipsis, slice(64, None, None))];  key_states_24 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_49: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_49;  x2_49 = None
        cat_50: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_49, x1_49), dim = -1);  neg_49 = x1_49 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_125: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_50 * sin_27;  cat_50 = sin_27 = None
        k_embed_24: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_124 + mul_125;  mul_124 = mul_125 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__48: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_24_keys.index_copy_(2, l_cache_position_, k_embed_24);  k_embed_24 = index_copy__48 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__49: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_24_values.index_copy_(2, l_cache_position_, value_states_24);  value_states_24 = index_copy__49 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_174: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_24_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_24_keys = None
        hidden_states_96: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_174.expand(1, 8, 4, 2048, 128);  getitem_174 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_48: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_96.reshape(1, 32, 2048, 128);  hidden_states_96 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_175: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_24_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_24_values = None
        hidden_states_97: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_175.expand(1, 8, 4, 2048, 128);  getitem_175 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_48: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_97.reshape(1, 32, 2048, 128);  hidden_states_97 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_24: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_24: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_24.contiguous();  q_embed_24 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_49: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_48.contiguous();  key_48 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_49: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_48.contiguous();  value_48 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_96: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_24, key_49, value_49, attn_mask = causal_mask_24, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_24 = key_49 = value_49 = causal_mask_24 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_100: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_96.transpose(1, 2);  attn_output_96 = None
        attn_output_97: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_100.contiguous();  transpose_100 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_74: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_97.reshape(1, 1, -1);  attn_output_97 = None
        attn_output_98: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_74.contiguous();  reshape_74 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_99: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_98, l_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_98 = l_self_modules_model_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_98: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_95 + attn_output_99;  hidden_states_95 = attn_output_99 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_49 = torch.autograd.function.FunctionCtx();  function_ctx_49 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_223: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_98.contiguous()
        contiguous_224: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_49: "f16[1, 4096][4096, 1]cuda:0" = contiguous_223.view(-1, 4096);  contiguous_223 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_98: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_49: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_49 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 375, constant_args_idx = 374, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_98, 'X_ptr': X_49, 'W_ptr': contiguous_224, 'RSTD_ptr': RSTD_49});  X_49 = contiguous_224 = RSTD_49 = triton_kernel_wrapper_mutation_49 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_99: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_98.view(1, 1, 4096);  Y_98 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_172: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_99, l_self_modules_model_modules_layers_modules_24_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_24_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_24: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_172, inplace = False);  linear_172 = None
        linear_173: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_99, l_self_modules_model_modules_layers_modules_24_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_99 = l_self_modules_model_modules_layers_modules_24_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_126: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_24 * linear_173;  silu_24 = linear_173 = None
        down_proj_24: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_126, l_self_modules_model_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_126 = l_self_modules_model_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_99: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_98 + down_proj_24;  hidden_states_98 = down_proj_24 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_50 = torch.autograd.function.FunctionCtx();  function_ctx_50 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_225: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_99.contiguous()
        contiguous_226: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_25_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_25_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_50: "f16[1, 4096][4096, 1]cuda:0" = contiguous_225.view(-1, 4096);  contiguous_225 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_100: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_50: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_50 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 376, constant_args_idx = 375, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_100, 'X_ptr': X_50, 'W_ptr': contiguous_226, 'RSTD_ptr': RSTD_50});  X_50 = contiguous_226 = RSTD_50 = triton_kernel_wrapper_mutation_50 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_101: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_100.view(1, 1, 4096);  Y_100 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_175: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_101, l_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_177: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_175.view((1, 1, -1, 128));  linear_175 = None
        query_states_25: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_177.transpose(1, 2);  view_177 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_176: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_101, l_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_178: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_176.view((1, 1, -1, 128));  linear_176 = None
        key_states_25: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_178.transpose(1, 2);  view_178 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_177: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_101, l_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_101 = l_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_179: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_177.view((1, 1, -1, 128));  linear_177 = None
        value_states_25: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_179.transpose(1, 2);  view_179 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_28: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_28: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_127: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_25 * cos_28
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_50: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_25[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_50: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_25[(Ellipsis, slice(64, None, None))];  query_states_25 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_50: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_50;  x2_50 = None
        cat_51: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_50, x1_50), dim = -1);  neg_50 = x1_50 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_128: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_51 * sin_28;  cat_51 = None
        q_embed_25: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_127 + mul_128;  mul_127 = mul_128 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_129: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_25 * cos_28;  cos_28 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_51: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_25[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_51: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_25[(Ellipsis, slice(64, None, None))];  key_states_25 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_51: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_51;  x2_51 = None
        cat_52: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_51, x1_51), dim = -1);  neg_51 = x1_51 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_130: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_52 * sin_28;  cat_52 = sin_28 = None
        k_embed_25: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_129 + mul_130;  mul_129 = mul_130 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__50: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_25_keys.index_copy_(2, l_cache_position_, k_embed_25);  k_embed_25 = index_copy__50 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__51: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_25_values.index_copy_(2, l_cache_position_, value_states_25);  value_states_25 = index_copy__51 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_181: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_25_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_25_keys = None
        hidden_states_100: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_181.expand(1, 8, 4, 2048, 128);  getitem_181 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_50: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_100.reshape(1, 32, 2048, 128);  hidden_states_100 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_182: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_25_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_25_values = None
        hidden_states_101: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_182.expand(1, 8, 4, 2048, 128);  getitem_182 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_50: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_101.reshape(1, 32, 2048, 128);  hidden_states_101 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_25: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_25: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_25.contiguous();  q_embed_25 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_51: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_50.contiguous();  key_50 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_51: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_50.contiguous();  value_50 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_100: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_25, key_51, value_51, attn_mask = causal_mask_25, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_25 = key_51 = value_51 = causal_mask_25 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_104: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_100.transpose(1, 2);  attn_output_100 = None
        attn_output_101: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_104.contiguous();  transpose_104 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_77: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_101.reshape(1, 1, -1);  attn_output_101 = None
        attn_output_102: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_77.contiguous();  reshape_77 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_103: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_102, l_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_102 = l_self_modules_model_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_102: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_99 + attn_output_103;  hidden_states_99 = attn_output_103 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_51 = torch.autograd.function.FunctionCtx();  function_ctx_51 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_232: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_102.contiguous()
        contiguous_233: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_51: "f16[1, 4096][4096, 1]cuda:0" = contiguous_232.view(-1, 4096);  contiguous_232 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_102: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_51: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_51 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 377, constant_args_idx = 376, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_102, 'X_ptr': X_51, 'W_ptr': contiguous_233, 'RSTD_ptr': RSTD_51});  X_51 = contiguous_233 = RSTD_51 = triton_kernel_wrapper_mutation_51 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_103: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_102.view(1, 1, 4096);  Y_102 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_179: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_103, l_self_modules_model_modules_layers_modules_25_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_25_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_25: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_179, inplace = False);  linear_179 = None
        linear_180: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_103, l_self_modules_model_modules_layers_modules_25_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_103 = l_self_modules_model_modules_layers_modules_25_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_131: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_25 * linear_180;  silu_25 = linear_180 = None
        down_proj_25: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_131, l_self_modules_model_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_131 = l_self_modules_model_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_103: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_102 + down_proj_25;  hidden_states_102 = down_proj_25 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_52 = torch.autograd.function.FunctionCtx();  function_ctx_52 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_234: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_103.contiguous()
        contiguous_235: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_26_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_26_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_52: "f16[1, 4096][4096, 1]cuda:0" = contiguous_234.view(-1, 4096);  contiguous_234 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_104: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_52: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_52 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 378, constant_args_idx = 377, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_104, 'X_ptr': X_52, 'W_ptr': contiguous_235, 'RSTD_ptr': RSTD_52});  X_52 = contiguous_235 = RSTD_52 = triton_kernel_wrapper_mutation_52 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_105: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_104.view(1, 1, 4096);  Y_104 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_182: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_105, l_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_184: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_182.view((1, 1, -1, 128));  linear_182 = None
        query_states_26: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_184.transpose(1, 2);  view_184 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_183: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_105, l_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_185: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_183.view((1, 1, -1, 128));  linear_183 = None
        key_states_26: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_185.transpose(1, 2);  view_185 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_184: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_105, l_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_105 = l_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_186: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_184.view((1, 1, -1, 128));  linear_184 = None
        value_states_26: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_186.transpose(1, 2);  view_186 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_29: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_29: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_132: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_26 * cos_29
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_52: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_26[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_52: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_26[(Ellipsis, slice(64, None, None))];  query_states_26 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_52: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_52;  x2_52 = None
        cat_53: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_52, x1_52), dim = -1);  neg_52 = x1_52 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_133: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_53 * sin_29;  cat_53 = None
        q_embed_26: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_132 + mul_133;  mul_132 = mul_133 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_134: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_26 * cos_29;  cos_29 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_53: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_26[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_53: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_26[(Ellipsis, slice(64, None, None))];  key_states_26 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_53: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_53;  x2_53 = None
        cat_54: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_53, x1_53), dim = -1);  neg_53 = x1_53 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_135: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_54 * sin_29;  cat_54 = sin_29 = None
        k_embed_26: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_134 + mul_135;  mul_134 = mul_135 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__52: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_26_keys.index_copy_(2, l_cache_position_, k_embed_26);  k_embed_26 = index_copy__52 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__53: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_26_values.index_copy_(2, l_cache_position_, value_states_26);  value_states_26 = index_copy__53 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_188: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_26_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_26_keys = None
        hidden_states_104: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_188.expand(1, 8, 4, 2048, 128);  getitem_188 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_52: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_104.reshape(1, 32, 2048, 128);  hidden_states_104 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_189: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_26_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_26_values = None
        hidden_states_105: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_189.expand(1, 8, 4, 2048, 128);  getitem_189 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_52: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_105.reshape(1, 32, 2048, 128);  hidden_states_105 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_26: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_26: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_26.contiguous();  q_embed_26 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_53: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_52.contiguous();  key_52 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_53: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_52.contiguous();  value_52 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_104: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_26, key_53, value_53, attn_mask = causal_mask_26, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_26 = key_53 = value_53 = causal_mask_26 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_108: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_104.transpose(1, 2);  attn_output_104 = None
        attn_output_105: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_108.contiguous();  transpose_108 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_80: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_105.reshape(1, 1, -1);  attn_output_105 = None
        attn_output_106: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_80.contiguous();  reshape_80 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_107: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_106, l_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_106 = l_self_modules_model_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_106: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_103 + attn_output_107;  hidden_states_103 = attn_output_107 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_53 = torch.autograd.function.FunctionCtx();  function_ctx_53 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_241: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_106.contiguous()
        contiguous_242: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_53: "f16[1, 4096][4096, 1]cuda:0" = contiguous_241.view(-1, 4096);  contiguous_241 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_106: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_53: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_53 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 379, constant_args_idx = 378, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_106, 'X_ptr': X_53, 'W_ptr': contiguous_242, 'RSTD_ptr': RSTD_53});  X_53 = contiguous_242 = RSTD_53 = triton_kernel_wrapper_mutation_53 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_107: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_106.view(1, 1, 4096);  Y_106 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_186: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_107, l_self_modules_model_modules_layers_modules_26_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_26_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_26: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_186, inplace = False);  linear_186 = None
        linear_187: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_107, l_self_modules_model_modules_layers_modules_26_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_107 = l_self_modules_model_modules_layers_modules_26_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_136: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_26 * linear_187;  silu_26 = linear_187 = None
        down_proj_26: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_136, l_self_modules_model_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_136 = l_self_modules_model_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_107: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_106 + down_proj_26;  hidden_states_106 = down_proj_26 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_54 = torch.autograd.function.FunctionCtx();  function_ctx_54 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_243: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_107.contiguous()
        contiguous_244: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_27_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_27_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_54: "f16[1, 4096][4096, 1]cuda:0" = contiguous_243.view(-1, 4096);  contiguous_243 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_108: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_54: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_54 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 380, constant_args_idx = 379, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_108, 'X_ptr': X_54, 'W_ptr': contiguous_244, 'RSTD_ptr': RSTD_54});  X_54 = contiguous_244 = RSTD_54 = triton_kernel_wrapper_mutation_54 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_109: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_108.view(1, 1, 4096);  Y_108 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_189: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_109, l_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_191: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_189.view((1, 1, -1, 128));  linear_189 = None
        query_states_27: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_191.transpose(1, 2);  view_191 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_190: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_109, l_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_192: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_190.view((1, 1, -1, 128));  linear_190 = None
        key_states_27: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_192.transpose(1, 2);  view_192 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_191: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_109, l_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_109 = l_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_193: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_191.view((1, 1, -1, 128));  linear_191 = None
        value_states_27: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_193.transpose(1, 2);  view_193 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_30: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_30: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_137: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_27 * cos_30
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_54: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_27[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_54: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_27[(Ellipsis, slice(64, None, None))];  query_states_27 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_54: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_54;  x2_54 = None
        cat_55: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_54, x1_54), dim = -1);  neg_54 = x1_54 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_138: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_55 * sin_30;  cat_55 = None
        q_embed_27: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_137 + mul_138;  mul_137 = mul_138 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_139: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_27 * cos_30;  cos_30 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_55: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_27[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_55: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_27[(Ellipsis, slice(64, None, None))];  key_states_27 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_55: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_55;  x2_55 = None
        cat_56: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_55, x1_55), dim = -1);  neg_55 = x1_55 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_140: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_56 * sin_30;  cat_56 = sin_30 = None
        k_embed_27: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_139 + mul_140;  mul_139 = mul_140 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__54: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_27_keys.index_copy_(2, l_cache_position_, k_embed_27);  k_embed_27 = index_copy__54 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__55: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_27_values.index_copy_(2, l_cache_position_, value_states_27);  value_states_27 = index_copy__55 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_195: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_27_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_27_keys = None
        hidden_states_108: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_195.expand(1, 8, 4, 2048, 128);  getitem_195 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_54: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_108.reshape(1, 32, 2048, 128);  hidden_states_108 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_196: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_27_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_27_values = None
        hidden_states_109: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_196.expand(1, 8, 4, 2048, 128);  getitem_196 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_54: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_109.reshape(1, 32, 2048, 128);  hidden_states_109 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_27: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_27: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_27.contiguous();  q_embed_27 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_55: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_54.contiguous();  key_54 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_55: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_54.contiguous();  value_54 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_108: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_27, key_55, value_55, attn_mask = causal_mask_27, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_27 = key_55 = value_55 = causal_mask_27 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_112: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_108.transpose(1, 2);  attn_output_108 = None
        attn_output_109: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_112.contiguous();  transpose_112 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_83: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_109.reshape(1, 1, -1);  attn_output_109 = None
        attn_output_110: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_83.contiguous();  reshape_83 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_111: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_110, l_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_110 = l_self_modules_model_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_110: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_107 + attn_output_111;  hidden_states_107 = attn_output_111 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_55 = torch.autograd.function.FunctionCtx();  function_ctx_55 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_250: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_110.contiguous()
        contiguous_251: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_55: "f16[1, 4096][4096, 1]cuda:0" = contiguous_250.view(-1, 4096);  contiguous_250 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_110: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_55: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_55 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 381, constant_args_idx = 380, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_110, 'X_ptr': X_55, 'W_ptr': contiguous_251, 'RSTD_ptr': RSTD_55});  X_55 = contiguous_251 = RSTD_55 = triton_kernel_wrapper_mutation_55 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_111: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_110.view(1, 1, 4096);  Y_110 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_193: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_111, l_self_modules_model_modules_layers_modules_27_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_27_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_27: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_193, inplace = False);  linear_193 = None
        linear_194: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_111, l_self_modules_model_modules_layers_modules_27_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_111 = l_self_modules_model_modules_layers_modules_27_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_141: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_27 * linear_194;  silu_27 = linear_194 = None
        down_proj_27: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_141, l_self_modules_model_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_141 = l_self_modules_model_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_111: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_110 + down_proj_27;  hidden_states_110 = down_proj_27 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_56 = torch.autograd.function.FunctionCtx();  function_ctx_56 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_252: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_111.contiguous()
        contiguous_253: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_28_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_28_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_56: "f16[1, 4096][4096, 1]cuda:0" = contiguous_252.view(-1, 4096);  contiguous_252 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_112: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_56: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_56 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 382, constant_args_idx = 381, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_112, 'X_ptr': X_56, 'W_ptr': contiguous_253, 'RSTD_ptr': RSTD_56});  X_56 = contiguous_253 = RSTD_56 = triton_kernel_wrapper_mutation_56 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_113: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_112.view(1, 1, 4096);  Y_112 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_196: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_113, l_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_198: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_196.view((1, 1, -1, 128));  linear_196 = None
        query_states_28: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_198.transpose(1, 2);  view_198 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_197: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_113, l_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_199: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_197.view((1, 1, -1, 128));  linear_197 = None
        key_states_28: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_199.transpose(1, 2);  view_199 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_198: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_113, l_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_113 = l_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_200: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_198.view((1, 1, -1, 128));  linear_198 = None
        value_states_28: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_200.transpose(1, 2);  view_200 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_31: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_31: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_142: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_28 * cos_31
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_56: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_28[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_56: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_28[(Ellipsis, slice(64, None, None))];  query_states_28 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_56: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_56;  x2_56 = None
        cat_57: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_56, x1_56), dim = -1);  neg_56 = x1_56 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_143: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_57 * sin_31;  cat_57 = None
        q_embed_28: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_142 + mul_143;  mul_142 = mul_143 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_144: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_28 * cos_31;  cos_31 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_57: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_28[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_57: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_28[(Ellipsis, slice(64, None, None))];  key_states_28 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_57: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_57;  x2_57 = None
        cat_58: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_57, x1_57), dim = -1);  neg_57 = x1_57 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_145: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_58 * sin_31;  cat_58 = sin_31 = None
        k_embed_28: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_144 + mul_145;  mul_144 = mul_145 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__56: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_28_keys.index_copy_(2, l_cache_position_, k_embed_28);  k_embed_28 = index_copy__56 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__57: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_28_values.index_copy_(2, l_cache_position_, value_states_28);  value_states_28 = index_copy__57 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_202: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_28_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_28_keys = None
        hidden_states_112: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_202.expand(1, 8, 4, 2048, 128);  getitem_202 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_56: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_112.reshape(1, 32, 2048, 128);  hidden_states_112 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_203: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_28_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_28_values = None
        hidden_states_113: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_203.expand(1, 8, 4, 2048, 128);  getitem_203 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_56: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_113.reshape(1, 32, 2048, 128);  hidden_states_113 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_28: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_28: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_28.contiguous();  q_embed_28 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_57: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_56.contiguous();  key_56 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_57: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_56.contiguous();  value_56 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_112: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_28, key_57, value_57, attn_mask = causal_mask_28, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_28 = key_57 = value_57 = causal_mask_28 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_116: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_112.transpose(1, 2);  attn_output_112 = None
        attn_output_113: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_116.contiguous();  transpose_116 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_86: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_113.reshape(1, 1, -1);  attn_output_113 = None
        attn_output_114: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_86.contiguous();  reshape_86 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_115: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_114, l_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_114 = l_self_modules_model_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_114: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_111 + attn_output_115;  hidden_states_111 = attn_output_115 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_57 = torch.autograd.function.FunctionCtx();  function_ctx_57 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_259: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_114.contiguous()
        contiguous_260: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_57: "f16[1, 4096][4096, 1]cuda:0" = contiguous_259.view(-1, 4096);  contiguous_259 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_114: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_57: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_57 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 383, constant_args_idx = 382, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_114, 'X_ptr': X_57, 'W_ptr': contiguous_260, 'RSTD_ptr': RSTD_57});  X_57 = contiguous_260 = RSTD_57 = triton_kernel_wrapper_mutation_57 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_115: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_114.view(1, 1, 4096);  Y_114 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_200: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_115, l_self_modules_model_modules_layers_modules_28_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_28_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_28: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_200, inplace = False);  linear_200 = None
        linear_201: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_115, l_self_modules_model_modules_layers_modules_28_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_115 = l_self_modules_model_modules_layers_modules_28_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_146: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_28 * linear_201;  silu_28 = linear_201 = None
        down_proj_28: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_146, l_self_modules_model_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_146 = l_self_modules_model_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_115: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_114 + down_proj_28;  hidden_states_114 = down_proj_28 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_58 = torch.autograd.function.FunctionCtx();  function_ctx_58 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_261: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_115.contiguous()
        contiguous_262: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_29_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_29_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_58: "f16[1, 4096][4096, 1]cuda:0" = contiguous_261.view(-1, 4096);  contiguous_261 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_116: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_58: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_58 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 384, constant_args_idx = 383, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_116, 'X_ptr': X_58, 'W_ptr': contiguous_262, 'RSTD_ptr': RSTD_58});  X_58 = contiguous_262 = RSTD_58 = triton_kernel_wrapper_mutation_58 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_117: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_116.view(1, 1, 4096);  Y_116 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_203: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_117, l_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_205: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_203.view((1, 1, -1, 128));  linear_203 = None
        query_states_29: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_205.transpose(1, 2);  view_205 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_204: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_117, l_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_206: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_204.view((1, 1, -1, 128));  linear_204 = None
        key_states_29: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_206.transpose(1, 2);  view_206 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_205: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_117, l_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_117 = l_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_207: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_205.view((1, 1, -1, 128));  linear_205 = None
        value_states_29: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_207.transpose(1, 2);  view_207 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_32: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_32: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_147: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_29 * cos_32
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_58: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_29[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_58: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_29[(Ellipsis, slice(64, None, None))];  query_states_29 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_58: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_58;  x2_58 = None
        cat_59: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_58, x1_58), dim = -1);  neg_58 = x1_58 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_148: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_59 * sin_32;  cat_59 = None
        q_embed_29: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_147 + mul_148;  mul_147 = mul_148 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_149: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_29 * cos_32;  cos_32 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_59: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_29[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_59: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_29[(Ellipsis, slice(64, None, None))];  key_states_29 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_59: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_59;  x2_59 = None
        cat_60: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_59, x1_59), dim = -1);  neg_59 = x1_59 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_150: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_60 * sin_32;  cat_60 = sin_32 = None
        k_embed_29: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_149 + mul_150;  mul_149 = mul_150 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__58: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_29_keys.index_copy_(2, l_cache_position_, k_embed_29);  k_embed_29 = index_copy__58 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__59: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_29_values.index_copy_(2, l_cache_position_, value_states_29);  value_states_29 = index_copy__59 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_209: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_29_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_29_keys = None
        hidden_states_116: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_209.expand(1, 8, 4, 2048, 128);  getitem_209 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_58: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_116.reshape(1, 32, 2048, 128);  hidden_states_116 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_210: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_29_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_29_values = None
        hidden_states_117: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_210.expand(1, 8, 4, 2048, 128);  getitem_210 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_58: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_117.reshape(1, 32, 2048, 128);  hidden_states_117 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_29: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_29: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_29.contiguous();  q_embed_29 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_59: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_58.contiguous();  key_58 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_59: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_58.contiguous();  value_58 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_116: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_29, key_59, value_59, attn_mask = causal_mask_29, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_29 = key_59 = value_59 = causal_mask_29 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_120: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_116.transpose(1, 2);  attn_output_116 = None
        attn_output_117: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_120.contiguous();  transpose_120 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_89: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_117.reshape(1, 1, -1);  attn_output_117 = None
        attn_output_118: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_89.contiguous();  reshape_89 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_119: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_118, l_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_118 = l_self_modules_model_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_118: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_115 + attn_output_119;  hidden_states_115 = attn_output_119 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_59 = torch.autograd.function.FunctionCtx();  function_ctx_59 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_268: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_118.contiguous()
        contiguous_269: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_59: "f16[1, 4096][4096, 1]cuda:0" = contiguous_268.view(-1, 4096);  contiguous_268 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_118: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_59: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_59 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 385, constant_args_idx = 384, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_118, 'X_ptr': X_59, 'W_ptr': contiguous_269, 'RSTD_ptr': RSTD_59});  X_59 = contiguous_269 = RSTD_59 = triton_kernel_wrapper_mutation_59 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_119: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_118.view(1, 1, 4096);  Y_118 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_207: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_119, l_self_modules_model_modules_layers_modules_29_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_29_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_29: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_207, inplace = False);  linear_207 = None
        linear_208: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_119, l_self_modules_model_modules_layers_modules_29_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_119 = l_self_modules_model_modules_layers_modules_29_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_151: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_29 * linear_208;  silu_29 = linear_208 = None
        down_proj_29: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_151, l_self_modules_model_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_151 = l_self_modules_model_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_119: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_118 + down_proj_29;  hidden_states_118 = down_proj_29 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_60 = torch.autograd.function.FunctionCtx();  function_ctx_60 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_270: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_119.contiguous()
        contiguous_271: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_30_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_30_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_60: "f16[1, 4096][4096, 1]cuda:0" = contiguous_270.view(-1, 4096);  contiguous_270 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_120: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_60: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_60 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 386, constant_args_idx = 385, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_120, 'X_ptr': X_60, 'W_ptr': contiguous_271, 'RSTD_ptr': RSTD_60});  X_60 = contiguous_271 = RSTD_60 = triton_kernel_wrapper_mutation_60 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_121: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_120.view(1, 1, 4096);  Y_120 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_210: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_121, l_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_212: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_210.view((1, 1, -1, 128));  linear_210 = None
        query_states_30: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_212.transpose(1, 2);  view_212 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_211: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_121, l_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_213: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_211.view((1, 1, -1, 128));  linear_211 = None
        key_states_30: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_213.transpose(1, 2);  view_213 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_212: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_121, l_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_121 = l_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_214: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_212.view((1, 1, -1, 128));  linear_212 = None
        value_states_30: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_214.transpose(1, 2);  view_214 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_33: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_33: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_152: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_30 * cos_33
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_60: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_30[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_60: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_30[(Ellipsis, slice(64, None, None))];  query_states_30 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_60: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_60;  x2_60 = None
        cat_61: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_60, x1_60), dim = -1);  neg_60 = x1_60 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_153: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_61 * sin_33;  cat_61 = None
        q_embed_30: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_152 + mul_153;  mul_152 = mul_153 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_154: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_30 * cos_33;  cos_33 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_61: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_30[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_61: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_30[(Ellipsis, slice(64, None, None))];  key_states_30 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_61: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_61;  x2_61 = None
        cat_62: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_61, x1_61), dim = -1);  neg_61 = x1_61 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_155: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_62 * sin_33;  cat_62 = sin_33 = None
        k_embed_30: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_154 + mul_155;  mul_154 = mul_155 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__60: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_30_keys.index_copy_(2, l_cache_position_, k_embed_30);  k_embed_30 = index_copy__60 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__61: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_30_values.index_copy_(2, l_cache_position_, value_states_30);  value_states_30 = index_copy__61 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_216: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_30_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_30_keys = None
        hidden_states_120: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_216.expand(1, 8, 4, 2048, 128);  getitem_216 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_60: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_120.reshape(1, 32, 2048, 128);  hidden_states_120 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_217: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_30_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_30_values = None
        hidden_states_121: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_217.expand(1, 8, 4, 2048, 128);  getitem_217 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_60: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_121.reshape(1, 32, 2048, 128);  hidden_states_121 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_30: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_30: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_30.contiguous();  q_embed_30 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_61: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_60.contiguous();  key_60 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_61: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_60.contiguous();  value_60 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_120: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_30, key_61, value_61, attn_mask = causal_mask_30, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_30 = key_61 = value_61 = causal_mask_30 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_124: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_120.transpose(1, 2);  attn_output_120 = None
        attn_output_121: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_124.contiguous();  transpose_124 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_92: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_121.reshape(1, 1, -1);  attn_output_121 = None
        attn_output_122: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_92.contiguous();  reshape_92 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_123: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_122, l_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_122 = l_self_modules_model_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_122: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_119 + attn_output_123;  hidden_states_119 = attn_output_123 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_61 = torch.autograd.function.FunctionCtx();  function_ctx_61 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_277: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_122.contiguous()
        contiguous_278: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_61: "f16[1, 4096][4096, 1]cuda:0" = contiguous_277.view(-1, 4096);  contiguous_277 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_122: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_61: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_61 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 387, constant_args_idx = 386, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_122, 'X_ptr': X_61, 'W_ptr': contiguous_278, 'RSTD_ptr': RSTD_61});  X_61 = contiguous_278 = RSTD_61 = triton_kernel_wrapper_mutation_61 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_123: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_122.view(1, 1, 4096);  Y_122 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_214: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_123, l_self_modules_model_modules_layers_modules_30_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_30_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_30: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_214, inplace = False);  linear_214 = None
        linear_215: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_123, l_self_modules_model_modules_layers_modules_30_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_123 = l_self_modules_model_modules_layers_modules_30_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_156: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_30 * linear_215;  silu_30 = linear_215 = None
        down_proj_30: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_156, l_self_modules_model_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_156 = l_self_modules_model_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_123: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_122 + down_proj_30;  hidden_states_122 = down_proj_30 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_62 = torch.autograd.function.FunctionCtx();  function_ctx_62 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_279: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_123.contiguous()
        contiguous_280: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_31_modules_input_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_31_modules_input_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_62: "f16[1, 4096][4096, 1]cuda:0" = contiguous_279.view(-1, 4096);  contiguous_279 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_124: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_62: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_62 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 388, constant_args_idx = 387, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_124, 'X_ptr': X_62, 'W_ptr': contiguous_280, 'RSTD_ptr': RSTD_62});  X_62 = contiguous_280 = RSTD_62 = triton_kernel_wrapper_mutation_62 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_125: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_124.view(1, 1, 4096);  Y_124 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_217: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(Y_125, l_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view_219: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = linear_217.view((1, 1, -1, 128));  linear_217 = None
        query_states_31: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = view_219.transpose(1, 2);  view_219 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_218: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_125, l_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_220: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_218.view((1, 1, -1, 128));  linear_218 = None
        key_states_31: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_220.transpose(1, 2);  view_220 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_219: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch._C._nn.linear(Y_125, l_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_v_proj_parameters_weight_, None);  Y_125 = l_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_221: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = linear_219.view((1, 1, -1, 128));  linear_219 = None
        value_states_31: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = view_221.transpose(1, 2);  view_221 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_34: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_34: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_157: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = query_states_31 * cos_34
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_62: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_31[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_62: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = query_states_31[(Ellipsis, slice(64, None, None))];  query_states_31 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_62: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = -x2_62;  x2_62 = None
        cat_63: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.cat((neg_62, x1_62), dim = -1);  neg_62 = x1_62 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_158: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = cat_63 * sin_34;  cat_63 = None
        q_embed_31: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = mul_157 + mul_158;  mul_157 = mul_158 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_159: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = key_states_31 * cos_34;  cos_34 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_63: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_31[(Ellipsis, slice(None, 64, None))]
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_63: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = key_states_31[(Ellipsis, slice(64, None, None))];  key_states_31 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_63: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = -x2_63;  x2_63 = None
        cat_64: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.cat((neg_63, x1_63), dim = -1);  neg_63 = x1_63 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_160: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = cat_64 * sin_34;  cat_64 = sin_34 = None
        k_embed_31: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = mul_159 + mul_160;  mul_159 = mul_160 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_copy__62: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_31_keys.index_copy_(2, l_cache_position_, k_embed_31);  k_embed_31 = index_copy__62 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_copy__63: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = l_past_key_values_layers_31_values.index_copy_(2, l_cache_position_, value_states_31);  l_cache_position_ = value_states_31 = index_copy__63 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_223: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_31_keys[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_31_keys = None
        hidden_states_124: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_223.expand(1, 8, 4, 2048, 128);  getitem_223 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key_62: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_124.reshape(1, 32, 2048, 128);  hidden_states_124 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:14 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_224: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = l_past_key_values_layers_31_values[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  l_past_key_values_layers_31_values = None
        hidden_states_125: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = getitem_224.expand(1, 8, 4, 2048, 128);  getitem_224 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value_62: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = hidden_states_125.reshape(1, 32, 2048, 128);  hidden_states_125 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        causal_mask_31: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = l_attention_mask_[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))];  l_attention_mask_ = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        query_31: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = q_embed_31.contiguous();  q_embed_31 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:40 in sdpa_attention_forward, code: key = key.contiguous()
        key_63: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = key_62.contiguous();  key_62 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:41 in sdpa_attention_forward, code: value = value.contiguous()
        value_63: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = value_62.contiguous();  value_62 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output_124: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(query_31, key_63, value_63, attn_mask = causal_mask_31, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query_31 = key_63 = value_63 = causal_mask_31 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_128: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = attn_output_124.transpose(1, 2);  attn_output_124 = None
        attn_output_125: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = transpose_128.contiguous();  transpose_128 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_95: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = attn_output_125.reshape(1, 1, -1);  attn_output_125 = None
        attn_output_126: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = reshape_95.contiguous();  reshape_95 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_127: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(attn_output_126, l_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_126 = l_self_modules_model_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        hidden_states_126: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_123 + attn_output_127;  hidden_states_123 = attn_output_127 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_63 = torch.autograd.function.FunctionCtx();  function_ctx_63 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_286: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_126.contiguous()
        contiguous_287: "f16[4096][1]cuda:0" = l_self_modules_model_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_.contiguous();  l_self_modules_model_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_63: "f16[1, 4096][4096, 1]cuda:0" = contiguous_286.view(-1, 4096);  contiguous_286 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_126: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_63: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_63 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 389, constant_args_idx = 388, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_126, 'X_ptr': X_63, 'W_ptr': contiguous_287, 'RSTD_ptr': RSTD_63});  X_63 = contiguous_287 = RSTD_63 = triton_kernel_wrapper_mutation_63 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_127: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_126.view(1, 1, 4096);  Y_126 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_221: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_127, l_self_modules_model_modules_layers_modules_31_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_31_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu_31: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.nn.functional.silu(linear_221, inplace = False);  linear_221 = None
        linear_222: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch._C._nn.linear(Y_127, l_self_modules_model_modules_layers_modules_31_modules_mlp_modules_up_proj_parameters_weight_, None);  Y_127 = l_self_modules_model_modules_layers_modules_31_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_161: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = silu_31 * linear_222;  silu_31 = linear_222 = None
        down_proj_31: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch._C._nn.linear(mul_161, l_self_modules_model_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_161 = l_self_modules_model_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_ = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        hidden_states_127: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_126 + down_proj_31;  hidden_states_126 = down_proj_31 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:666 in forward, code: return LigerRMSNormFunction.apply(
        function_ctx_64 = torch.autograd.function.FunctionCtx();  function_ctx_64 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:72 in maybe_to_contiguous, code: return x.contiguous() if isinstance(x, torch.Tensor) else x
        contiguous_288: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = hidden_states_127.contiguous();  hidden_states_127 = None
        contiguous_289: "f16[4096][1]cuda:0" = l_self_modules_model_modules_norm_parameters_weight_.contiguous();  l_self_modules_model_modules_norm_parameters_weight_ = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        X_64: "f16[1, 4096][4096, 1]cuda:0" = contiguous_288.view(-1, 4096);  contiguous_288 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        Y_128: "f16[1, 4096][4096, 1]cuda:0" = torch.empty((1, 4096), dtype = torch.float16, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        RSTD_64: "f32[1][1]cuda:0" = torch.empty(1, dtype = torch.float32, device = device(type='cuda', index=0))
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_mutation_64 = torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 390, constant_args_idx = 389, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': Y_128, 'X_ptr': X_64, 'W_ptr': contiguous_289, 'RSTD_ptr': RSTD_64});  X_64 = contiguous_289 = RSTD_64 = triton_kernel_wrapper_mutation_64 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:499 in rms_norm_forward, code: return Y.view(*shape), X, RSTD, BLOCK_SIZE, num_warps, casting_mode
        Y_129: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_128.view(1, 1, 4096);  Y_128 = None
        
         # File: /app/low_bit_inference/model.py:451 in forward, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
        getitem_226: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = Y_129[(slice(None, None, None), slice(-1, None, None), slice(None, None, None))];  Y_129 = None
        logits: "f16[1, 1, 128256][128256, 128256, 1]cuda:0" = torch._C._nn.linear(getitem_226, l_self_modules_lm_head_parameters_weight_, None);  getitem_226 = l_self_modules_lm_head_parameters_weight_ = None
        return (logits,)
        