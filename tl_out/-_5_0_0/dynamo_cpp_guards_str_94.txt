
TREE_GUARD_MANAGER:
+- RootGuardManager
| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:520 in init_ambient_guards
| +- GLOBAL_STATE: ___check_global_state()
| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
| +- GuardManager: source=L['self'], accessed_by=FrameLocalsGuardAccessor(key='self', framelocals_idx=0)
| | +- TYPE_MATCH: ___check_type_id(L['self'], 879153888)                      
| | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | +- GuardManager: source=L['self']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules, 21447360)              
| | | | +- GuardManager: source=L['self']._modules['model'], accessed_by=DictGetItemGuardAccessor('model')
| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model'], 879152112)    
| | | | | +- GuardManager: source=L['self']._modules['model'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model'].__dict__)
| | | | | | +- GuardManager: source=L['self']._modules['model'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model'].config, 879143152)
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn'].config
| | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn'].config
| | | | | | | +- GuardManager: source=object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'), accessed_by=GenericGetAttrGuardAccessor(attribute_map)
| | | | | | | | +- TYPE_MATCH: ___check_type_id(object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'), 21447360)
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('num_hidden_layers', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_attn_implementation', object.__getattribute__(L['self']._modules['model'].config, 'attribute_map'))
| | | | | | | +- GuardManager: source=L['self']._modules['model'].config.__getattribute__, accessed_by=GetAttrGuardAccessor(__getattribute__)
| | | | | | | | +- GuardManager: source=L['self']._modules['model'].config.__getattribute__.__closure__, accessed_by=GetAttrGuardAccessor(__closure__)
| | | | | | | | | +- GuardManager: source=L['self']._modules['model'].config.__getattribute__.__closure__[0], accessed_by=TupleGetItemGuardAccessor(0)
| | | | | | | | | | +- GuardManager: source=L['self']._modules['model'].config.__getattribute__.__closure__[0].cell_contents, accessed_by=GetAttrGuardAccessor(cell_contents)
| | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model'].config.__getattribute__.__closure__[0].cell_contents, 783905248)
| | | | | | | +- GuardManager: source=object.__getattribute__(L['self']._modules['model'].config, 'num_hidden_layers'), accessed_by=GenericGetAttrGuardAccessor(num_hidden_layers)
| | | | | | | | +- EQUALS_MATCH: object.__getattribute__(L['self']._modules['model'].config, 'num_hidden_layers') == 32
| | | | | | | +- GuardManager: source=object.__getattribute__(L['self']._modules['model'].config, '_attn_implementation'), accessed_by=GenericGetAttrGuardAccessor(_attn_implementation)
| | | | | | | | +- EQUALS_MATCH: object.__getattribute__(L['self']._modules['model'].config, '_attn_implementation') == 'sdpa'
| | | | | | +- GuardManager: source=L['self']._modules['model']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules, 21447360)
| | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['norm'], accessed_by=DictGetItemGuardAccessor('norm')
| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['norm'], 783312032)
| | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['norm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['norm'].__dict__)
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['norm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['norm'].offset == 0.0  
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['norm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['norm'].in_place, 21370792)
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['norm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['norm'].row_mode, 21458960)
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['norm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['norm']._parameters, 21447360)
| | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['norm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['norm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['norm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['norm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['norm'].casting_mode == 'llama'
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['norm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['norm'].variance_epsilon == 1e-05
| | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers'], accessed_by=DictGetItemGuardAccessor('layers')
| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers'], 759874464)
| | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers'].__class__, accessed_by=GetAttrGuardAccessor(__class__)
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers'].__class__.call_super_init, accessed_by=GetAttrGuardAccessor(call_super_init)
| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers'].__class__.call_super_init, 21370824)
| | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | +- DictGuardManager: source=L['self']._modules['model']._modules['layers']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | +- KeyValueManager pair at index=0
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[0]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[0] == '0'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['0'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn'].layer_idx == 0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['0']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['0']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['0']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['0']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['0']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=1
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[1]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[1] == '1'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['1'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn'].layer_idx == 1
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['1']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['1']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['1']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['1']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['1']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=2
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[2]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[2] == '2'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['2'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn'].layer_idx == 2
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['2']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['2']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['2']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['2']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['2']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=3
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[3]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[3] == '3'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['3'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn'].layer_idx == 3
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['3']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['3']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['3']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['3']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['3']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=4
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[4]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[4] == '4'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['4'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn'].layer_idx == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['4']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['4']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['4']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['4']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['4']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=5
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[5]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[5] == '5'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['5'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn'].layer_idx == 5
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['5']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['5']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['5']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['5']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['5']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=6
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[6]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[6] == '6'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['6'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn'].layer_idx == 6
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['6']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['6']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['6']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['6']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['6']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=7
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[7]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[7] == '7'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['7'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn'].layer_idx == 7
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['7']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['7']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['7']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['7']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['7']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=8
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[8]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[8] == '8'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['8'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn'].layer_idx == 8
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['8']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['8']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['8']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['8']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['8']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=9
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[9]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[9] == '9'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['9'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn'].layer_idx == 9
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['9']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['9']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['9']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['9']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['9']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=10
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[10]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[10] == '10'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['10'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn'].layer_idx == 10
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['10']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['10']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['10']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['10']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['10']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=11
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[11]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[11] == '11'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['11'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn'].layer_idx == 11
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['11']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['11']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['11']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['11']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['11']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=12
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[12]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[12] == '12'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['12'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn'].layer_idx == 12
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['12']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['12']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['12']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['12']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['12']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=13
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[13]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[13] == '13'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['13'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn'].layer_idx == 13
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['13']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['13']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['13']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['13']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['13']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=14
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[14]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[14] == '14'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['14'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn'].layer_idx == 14
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['14']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['14']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['14']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['14']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['14']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=15
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[15]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[15] == '15'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['15'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn'].layer_idx == 15
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['15']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['15']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['15']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['15']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['15']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=16
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[16]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[16] == '16'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['16'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn'].layer_idx == 16
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['16']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['16']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['16']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['16']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['16']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=17
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[17]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[17] == '17'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['17'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn'].layer_idx == 17
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['17']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['17']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['17']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['17']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['17']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=18
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[18]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[18] == '18'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['18'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn'].layer_idx == 18
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['18']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['18']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['18']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['18']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['18']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=19
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[19]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[19] == '19'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['19'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn'].layer_idx == 19
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['19']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['19']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['19']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['19']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['19']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=20
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[20]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[20] == '20'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['20'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn'].layer_idx == 20
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['20']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['20']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['20']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['20']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['20']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=21
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[21]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[21] == '21'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['21'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn'].layer_idx == 21
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['21']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['21']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['21']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['21']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['21']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=22
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[22]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[22] == '22'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['22'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn'].layer_idx == 22
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['22']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['22']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['22']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['22']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['22']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=23
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[23]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[23] == '23'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['23'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn'].layer_idx == 23
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['23']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['23']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['23']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['23']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['23']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=24
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[24]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[24] == '24'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['24'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn'].layer_idx == 24
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['24']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['24']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['24']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['24']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['24']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=25
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[25]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[25] == '25'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['25'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn'].layer_idx == 25
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['25']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['25']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['25']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['25']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['25']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=26
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[26]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[26] == '26'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['26'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn'].layer_idx == 26
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['26']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['26']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['26']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['26']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['26']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=27
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[27]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[27] == '27'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['27'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn'].layer_idx == 27
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['27']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['27']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['27']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['27']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['27']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=28
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[28]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[28] == '28'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['28'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn'].layer_idx == 28
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['28']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['28']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['28']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['28']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['28']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=29
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[29]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[29] == '29'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['29'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn'].layer_idx == 29
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['29']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['29']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['29']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['29']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['29']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=30
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[30]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[30] == '30'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['30'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn'].layer_idx == 30
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['30']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['30']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['30']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['30']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['30']._parameters, 21447360)
| | | | | | | | | | +- KeyValueManager pair at index=31
| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[31]
| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['model']._modules['layers']._modules))[31] == '31'
| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']
| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31'], 820875504)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['31'].__dict__)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp'], 820902560)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['act_fn'], accessed_by=DictGetItemGuardAccessor('act_fn')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['act_fn'], 759497776)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['act_fn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['act_fn'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['act_fn'].inplace, accessed_by=DictGetItemGuardAccessor('inplace')
| | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['act_fn'].inplace, 21370824)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['up_proj'], accessed_by=DictGetItemGuardAccessor('up_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['up_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['up_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['up_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['up_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['up_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['up_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['up_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['up_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['up_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['down_proj'], accessed_by=DictGetItemGuardAccessor('down_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['down_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['down_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['down_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['down_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['down_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['down_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['down_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['down_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['down_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 14336], stride=[14336, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['gate_proj'], accessed_by=DictGetItemGuardAccessor('gate_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['gate_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['gate_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['gate_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['gate_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['gate_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['gate_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['gate_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['gate_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._modules['gate_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[14336, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['mlp']._parameters, 21447360)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn'], accessed_by=DictGetItemGuardAccessor('self_attn')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn'], 820873728)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn'].scaling, accessed_by=DictGetItemGuardAccessor('scaling')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn'].scaling == 0.08838834764831845
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['k_proj'], accessed_by=DictGetItemGuardAccessor('k_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['k_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['k_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['k_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['k_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['k_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['k_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['k_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['k_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['k_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['o_proj'], accessed_by=DictGetItemGuardAccessor('o_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['o_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['o_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['o_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['o_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['o_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['o_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['o_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['o_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['o_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['q_proj'], accessed_by=DictGetItemGuardAccessor('q_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['q_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['q_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['q_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['q_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['q_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['q_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['q_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['q_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['q_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['v_proj'], accessed_by=DictGetItemGuardAccessor('v_proj')
| | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['v_proj'], 759894080)
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['v_proj'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['v_proj'].__dict__)
| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['v_proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['v_proj']._parameters, 21447360)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['v_proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['v_proj']._parameters['bias'], 21458960)
| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['v_proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._modules['v_proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[1024, 4096], stride=[4096, 1])
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn'].head_dim, accessed_by=DictGetItemGuardAccessor('head_dim')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn'].head_dim == 128
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn'].training, accessed_by=DictGetItemGuardAccessor('training')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn'].training, 21370824)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn'].layer_idx, accessed_by=DictGetItemGuardAccessor('layer_idx')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn'].layer_idx == 31
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn']._parameters, 21447360)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn'].num_key_value_groups, accessed_by=DictGetItemGuardAccessor('num_key_value_groups')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn'].num_key_value_groups == 4
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn'].config, accessed_by=DictGetItemGuardAccessor('config')
| | | | | | | | | | | | | | | | | +- OBJECT_ALIASING: L['self']._modules['model'].config is L['self']._modules['model']._modules['layers']._modules['31']._modules['self_attn'].config
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm'], accessed_by=DictGetItemGuardAccessor('input_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['31']._modules['input_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm'], accessed_by=DictGetItemGuardAccessor('post_attention_layernorm')
| | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm'], 783312032)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm'].__dict__)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm'].offset, accessed_by=DictGetItemGuardAccessor('offset')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm'].offset == 0.0
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm'].in_place, accessed_by=DictGetItemGuardAccessor('in_place')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm'].in_place, 21370792)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm'].row_mode, accessed_by=DictGetItemGuardAccessor('row_mode')
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm'].row_mode, 21458960)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm']._parameters, 21447360)
| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[4096], stride=[1])
| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm']._parameters['weight'].contiguous, accessed_by=GetAttrGuardAccessor(contiguous)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm'].casting_mode, accessed_by=DictGetItemGuardAccessor('casting_mode')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm'].casting_mode == 'llama'
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm'].variance_epsilon, accessed_by=DictGetItemGuardAccessor('variance_epsilon')
| | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['layers']._modules['31']._modules['post_attention_layernorm'].variance_epsilon == 1e-05
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['layers']._modules['31']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['layers']._modules['31']._parameters, 21447360)
| | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'], accessed_by=DictGetItemGuardAccessor('rotary_emb')
| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['rotary_emb'], 820429760)
| | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['rotary_emb'].__dict__)
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb']._buffers, accessed_by=DictGetItemGuardAccessor('_buffers')
| | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['rotary_emb']._buffers, 21447360)
| | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb']._buffers['inv_freq'], accessed_by=DictGetItemGuardAccessor('inv_freq')
| | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['rotary_emb']._buffers['inv_freq'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float32, device=0, requires_grad=False, size=[64], stride=[1])
| | | | | | | | | | | +- NO_TENSOR_ALIASING: check_no_aliasing(L['input_ids'], L['position_ids'], L['attention_mask'], L['cache_position'], L['past_key_values'].layers[0].keys, L['past_key_values'].layers[1].keys, L['past_key_values'].layers[2].keys, L['past_key_values'].layers[3].keys, L['past_key_values'].layers[4].keys, L['past_key_values'].layers[5].keys, L['past_key_values'].layers[6].keys, L['past_key_values'].layers[7].keys, L['past_key_values'].layers[8].keys, L['past_key_values'].layers[9].keys, L['past_key_values'].layers[10].keys, L['past_key_values'].layers[11].keys, L['past_key_values'].layers[12].keys, L['past_key_values'].layers[13].keys, L['past_key_values'].layers[14].keys, L['past_key_values'].layers[15].keys, L['past_key_values'].layers[16].keys, L['past_key_values'].layers[17].keys, L['past_key_values'].layers[18].keys, L['past_key_values'].layers[19].keys, L['past_key_values'].layers[20].keys, L['past_key_values'].layers[21].keys, L['past_key_values'].layers[22].keys, L['past_key_values'].layers[23].keys, L['past_key_values'].layers[24].keys, L['past_key_values'].layers[25].keys, L['past_key_values'].layers[26].keys, L['past_key_values'].layers[27].keys, L['past_key_values'].layers[28].keys, L['past_key_values'].layers[29].keys, L['past_key_values'].layers[30].keys, L['past_key_values'].layers[31].keys, L['past_key_values'].layers[0].values, L['past_key_values'].layers[1].values, L['past_key_values'].layers[2].values, L['past_key_values'].layers[3].values, L['past_key_values'].layers[4].values, L['past_key_values'].layers[5].values, L['past_key_values'].layers[6].values, L['past_key_values'].layers[7].values, L['past_key_values'].layers[8].values, L['past_key_values'].layers[9].values, L['past_key_values'].layers[10].values, L['past_key_values'].layers[11].values, L['past_key_values'].layers[12].values, L['past_key_values'].layers[13].values, L['past_key_values'].layers[14].values, L['past_key_values'].layers[15].values, L['past_key_values'].layers[16].values, L['past_key_values'].layers[17].values, L['past_key_values'].layers[18].values, L['past_key_values'].layers[19].values, L['past_key_values'].layers[20].values, L['past_key_values'].layers[21].values, L['past_key_values'].layers[22].values, L['past_key_values'].layers[23].values, L['past_key_values'].layers[24].values, L['past_key_values'].layers[25].values, L['past_key_values'].layers[26].values, L['past_key_values'].layers[27].values, L['past_key_values'].layers[28].values, L['past_key_values'].layers[29].values, L['past_key_values'].layers[30].values, L['past_key_values'].layers[31].values, L['self']._modules['model']._modules['rotary_emb']._buffers['inv_freq'])
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb']._modules, accessed_by=DictGetItemGuardAccessor('_modules')
| | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['rotary_emb']._modules, 21447360)
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].rope_type, accessed_by=DictGetItemGuardAccessor('rope_type')
| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['rotary_emb'].rope_type == 'llama3'
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['rotary_emb']._parameters, 21447360)
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].attention_scaling, accessed_by=DictGetItemGuardAccessor('attention_scaling')
| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['rotary_emb'].attention_scaling == 1.0
| | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__, accessed_by=GetAttrGuardAccessor(__class__)
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__.forward, accessed_by=GetAttrGuardAccessor(forward)
| | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__, accessed_by=GetAttrGuardAccessor(__closure__)
| | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[0], accessed_by=TupleGetItemGuardAccessor(0)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[0].cell_contents, accessed_by=GetAttrGuardAccessor(cell_contents)
| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[0].cell_contents, 21380576)
| | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[1], accessed_by=TupleGetItemGuardAccessor(1)
| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[1].cell_contents, accessed_by=GetAttrGuardAccessor(cell_contents)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[1].cell_contents.__code__, accessed_by=GetAttrGuardAccessor(__code__)
| | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[1].cell_contents.__code__, 124488323981264)
| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[1].cell_contents.__closure__, accessed_by=GetAttrGuardAccessor(__closure__)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[1].cell_contents.__closure__[0], accessed_by=TupleGetItemGuardAccessor(0)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[1].cell_contents.__closure__[0].cell_contents, accessed_by=GetAttrGuardAccessor(cell_contents)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[1].cell_contents.__closure__[0].cell_contents.__code__, accessed_by=GetAttrGuardAccessor(__code__)
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[1].cell_contents.__closure__[0].cell_contents.__code__, 820434896)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[1].cell_contents.__closure__[1], accessed_by=TupleGetItemGuardAccessor(1)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[1].cell_contents.__closure__[1].cell_contents, accessed_by=GetAttrGuardAccessor(cell_contents)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[1].cell_contents.__closure__[1].cell_contents.__code__, accessed_by=GetAttrGuardAccessor(__code__)
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[1].cell_contents.__closure__[1].cell_contents.__code__, 820395264)
| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[1].cell_contents.__closure__[2], accessed_by=TupleGetItemGuardAccessor(2)
| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[1].cell_contents.__closure__[2].cell_contents, accessed_by=GetAttrGuardAccessor(cell_contents)
| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[1].cell_contents.__closure__[2].cell_contents.__code__, accessed_by=GetAttrGuardAccessor(__code__)
| | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['rotary_emb'].__class__.forward.__closure__[1].cell_contents.__closure__[2].cell_contents.__code__, 783954912)
| | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['embed_tokens'], accessed_by=DictGetItemGuardAccessor('embed_tokens')
| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['embed_tokens'], 761156720)
| | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['embed_tokens'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['model']._modules['embed_tokens'].__dict__)
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['embed_tokens'].sparse, accessed_by=DictGetItemGuardAccessor('sparse')
| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['embed_tokens'].sparse, 21370824)
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['embed_tokens'].max_norm, accessed_by=DictGetItemGuardAccessor('max_norm')
| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['embed_tokens'].max_norm, 21458960)
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['embed_tokens'].norm_type, accessed_by=DictGetItemGuardAccessor('norm_type')
| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['embed_tokens'].norm_type == 2.0
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['embed_tokens']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._modules['embed_tokens']._parameters, 21447360)
| | | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['embed_tokens']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['model']._modules['embed_tokens']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[128256, 4096], stride=[4096, 1])
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['embed_tokens'].padding_idx, accessed_by=DictGetItemGuardAccessor('padding_idx')
| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['model']._modules['embed_tokens'].padding_idx == 128004
| | | | | | | | | +- GuardManager: source=L['self']._modules['model']._modules['embed_tokens'].scale_grad_by_freq, accessed_by=DictGetItemGuardAccessor('scale_grad_by_freq')
| | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['model']._modules['embed_tokens'].scale_grad_by_freq, 21370824)
| | | | | | +- GuardManager: source=L['self']._modules['model']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['model']._parameters, 21447360)
| | | | +- GuardManager: source=L['self']._modules['lm_head'], accessed_by=DictGetItemGuardAccessor('lm_head')
| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['lm_head'], 759894080)  
| | | | | +- GuardManager: source=L['self']._modules['lm_head'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['lm_head'].__dict__)
| | | | | | +- GuardManager: source=L['self']._modules['lm_head']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['lm_head']._parameters, 21447360)
| | | | | | | +- GuardManager: source=L['self']._modules['lm_head']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')
| | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['lm_head']._parameters['bias'], 21458960)
| | | | | | | +- GuardManager: source=L['self']._modules['lm_head']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')
| | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['lm_head']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView), torch.float16, device=0, requires_grad=True, size=[128256, 4096], stride=[4096, 1])
| | | +- GuardManager: source=L['self']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')
| | | | +- TYPE_MATCH: ___check_type_id(L['self']._parameters, 21447360)           
| +- GuardManager: source=L['kwargs'], accessed_by=FrameLocalsGuardAccessor(key='kwargs', framelocals_idx=10)
| | +- TYPE_MATCH: ___check_type_id(L['kwargs'], 21447360)                     
| | +- DICT_LENGTH: len(L['kwargs']) == 1                                       
| | +- GuardManager: source=L['kwargs']['return_dict'], accessed_by=DictGetItemGuardAccessor('return_dict')
| +- GuardManager: source=L['labels'], accessed_by=FrameLocalsGuardAccessor(key='labels', framelocals_idx=6)
| | +- ID_MATCH: ___check_obj_id(L['labels'], 21458960)                      
| +- GuardManager: source=L['input_ids'], accessed_by=FrameLocalsGuardAccessor(key='input_ids', framelocals_idx=1)
| | +- TENSOR_MATCH: check_tensor(L['input_ids'], Tensor, DispatchKeySet(CUDA, BackendSelect), torch.int64, device=0, requires_grad=False, size=[1, 1], stride=[1, 1])
| | +- NO_HASATTR: hasattr(L['input_ids'], '_dynamo_dynamic_indices') == False 
| | +- NO_TENSOR_ALIASING
| +- GuardManager: source=L['use_cache'], accessed_by=FrameLocalsGuardAccessor(key='use_cache', framelocals_idx=7)
| | +- ID_MATCH: ___check_obj_id(L['use_cache'], 21370792)                   
| +- GuardManager: source=L['position_ids'], accessed_by=FrameLocalsGuardAccessor(key='position_ids', framelocals_idx=3)
| | +- TENSOR_MATCH: check_tensor(L['position_ids'], Tensor, DispatchKeySet(CUDA, BackendSelect), torch.int64, device=0, requires_grad=False, size=[1, 1], stride=[1, 1])
| | +- NO_HASATTR: hasattr(L['position_ids'], '_dynamo_dynamic_indices') == False
| | +- NO_TENSOR_ALIASING
| +- GuardManager: source=L['inputs_embeds'], accessed_by=FrameLocalsGuardAccessor(key='inputs_embeds', framelocals_idx=5)
| | +- ID_MATCH: ___check_obj_id(L['inputs_embeds'], 21458960)               
| +- GuardManager: source=L['attention_mask'], accessed_by=FrameLocalsGuardAccessor(key='attention_mask', framelocals_idx=2)
| | +- TENSOR_MATCH: check_tensor(L['attention_mask'], Tensor, DispatchKeySet(CUDA, BackendSelect), torch.bool, device=0, requires_grad=False, size=[1, 1, 1, 2048], stride=[2048, 2048, 2048, 1])
| | +- NO_HASATTR: hasattr(L['attention_mask'], '_dynamo_dynamic_indices') == False
| | +- NO_TENSOR_ALIASING
| +- GuardManager: source=L['cache_position'], accessed_by=FrameLocalsGuardAccessor(key='cache_position', framelocals_idx=8)
| | +- TENSOR_MATCH: check_tensor(L['cache_position'], Tensor, DispatchKeySet(CUDA, BackendSelect), torch.int64, device=0, requires_grad=False, size=[1], stride=[1])
| | +- NO_HASATTR: hasattr(L['cache_position'], '_dynamo_dynamic_indices') == False
| | +- NO_TENSOR_ALIASING
| +- GuardManager: source=L['logits_to_keep'], accessed_by=FrameLocalsGuardAccessor(key='logits_to_keep', framelocals_idx=9)
| | +- EQUALS_MATCH: L['logits_to_keep'] == 1                                    
| +- GuardManager: source=L['past_key_values'], accessed_by=FrameLocalsGuardAccessor(key='past_key_values', framelocals_idx=4)
| | +- TYPE_MATCH: ___check_type_id(L['past_key_values'], 820345104)           
| | +- GuardManager: source=L['past_key_values'].layers, accessed_by=GetAttrGuardAccessor(layers)
| | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers, 21438672)     
| | | +- LENGTH_CHECK: len(L['past_key_values'].layers) == 32                      
| | | +- GuardManager: source=L['past_key_values'].layers[0], accessed_by=ListGetItemGuardAccessor(0)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[0], 820340400) 
| | | | +- GuardManager: source=L['past_key_values'].layers[0].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[0].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[0].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[0].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[0].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[0].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[0].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[0].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[1], accessed_by=ListGetItemGuardAccessor(1)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[1], 820340400) 
| | | | +- GuardManager: source=L['past_key_values'].layers[1].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[1].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[1].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[1].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[1].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[1].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[1].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[1].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[2], accessed_by=ListGetItemGuardAccessor(2)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[2], 820340400) 
| | | | +- GuardManager: source=L['past_key_values'].layers[2].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[2].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[2].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[2].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[2].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[2].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[2].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[2].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[3], accessed_by=ListGetItemGuardAccessor(3)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[3], 820340400) 
| | | | +- GuardManager: source=L['past_key_values'].layers[3].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[3].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[3].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[3].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[3].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[3].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[3].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[3].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[4], accessed_by=ListGetItemGuardAccessor(4)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[4], 820340400) 
| | | | +- GuardManager: source=L['past_key_values'].layers[4].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[4].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[4].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[4].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[4].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[4].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[4].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[4].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[5], accessed_by=ListGetItemGuardAccessor(5)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[5], 820340400) 
| | | | +- GuardManager: source=L['past_key_values'].layers[5].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[5].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[5].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[5].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[5].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[5].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[5].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[5].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[6], accessed_by=ListGetItemGuardAccessor(6)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[6], 820340400) 
| | | | +- GuardManager: source=L['past_key_values'].layers[6].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[6].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[6].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[6].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[6].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[6].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[6].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[6].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[7], accessed_by=ListGetItemGuardAccessor(7)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[7], 820340400) 
| | | | +- GuardManager: source=L['past_key_values'].layers[7].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[7].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[7].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[7].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[7].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[7].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[7].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[7].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[8], accessed_by=ListGetItemGuardAccessor(8)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[8], 820340400) 
| | | | +- GuardManager: source=L['past_key_values'].layers[8].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[8].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[8].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[8].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[8].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[8].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[8].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[8].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[9], accessed_by=ListGetItemGuardAccessor(9)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[9], 820340400) 
| | | | +- GuardManager: source=L['past_key_values'].layers[9].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[9].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[9].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[9].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[9].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[9].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[9].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[9].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[10], accessed_by=ListGetItemGuardAccessor(10)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[10], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[10].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[10].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[10].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[10].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[10].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[10].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[10].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[10].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[11], accessed_by=ListGetItemGuardAccessor(11)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[11], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[11].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[11].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[11].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[11].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[11].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[11].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[11].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[11].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[12], accessed_by=ListGetItemGuardAccessor(12)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[12], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[12].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[12].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[12].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[12].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[12].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[12].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[12].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[12].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[13], accessed_by=ListGetItemGuardAccessor(13)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[13], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[13].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[13].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[13].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[13].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[13].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[13].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[13].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[13].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[14], accessed_by=ListGetItemGuardAccessor(14)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[14], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[14].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[14].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[14].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[14].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[14].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[14].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[14].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[14].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[15], accessed_by=ListGetItemGuardAccessor(15)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[15], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[15].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[15].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[15].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[15].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[15].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[15].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[15].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[15].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[16], accessed_by=ListGetItemGuardAccessor(16)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[16], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[16].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[16].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[16].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[16].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[16].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[16].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[16].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[16].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[17], accessed_by=ListGetItemGuardAccessor(17)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[17], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[17].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[17].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[17].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[17].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[17].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[17].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[17].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[17].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[18], accessed_by=ListGetItemGuardAccessor(18)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[18], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[18].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[18].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[18].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[18].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[18].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[18].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[18].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[18].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[19], accessed_by=ListGetItemGuardAccessor(19)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[19], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[19].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[19].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[19].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[19].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[19].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[19].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[19].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[19].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[20], accessed_by=ListGetItemGuardAccessor(20)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[20], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[20].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[20].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[20].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[20].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[20].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[20].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[20].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[20].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[21], accessed_by=ListGetItemGuardAccessor(21)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[21], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[21].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[21].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[21].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[21].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[21].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[21].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[21].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[21].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[22], accessed_by=ListGetItemGuardAccessor(22)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[22], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[22].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[22].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[22].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[22].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[22].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[22].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[22].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[22].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[23], accessed_by=ListGetItemGuardAccessor(23)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[23], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[23].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[23].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[23].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[23].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[23].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[23].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[23].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[23].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[24], accessed_by=ListGetItemGuardAccessor(24)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[24], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[24].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[24].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[24].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[24].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[24].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[24].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[24].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[24].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[25], accessed_by=ListGetItemGuardAccessor(25)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[25], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[25].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[25].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[25].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[25].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[25].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[25].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[25].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[25].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[26], accessed_by=ListGetItemGuardAccessor(26)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[26], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[26].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[26].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[26].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[26].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[26].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[26].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[26].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[26].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[27], accessed_by=ListGetItemGuardAccessor(27)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[27], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[27].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[27].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[27].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[27].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[27].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[27].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[27].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[27].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[28], accessed_by=ListGetItemGuardAccessor(28)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[28], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[28].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[28].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[28].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[28].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[28].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[28].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[28].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[28].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[29], accessed_by=ListGetItemGuardAccessor(29)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[29], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[29].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[29].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[29].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[29].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[29].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[29].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[29].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[29].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[30], accessed_by=ListGetItemGuardAccessor(30)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[30], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[30].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[30].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[30].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[30].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[30].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[30].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[30].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[30].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | +- GuardManager: source=L['past_key_values'].layers[31], accessed_by=ListGetItemGuardAccessor(31)
| | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].layers[31], 820340400)
| | | | +- GuardManager: source=L['past_key_values'].layers[31].keys, accessed_by=GetAttrGuardAccessor(keys)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[31].keys, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[31].keys, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[31].keys.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | | | +- GuardManager: source=L['past_key_values'].layers[31].values, accessed_by=GetAttrGuardAccessor(values)
| | | | | +- TENSOR_MATCH: check_tensor(L['past_key_values'].layers[31].values, Tensor, DispatchKeySet(CUDA, BackendSelect), torch.float16, device=0, requires_grad=False, size=[1, 8, 2048, 128], stride=[2097152, 262144, 128, 1])
| | | | | +- NO_HASATTR: hasattr(L['past_key_values'].layers[31].values, '_dynamo_dynamic_indices') == False
| | | | | +- NO_TENSOR_ALIASING
| | | | | +- GuardManager: source=L['past_key_values'].layers[31].values.index_copy_, accessed_by=GetAttrGuardAccessor(index_copy_)
| | +- GuardManager: source=L['past_key_values'].offloading, accessed_by=GetAttrGuardAccessor(offloading)
| | | +- ID_MATCH: ___check_obj_id(L['past_key_values'].offloading, 21370824)  
| | +- GuardManager: source=L['past_key_values'].layer_class_to_replicate, accessed_by=GetAttrGuardAccessor(layer_class_to_replicate)
| | | +- ID_MATCH: ___check_obj_id(L['past_key_values'].layer_class_to_replicate, 21458960)
| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
| | +- GuardManager: source=G['rotate_half'], accessed_by=DictGetItemGuardAccessor('rotate_half')
| | | +- GuardManager: source=G['rotate_half'].__code__, accessed_by=GetAttrGuardAccessor(__code__)
| | | | +- ID_MATCH: ___check_obj_id(G['rotate_half'].__code__, 124488325487888) 
| | +- GuardManager: source=G['create_causal_mask'], accessed_by=DictGetItemGuardAccessor('create_causal_mask')
| | | +- GuardManager: source=G['create_causal_mask'].__code__, accessed_by=GetAttrGuardAccessor(__code__)
| | | | +- ID_MATCH: ___check_obj_id(G['create_causal_mask'].__code__, 820052032)
| | +- GuardManager: source=G['apply_rotary_pos_emb'], accessed_by=DictGetItemGuardAccessor('apply_rotary_pos_emb')
| | | +- GuardManager: source=G['apply_rotary_pos_emb'].__code__, accessed_by=GetAttrGuardAccessor(__code__)
| | | | +- ID_MATCH: ___check_obj_id(G['apply_rotary_pos_emb'].__code__, 124488325488240)
| | | +- GuardManager: source=G['apply_rotary_pos_emb'], accessed_by=FuncDefaultsGuardAccessor
| | | | +- GuardManager: source=G['apply_rotary_pos_emb'].__defaults__[1], accessed_by=GetItemGuardAccessor(1)
| | | | | +- EQUALS_MATCH: G['apply_rotary_pos_emb'].__defaults__[1] == 1              
| | +- GuardManager: source=G['CausalLMOutputWithPast'], accessed_by=DictGetItemGuardAccessor('CausalLMOutputWithPast')
| | | +- ID_MATCH: ___check_obj_id(G['CausalLMOutputWithPast'], 820381440)     
| | +- GuardManager: source=G['ALL_ATTENTION_FUNCTIONS'], accessed_by=DictGetItemGuardAccessor('ALL_ATTENTION_FUNCTIONS')
| | | +- TYPE_MATCH: ___check_type_id(G['ALL_ATTENTION_FUNCTIONS'], 879141376)   
| | | +- GuardManager: source=G['ALL_ATTENTION_FUNCTIONS']._local_mapping, accessed_by=GetAttrGuardAccessor(_local_mapping)
| | | | +- TYPE_MATCH: ___check_type_id(G['ALL_ATTENTION_FUNCTIONS']._local_mapping, 21447360)
| | | | +- DICT_LENGTH: not G['ALL_ATTENTION_FUNCTIONS']._local_mapping             
| | | +- GuardManager: source=G['ALL_ATTENTION_FUNCTIONS']._global_mapping, accessed_by=GetAttrGuardAccessor(_global_mapping)
| | | | +- TYPE_MATCH: ___check_type_id(G['ALL_ATTENTION_FUNCTIONS']._global_mapping, 21447360)
| | | | +- GuardManager: source=G['ALL_ATTENTION_FUNCTIONS']._global_mapping['sdpa'], accessed_by=DictGetItemGuardAccessor('sdpa')
| | | | | +- GuardManager: source=G['ALL_ATTENTION_FUNCTIONS']._global_mapping['sdpa'].__code__, accessed_by=GetAttrGuardAccessor(__code__)
| | | | | | +- ID_MATCH: ___check_obj_id(G['ALL_ATTENTION_FUNCTIONS']._global_mapping['sdpa'].__code__, 869234336)
| | | | | +- GuardManager: source=G['ALL_ATTENTION_FUNCTIONS']._global_mapping['sdpa'], accessed_by=FuncDefaultsGuardAccessor
| | | | | | +- GuardManager: source=G['ALL_ATTENTION_FUNCTIONS']._global_mapping['sdpa'].__defaults__[2], accessed_by=GetItemGuardAccessor(2)
| | | | | | | +- ID_MATCH: ___check_obj_id(G['ALL_ATTENTION_FUNCTIONS']._global_mapping['sdpa'].__defaults__[2], 21458960)
| | +- GuardManager: source=G['BaseModelOutputWithPast'], accessed_by=DictGetItemGuardAccessor('BaseModelOutputWithPast')
| | | +- ID_MATCH: ___check_obj_id(G['BaseModelOutputWithPast'], 820356928)    
| | +- GuardManager: source=G['eager_attention_forward'], accessed_by=DictGetItemGuardAccessor('eager_attention_forward')
| | | +- GuardManager: source=G['eager_attention_forward'].__code__, accessed_by=GetAttrGuardAccessor(__code__)
| | | | +- ID_MATCH: ___check_obj_id(G['eager_attention_forward'].__code__, 820137712)
| | +- GuardManager: source=G['__builtins_dict___10'], accessed_by=DictGetItemGuardAccessor('__builtins_dict___10')
| | | +- GuardManager: source=G['__builtins_dict___10']['all'], accessed_by=DictGetItemGuardAccessor('all')
| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['all'], 124493385843632)
| | | +- GuardManager: source=G['__builtins_dict___10']['int'], accessed_by=DictGetItemGuardAccessor('int')
| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['int'], 21441248) 
| | | +- GuardManager: source=G['__builtins_dict___10']['len'], accessed_by=DictGetItemGuardAccessor('len')
| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['len'], 124493385845632)
| | | +- GuardManager: source=G['__builtins_dict___10']['set'], accessed_by=DictGetItemGuardAccessor('set')
| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['set'], 21464880) 
| | | +- GuardManager: source=G['__builtins_dict___10']['str'], accessed_by=DictGetItemGuardAccessor('str')
| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['str'], 21487744) 
| | | +- GuardManager: source=G['__builtins_dict___10']['bool'], accessed_by=DictGetItemGuardAccessor('bool')
| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['bool'], 21370376)
| | | +- GuardManager: source=G['__builtins_dict___10']['iter'], accessed_by=DictGetItemGuardAccessor('iter')
| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['iter'], 124493385845472)
| | | +- GuardManager: source=G['__builtins_dict___10']['list'], accessed_by=DictGetItemGuardAccessor('list')
| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['list'], 21438672)
| | | +- GuardManager: source=G['__builtins_dict___10']['slice'], accessed_by=DictGetItemGuardAccessor('slice')
| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['slice'], 21466048)
| | | +- GuardManager: source=G['__builtins_dict___10']['super'], accessed_by=DictGetItemGuardAccessor('super')
| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['super'], 21475840)
| | | +- GuardManager: source=G['__builtins_dict___10']['getattr'], accessed_by=DictGetItemGuardAccessor('getattr')
| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['getattr'], 124493385844752)
| | | +- GuardManager: source=G['__builtins_dict___10']['hasattr'], accessed_by=DictGetItemGuardAccessor('hasattr')
| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['hasattr'], 124493385844912)
| | | +- GuardManager: source=G['__builtins_dict___10']['enumerate'], accessed_by=DictGetItemGuardAccessor('enumerate')
| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['enumerate'], 21391040)
| | | +- GuardManager: source=G['__builtins_dict___10']['isinstance'], accessed_by=DictGetItemGuardAccessor('isinstance')
| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___10']['isinstance'], 124493385845312)
| | +- GuardManager: source=G['__import_torch_dot__dynamo_dot_polyfills'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot__dynamo_dot_polyfills')
| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot__dynamo_dot_polyfills'], 124488478513872)
| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot_nn_dot_modules_dot_linear')
| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'], 124488690976464)
| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F, accessed_by=GetAttrGuardAccessor(F)
| | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_sparse'].F is G['__import_torch_dot_nn_dot_modules_dot_linear'].F
| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot_nn_dot_modules_dot_module')
| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'], 124488807000368)
| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].Module, accessed_by=GetAttrGuardAccessor(Module)
| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].Module, 757579120)
| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].OrderedDict, accessed_by=GetAttrGuardAccessor(OrderedDict)
| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].OrderedDict, 21451216)
| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_hooks)
| | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, 21451216)
| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_hooks)
| | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, 21451216)
| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_pre_hooks)
| | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, 21451216)
| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_pre_hooks)
| | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, 21451216)
| | | +- DictGuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_module_registration_hooks, accessed_by=GetAttrGuardAccessor(_global_module_registration_hooks)
| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].torch, accessed_by=GetAttrGuardAccessor(torch)
| | | | +- OBJECT_ALIASING: G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch is G['__import_torch_dot_nn_dot_modules_dot_module'].torch
| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_sparse'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot_nn_dot_modules_dot_sparse')
| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_sparse'], 124488685521152)
| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_sparse'].F, accessed_by=GetAttrGuardAccessor(F)
| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_sparse'].F, 124488690976624)
| | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_sparse'].F is G['__import_torch_dot_nn_dot_modules_dot_linear'].F
| | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_sparse'].F is G['__import_torch_dot_nn_dot_modules_dot_activation'].F
| | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_sparse'].F is G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.nn.functional
| | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_sparse'].F.silu, accessed_by=GetAttrGuardAccessor(silu)
| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_sparse'].F.silu, 124488688030944)
| | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_sparse'].F.linear, accessed_by=GetAttrGuardAccessor(linear)
| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_sparse'].F.linear, 124489089967792)
| | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_sparse'].F.embedding, accessed_by=GetAttrGuardAccessor(embedding)
| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_sparse'].F.embedding, 124488688031584)
| | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_sparse'].F.scaled_dot_product_attention, accessed_by=GetAttrGuardAccessor(scaled_dot_product_attention)
| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_sparse'].F.scaled_dot_product_attention, 124489089970272)
| | +- GuardManager: source=G['__import_transformers_dot_configuration_utils'], accessed_by=DictGetItemGuardAccessor('__import_transformers_dot_configuration_utils')
| | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_configuration_utils'], 124488646223056)
| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_container'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot_nn_dot_modules_dot_container')
| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_container'], 124488686640144)
| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_container'].container_abcs, accessed_by=GetAttrGuardAccessor(container_abcs)
| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_container'].container_abcs, 124493383361824)
| | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_container'].container_abcs.Iterable, accessed_by=GetAttrGuardAccessor(Iterable)
| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_container'].container_abcs.Iterable, 676396416)
| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_activation'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot_nn_dot_modules_dot_activation')
| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_activation'], 124488688528624)
| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_activation'].F, accessed_by=GetAttrGuardAccessor(F)
| | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_sparse'].F is G['__import_torch_dot_nn_dot_modules_dot_activation'].F
| | +- GuardManager: source=G['__import_torch_dot__dynamo_dot_polyfills_dot_builtins'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot__dynamo_dot_polyfills_dot_builtins')
| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot__dynamo_dot_polyfills_dot_builtins'], 124488470636528)
| | +- GuardManager: source=G['__import_low_bit_inference_dot_utils_dot_generic_utils'], accessed_by=DictGetItemGuardAccessor('__import_low_bit_inference_dot_utils_dot_generic_utils')
| | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_utils_dot_generic_utils'], 124488324197456)
| | | +- GuardManager: source=G['__import_low_bit_inference_dot_utils_dot_generic_utils'].fields, accessed_by=GetAttrGuardAccessor(fields)
| | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_utils_dot_generic_utils'].fields, 124489024432352)
| | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_masking_optim'], accessed_by=DictGetItemGuardAccessor('__import_low_bit_inference_dot_optims_dot_masking_optim')
| | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_masking_optim'], 124488324060464)
| | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch, accessed_by=GetAttrGuardAccessor(torch)
| | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch, 124493383935424)
| | | | +- OBJECT_ALIASING: G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch is G['torch']
| | | | +- OBJECT_ALIASING: G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch is G['__import_torch_dot_nn_dot_modules_dot_module'].torch
| | | | +- OBJECT_ALIASING: G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch is G['__import_transformers_dot_integrations_dot_sdpa_attention'].torch
| | | | +- OBJECT_ALIASING: G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch is G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].torch
| | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch._C, accessed_by=GetAttrGuardAccessor(_C)
| | | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch._C, 124493379232416)
| | | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch._C._log_api_usage_once, accessed_by=GetAttrGuardAccessor(_log_api_usage_once)
| | | | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch._C._log_api_usage_once, 124489090158720)
| | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.nn, accessed_by=GetAttrGuardAccessor(nn)
| | | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.nn, 124488806994768)
| | | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.nn.functional, accessed_by=GetAttrGuardAccessor(functional)
| | | | | | +- OBJECT_ALIASING: G['__import_torch_dot_nn_dot_modules_dot_sparse'].F is G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.nn.functional
| | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.cat, accessed_by=GetAttrGuardAccessor(cat)
| | | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.cat, 124493379612608)
| | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.jit, accessed_by=GetAttrGuardAccessor(jit)
| | | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.jit, 124488685411184)
| | | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.jit.is_tracing, accessed_by=GetAttrGuardAccessor(is_tracing)
| | | | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.jit.is_tracing, 124488684107744)
| | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.empty, accessed_by=GetAttrGuardAccessor(empty)
| | | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.empty, 124493379614528)
| | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.Tensor, accessed_by=GetAttrGuardAccessor(Tensor)
| | | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.Tensor, 752318480)
| | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.float32, accessed_by=GetAttrGuardAccessor(float32)
| | | | | +- EQUALS_MATCH: G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.float32 == torch.float32
| | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.autocast, accessed_by=GetAttrGuardAccessor(autocast)
| | | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch.autocast, 750651392)
| | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_masking_optim'].BlockMask, accessed_by=GetAttrGuardAccessor(BlockMask)
| | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_masking_optim'].BlockMask, 820277664)
| | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_masking_optim']._preprocess_mask_arguments, accessed_by=GetAttrGuardAccessor(_preprocess_mask_arguments)
| | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_masking_optim']._preprocess_mask_arguments.__code__, accessed_by=GetAttrGuardAccessor(__code__)
| | | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_masking_optim']._preprocess_mask_arguments.__code__, 819970880)
| | +- GuardManager: source=G['__import_transformers_dot_integrations_dot_sdpa_attention'], accessed_by=DictGetItemGuardAccessor('__import_transformers_dot_integrations_dot_sdpa_attention')
| | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_integrations_dot_sdpa_attention'], 124487404262144)
| | | +- GuardManager: source=G['__import_transformers_dot_integrations_dot_sdpa_attention'].repeat_kv, accessed_by=GetAttrGuardAccessor(repeat_kv)
| | | | +- GuardManager: source=G['__import_transformers_dot_integrations_dot_sdpa_attention'].repeat_kv.__code__, accessed_by=GetAttrGuardAccessor(__code__)
| | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_integrations_dot_sdpa_attention'].repeat_kv.__code__, 124488302438880)
| | | +- GuardManager: source=G['__import_transformers_dot_integrations_dot_sdpa_attention'].torch, accessed_by=GetAttrGuardAccessor(torch)
| | | | +- OBJECT_ALIASING: G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch is G['__import_transformers_dot_integrations_dot_sdpa_attention'].torch
| | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'], accessed_by=DictGetItemGuardAccessor('__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm')
| | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'], 124488323922528)
| | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].triton, accessed_by=GetAttrGuardAccessor(triton)
| | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].triton, 124488614461040)
| | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].triton.next_power_of_2, accessed_by=GetAttrGuardAccessor(next_power_of_2)
| | | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].triton.next_power_of_2.__code__, accessed_by=GetAttrGuardAccessor(__code__)
| | | | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].triton.next_power_of_2.__code__, 124488615734576)
| | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].rms_norm_forward, accessed_by=GetAttrGuardAccessor(rms_norm_forward)
| | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].rms_norm_forward.__code__, accessed_by=GetAttrGuardAccessor(__code__)
| | | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].rms_norm_forward.__code__, 783316704)
| | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].calculate_settings, accessed_by=GetAttrGuardAccessor(calculate_settings)
| | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].calculate_settings.__code__, accessed_by=GetAttrGuardAccessor(__code__)
| | | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].calculate_settings.__code__, 124488323974640)
| | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm']._CASTING_MODE_GEMMA, accessed_by=GetAttrGuardAccessor(_CASTING_MODE_GEMMA)
| | | | +- TYPE_MATCH: ___check_type_id(G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm']._CASTING_MODE_GEMMA, 798281360)
| | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm']._CASTING_MODE_LLAMA, accessed_by=GetAttrGuardAccessor(_CASTING_MODE_LLAMA)
| | | | +- TYPE_MATCH: ___check_type_id(G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm']._CASTING_MODE_LLAMA, 798281360)
| | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm']._CASTING_MODE_LLAMA.value, accessed_by=GetAttrGuardAccessor(value)
| | | | | +- EQUALS_MATCH: G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm']._CASTING_MODE_LLAMA.value == 0
| | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].LigerRMSNormFunction, accessed_by=GetAttrGuardAccessor(LigerRMSNormFunction)
| | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].LigerRMSNormFunction, 778956944)
| | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].LigerRMSNormFunction.forward, accessed_by=GetAttrGuardAccessor(forward)
| | | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].LigerRMSNormFunction.forward.__closure__, accessed_by=GetAttrGuardAccessor(__closure__)
| | | | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].LigerRMSNormFunction.forward.__closure__[0], accessed_by=TupleGetItemGuardAccessor(0)
| | | | | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].LigerRMSNormFunction.forward.__closure__[0].cell_contents, accessed_by=GetAttrGuardAccessor(cell_contents)
| | | | | | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].LigerRMSNormFunction.forward.__closure__[0].cell_contents.__code__, accessed_by=GetAttrGuardAccessor(__code__)
| | | | | | | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].LigerRMSNormFunction.forward.__closure__[0].cell_contents.__code__, 124488323975008)
| | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm']._str_to_casting_mode, accessed_by=GetAttrGuardAccessor(_str_to_casting_mode)
| | | | +- TYPE_MATCH: ___check_type_id(G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm']._str_to_casting_mode, 21447360)
| | | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm']._str_to_casting_mode['llama'], accessed_by=DictGetItemGuardAccessor('llama')
| | | | | +- EQUALS_MATCH: G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm']._str_to_casting_mode['llama'] == 0
| | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm']._rms_norm_forward_kernel, accessed_by=GetAttrGuardAccessor(_rms_norm_forward_kernel)
| | | | +- ID_MATCH: ___check_obj_id(G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm']._rms_norm_forward_kernel, 124488324025856)
| | | +- GuardManager: source=G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].torch, accessed_by=GetAttrGuardAccessor(torch)
| | | | +- OBJECT_ALIASING: G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch is G['__import_low_bit_inference_dot_optims_dot_rms_norm_kernels_dot_liger_rms_norm'].torch
| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor('torch')
| | | +- OBJECT_ALIASING: G['__import_low_bit_inference_dot_optims_dot_masking_optim'].torch is G['torch']

Guard latency = 340.42 us