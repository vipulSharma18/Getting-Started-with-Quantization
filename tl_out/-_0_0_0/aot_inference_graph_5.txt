class <lambda>(torch.nn.Module):
    def forward(self, arg0_1: "Sym(s0)", arg1_1: "i64[1, s0][s0, 1]cuda:0", arg2_1: "f16[128256, 4096][4096, 1]cuda:0", arg3_1: "i64[s0][1]cuda:0", arg4_1: "i64[1, s0][s0, 1]cuda:0", arg5_1: "Sym(s1)", arg6_1: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0", arg7_1: "f32[64][1]cuda:0", arg8_1: "f16[4096][1]cuda:0", arg9_1: "f16[4096, 4096][4096, 1]cuda:0", arg10_1: "f16[1024, 4096][4096, 1]cuda:0", arg11_1: "f16[1024, 4096][4096, 1]cuda:0", arg12_1: "Sym(s1)", arg13_1: "f16[4096, 4096][4096, 1]cuda:0", arg14_1: "f16[4096][1]cuda:0", arg15_1: "f16[14336, 4096][4096, 1]cuda:0", arg16_1: "f16[14336, 4096][4096, 1]cuda:0", arg17_1: "f16[4096, 14336][14336, 1]cuda:0", arg18_1: "f16[4096][1]cuda:0", arg19_1: "f16[4096, 4096][4096, 1]cuda:0", arg20_1: "f16[1024, 4096][4096, 1]cuda:0", arg21_1: "f16[1024, 4096][4096, 1]cuda:0", arg22_1: "Sym(s1)", arg23_1: "f16[4096, 4096][4096, 1]cuda:0", arg24_1: "f16[4096][1]cuda:0", arg25_1: "f16[14336, 4096][4096, 1]cuda:0", arg26_1: "f16[14336, 4096][4096, 1]cuda:0", arg27_1: "f16[4096, 14336][14336, 1]cuda:0", arg28_1: "f16[4096][1]cuda:0", arg29_1: "f16[4096, 4096][4096, 1]cuda:0", arg30_1: "f16[1024, 4096][4096, 1]cuda:0", arg31_1: "f16[1024, 4096][4096, 1]cuda:0", arg32_1: "Sym(s1)", arg33_1: "f16[4096, 4096][4096, 1]cuda:0", arg34_1: "f16[4096][1]cuda:0", arg35_1: "f16[14336, 4096][4096, 1]cuda:0", arg36_1: "f16[14336, 4096][4096, 1]cuda:0", arg37_1: "f16[4096, 14336][14336, 1]cuda:0", arg38_1: "f16[4096][1]cuda:0", arg39_1: "f16[4096, 4096][4096, 1]cuda:0", arg40_1: "f16[1024, 4096][4096, 1]cuda:0", arg41_1: "f16[1024, 4096][4096, 1]cuda:0", arg42_1: "Sym(s1)", arg43_1: "f16[4096, 4096][4096, 1]cuda:0", arg44_1: "f16[4096][1]cuda:0", arg45_1: "f16[14336, 4096][4096, 1]cuda:0", arg46_1: "f16[14336, 4096][4096, 1]cuda:0", arg47_1: "f16[4096, 14336][14336, 1]cuda:0", arg48_1: "f16[4096][1]cuda:0", arg49_1: "f16[4096, 4096][4096, 1]cuda:0", arg50_1: "f16[1024, 4096][4096, 1]cuda:0", arg51_1: "f16[1024, 4096][4096, 1]cuda:0", arg52_1: "Sym(s1)", arg53_1: "f16[4096, 4096][4096, 1]cuda:0", arg54_1: "f16[4096][1]cuda:0", arg55_1: "f16[14336, 4096][4096, 1]cuda:0", arg56_1: "f16[14336, 4096][4096, 1]cuda:0", arg57_1: "f16[4096, 14336][14336, 1]cuda:0", arg58_1: "f16[4096][1]cuda:0", arg59_1: "f16[4096, 4096][4096, 1]cuda:0", arg60_1: "f16[1024, 4096][4096, 1]cuda:0", arg61_1: "f16[1024, 4096][4096, 1]cuda:0", arg62_1: "Sym(s1)", arg63_1: "f16[4096, 4096][4096, 1]cuda:0", arg64_1: "f16[4096][1]cuda:0", arg65_1: "f16[14336, 4096][4096, 1]cuda:0", arg66_1: "f16[14336, 4096][4096, 1]cuda:0", arg67_1: "f16[4096, 14336][14336, 1]cuda:0", arg68_1: "f16[4096][1]cuda:0", arg69_1: "f16[4096, 4096][4096, 1]cuda:0", arg70_1: "f16[1024, 4096][4096, 1]cuda:0", arg71_1: "f16[1024, 4096][4096, 1]cuda:0", arg72_1: "Sym(s1)", arg73_1: "f16[4096, 4096][4096, 1]cuda:0", arg74_1: "f16[4096][1]cuda:0", arg75_1: "f16[14336, 4096][4096, 1]cuda:0", arg76_1: "f16[14336, 4096][4096, 1]cuda:0", arg77_1: "f16[4096, 14336][14336, 1]cuda:0", arg78_1: "f16[4096][1]cuda:0", arg79_1: "f16[4096, 4096][4096, 1]cuda:0", arg80_1: "f16[1024, 4096][4096, 1]cuda:0", arg81_1: "f16[1024, 4096][4096, 1]cuda:0", arg82_1: "Sym(s1)", arg83_1: "f16[4096, 4096][4096, 1]cuda:0", arg84_1: "f16[4096][1]cuda:0", arg85_1: "f16[14336, 4096][4096, 1]cuda:0", arg86_1: "f16[14336, 4096][4096, 1]cuda:0", arg87_1: "f16[4096, 14336][14336, 1]cuda:0", arg88_1: "f16[4096][1]cuda:0", arg89_1: "f16[4096, 4096][4096, 1]cuda:0", arg90_1: "f16[1024, 4096][4096, 1]cuda:0", arg91_1: "f16[1024, 4096][4096, 1]cuda:0", arg92_1: "Sym(s1)", arg93_1: "f16[4096, 4096][4096, 1]cuda:0", arg94_1: "f16[4096][1]cuda:0", arg95_1: "f16[14336, 4096][4096, 1]cuda:0", arg96_1: "f16[14336, 4096][4096, 1]cuda:0", arg97_1: "f16[4096, 14336][14336, 1]cuda:0", arg98_1: "f16[4096][1]cuda:0", arg99_1: "f16[4096, 4096][4096, 1]cuda:0", arg100_1: "f16[1024, 4096][4096, 1]cuda:0", arg101_1: "f16[1024, 4096][4096, 1]cuda:0", arg102_1: "Sym(s1)", arg103_1: "f16[4096, 4096][4096, 1]cuda:0", arg104_1: "f16[4096][1]cuda:0", arg105_1: "f16[14336, 4096][4096, 1]cuda:0", arg106_1: "f16[14336, 4096][4096, 1]cuda:0", arg107_1: "f16[4096, 14336][14336, 1]cuda:0", arg108_1: "f16[4096][1]cuda:0", arg109_1: "f16[4096, 4096][4096, 1]cuda:0", arg110_1: "f16[1024, 4096][4096, 1]cuda:0", arg111_1: "f16[1024, 4096][4096, 1]cuda:0", arg112_1: "Sym(s1)", arg113_1: "f16[4096, 4096][4096, 1]cuda:0", arg114_1: "f16[4096][1]cuda:0", arg115_1: "f16[14336, 4096][4096, 1]cuda:0", arg116_1: "f16[14336, 4096][4096, 1]cuda:0", arg117_1: "f16[4096, 14336][14336, 1]cuda:0", arg118_1: "f16[4096][1]cuda:0", arg119_1: "f16[4096, 4096][4096, 1]cuda:0", arg120_1: "f16[1024, 4096][4096, 1]cuda:0", arg121_1: "f16[1024, 4096][4096, 1]cuda:0", arg122_1: "Sym(s1)", arg123_1: "f16[4096, 4096][4096, 1]cuda:0", arg124_1: "f16[4096][1]cuda:0", arg125_1: "f16[14336, 4096][4096, 1]cuda:0", arg126_1: "f16[14336, 4096][4096, 1]cuda:0", arg127_1: "f16[4096, 14336][14336, 1]cuda:0", arg128_1: "f16[4096][1]cuda:0", arg129_1: "f16[4096, 4096][4096, 1]cuda:0", arg130_1: "f16[1024, 4096][4096, 1]cuda:0", arg131_1: "f16[1024, 4096][4096, 1]cuda:0", arg132_1: "Sym(s1)", arg133_1: "f16[4096, 4096][4096, 1]cuda:0", arg134_1: "f16[4096][1]cuda:0", arg135_1: "f16[14336, 4096][4096, 1]cuda:0", arg136_1: "f16[14336, 4096][4096, 1]cuda:0", arg137_1: "f16[4096, 14336][14336, 1]cuda:0", arg138_1: "f16[4096][1]cuda:0", arg139_1: "f16[4096, 4096][4096, 1]cuda:0", arg140_1: "f16[1024, 4096][4096, 1]cuda:0", arg141_1: "f16[1024, 4096][4096, 1]cuda:0", arg142_1: "Sym(s1)", arg143_1: "f16[4096, 4096][4096, 1]cuda:0", arg144_1: "f16[4096][1]cuda:0", arg145_1: "f16[14336, 4096][4096, 1]cuda:0", arg146_1: "f16[14336, 4096][4096, 1]cuda:0", arg147_1: "f16[4096, 14336][14336, 1]cuda:0", arg148_1: "f16[4096][1]cuda:0", arg149_1: "f16[4096, 4096][4096, 1]cuda:0", arg150_1: "f16[1024, 4096][4096, 1]cuda:0", arg151_1: "f16[1024, 4096][4096, 1]cuda:0", arg152_1: "Sym(s1)", arg153_1: "f16[4096, 4096][4096, 1]cuda:0", arg154_1: "f16[4096][1]cuda:0", arg155_1: "f16[14336, 4096][4096, 1]cuda:0", arg156_1: "f16[14336, 4096][4096, 1]cuda:0", arg157_1: "f16[4096, 14336][14336, 1]cuda:0", arg158_1: "f16[4096][1]cuda:0", arg159_1: "f16[4096, 4096][4096, 1]cuda:0", arg160_1: "f16[1024, 4096][4096, 1]cuda:0", arg161_1: "f16[1024, 4096][4096, 1]cuda:0", arg162_1: "Sym(s1)", arg163_1: "f16[4096, 4096][4096, 1]cuda:0", arg164_1: "f16[4096][1]cuda:0", arg165_1: "f16[14336, 4096][4096, 1]cuda:0", arg166_1: "f16[14336, 4096][4096, 1]cuda:0", arg167_1: "f16[4096, 14336][14336, 1]cuda:0", arg168_1: "f16[4096][1]cuda:0", arg169_1: "f16[4096, 4096][4096, 1]cuda:0", arg170_1: "f16[1024, 4096][4096, 1]cuda:0", arg171_1: "f16[1024, 4096][4096, 1]cuda:0", arg172_1: "Sym(s1)", arg173_1: "f16[4096, 4096][4096, 1]cuda:0", arg174_1: "f16[4096][1]cuda:0", arg175_1: "f16[14336, 4096][4096, 1]cuda:0", arg176_1: "f16[14336, 4096][4096, 1]cuda:0", arg177_1: "f16[4096, 14336][14336, 1]cuda:0", arg178_1: "f16[4096][1]cuda:0", arg179_1: "f16[4096, 4096][4096, 1]cuda:0", arg180_1: "f16[1024, 4096][4096, 1]cuda:0", arg181_1: "f16[1024, 4096][4096, 1]cuda:0", arg182_1: "Sym(s1)", arg183_1: "f16[4096, 4096][4096, 1]cuda:0", arg184_1: "f16[4096][1]cuda:0", arg185_1: "f16[14336, 4096][4096, 1]cuda:0", arg186_1: "f16[14336, 4096][4096, 1]cuda:0", arg187_1: "f16[4096, 14336][14336, 1]cuda:0", arg188_1: "f16[4096][1]cuda:0", arg189_1: "f16[4096, 4096][4096, 1]cuda:0", arg190_1: "f16[1024, 4096][4096, 1]cuda:0", arg191_1: "f16[1024, 4096][4096, 1]cuda:0", arg192_1: "Sym(s1)", arg193_1: "f16[4096, 4096][4096, 1]cuda:0", arg194_1: "f16[4096][1]cuda:0", arg195_1: "f16[14336, 4096][4096, 1]cuda:0", arg196_1: "f16[14336, 4096][4096, 1]cuda:0", arg197_1: "f16[4096, 14336][14336, 1]cuda:0", arg198_1: "f16[4096][1]cuda:0", arg199_1: "f16[4096, 4096][4096, 1]cuda:0", arg200_1: "f16[1024, 4096][4096, 1]cuda:0", arg201_1: "f16[1024, 4096][4096, 1]cuda:0", arg202_1: "Sym(s1)", arg203_1: "f16[4096, 4096][4096, 1]cuda:0", arg204_1: "f16[4096][1]cuda:0", arg205_1: "f16[14336, 4096][4096, 1]cuda:0", arg206_1: "f16[14336, 4096][4096, 1]cuda:0", arg207_1: "f16[4096, 14336][14336, 1]cuda:0", arg208_1: "f16[4096][1]cuda:0", arg209_1: "f16[4096, 4096][4096, 1]cuda:0", arg210_1: "f16[1024, 4096][4096, 1]cuda:0", arg211_1: "f16[1024, 4096][4096, 1]cuda:0", arg212_1: "Sym(s1)", arg213_1: "f16[4096, 4096][4096, 1]cuda:0", arg214_1: "f16[4096][1]cuda:0", arg215_1: "f16[14336, 4096][4096, 1]cuda:0", arg216_1: "f16[14336, 4096][4096, 1]cuda:0", arg217_1: "f16[4096, 14336][14336, 1]cuda:0", arg218_1: "f16[4096][1]cuda:0", arg219_1: "f16[4096, 4096][4096, 1]cuda:0", arg220_1: "f16[1024, 4096][4096, 1]cuda:0", arg221_1: "f16[1024, 4096][4096, 1]cuda:0", arg222_1: "Sym(s1)", arg223_1: "f16[4096, 4096][4096, 1]cuda:0", arg224_1: "f16[4096][1]cuda:0", arg225_1: "f16[14336, 4096][4096, 1]cuda:0", arg226_1: "f16[14336, 4096][4096, 1]cuda:0", arg227_1: "f16[4096, 14336][14336, 1]cuda:0", arg228_1: "f16[4096][1]cuda:0", arg229_1: "f16[4096, 4096][4096, 1]cuda:0", arg230_1: "f16[1024, 4096][4096, 1]cuda:0", arg231_1: "f16[1024, 4096][4096, 1]cuda:0", arg232_1: "Sym(s1)", arg233_1: "f16[4096, 4096][4096, 1]cuda:0", arg234_1: "f16[4096][1]cuda:0", arg235_1: "f16[14336, 4096][4096, 1]cuda:0", arg236_1: "f16[14336, 4096][4096, 1]cuda:0", arg237_1: "f16[4096, 14336][14336, 1]cuda:0", arg238_1: "f16[4096][1]cuda:0", arg239_1: "f16[4096, 4096][4096, 1]cuda:0", arg240_1: "f16[1024, 4096][4096, 1]cuda:0", arg241_1: "f16[1024, 4096][4096, 1]cuda:0", arg242_1: "Sym(s1)", arg243_1: "f16[4096, 4096][4096, 1]cuda:0", arg244_1: "f16[4096][1]cuda:0", arg245_1: "f16[14336, 4096][4096, 1]cuda:0", arg246_1: "f16[14336, 4096][4096, 1]cuda:0", arg247_1: "f16[4096, 14336][14336, 1]cuda:0", arg248_1: "f16[4096][1]cuda:0", arg249_1: "f16[4096, 4096][4096, 1]cuda:0", arg250_1: "f16[1024, 4096][4096, 1]cuda:0", arg251_1: "f16[1024, 4096][4096, 1]cuda:0", arg252_1: "Sym(s1)", arg253_1: "f16[4096, 4096][4096, 1]cuda:0", arg254_1: "f16[4096][1]cuda:0", arg255_1: "f16[14336, 4096][4096, 1]cuda:0", arg256_1: "f16[14336, 4096][4096, 1]cuda:0", arg257_1: "f16[4096, 14336][14336, 1]cuda:0", arg258_1: "f16[4096][1]cuda:0", arg259_1: "f16[4096, 4096][4096, 1]cuda:0", arg260_1: "f16[1024, 4096][4096, 1]cuda:0", arg261_1: "f16[1024, 4096][4096, 1]cuda:0", arg262_1: "Sym(s1)", arg263_1: "f16[4096, 4096][4096, 1]cuda:0", arg264_1: "f16[4096][1]cuda:0", arg265_1: "f16[14336, 4096][4096, 1]cuda:0", arg266_1: "f16[14336, 4096][4096, 1]cuda:0", arg267_1: "f16[4096, 14336][14336, 1]cuda:0", arg268_1: "f16[4096][1]cuda:0", arg269_1: "f16[4096, 4096][4096, 1]cuda:0", arg270_1: "f16[1024, 4096][4096, 1]cuda:0", arg271_1: "f16[1024, 4096][4096, 1]cuda:0", arg272_1: "Sym(s1)", arg273_1: "f16[4096, 4096][4096, 1]cuda:0", arg274_1: "f16[4096][1]cuda:0", arg275_1: "f16[14336, 4096][4096, 1]cuda:0", arg276_1: "f16[14336, 4096][4096, 1]cuda:0", arg277_1: "f16[4096, 14336][14336, 1]cuda:0", arg278_1: "f16[4096][1]cuda:0", arg279_1: "f16[4096, 4096][4096, 1]cuda:0", arg280_1: "f16[1024, 4096][4096, 1]cuda:0", arg281_1: "f16[1024, 4096][4096, 1]cuda:0", arg282_1: "Sym(s1)", arg283_1: "f16[4096, 4096][4096, 1]cuda:0", arg284_1: "f16[4096][1]cuda:0", arg285_1: "f16[14336, 4096][4096, 1]cuda:0", arg286_1: "f16[14336, 4096][4096, 1]cuda:0", arg287_1: "f16[4096, 14336][14336, 1]cuda:0", arg288_1: "f16[4096][1]cuda:0", arg289_1: "f16[4096, 4096][4096, 1]cuda:0", arg290_1: "f16[1024, 4096][4096, 1]cuda:0", arg291_1: "f16[1024, 4096][4096, 1]cuda:0", arg292_1: "Sym(s1)", arg293_1: "f16[4096, 4096][4096, 1]cuda:0", arg294_1: "f16[4096][1]cuda:0", arg295_1: "f16[14336, 4096][4096, 1]cuda:0", arg296_1: "f16[14336, 4096][4096, 1]cuda:0", arg297_1: "f16[4096, 14336][14336, 1]cuda:0", arg298_1: "f16[4096][1]cuda:0", arg299_1: "f16[4096, 4096][4096, 1]cuda:0", arg300_1: "f16[1024, 4096][4096, 1]cuda:0", arg301_1: "f16[1024, 4096][4096, 1]cuda:0", arg302_1: "Sym(s1)", arg303_1: "f16[4096, 4096][4096, 1]cuda:0", arg304_1: "f16[4096][1]cuda:0", arg305_1: "f16[14336, 4096][4096, 1]cuda:0", arg306_1: "f16[14336, 4096][4096, 1]cuda:0", arg307_1: "f16[4096, 14336][14336, 1]cuda:0", arg308_1: "f16[4096][1]cuda:0", arg309_1: "f16[4096, 4096][4096, 1]cuda:0", arg310_1: "f16[1024, 4096][4096, 1]cuda:0", arg311_1: "f16[1024, 4096][4096, 1]cuda:0", arg312_1: "Sym(s1)", arg313_1: "f16[4096, 4096][4096, 1]cuda:0", arg314_1: "f16[4096][1]cuda:0", arg315_1: "f16[14336, 4096][4096, 1]cuda:0", arg316_1: "f16[14336, 4096][4096, 1]cuda:0", arg317_1: "f16[4096, 14336][14336, 1]cuda:0", arg318_1: "f16[4096][1]cuda:0", arg319_1: "f16[4096, 4096][4096, 1]cuda:0", arg320_1: "f16[1024, 4096][4096, 1]cuda:0", arg321_1: "f16[1024, 4096][4096, 1]cuda:0", arg322_1: "Sym(s1)", arg323_1: "f16[4096, 4096][4096, 1]cuda:0", arg324_1: "f16[4096][1]cuda:0", arg325_1: "f16[14336, 4096][4096, 1]cuda:0", arg326_1: "f16[14336, 4096][4096, 1]cuda:0", arg327_1: "f16[4096, 14336][14336, 1]cuda:0", arg328_1: "f16[4096][1]cuda:0", arg329_1: "Sym(1)", arg330_1: "f16[128256, 4096][4096, 1]cuda:0"):
         # File: /app/low_bit_inference/model.py:346 in forward, code: inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)
        embedding: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.embedding.default(arg2_1, arg1_1, 128004);  arg2_1 = arg1_1 = None
        
         # File: /app/low_bit_inference/model.py:65 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        unsqueeze: "f32[1, 64][64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(arg7_1, 0);  arg7_1 = None
        slice_1: "f32[1, 64][64, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze, 1, 0, 9223372036854775807);  unsqueeze = None
        unsqueeze_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_1, 2);  slice_1 = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_1, [1, -1, 1]);  unsqueeze_1 = None
        
         # File: /app/low_bit_inference/model.py:66 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        slice_2: "i64[1, s0][s0, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg4_1, 0, 0, 9223372036854775807);  arg4_1 = None
        unsqueeze_2: "i64[1, 1, s0][s0, s0, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_2, 1);  slice_2 = None
        slice_3: "i64[1, 1, s0][s0, s0, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_2, 2, 0, 9223372036854775807);  unsqueeze_2 = None
        convert_element_type: "f32[1, 1, s0][s0, s0, 1]cuda:0" = torch.ops.prims.convert_element_type.default(slice_3, torch.float32);  slice_3 = None
        
         # File: /app/low_bit_inference/model.py:70 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        expand_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = torch.ops.aten.expand.default(expand, [1, 64, 1]);  expand = None
        view: "f32[1, 64, 1][64, 1, 1]cuda:0" = torch.ops.aten.view.default(expand_1, [1, 64, 1]);  expand_1 = None
        expand_2: "f32[1, 1, s0][s0, s0, 1]cuda:0" = torch.ops.aten.expand.default(convert_element_type, [1, 1, arg0_1]);  convert_element_type = None
        view_1: "f32[1, 1, s0][s0, s0, 1]cuda:0" = torch.ops.aten.view.default(expand_2, [1, 1, arg0_1]);  expand_2 = None
        bmm: "f32[1, 64, s0][64*s0, s0, 1]cuda:0" = torch.ops.aten.bmm.default(view, view_1);  view = view_1 = None
        view_2: "f32[1, 64, s0][64*s0, s0, 1]cuda:0" = torch.ops.aten.view.default(bmm, [1, 64, arg0_1]);  bmm = None
        permute: "f32[1, s0, 64][64*s0, 1, s0]cuda:0" = torch.ops.aten.permute.default(view_2, [0, 2, 1]);  view_2 = None
        
         # File: /app/low_bit_inference/model.py:71 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        unsqueeze_3: "f32[1, s0, 1, 64][64*s0, 1, 64*s0, s0]cuda:0" = torch.ops.aten.unsqueeze.default(permute, 2);  permute = None
        expand_3: "f32[1, s0, 2, 64][64*s0, 1, 0, s0]cuda:0" = torch.ops.aten.expand.default(unsqueeze_3, [1, arg0_1, 2, 64]);  unsqueeze_3 = None
        clone: "f32[1, s0, 2, 64][128*s0, 128, 64, 1]cuda:0" = torch.ops.aten.clone.default(expand_3, memory_format = torch.contiguous_format);  expand_3 = None
        view_3: "f32[1, s0, 128][128*s0, 128, 1]cuda:0" = torch.ops.aten.view.default(clone, [1, arg0_1, 128]);  clone = None
        clone_1: "f32[1, s0, 128][128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(view_3);  view_3 = None
        
         # File: /app/low_bit_inference/model.py:72 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, s0, 128][128*s0, 128, 1]cuda:0" = torch.ops.aten.cos.default(clone_1)
        mul_39: "f32[1, s0, 128][128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None
        
         # File: /app/low_bit_inference/model.py:73 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, s0, 128][128*s0, 128, 1]cuda:0" = torch.ops.aten.sin.default(clone_1);  clone_1 = None
        mul_46: "f32[1, s0, 128][128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
        
         # File: /app/low_bit_inference/model.py:75 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        convert_element_type_1: "f16[1, s0, 128][128*s0, 128, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_39, torch.float16);  mul_39 = None
        convert_element_type_2: "f16[1, s0, 128][128*s0, 128, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_46, torch.float16);  mul_46 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_4: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(embedding, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty);  empty = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_1: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_1: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_1);  empty_1 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 1, constant_args_idx = 0, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias, 'X_ptr': view_4, 'W_ptr': arg8_1, 'RSTD_ptr': alias_1}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias = view_4 = arg8_1 = alias_1 = None
        getitem: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy['Y_ptr'];  triton_kernel_wrapper_functional_proxy = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_1: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg9_1, [1, 0]);  arg9_1 = None
        alias_4: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem)
        view_7: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_4, [1, arg0_1, 4096]);  alias_4 = None
        view_8: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_7, [arg0_1, 4096]);  view_7 = None
        mm: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_8, permute_1);  view_8 = permute_1 = None
        view_9: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm, [1, arg0_1, 4096]);  mm = None
        view_10: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_9, [1, arg0_1, -1, 128]);  view_9 = None
        permute_2: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_10, [0, 2, 1, 3]);  view_10 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_3: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg10_1, [1, 0]);  arg10_1 = None
        alias_5: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem)
        view_12: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_5, [1, arg0_1, 4096]);  alias_5 = None
        view_13: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_12, [arg0_1, 4096]);  view_12 = None
        mm_1: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_13, permute_3);  view_13 = permute_3 = None
        view_14: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_1, [1, arg0_1, 1024]);  mm_1 = None
        view_15: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_14, [1, arg0_1, -1, 128]);  view_14 = None
        permute_4: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_5: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg11_1, [1, 0]);  arg11_1 = None
        alias_6: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem);  getitem = None
        view_17: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_6, [1, arg0_1, 4096]);  alias_6 = None
        view_18: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_17, [arg0_1, 4096]);  view_17 = None
        mm_2: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_18, permute_5);  view_18 = permute_5 = None
        view_19: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_2, [1, arg0_1, 1024]);  mm_2 = None
        view_20: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_19, [1, arg0_1, -1, 128]);  view_19 = None
        permute_6: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_20, [0, 2, 1, 3]);  view_20 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_4: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_5: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_136: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_2, unsqueeze_4)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_4: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_2, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_5: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_2, 3, 64, 9223372036854775807);  permute_2 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_5);  slice_5 = None
        cat: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg, slice_4], -1);  neg = slice_4 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_153: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat, unsqueeze_5);  cat = None
        add_135: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_136, mul_153);  mul_136 = mul_153 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_161: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_4, unsqueeze_4);  unsqueeze_4 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_6: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_4, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_7: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_4, 3, 64, 9223372036854775807);  permute_4 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_7);  slice_7 = None
        cat_1: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_1, slice_6], -1);  neg_1 = slice_6 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_178: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_1, unsqueeze_5);  cat_1 = unsqueeze_5 = None
        add_159: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_161, mul_178);  mul_161 = mul_178 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg12_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_7: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full);  full = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_1: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg12_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg12_1 = None
        alias_8: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_1);  full_1 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_7, [None, None, arg3_1], add_159);  alias_7 = add_159 = None
        alias_9: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_1: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_8, [None, None, arg3_1], permute_6);  alias_8 = permute_6 = None
        alias_10: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_1)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_11: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put);  index_put = None
        slice_12: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_11, 0, 0, 9223372036854775807);  alias_11 = None
        slice_13: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_12, 1, 0, 9223372036854775807);  slice_12 = None
        unsqueeze_7: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_13, 2);  slice_13 = None
        slice_14: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_7, 3, 0, 9223372036854775807);  unsqueeze_7 = None
        slice_15: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_14, 4, 0, 9223372036854775807);  slice_14 = None
        expand_5: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_15, [1, 8, 4, arg5_1, 128]);  slice_15 = None
        clone_2: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_5, memory_format = torch.contiguous_format);  expand_5 = None
        view_21: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_2, [1, 32, arg5_1, 128]);  clone_2 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_12: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_1);  index_put_1 = None
        slice_20: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_12, 0, 0, 9223372036854775807);  alias_12 = None
        slice_21: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_20, 1, 0, 9223372036854775807);  slice_20 = None
        unsqueeze_9: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_21, 2);  slice_21 = None
        slice_22: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_9, 3, 0, 9223372036854775807);  unsqueeze_9 = None
        slice_23: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_22, 4, 0, 9223372036854775807);  slice_22 = None
        expand_7: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_23, [1, 8, 4, arg5_1, 128]);  slice_23 = None
        clone_3: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_7, memory_format = torch.contiguous_format);  expand_7 = None
        view_22: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_3, [1, 32, arg5_1, 128]);  clone_3 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_24: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_25: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_24, 1, 0, 9223372036854775807);  slice_24 = None
        slice_26: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_25, 2, 0, 9223372036854775807);  slice_25 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_4: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_135, memory_format = torch.contiguous_format);  add_135 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_1: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_26, scalar_tensor_1, scalar_tensor);  slice_26 = scalar_tensor_1 = scalar_tensor = None
        expand_8: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where, [1, 32, arg0_1, arg5_1]);  where = None
        _scaled_dot_product_efficient_attention = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_4, view_21, view_22, expand_8, False, scale = 0.08838834764831845);  clone_4 = view_21 = view_22 = expand_8 = None
        getitem_2: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention[0];  _scaled_dot_product_efficient_attention = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_7: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_2, [0, 2, 1, 3]);  getitem_2 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_23: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_7, [1, arg0_1, -1]);  permute_7 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_8: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg13_1, [1, 0]);  arg13_1 = None
        view_24: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_23, [arg0_1, 4096]);  view_23 = None
        mm_3: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_24, permute_8);  view_24 = permute_8 = None
        view_25: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_3, [1, arg0_1, 4096]);  mm_3 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_317: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(embedding, view_25);  embedding = view_25 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_26: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_317, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_2: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_13: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_2);  empty_2 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_3: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_14: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_3);  empty_3 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_1 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 2, constant_args_idx = 1, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_13, 'X_ptr': view_26, 'W_ptr': arg14_1, 'RSTD_ptr': alias_14}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_13 = view_26 = arg14_1 = alias_14 = None
        getitem_6: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_1['Y_ptr'];  triton_kernel_wrapper_functional_proxy_1 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_9: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg15_1, [1, 0]);  arg15_1 = None
        alias_17: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_6)
        view_29: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_17, [1, arg0_1, 4096]);  alias_17 = None
        view_30: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_29, [arg0_1, 4096]);  view_29 = None
        mm_4: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_30, permute_9);  view_30 = permute_9 = None
        view_31: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_4, [1, arg0_1, 14336]);  mm_4 = None
        convert_element_type_13: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_31, torch.float32);  view_31 = None
        sigmoid: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_13)
        mul_426: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_13, sigmoid);  convert_element_type_13 = sigmoid = None
        convert_element_type_14: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_426, torch.float16);  mul_426 = None
        permute_10: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg16_1, [1, 0]);  arg16_1 = None
        alias_18: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_6);  getitem_6 = None
        view_33: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_18, [1, arg0_1, 4096]);  alias_18 = None
        view_34: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_33, [arg0_1, 4096]);  view_33 = None
        mm_5: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_34, permute_10);  view_34 = permute_10 = None
        view_35: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_5, [1, arg0_1, 14336]);  mm_5 = None
        mul_443: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_14, view_35);  convert_element_type_14 = view_35 = None
        permute_11: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg17_1, [1, 0]);  arg17_1 = None
        view_36: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_443, [arg0_1, 14336]);  mul_443 = None
        mm_6: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_36, permute_11);  view_36 = permute_11 = None
        view_37: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_6, [1, arg0_1, 4096]);  mm_6 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_370: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_317, view_37);  add_317 = view_37 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_38: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_370, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_4: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_19: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_4);  empty_4 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_5: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_20: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_5);  empty_5 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_2 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 3, constant_args_idx = 2, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_19, 'X_ptr': view_38, 'W_ptr': arg18_1, 'RSTD_ptr': alias_20}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_19 = view_38 = arg18_1 = alias_20 = None
        getitem_8: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_2['Y_ptr'];  triton_kernel_wrapper_functional_proxy_2 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_12: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg19_1, [1, 0]);  arg19_1 = None
        alias_23: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_8)
        view_41: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_23, [1, arg0_1, 4096]);  alias_23 = None
        view_42: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_41, [arg0_1, 4096]);  view_41 = None
        mm_7: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_42, permute_12);  view_42 = permute_12 = None
        view_43: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_7, [1, arg0_1, 4096]);  mm_7 = None
        view_44: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_43, [1, arg0_1, -1, 128]);  view_43 = None
        permute_13: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_44, [0, 2, 1, 3]);  view_44 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_14: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg20_1, [1, 0]);  arg20_1 = None
        alias_24: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_8)
        view_46: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_24, [1, arg0_1, 4096]);  alias_24 = None
        view_47: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_46, [arg0_1, 4096]);  view_46 = None
        mm_8: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_47, permute_14);  view_47 = permute_14 = None
        view_48: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_8, [1, arg0_1, 1024]);  mm_8 = None
        view_49: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_48, [1, arg0_1, -1, 128]);  view_48 = None
        permute_15: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_49, [0, 2, 1, 3]);  view_49 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_16: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg21_1, [1, 0]);  arg21_1 = None
        alias_25: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_8);  getitem_8 = None
        view_51: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_25, [1, arg0_1, 4096]);  alias_25 = None
        view_52: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_51, [arg0_1, 4096]);  view_51 = None
        mm_9: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_52, permute_16);  view_52 = permute_16 = None
        view_53: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_9, [1, arg0_1, 1024]);  mm_9 = None
        view_54: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_53, [1, arg0_1, -1, 128]);  view_53 = None
        permute_17: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_54, [0, 2, 1, 3]);  view_54 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_10: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_11: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_545: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_13, unsqueeze_10)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_27: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_13, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_28: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_13, 3, 64, 9223372036854775807);  permute_13 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_2: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_28);  slice_28 = None
        cat_2: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_2, slice_27], -1);  neg_2 = slice_27 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_562: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_2, unsqueeze_11);  cat_2 = None
        add_464: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_545, mul_562);  mul_545 = mul_562 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_570: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_15, unsqueeze_10);  unsqueeze_10 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_29: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_15, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_30: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_15, 3, 64, 9223372036854775807);  permute_15 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_3: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_30);  slice_30 = None
        cat_3: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_3, slice_29], -1);  neg_3 = slice_29 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_587: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_3, unsqueeze_11);  cat_3 = unsqueeze_11 = None
        add_488: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_570, mul_587);  mul_570 = mul_587 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_2: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg22_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_26: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_2);  full_2 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_3: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg22_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg22_1 = None
        alias_27: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_3);  full_3 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_2: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_26, [None, None, arg3_1], add_488);  alias_26 = add_488 = None
        alias_28: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_2)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_3: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_27, [None, None, arg3_1], permute_17);  alias_27 = permute_17 = None
        alias_29: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_3)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_30: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_2);  index_put_2 = None
        slice_35: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_30, 0, 0, 9223372036854775807);  alias_30 = None
        slice_36: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_35, 1, 0, 9223372036854775807);  slice_35 = None
        unsqueeze_13: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_36, 2);  slice_36 = None
        slice_37: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_13, 3, 0, 9223372036854775807);  unsqueeze_13 = None
        slice_38: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_37, 4, 0, 9223372036854775807);  slice_37 = None
        expand_10: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_38, [1, 8, 4, arg5_1, 128]);  slice_38 = None
        clone_5: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_10, memory_format = torch.contiguous_format);  expand_10 = None
        view_55: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_5, [1, 32, arg5_1, 128]);  clone_5 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_31: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_3);  index_put_3 = None
        slice_43: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_31, 0, 0, 9223372036854775807);  alias_31 = None
        slice_44: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_43, 1, 0, 9223372036854775807);  slice_43 = None
        unsqueeze_15: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_44, 2);  slice_44 = None
        slice_45: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_15, 3, 0, 9223372036854775807);  unsqueeze_15 = None
        slice_46: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_45, 4, 0, 9223372036854775807);  slice_45 = None
        expand_12: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_46, [1, 8, 4, arg5_1, 128]);  slice_46 = None
        clone_6: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_12, memory_format = torch.contiguous_format);  expand_12 = None
        view_56: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_6, [1, 32, arg5_1, 128]);  clone_6 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_47: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_48: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_47, 1, 0, 9223372036854775807);  slice_47 = None
        slice_49: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_48, 2, 0, 9223372036854775807);  slice_48 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_7: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_464, memory_format = torch.contiguous_format);  add_464 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_2: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_3: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_1: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_49, scalar_tensor_3, scalar_tensor_2);  slice_49 = scalar_tensor_3 = scalar_tensor_2 = None
        expand_13: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_1, [1, 32, arg0_1, arg5_1]);  where_1 = None
        _scaled_dot_product_efficient_attention_1 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_7, view_55, view_56, expand_13, False, scale = 0.08838834764831845);  clone_7 = view_55 = view_56 = expand_13 = None
        getitem_10: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_1[0];  _scaled_dot_product_efficient_attention_1 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_18: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_10, [0, 2, 1, 3]);  getitem_10 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_57: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_18, [1, arg0_1, -1]);  permute_18 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_19: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg23_1, [1, 0]);  arg23_1 = None
        view_58: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_57, [arg0_1, 4096]);  view_57 = None
        mm_10: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_58, permute_19);  view_58 = permute_19 = None
        view_59: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_10, [1, arg0_1, 4096]);  mm_10 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_646: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_370, view_59);  add_370 = view_59 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_60: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_646, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_6: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_32: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_6);  empty_6 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_7: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_33: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_7);  empty_7 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_3 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 3, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_32, 'X_ptr': view_60, 'W_ptr': arg24_1, 'RSTD_ptr': alias_33}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_32 = view_60 = arg24_1 = alias_33 = None
        getitem_14: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_3['Y_ptr'];  triton_kernel_wrapper_functional_proxy_3 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_20: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg25_1, [1, 0]);  arg25_1 = None
        alias_36: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_14)
        view_63: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_36, [1, arg0_1, 4096]);  alias_36 = None
        view_64: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_63, [arg0_1, 4096]);  view_63 = None
        mm_11: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_64, permute_20);  view_64 = permute_20 = None
        view_65: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_11, [1, arg0_1, 14336]);  mm_11 = None
        convert_element_type_29: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_65, torch.float32);  view_65 = None
        sigmoid_1: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_29)
        mul_835: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_29, sigmoid_1);  convert_element_type_29 = sigmoid_1 = None
        convert_element_type_30: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_835, torch.float16);  mul_835 = None
        permute_21: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg26_1, [1, 0]);  arg26_1 = None
        alias_37: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_14);  getitem_14 = None
        view_67: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_37, [1, arg0_1, 4096]);  alias_37 = None
        view_68: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_67, [arg0_1, 4096]);  view_67 = None
        mm_12: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_68, permute_21);  view_68 = permute_21 = None
        view_69: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_12, [1, arg0_1, 14336]);  mm_12 = None
        mul_852: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_30, view_69);  convert_element_type_30 = view_69 = None
        permute_22: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg27_1, [1, 0]);  arg27_1 = None
        view_70: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_852, [arg0_1, 14336]);  mul_852 = None
        mm_13: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_70, permute_22);  view_70 = permute_22 = None
        view_71: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_13, [1, arg0_1, 4096]);  mm_13 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_699: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_646, view_71);  add_646 = view_71 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_72: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_699, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_8: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_38: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_8);  empty_8 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_9: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_39: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_9);  empty_9 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_4 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 4, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_38, 'X_ptr': view_72, 'W_ptr': arg28_1, 'RSTD_ptr': alias_39}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_38 = view_72 = arg28_1 = alias_39 = None
        getitem_16: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_4['Y_ptr'];  triton_kernel_wrapper_functional_proxy_4 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_23: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg29_1, [1, 0]);  arg29_1 = None
        alias_42: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_16)
        view_75: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_42, [1, arg0_1, 4096]);  alias_42 = None
        view_76: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_75, [arg0_1, 4096]);  view_75 = None
        mm_14: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_76, permute_23);  view_76 = permute_23 = None
        view_77: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_14, [1, arg0_1, 4096]);  mm_14 = None
        view_78: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_77, [1, arg0_1, -1, 128]);  view_77 = None
        permute_24: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_78, [0, 2, 1, 3]);  view_78 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_25: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg30_1, [1, 0]);  arg30_1 = None
        alias_43: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_16)
        view_80: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_43, [1, arg0_1, 4096]);  alias_43 = None
        view_81: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_80, [arg0_1, 4096]);  view_80 = None
        mm_15: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_81, permute_25);  view_81 = permute_25 = None
        view_82: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_15, [1, arg0_1, 1024]);  mm_15 = None
        view_83: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_82, [1, arg0_1, -1, 128]);  view_82 = None
        permute_26: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_83, [0, 2, 1, 3]);  view_83 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_27: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg31_1, [1, 0]);  arg31_1 = None
        alias_44: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_16);  getitem_16 = None
        view_85: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_44, [1, arg0_1, 4096]);  alias_44 = None
        view_86: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_85, [arg0_1, 4096]);  view_85 = None
        mm_16: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_86, permute_27);  view_86 = permute_27 = None
        view_87: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_16, [1, arg0_1, 1024]);  mm_16 = None
        view_88: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_87, [1, arg0_1, -1, 128]);  view_87 = None
        permute_28: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_88, [0, 2, 1, 3]);  view_88 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_16: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_17: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_954: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_24, unsqueeze_16)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_50: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_24, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_51: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_24, 3, 64, 9223372036854775807);  permute_24 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_4: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_51);  slice_51 = None
        cat_4: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_4, slice_50], -1);  neg_4 = slice_50 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_971: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_4, unsqueeze_17);  cat_4 = None
        add_793: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_954, mul_971);  mul_954 = mul_971 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_979: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_26, unsqueeze_16);  unsqueeze_16 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_52: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_26, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_53: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_26, 3, 64, 9223372036854775807);  permute_26 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_5: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_53);  slice_53 = None
        cat_5: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_5, slice_52], -1);  neg_5 = slice_52 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_996: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_5, unsqueeze_17);  cat_5 = unsqueeze_17 = None
        add_817: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_979, mul_996);  mul_979 = mul_996 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_4: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg32_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_45: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_4);  full_4 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_5: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg32_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg32_1 = None
        alias_46: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_5);  full_5 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_4: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_45, [None, None, arg3_1], add_817);  alias_45 = add_817 = None
        alias_47: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_4)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_5: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_46, [None, None, arg3_1], permute_28);  alias_46 = permute_28 = None
        alias_48: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_5)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_49: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_4);  index_put_4 = None
        slice_58: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_49, 0, 0, 9223372036854775807);  alias_49 = None
        slice_59: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_58, 1, 0, 9223372036854775807);  slice_58 = None
        unsqueeze_19: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_59, 2);  slice_59 = None
        slice_60: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_19, 3, 0, 9223372036854775807);  unsqueeze_19 = None
        slice_61: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_60, 4, 0, 9223372036854775807);  slice_60 = None
        expand_15: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_61, [1, 8, 4, arg5_1, 128]);  slice_61 = None
        clone_8: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_15, memory_format = torch.contiguous_format);  expand_15 = None
        view_89: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_8, [1, 32, arg5_1, 128]);  clone_8 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_50: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_5);  index_put_5 = None
        slice_66: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_50, 0, 0, 9223372036854775807);  alias_50 = None
        slice_67: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_66, 1, 0, 9223372036854775807);  slice_66 = None
        unsqueeze_21: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_67, 2);  slice_67 = None
        slice_68: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_21, 3, 0, 9223372036854775807);  unsqueeze_21 = None
        slice_69: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_68, 4, 0, 9223372036854775807);  slice_68 = None
        expand_17: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_69, [1, 8, 4, arg5_1, 128]);  slice_69 = None
        clone_9: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_17, memory_format = torch.contiguous_format);  expand_17 = None
        view_90: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_9, [1, 32, arg5_1, 128]);  clone_9 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_70: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_71: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_70, 1, 0, 9223372036854775807);  slice_70 = None
        slice_72: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_71, 2, 0, 9223372036854775807);  slice_71 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_10: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_793, memory_format = torch.contiguous_format);  add_793 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_4: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_5: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_2: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_72, scalar_tensor_5, scalar_tensor_4);  slice_72 = scalar_tensor_5 = scalar_tensor_4 = None
        expand_18: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_2, [1, 32, arg0_1, arg5_1]);  where_2 = None
        _scaled_dot_product_efficient_attention_2 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_10, view_89, view_90, expand_18, False, scale = 0.08838834764831845);  clone_10 = view_89 = view_90 = expand_18 = None
        getitem_18: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_2[0];  _scaled_dot_product_efficient_attention_2 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_29: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_18, [0, 2, 1, 3]);  getitem_18 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_91: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_29, [1, arg0_1, -1]);  permute_29 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_30: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg33_1, [1, 0]);  arg33_1 = None
        view_92: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_91, [arg0_1, 4096]);  view_91 = None
        mm_17: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_92, permute_30);  view_92 = permute_30 = None
        view_93: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_17, [1, arg0_1, 4096]);  mm_17 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_975: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_699, view_93);  add_699 = view_93 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_94: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_975, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_10: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_51: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_10);  empty_10 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_11: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_52: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_11);  empty_11 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_5 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 6, constant_args_idx = 5, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_51, 'X_ptr': view_94, 'W_ptr': arg34_1, 'RSTD_ptr': alias_52}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_51 = view_94 = arg34_1 = alias_52 = None
        getitem_22: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_5['Y_ptr'];  triton_kernel_wrapper_functional_proxy_5 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_31: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg35_1, [1, 0]);  arg35_1 = None
        alias_55: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_22)
        view_97: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_55, [1, arg0_1, 4096]);  alias_55 = None
        view_98: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_97, [arg0_1, 4096]);  view_97 = None
        mm_18: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_98, permute_31);  view_98 = permute_31 = None
        view_99: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_18, [1, arg0_1, 14336]);  mm_18 = None
        convert_element_type_45: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_99, torch.float32);  view_99 = None
        sigmoid_2: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_45)
        mul_1244: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_45, sigmoid_2);  convert_element_type_45 = sigmoid_2 = None
        convert_element_type_46: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1244, torch.float16);  mul_1244 = None
        permute_32: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg36_1, [1, 0]);  arg36_1 = None
        alias_56: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_22);  getitem_22 = None
        view_101: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_56, [1, arg0_1, 4096]);  alias_56 = None
        view_102: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_101, [arg0_1, 4096]);  view_101 = None
        mm_19: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_102, permute_32);  view_102 = permute_32 = None
        view_103: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_19, [1, arg0_1, 14336]);  mm_19 = None
        mul_1261: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_46, view_103);  convert_element_type_46 = view_103 = None
        permute_33: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg37_1, [1, 0]);  arg37_1 = None
        view_104: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_1261, [arg0_1, 14336]);  mul_1261 = None
        mm_20: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_104, permute_33);  view_104 = permute_33 = None
        view_105: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_20, [1, arg0_1, 4096]);  mm_20 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_1028: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_975, view_105);  add_975 = view_105 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_106: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_1028, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_12: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_57: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_12);  empty_12 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_13: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_58: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_13);  empty_13 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_6 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 7, constant_args_idx = 6, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_57, 'X_ptr': view_106, 'W_ptr': arg38_1, 'RSTD_ptr': alias_58}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_57 = view_106 = arg38_1 = alias_58 = None
        getitem_24: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_6['Y_ptr'];  triton_kernel_wrapper_functional_proxy_6 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_34: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg39_1, [1, 0]);  arg39_1 = None
        alias_61: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_24)
        view_109: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_61, [1, arg0_1, 4096]);  alias_61 = None
        view_110: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_109, [arg0_1, 4096]);  view_109 = None
        mm_21: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_110, permute_34);  view_110 = permute_34 = None
        view_111: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_21, [1, arg0_1, 4096]);  mm_21 = None
        view_112: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_111, [1, arg0_1, -1, 128]);  view_111 = None
        permute_35: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_112, [0, 2, 1, 3]);  view_112 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_36: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg40_1, [1, 0]);  arg40_1 = None
        alias_62: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_24)
        view_114: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_62, [1, arg0_1, 4096]);  alias_62 = None
        view_115: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_114, [arg0_1, 4096]);  view_114 = None
        mm_22: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_115, permute_36);  view_115 = permute_36 = None
        view_116: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_22, [1, arg0_1, 1024]);  mm_22 = None
        view_117: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_116, [1, arg0_1, -1, 128]);  view_116 = None
        permute_37: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_117, [0, 2, 1, 3]);  view_117 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_38: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg41_1, [1, 0]);  arg41_1 = None
        alias_63: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_24);  getitem_24 = None
        view_119: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_63, [1, arg0_1, 4096]);  alias_63 = None
        view_120: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_119, [arg0_1, 4096]);  view_119 = None
        mm_23: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_120, permute_38);  view_120 = permute_38 = None
        view_121: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_23, [1, arg0_1, 1024]);  mm_23 = None
        view_122: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_121, [1, arg0_1, -1, 128]);  view_121 = None
        permute_39: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_122, [0, 2, 1, 3]);  view_122 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_22: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_23: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_1363: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_35, unsqueeze_22)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_73: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_35, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_74: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_35, 3, 64, 9223372036854775807);  permute_35 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_6: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_74);  slice_74 = None
        cat_6: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_6, slice_73], -1);  neg_6 = slice_73 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_1380: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_6, unsqueeze_23);  cat_6 = None
        add_1122: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_1363, mul_1380);  mul_1363 = mul_1380 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_1388: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_37, unsqueeze_22);  unsqueeze_22 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_75: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_37, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_76: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_37, 3, 64, 9223372036854775807);  permute_37 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_7: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_76);  slice_76 = None
        cat_7: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_7, slice_75], -1);  neg_7 = slice_75 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_1405: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_7, unsqueeze_23);  cat_7 = unsqueeze_23 = None
        add_1146: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_1388, mul_1405);  mul_1388 = mul_1405 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_6: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg42_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_64: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_6);  full_6 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_7: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg42_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg42_1 = None
        alias_65: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_7);  full_7 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_6: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_64, [None, None, arg3_1], add_1146);  alias_64 = add_1146 = None
        alias_66: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_6)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_7: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_65, [None, None, arg3_1], permute_39);  alias_65 = permute_39 = None
        alias_67: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_7)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_68: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_6);  index_put_6 = None
        slice_81: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_68, 0, 0, 9223372036854775807);  alias_68 = None
        slice_82: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_81, 1, 0, 9223372036854775807);  slice_81 = None
        unsqueeze_25: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_82, 2);  slice_82 = None
        slice_83: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_25, 3, 0, 9223372036854775807);  unsqueeze_25 = None
        slice_84: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_83, 4, 0, 9223372036854775807);  slice_83 = None
        expand_20: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_84, [1, 8, 4, arg5_1, 128]);  slice_84 = None
        clone_11: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_20, memory_format = torch.contiguous_format);  expand_20 = None
        view_123: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_11, [1, 32, arg5_1, 128]);  clone_11 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_69: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_7);  index_put_7 = None
        slice_89: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_69, 0, 0, 9223372036854775807);  alias_69 = None
        slice_90: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_89, 1, 0, 9223372036854775807);  slice_89 = None
        unsqueeze_27: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_90, 2);  slice_90 = None
        slice_91: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_27, 3, 0, 9223372036854775807);  unsqueeze_27 = None
        slice_92: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_91, 4, 0, 9223372036854775807);  slice_91 = None
        expand_22: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_92, [1, 8, 4, arg5_1, 128]);  slice_92 = None
        clone_12: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_22, memory_format = torch.contiguous_format);  expand_22 = None
        view_124: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_12, [1, 32, arg5_1, 128]);  clone_12 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_93: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_94: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_93, 1, 0, 9223372036854775807);  slice_93 = None
        slice_95: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_94, 2, 0, 9223372036854775807);  slice_94 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_13: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_1122, memory_format = torch.contiguous_format);  add_1122 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_6: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_7: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_3: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_95, scalar_tensor_7, scalar_tensor_6);  slice_95 = scalar_tensor_7 = scalar_tensor_6 = None
        expand_23: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_3, [1, 32, arg0_1, arg5_1]);  where_3 = None
        _scaled_dot_product_efficient_attention_3 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_13, view_123, view_124, expand_23, False, scale = 0.08838834764831845);  clone_13 = view_123 = view_124 = expand_23 = None
        getitem_26: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_3[0];  _scaled_dot_product_efficient_attention_3 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_40: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_26, [0, 2, 1, 3]);  getitem_26 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_125: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_40, [1, arg0_1, -1]);  permute_40 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_41: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg43_1, [1, 0]);  arg43_1 = None
        view_126: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_125, [arg0_1, 4096]);  view_125 = None
        mm_24: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_126, permute_41);  view_126 = permute_41 = None
        view_127: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_24, [1, arg0_1, 4096]);  mm_24 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_1304: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_1028, view_127);  add_1028 = view_127 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_128: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_1304, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_14: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_70: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_14);  empty_14 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_15: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_71: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_15);  empty_15 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_7 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 8, constant_args_idx = 7, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_70, 'X_ptr': view_128, 'W_ptr': arg44_1, 'RSTD_ptr': alias_71}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_70 = view_128 = arg44_1 = alias_71 = None
        getitem_30: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_7['Y_ptr'];  triton_kernel_wrapper_functional_proxy_7 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_42: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg45_1, [1, 0]);  arg45_1 = None
        alias_74: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_30)
        view_131: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_74, [1, arg0_1, 4096]);  alias_74 = None
        view_132: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_131, [arg0_1, 4096]);  view_131 = None
        mm_25: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_132, permute_42);  view_132 = permute_42 = None
        view_133: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_25, [1, arg0_1, 14336]);  mm_25 = None
        convert_element_type_61: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_133, torch.float32);  view_133 = None
        sigmoid_3: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_61)
        mul_1653: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_61, sigmoid_3);  convert_element_type_61 = sigmoid_3 = None
        convert_element_type_62: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1653, torch.float16);  mul_1653 = None
        permute_43: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg46_1, [1, 0]);  arg46_1 = None
        alias_75: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_30);  getitem_30 = None
        view_135: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_75, [1, arg0_1, 4096]);  alias_75 = None
        view_136: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_135, [arg0_1, 4096]);  view_135 = None
        mm_26: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_136, permute_43);  view_136 = permute_43 = None
        view_137: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_26, [1, arg0_1, 14336]);  mm_26 = None
        mul_1670: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_62, view_137);  convert_element_type_62 = view_137 = None
        permute_44: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg47_1, [1, 0]);  arg47_1 = None
        view_138: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_1670, [arg0_1, 14336]);  mul_1670 = None
        mm_27: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_138, permute_44);  view_138 = permute_44 = None
        view_139: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_27, [1, arg0_1, 4096]);  mm_27 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_1357: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_1304, view_139);  add_1304 = view_139 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_140: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_1357, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_16: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_76: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_16);  empty_16 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_17: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_77: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_17);  empty_17 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_8 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 9, constant_args_idx = 8, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_76, 'X_ptr': view_140, 'W_ptr': arg48_1, 'RSTD_ptr': alias_77}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_76 = view_140 = arg48_1 = alias_77 = None
        getitem_32: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_8['Y_ptr'];  triton_kernel_wrapper_functional_proxy_8 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_45: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg49_1, [1, 0]);  arg49_1 = None
        alias_80: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_32)
        view_143: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_80, [1, arg0_1, 4096]);  alias_80 = None
        view_144: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_143, [arg0_1, 4096]);  view_143 = None
        mm_28: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_144, permute_45);  view_144 = permute_45 = None
        view_145: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_28, [1, arg0_1, 4096]);  mm_28 = None
        view_146: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_145, [1, arg0_1, -1, 128]);  view_145 = None
        permute_46: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_146, [0, 2, 1, 3]);  view_146 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_47: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg50_1, [1, 0]);  arg50_1 = None
        alias_81: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_32)
        view_148: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_81, [1, arg0_1, 4096]);  alias_81 = None
        view_149: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_148, [arg0_1, 4096]);  view_148 = None
        mm_29: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_149, permute_47);  view_149 = permute_47 = None
        view_150: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_29, [1, arg0_1, 1024]);  mm_29 = None
        view_151: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_150, [1, arg0_1, -1, 128]);  view_150 = None
        permute_48: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_151, [0, 2, 1, 3]);  view_151 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_49: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg51_1, [1, 0]);  arg51_1 = None
        alias_82: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_32);  getitem_32 = None
        view_153: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_82, [1, arg0_1, 4096]);  alias_82 = None
        view_154: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_153, [arg0_1, 4096]);  view_153 = None
        mm_30: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_154, permute_49);  view_154 = permute_49 = None
        view_155: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_30, [1, arg0_1, 1024]);  mm_30 = None
        view_156: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_155, [1, arg0_1, -1, 128]);  view_155 = None
        permute_50: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_156, [0, 2, 1, 3]);  view_156 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_28: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_29: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_1772: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_46, unsqueeze_28)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_96: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_46, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_97: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_46, 3, 64, 9223372036854775807);  permute_46 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_8: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_97);  slice_97 = None
        cat_8: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_8, slice_96], -1);  neg_8 = slice_96 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_1789: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_8, unsqueeze_29);  cat_8 = None
        add_1451: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_1772, mul_1789);  mul_1772 = mul_1789 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_1797: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_48, unsqueeze_28);  unsqueeze_28 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_98: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_48, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_99: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_48, 3, 64, 9223372036854775807);  permute_48 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_9: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_99);  slice_99 = None
        cat_9: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_9, slice_98], -1);  neg_9 = slice_98 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_1814: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_9, unsqueeze_29);  cat_9 = unsqueeze_29 = None
        add_1475: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_1797, mul_1814);  mul_1797 = mul_1814 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_8: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg52_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_83: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_8);  full_8 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_9: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg52_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg52_1 = None
        alias_84: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_9);  full_9 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_8: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_83, [None, None, arg3_1], add_1475);  alias_83 = add_1475 = None
        alias_85: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_8)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_9: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_84, [None, None, arg3_1], permute_50);  alias_84 = permute_50 = None
        alias_86: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_9)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_87: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_8);  index_put_8 = None
        slice_104: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_87, 0, 0, 9223372036854775807);  alias_87 = None
        slice_105: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_104, 1, 0, 9223372036854775807);  slice_104 = None
        unsqueeze_31: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_105, 2);  slice_105 = None
        slice_106: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_31, 3, 0, 9223372036854775807);  unsqueeze_31 = None
        slice_107: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_106, 4, 0, 9223372036854775807);  slice_106 = None
        expand_25: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_107, [1, 8, 4, arg5_1, 128]);  slice_107 = None
        clone_14: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_25, memory_format = torch.contiguous_format);  expand_25 = None
        view_157: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_14, [1, 32, arg5_1, 128]);  clone_14 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_88: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_9);  index_put_9 = None
        slice_112: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_88, 0, 0, 9223372036854775807);  alias_88 = None
        slice_113: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_112, 1, 0, 9223372036854775807);  slice_112 = None
        unsqueeze_33: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_113, 2);  slice_113 = None
        slice_114: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_33, 3, 0, 9223372036854775807);  unsqueeze_33 = None
        slice_115: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_114, 4, 0, 9223372036854775807);  slice_114 = None
        expand_27: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_115, [1, 8, 4, arg5_1, 128]);  slice_115 = None
        clone_15: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_27, memory_format = torch.contiguous_format);  expand_27 = None
        view_158: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_15, [1, 32, arg5_1, 128]);  clone_15 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_116: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_117: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_116, 1, 0, 9223372036854775807);  slice_116 = None
        slice_118: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_117, 2, 0, 9223372036854775807);  slice_117 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_16: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_1451, memory_format = torch.contiguous_format);  add_1451 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_8: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_9: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_4: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_118, scalar_tensor_9, scalar_tensor_8);  slice_118 = scalar_tensor_9 = scalar_tensor_8 = None
        expand_28: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_4, [1, 32, arg0_1, arg5_1]);  where_4 = None
        _scaled_dot_product_efficient_attention_4 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_16, view_157, view_158, expand_28, False, scale = 0.08838834764831845);  clone_16 = view_157 = view_158 = expand_28 = None
        getitem_34: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_4[0];  _scaled_dot_product_efficient_attention_4 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_51: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_34, [0, 2, 1, 3]);  getitem_34 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_159: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_51, [1, arg0_1, -1]);  permute_51 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_52: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg53_1, [1, 0]);  arg53_1 = None
        view_160: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_159, [arg0_1, 4096]);  view_159 = None
        mm_31: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_160, permute_52);  view_160 = permute_52 = None
        view_161: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_31, [1, arg0_1, 4096]);  mm_31 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_1633: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_1357, view_161);  add_1357 = view_161 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_162: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_1633, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_18: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_89: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_18);  empty_18 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_19: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_90: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_19);  empty_19 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_9 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 10, constant_args_idx = 9, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_89, 'X_ptr': view_162, 'W_ptr': arg54_1, 'RSTD_ptr': alias_90}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_89 = view_162 = arg54_1 = alias_90 = None
        getitem_38: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_9['Y_ptr'];  triton_kernel_wrapper_functional_proxy_9 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_53: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg55_1, [1, 0]);  arg55_1 = None
        alias_93: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_38)
        view_165: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_93, [1, arg0_1, 4096]);  alias_93 = None
        view_166: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_165, [arg0_1, 4096]);  view_165 = None
        mm_32: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_166, permute_53);  view_166 = permute_53 = None
        view_167: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_32, [1, arg0_1, 14336]);  mm_32 = None
        convert_element_type_77: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_167, torch.float32);  view_167 = None
        sigmoid_4: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_77)
        mul_2062: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_77, sigmoid_4);  convert_element_type_77 = sigmoid_4 = None
        convert_element_type_78: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_2062, torch.float16);  mul_2062 = None
        permute_54: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg56_1, [1, 0]);  arg56_1 = None
        alias_94: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_38);  getitem_38 = None
        view_169: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_94, [1, arg0_1, 4096]);  alias_94 = None
        view_170: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_169, [arg0_1, 4096]);  view_169 = None
        mm_33: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_170, permute_54);  view_170 = permute_54 = None
        view_171: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_33, [1, arg0_1, 14336]);  mm_33 = None
        mul_2079: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_78, view_171);  convert_element_type_78 = view_171 = None
        permute_55: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg57_1, [1, 0]);  arg57_1 = None
        view_172: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_2079, [arg0_1, 14336]);  mul_2079 = None
        mm_34: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_172, permute_55);  view_172 = permute_55 = None
        view_173: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_34, [1, arg0_1, 4096]);  mm_34 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_1686: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_1633, view_173);  add_1633 = view_173 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_174: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_1686, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_20: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_95: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_20);  empty_20 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_21: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_96: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_21);  empty_21 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_10 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 11, constant_args_idx = 10, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_95, 'X_ptr': view_174, 'W_ptr': arg58_1, 'RSTD_ptr': alias_96}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_95 = view_174 = arg58_1 = alias_96 = None
        getitem_40: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_10['Y_ptr'];  triton_kernel_wrapper_functional_proxy_10 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_56: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg59_1, [1, 0]);  arg59_1 = None
        alias_99: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_40)
        view_177: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_99, [1, arg0_1, 4096]);  alias_99 = None
        view_178: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_177, [arg0_1, 4096]);  view_177 = None
        mm_35: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_178, permute_56);  view_178 = permute_56 = None
        view_179: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_35, [1, arg0_1, 4096]);  mm_35 = None
        view_180: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_179, [1, arg0_1, -1, 128]);  view_179 = None
        permute_57: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_180, [0, 2, 1, 3]);  view_180 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_58: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg60_1, [1, 0]);  arg60_1 = None
        alias_100: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_40)
        view_182: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_100, [1, arg0_1, 4096]);  alias_100 = None
        view_183: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_182, [arg0_1, 4096]);  view_182 = None
        mm_36: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_183, permute_58);  view_183 = permute_58 = None
        view_184: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_36, [1, arg0_1, 1024]);  mm_36 = None
        view_185: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_184, [1, arg0_1, -1, 128]);  view_184 = None
        permute_59: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_185, [0, 2, 1, 3]);  view_185 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_60: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg61_1, [1, 0]);  arg61_1 = None
        alias_101: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_40);  getitem_40 = None
        view_187: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_101, [1, arg0_1, 4096]);  alias_101 = None
        view_188: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_187, [arg0_1, 4096]);  view_187 = None
        mm_37: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_188, permute_60);  view_188 = permute_60 = None
        view_189: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_37, [1, arg0_1, 1024]);  mm_37 = None
        view_190: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_189, [1, arg0_1, -1, 128]);  view_189 = None
        permute_61: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_190, [0, 2, 1, 3]);  view_190 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_34: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_35: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_2181: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_57, unsqueeze_34)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_119: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_57, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_120: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_57, 3, 64, 9223372036854775807);  permute_57 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_10: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_120);  slice_120 = None
        cat_10: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_10, slice_119], -1);  neg_10 = slice_119 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_2198: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_10, unsqueeze_35);  cat_10 = None
        add_1780: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_2181, mul_2198);  mul_2181 = mul_2198 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_2206: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_59, unsqueeze_34);  unsqueeze_34 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_121: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_59, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_122: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_59, 3, 64, 9223372036854775807);  permute_59 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_11: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_122);  slice_122 = None
        cat_11: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_11, slice_121], -1);  neg_11 = slice_121 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_2223: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_11, unsqueeze_35);  cat_11 = unsqueeze_35 = None
        add_1804: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_2206, mul_2223);  mul_2206 = mul_2223 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_10: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg62_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_102: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_10);  full_10 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_11: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg62_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg62_1 = None
        alias_103: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_11);  full_11 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_10: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_102, [None, None, arg3_1], add_1804);  alias_102 = add_1804 = None
        alias_104: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_10)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_11: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_103, [None, None, arg3_1], permute_61);  alias_103 = permute_61 = None
        alias_105: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_11)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_106: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_10);  index_put_10 = None
        slice_127: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_106, 0, 0, 9223372036854775807);  alias_106 = None
        slice_128: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_127, 1, 0, 9223372036854775807);  slice_127 = None
        unsqueeze_37: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_128, 2);  slice_128 = None
        slice_129: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_37, 3, 0, 9223372036854775807);  unsqueeze_37 = None
        slice_130: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_129, 4, 0, 9223372036854775807);  slice_129 = None
        expand_30: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_130, [1, 8, 4, arg5_1, 128]);  slice_130 = None
        clone_17: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_30, memory_format = torch.contiguous_format);  expand_30 = None
        view_191: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_17, [1, 32, arg5_1, 128]);  clone_17 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_107: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_11);  index_put_11 = None
        slice_135: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_107, 0, 0, 9223372036854775807);  alias_107 = None
        slice_136: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_135, 1, 0, 9223372036854775807);  slice_135 = None
        unsqueeze_39: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_136, 2);  slice_136 = None
        slice_137: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_39, 3, 0, 9223372036854775807);  unsqueeze_39 = None
        slice_138: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_137, 4, 0, 9223372036854775807);  slice_137 = None
        expand_32: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_138, [1, 8, 4, arg5_1, 128]);  slice_138 = None
        clone_18: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_32, memory_format = torch.contiguous_format);  expand_32 = None
        view_192: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_18, [1, 32, arg5_1, 128]);  clone_18 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_139: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_140: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_139, 1, 0, 9223372036854775807);  slice_139 = None
        slice_141: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_140, 2, 0, 9223372036854775807);  slice_140 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_19: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_1780, memory_format = torch.contiguous_format);  add_1780 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_10: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_11: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_5: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_141, scalar_tensor_11, scalar_tensor_10);  slice_141 = scalar_tensor_11 = scalar_tensor_10 = None
        expand_33: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_5, [1, 32, arg0_1, arg5_1]);  where_5 = None
        _scaled_dot_product_efficient_attention_5 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_19, view_191, view_192, expand_33, False, scale = 0.08838834764831845);  clone_19 = view_191 = view_192 = expand_33 = None
        getitem_42: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_5[0];  _scaled_dot_product_efficient_attention_5 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_62: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_42, [0, 2, 1, 3]);  getitem_42 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_193: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_62, [1, arg0_1, -1]);  permute_62 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_63: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg63_1, [1, 0]);  arg63_1 = None
        view_194: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_193, [arg0_1, 4096]);  view_193 = None
        mm_38: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_194, permute_63);  view_194 = permute_63 = None
        view_195: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_38, [1, arg0_1, 4096]);  mm_38 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_1962: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_1686, view_195);  add_1686 = view_195 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_196: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_1962, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_22: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_108: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_22);  empty_22 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_23: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_109: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_23);  empty_23 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_11 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 12, constant_args_idx = 11, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_108, 'X_ptr': view_196, 'W_ptr': arg64_1, 'RSTD_ptr': alias_109}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_108 = view_196 = arg64_1 = alias_109 = None
        getitem_46: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_11['Y_ptr'];  triton_kernel_wrapper_functional_proxy_11 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_64: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg65_1, [1, 0]);  arg65_1 = None
        alias_112: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_46)
        view_199: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_112, [1, arg0_1, 4096]);  alias_112 = None
        view_200: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_199, [arg0_1, 4096]);  view_199 = None
        mm_39: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_200, permute_64);  view_200 = permute_64 = None
        view_201: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_39, [1, arg0_1, 14336]);  mm_39 = None
        convert_element_type_93: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_201, torch.float32);  view_201 = None
        sigmoid_5: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_93)
        mul_2471: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_93, sigmoid_5);  convert_element_type_93 = sigmoid_5 = None
        convert_element_type_94: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_2471, torch.float16);  mul_2471 = None
        permute_65: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg66_1, [1, 0]);  arg66_1 = None
        alias_113: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_46);  getitem_46 = None
        view_203: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_113, [1, arg0_1, 4096]);  alias_113 = None
        view_204: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_203, [arg0_1, 4096]);  view_203 = None
        mm_40: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_204, permute_65);  view_204 = permute_65 = None
        view_205: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_40, [1, arg0_1, 14336]);  mm_40 = None
        mul_2488: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_94, view_205);  convert_element_type_94 = view_205 = None
        permute_66: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg67_1, [1, 0]);  arg67_1 = None
        view_206: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_2488, [arg0_1, 14336]);  mul_2488 = None
        mm_41: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_206, permute_66);  view_206 = permute_66 = None
        view_207: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_41, [1, arg0_1, 4096]);  mm_41 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_2015: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_1962, view_207);  add_1962 = view_207 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_208: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_2015, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_24: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_114: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_24);  empty_24 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_25: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_115: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_25);  empty_25 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_12 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 13, constant_args_idx = 12, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_114, 'X_ptr': view_208, 'W_ptr': arg68_1, 'RSTD_ptr': alias_115}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_114 = view_208 = arg68_1 = alias_115 = None
        getitem_48: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_12['Y_ptr'];  triton_kernel_wrapper_functional_proxy_12 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_67: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg69_1, [1, 0]);  arg69_1 = None
        alias_118: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_48)
        view_211: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_118, [1, arg0_1, 4096]);  alias_118 = None
        view_212: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_211, [arg0_1, 4096]);  view_211 = None
        mm_42: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_212, permute_67);  view_212 = permute_67 = None
        view_213: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_42, [1, arg0_1, 4096]);  mm_42 = None
        view_214: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_213, [1, arg0_1, -1, 128]);  view_213 = None
        permute_68: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_214, [0, 2, 1, 3]);  view_214 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_69: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg70_1, [1, 0]);  arg70_1 = None
        alias_119: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_48)
        view_216: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_119, [1, arg0_1, 4096]);  alias_119 = None
        view_217: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_216, [arg0_1, 4096]);  view_216 = None
        mm_43: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_217, permute_69);  view_217 = permute_69 = None
        view_218: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_43, [1, arg0_1, 1024]);  mm_43 = None
        view_219: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_218, [1, arg0_1, -1, 128]);  view_218 = None
        permute_70: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_219, [0, 2, 1, 3]);  view_219 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_71: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg71_1, [1, 0]);  arg71_1 = None
        alias_120: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_48);  getitem_48 = None
        view_221: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_120, [1, arg0_1, 4096]);  alias_120 = None
        view_222: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_221, [arg0_1, 4096]);  view_221 = None
        mm_44: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_222, permute_71);  view_222 = permute_71 = None
        view_223: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_44, [1, arg0_1, 1024]);  mm_44 = None
        view_224: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_223, [1, arg0_1, -1, 128]);  view_223 = None
        permute_72: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_224, [0, 2, 1, 3]);  view_224 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_40: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_41: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_2590: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_68, unsqueeze_40)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_142: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_68, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_143: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_68, 3, 64, 9223372036854775807);  permute_68 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_12: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_143);  slice_143 = None
        cat_12: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_12, slice_142], -1);  neg_12 = slice_142 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_2607: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_12, unsqueeze_41);  cat_12 = None
        add_2109: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_2590, mul_2607);  mul_2590 = mul_2607 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_2615: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_70, unsqueeze_40);  unsqueeze_40 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_144: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_70, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_145: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_70, 3, 64, 9223372036854775807);  permute_70 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_13: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_145);  slice_145 = None
        cat_13: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_13, slice_144], -1);  neg_13 = slice_144 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_2632: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_13, unsqueeze_41);  cat_13 = unsqueeze_41 = None
        add_2133: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_2615, mul_2632);  mul_2615 = mul_2632 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_12: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg72_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_121: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_12);  full_12 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_13: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg72_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg72_1 = None
        alias_122: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_13);  full_13 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_12: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_121, [None, None, arg3_1], add_2133);  alias_121 = add_2133 = None
        alias_123: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_12)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_13: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_122, [None, None, arg3_1], permute_72);  alias_122 = permute_72 = None
        alias_124: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_13)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_125: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_12);  index_put_12 = None
        slice_150: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_125, 0, 0, 9223372036854775807);  alias_125 = None
        slice_151: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_150, 1, 0, 9223372036854775807);  slice_150 = None
        unsqueeze_43: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_151, 2);  slice_151 = None
        slice_152: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_43, 3, 0, 9223372036854775807);  unsqueeze_43 = None
        slice_153: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_152, 4, 0, 9223372036854775807);  slice_152 = None
        expand_35: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_153, [1, 8, 4, arg5_1, 128]);  slice_153 = None
        clone_20: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_35, memory_format = torch.contiguous_format);  expand_35 = None
        view_225: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_20, [1, 32, arg5_1, 128]);  clone_20 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_126: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_13);  index_put_13 = None
        slice_158: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_126, 0, 0, 9223372036854775807);  alias_126 = None
        slice_159: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_158, 1, 0, 9223372036854775807);  slice_158 = None
        unsqueeze_45: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_159, 2);  slice_159 = None
        slice_160: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_45, 3, 0, 9223372036854775807);  unsqueeze_45 = None
        slice_161: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_160, 4, 0, 9223372036854775807);  slice_160 = None
        expand_37: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_161, [1, 8, 4, arg5_1, 128]);  slice_161 = None
        clone_21: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_37, memory_format = torch.contiguous_format);  expand_37 = None
        view_226: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_21, [1, 32, arg5_1, 128]);  clone_21 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_162: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_163: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_162, 1, 0, 9223372036854775807);  slice_162 = None
        slice_164: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_163, 2, 0, 9223372036854775807);  slice_163 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_22: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_2109, memory_format = torch.contiguous_format);  add_2109 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_12: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_13: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_6: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_164, scalar_tensor_13, scalar_tensor_12);  slice_164 = scalar_tensor_13 = scalar_tensor_12 = None
        expand_38: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_6, [1, 32, arg0_1, arg5_1]);  where_6 = None
        _scaled_dot_product_efficient_attention_6 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_22, view_225, view_226, expand_38, False, scale = 0.08838834764831845);  clone_22 = view_225 = view_226 = expand_38 = None
        getitem_50: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_6[0];  _scaled_dot_product_efficient_attention_6 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_73: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_50, [0, 2, 1, 3]);  getitem_50 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_227: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_73, [1, arg0_1, -1]);  permute_73 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_74: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg73_1, [1, 0]);  arg73_1 = None
        view_228: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_227, [arg0_1, 4096]);  view_227 = None
        mm_45: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_228, permute_74);  view_228 = permute_74 = None
        view_229: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_45, [1, arg0_1, 4096]);  mm_45 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_2291: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_2015, view_229);  add_2015 = view_229 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_230: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_2291, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_26: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_127: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_26);  empty_26 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_27: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_128: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_27);  empty_27 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_13 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 14, constant_args_idx = 13, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_127, 'X_ptr': view_230, 'W_ptr': arg74_1, 'RSTD_ptr': alias_128}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_127 = view_230 = arg74_1 = alias_128 = None
        getitem_54: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_13['Y_ptr'];  triton_kernel_wrapper_functional_proxy_13 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_75: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg75_1, [1, 0]);  arg75_1 = None
        alias_131: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_54)
        view_233: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_131, [1, arg0_1, 4096]);  alias_131 = None
        view_234: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_233, [arg0_1, 4096]);  view_233 = None
        mm_46: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_234, permute_75);  view_234 = permute_75 = None
        view_235: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_46, [1, arg0_1, 14336]);  mm_46 = None
        convert_element_type_109: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_235, torch.float32);  view_235 = None
        sigmoid_6: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_109)
        mul_2880: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_109, sigmoid_6);  convert_element_type_109 = sigmoid_6 = None
        convert_element_type_110: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_2880, torch.float16);  mul_2880 = None
        permute_76: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg76_1, [1, 0]);  arg76_1 = None
        alias_132: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_54);  getitem_54 = None
        view_237: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_132, [1, arg0_1, 4096]);  alias_132 = None
        view_238: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_237, [arg0_1, 4096]);  view_237 = None
        mm_47: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_238, permute_76);  view_238 = permute_76 = None
        view_239: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_47, [1, arg0_1, 14336]);  mm_47 = None
        mul_2897: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_110, view_239);  convert_element_type_110 = view_239 = None
        permute_77: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg77_1, [1, 0]);  arg77_1 = None
        view_240: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_2897, [arg0_1, 14336]);  mul_2897 = None
        mm_48: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_240, permute_77);  view_240 = permute_77 = None
        view_241: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_48, [1, arg0_1, 4096]);  mm_48 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_2344: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_2291, view_241);  add_2291 = view_241 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_242: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_2344, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_28: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_133: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_28);  empty_28 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_29: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_134: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_29);  empty_29 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_14 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 15, constant_args_idx = 14, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_133, 'X_ptr': view_242, 'W_ptr': arg78_1, 'RSTD_ptr': alias_134}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_133 = view_242 = arg78_1 = alias_134 = None
        getitem_56: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_14['Y_ptr'];  triton_kernel_wrapper_functional_proxy_14 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_78: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg79_1, [1, 0]);  arg79_1 = None
        alias_137: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_56)
        view_245: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_137, [1, arg0_1, 4096]);  alias_137 = None
        view_246: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_245, [arg0_1, 4096]);  view_245 = None
        mm_49: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_246, permute_78);  view_246 = permute_78 = None
        view_247: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_49, [1, arg0_1, 4096]);  mm_49 = None
        view_248: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_247, [1, arg0_1, -1, 128]);  view_247 = None
        permute_79: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_248, [0, 2, 1, 3]);  view_248 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_80: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg80_1, [1, 0]);  arg80_1 = None
        alias_138: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_56)
        view_250: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_138, [1, arg0_1, 4096]);  alias_138 = None
        view_251: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_250, [arg0_1, 4096]);  view_250 = None
        mm_50: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_251, permute_80);  view_251 = permute_80 = None
        view_252: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_50, [1, arg0_1, 1024]);  mm_50 = None
        view_253: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_252, [1, arg0_1, -1, 128]);  view_252 = None
        permute_81: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_253, [0, 2, 1, 3]);  view_253 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_82: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg81_1, [1, 0]);  arg81_1 = None
        alias_139: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_56);  getitem_56 = None
        view_255: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_139, [1, arg0_1, 4096]);  alias_139 = None
        view_256: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_255, [arg0_1, 4096]);  view_255 = None
        mm_51: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_256, permute_82);  view_256 = permute_82 = None
        view_257: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_51, [1, arg0_1, 1024]);  mm_51 = None
        view_258: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_257, [1, arg0_1, -1, 128]);  view_257 = None
        permute_83: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_258, [0, 2, 1, 3]);  view_258 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_46: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_47: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_2999: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_79, unsqueeze_46)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_165: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_79, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_166: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_79, 3, 64, 9223372036854775807);  permute_79 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_14: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_166);  slice_166 = None
        cat_14: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_14, slice_165], -1);  neg_14 = slice_165 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_3016: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_14, unsqueeze_47);  cat_14 = None
        add_2438: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_2999, mul_3016);  mul_2999 = mul_3016 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_3024: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_81, unsqueeze_46);  unsqueeze_46 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_167: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_81, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_168: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_81, 3, 64, 9223372036854775807);  permute_81 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_15: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_168);  slice_168 = None
        cat_15: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_15, slice_167], -1);  neg_15 = slice_167 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_3041: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_15, unsqueeze_47);  cat_15 = unsqueeze_47 = None
        add_2462: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_3024, mul_3041);  mul_3024 = mul_3041 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_14: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg82_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_140: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_14);  full_14 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_15: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg82_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg82_1 = None
        alias_141: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_15);  full_15 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_14: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_140, [None, None, arg3_1], add_2462);  alias_140 = add_2462 = None
        alias_142: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_14)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_15: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_141, [None, None, arg3_1], permute_83);  alias_141 = permute_83 = None
        alias_143: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_15)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_144: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_14);  index_put_14 = None
        slice_173: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_144, 0, 0, 9223372036854775807);  alias_144 = None
        slice_174: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_173, 1, 0, 9223372036854775807);  slice_173 = None
        unsqueeze_49: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_174, 2);  slice_174 = None
        slice_175: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_49, 3, 0, 9223372036854775807);  unsqueeze_49 = None
        slice_176: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_175, 4, 0, 9223372036854775807);  slice_175 = None
        expand_40: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_176, [1, 8, 4, arg5_1, 128]);  slice_176 = None
        clone_23: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_40, memory_format = torch.contiguous_format);  expand_40 = None
        view_259: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_23, [1, 32, arg5_1, 128]);  clone_23 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_145: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_15);  index_put_15 = None
        slice_181: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_145, 0, 0, 9223372036854775807);  alias_145 = None
        slice_182: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_181, 1, 0, 9223372036854775807);  slice_181 = None
        unsqueeze_51: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_182, 2);  slice_182 = None
        slice_183: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_51, 3, 0, 9223372036854775807);  unsqueeze_51 = None
        slice_184: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_183, 4, 0, 9223372036854775807);  slice_183 = None
        expand_42: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_184, [1, 8, 4, arg5_1, 128]);  slice_184 = None
        clone_24: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_42, memory_format = torch.contiguous_format);  expand_42 = None
        view_260: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_24, [1, 32, arg5_1, 128]);  clone_24 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_185: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_186: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_185, 1, 0, 9223372036854775807);  slice_185 = None
        slice_187: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_186, 2, 0, 9223372036854775807);  slice_186 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_25: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_2438, memory_format = torch.contiguous_format);  add_2438 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_14: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_15: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_7: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_187, scalar_tensor_15, scalar_tensor_14);  slice_187 = scalar_tensor_15 = scalar_tensor_14 = None
        expand_43: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_7, [1, 32, arg0_1, arg5_1]);  where_7 = None
        _scaled_dot_product_efficient_attention_7 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_25, view_259, view_260, expand_43, False, scale = 0.08838834764831845);  clone_25 = view_259 = view_260 = expand_43 = None
        getitem_58: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_7[0];  _scaled_dot_product_efficient_attention_7 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_84: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_58, [0, 2, 1, 3]);  getitem_58 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_261: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_84, [1, arg0_1, -1]);  permute_84 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_85: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg83_1, [1, 0]);  arg83_1 = None
        view_262: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_261, [arg0_1, 4096]);  view_261 = None
        mm_52: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_262, permute_85);  view_262 = permute_85 = None
        view_263: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_52, [1, arg0_1, 4096]);  mm_52 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_2620: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_2344, view_263);  add_2344 = view_263 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_264: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_2620, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_30: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_146: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_30);  empty_30 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_31: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_147: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_31);  empty_31 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_15 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 16, constant_args_idx = 15, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_146, 'X_ptr': view_264, 'W_ptr': arg84_1, 'RSTD_ptr': alias_147}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_146 = view_264 = arg84_1 = alias_147 = None
        getitem_62: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_15['Y_ptr'];  triton_kernel_wrapper_functional_proxy_15 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_86: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg85_1, [1, 0]);  arg85_1 = None
        alias_150: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_62)
        view_267: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_150, [1, arg0_1, 4096]);  alias_150 = None
        view_268: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_267, [arg0_1, 4096]);  view_267 = None
        mm_53: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_268, permute_86);  view_268 = permute_86 = None
        view_269: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_53, [1, arg0_1, 14336]);  mm_53 = None
        convert_element_type_125: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_269, torch.float32);  view_269 = None
        sigmoid_7: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_125)
        mul_3289: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_125, sigmoid_7);  convert_element_type_125 = sigmoid_7 = None
        convert_element_type_126: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_3289, torch.float16);  mul_3289 = None
        permute_87: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg86_1, [1, 0]);  arg86_1 = None
        alias_151: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_62);  getitem_62 = None
        view_271: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_151, [1, arg0_1, 4096]);  alias_151 = None
        view_272: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_271, [arg0_1, 4096]);  view_271 = None
        mm_54: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_272, permute_87);  view_272 = permute_87 = None
        view_273: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_54, [1, arg0_1, 14336]);  mm_54 = None
        mul_3306: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_126, view_273);  convert_element_type_126 = view_273 = None
        permute_88: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg87_1, [1, 0]);  arg87_1 = None
        view_274: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_3306, [arg0_1, 14336]);  mul_3306 = None
        mm_55: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_274, permute_88);  view_274 = permute_88 = None
        view_275: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_55, [1, arg0_1, 4096]);  mm_55 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_2673: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_2620, view_275);  add_2620 = view_275 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_276: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_2673, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_32: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_152: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_32);  empty_32 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_33: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_153: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_33);  empty_33 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_16 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 17, constant_args_idx = 16, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_152, 'X_ptr': view_276, 'W_ptr': arg88_1, 'RSTD_ptr': alias_153}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_152 = view_276 = arg88_1 = alias_153 = None
        getitem_64: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_16['Y_ptr'];  triton_kernel_wrapper_functional_proxy_16 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_89: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg89_1, [1, 0]);  arg89_1 = None
        alias_156: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_64)
        view_279: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_156, [1, arg0_1, 4096]);  alias_156 = None
        view_280: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_279, [arg0_1, 4096]);  view_279 = None
        mm_56: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_280, permute_89);  view_280 = permute_89 = None
        view_281: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_56, [1, arg0_1, 4096]);  mm_56 = None
        view_282: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_281, [1, arg0_1, -1, 128]);  view_281 = None
        permute_90: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_282, [0, 2, 1, 3]);  view_282 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_91: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg90_1, [1, 0]);  arg90_1 = None
        alias_157: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_64)
        view_284: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_157, [1, arg0_1, 4096]);  alias_157 = None
        view_285: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_284, [arg0_1, 4096]);  view_284 = None
        mm_57: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_285, permute_91);  view_285 = permute_91 = None
        view_286: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_57, [1, arg0_1, 1024]);  mm_57 = None
        view_287: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_286, [1, arg0_1, -1, 128]);  view_286 = None
        permute_92: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_287, [0, 2, 1, 3]);  view_287 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_93: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg91_1, [1, 0]);  arg91_1 = None
        alias_158: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_64);  getitem_64 = None
        view_289: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_158, [1, arg0_1, 4096]);  alias_158 = None
        view_290: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_289, [arg0_1, 4096]);  view_289 = None
        mm_58: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_290, permute_93);  view_290 = permute_93 = None
        view_291: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_58, [1, arg0_1, 1024]);  mm_58 = None
        view_292: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_291, [1, arg0_1, -1, 128]);  view_291 = None
        permute_94: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_292, [0, 2, 1, 3]);  view_292 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_52: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_53: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_3408: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_90, unsqueeze_52)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_188: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_90, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_189: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_90, 3, 64, 9223372036854775807);  permute_90 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_16: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_189);  slice_189 = None
        cat_16: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_16, slice_188], -1);  neg_16 = slice_188 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_3425: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_16, unsqueeze_53);  cat_16 = None
        add_2767: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_3408, mul_3425);  mul_3408 = mul_3425 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_3433: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_92, unsqueeze_52);  unsqueeze_52 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_190: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_92, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_191: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_92, 3, 64, 9223372036854775807);  permute_92 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_17: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_191);  slice_191 = None
        cat_17: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_17, slice_190], -1);  neg_17 = slice_190 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_3450: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_17, unsqueeze_53);  cat_17 = unsqueeze_53 = None
        add_2791: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_3433, mul_3450);  mul_3433 = mul_3450 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_16: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg92_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_159: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_16);  full_16 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_17: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg92_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg92_1 = None
        alias_160: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_17);  full_17 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_16: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_159, [None, None, arg3_1], add_2791);  alias_159 = add_2791 = None
        alias_161: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_16)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_17: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_160, [None, None, arg3_1], permute_94);  alias_160 = permute_94 = None
        alias_162: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_17)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_163: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_16);  index_put_16 = None
        slice_196: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_163, 0, 0, 9223372036854775807);  alias_163 = None
        slice_197: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_196, 1, 0, 9223372036854775807);  slice_196 = None
        unsqueeze_55: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_197, 2);  slice_197 = None
        slice_198: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_55, 3, 0, 9223372036854775807);  unsqueeze_55 = None
        slice_199: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_198, 4, 0, 9223372036854775807);  slice_198 = None
        expand_45: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_199, [1, 8, 4, arg5_1, 128]);  slice_199 = None
        clone_26: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_45, memory_format = torch.contiguous_format);  expand_45 = None
        view_293: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_26, [1, 32, arg5_1, 128]);  clone_26 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_164: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_17);  index_put_17 = None
        slice_204: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_164, 0, 0, 9223372036854775807);  alias_164 = None
        slice_205: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_204, 1, 0, 9223372036854775807);  slice_204 = None
        unsqueeze_57: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_205, 2);  slice_205 = None
        slice_206: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_57, 3, 0, 9223372036854775807);  unsqueeze_57 = None
        slice_207: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_206, 4, 0, 9223372036854775807);  slice_206 = None
        expand_47: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_207, [1, 8, 4, arg5_1, 128]);  slice_207 = None
        clone_27: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_47, memory_format = torch.contiguous_format);  expand_47 = None
        view_294: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_27, [1, 32, arg5_1, 128]);  clone_27 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_208: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_209: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_208, 1, 0, 9223372036854775807);  slice_208 = None
        slice_210: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_209, 2, 0, 9223372036854775807);  slice_209 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_28: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_2767, memory_format = torch.contiguous_format);  add_2767 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_16: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_17: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_8: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_210, scalar_tensor_17, scalar_tensor_16);  slice_210 = scalar_tensor_17 = scalar_tensor_16 = None
        expand_48: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_8, [1, 32, arg0_1, arg5_1]);  where_8 = None
        _scaled_dot_product_efficient_attention_8 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_28, view_293, view_294, expand_48, False, scale = 0.08838834764831845);  clone_28 = view_293 = view_294 = expand_48 = None
        getitem_66: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_8[0];  _scaled_dot_product_efficient_attention_8 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_95: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_66, [0, 2, 1, 3]);  getitem_66 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_295: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_95, [1, arg0_1, -1]);  permute_95 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_96: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg93_1, [1, 0]);  arg93_1 = None
        view_296: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_295, [arg0_1, 4096]);  view_295 = None
        mm_59: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_296, permute_96);  view_296 = permute_96 = None
        view_297: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_59, [1, arg0_1, 4096]);  mm_59 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_2949: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_2673, view_297);  add_2673 = view_297 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_298: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_2949, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_34: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_165: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_34);  empty_34 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_35: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_166: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_35);  empty_35 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_17 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 18, constant_args_idx = 17, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_165, 'X_ptr': view_298, 'W_ptr': arg94_1, 'RSTD_ptr': alias_166}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_165 = view_298 = arg94_1 = alias_166 = None
        getitem_70: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_17['Y_ptr'];  triton_kernel_wrapper_functional_proxy_17 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_97: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg95_1, [1, 0]);  arg95_1 = None
        alias_169: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_70)
        view_301: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_169, [1, arg0_1, 4096]);  alias_169 = None
        view_302: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_301, [arg0_1, 4096]);  view_301 = None
        mm_60: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_302, permute_97);  view_302 = permute_97 = None
        view_303: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_60, [1, arg0_1, 14336]);  mm_60 = None
        convert_element_type_141: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_303, torch.float32);  view_303 = None
        sigmoid_8: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_141)
        mul_3698: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_141, sigmoid_8);  convert_element_type_141 = sigmoid_8 = None
        convert_element_type_142: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_3698, torch.float16);  mul_3698 = None
        permute_98: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg96_1, [1, 0]);  arg96_1 = None
        alias_170: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_70);  getitem_70 = None
        view_305: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_170, [1, arg0_1, 4096]);  alias_170 = None
        view_306: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_305, [arg0_1, 4096]);  view_305 = None
        mm_61: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_306, permute_98);  view_306 = permute_98 = None
        view_307: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_61, [1, arg0_1, 14336]);  mm_61 = None
        mul_3715: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_142, view_307);  convert_element_type_142 = view_307 = None
        permute_99: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg97_1, [1, 0]);  arg97_1 = None
        view_308: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_3715, [arg0_1, 14336]);  mul_3715 = None
        mm_62: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_308, permute_99);  view_308 = permute_99 = None
        view_309: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_62, [1, arg0_1, 4096]);  mm_62 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_3002: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_2949, view_309);  add_2949 = view_309 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_310: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_3002, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_36: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_171: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_36);  empty_36 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_37: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_172: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_37);  empty_37 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_18 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 19, constant_args_idx = 18, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_171, 'X_ptr': view_310, 'W_ptr': arg98_1, 'RSTD_ptr': alias_172}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_171 = view_310 = arg98_1 = alias_172 = None
        getitem_72: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_18['Y_ptr'];  triton_kernel_wrapper_functional_proxy_18 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_100: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg99_1, [1, 0]);  arg99_1 = None
        alias_175: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_72)
        view_313: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_175, [1, arg0_1, 4096]);  alias_175 = None
        view_314: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_313, [arg0_1, 4096]);  view_313 = None
        mm_63: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_314, permute_100);  view_314 = permute_100 = None
        view_315: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_63, [1, arg0_1, 4096]);  mm_63 = None
        view_316: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_315, [1, arg0_1, -1, 128]);  view_315 = None
        permute_101: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_316, [0, 2, 1, 3]);  view_316 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_102: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg100_1, [1, 0]);  arg100_1 = None
        alias_176: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_72)
        view_318: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_176, [1, arg0_1, 4096]);  alias_176 = None
        view_319: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_318, [arg0_1, 4096]);  view_318 = None
        mm_64: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_319, permute_102);  view_319 = permute_102 = None
        view_320: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_64, [1, arg0_1, 1024]);  mm_64 = None
        view_321: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_320, [1, arg0_1, -1, 128]);  view_320 = None
        permute_103: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_321, [0, 2, 1, 3]);  view_321 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_104: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg101_1, [1, 0]);  arg101_1 = None
        alias_177: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_72);  getitem_72 = None
        view_323: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_177, [1, arg0_1, 4096]);  alias_177 = None
        view_324: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_323, [arg0_1, 4096]);  view_323 = None
        mm_65: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_324, permute_104);  view_324 = permute_104 = None
        view_325: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_65, [1, arg0_1, 1024]);  mm_65 = None
        view_326: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_325, [1, arg0_1, -1, 128]);  view_325 = None
        permute_105: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_326, [0, 2, 1, 3]);  view_326 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_58: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_59: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_3817: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_101, unsqueeze_58)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_211: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_101, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_212: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_101, 3, 64, 9223372036854775807);  permute_101 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_18: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_212);  slice_212 = None
        cat_18: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_18, slice_211], -1);  neg_18 = slice_211 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_3834: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_18, unsqueeze_59);  cat_18 = None
        add_3096: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_3817, mul_3834);  mul_3817 = mul_3834 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_3842: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_103, unsqueeze_58);  unsqueeze_58 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_213: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_103, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_214: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_103, 3, 64, 9223372036854775807);  permute_103 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_19: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_214);  slice_214 = None
        cat_19: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_19, slice_213], -1);  neg_19 = slice_213 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_3859: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_19, unsqueeze_59);  cat_19 = unsqueeze_59 = None
        add_3120: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_3842, mul_3859);  mul_3842 = mul_3859 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_18: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg102_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_178: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_18);  full_18 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_19: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg102_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg102_1 = None
        alias_179: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_19);  full_19 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_18: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_178, [None, None, arg3_1], add_3120);  alias_178 = add_3120 = None
        alias_180: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_18)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_19: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_179, [None, None, arg3_1], permute_105);  alias_179 = permute_105 = None
        alias_181: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_19)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_182: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_18);  index_put_18 = None
        slice_219: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_182, 0, 0, 9223372036854775807);  alias_182 = None
        slice_220: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_219, 1, 0, 9223372036854775807);  slice_219 = None
        unsqueeze_61: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_220, 2);  slice_220 = None
        slice_221: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_61, 3, 0, 9223372036854775807);  unsqueeze_61 = None
        slice_222: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_221, 4, 0, 9223372036854775807);  slice_221 = None
        expand_50: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_222, [1, 8, 4, arg5_1, 128]);  slice_222 = None
        clone_29: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_50, memory_format = torch.contiguous_format);  expand_50 = None
        view_327: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_29, [1, 32, arg5_1, 128]);  clone_29 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_183: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_19);  index_put_19 = None
        slice_227: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_183, 0, 0, 9223372036854775807);  alias_183 = None
        slice_228: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_227, 1, 0, 9223372036854775807);  slice_227 = None
        unsqueeze_63: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_228, 2);  slice_228 = None
        slice_229: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_63, 3, 0, 9223372036854775807);  unsqueeze_63 = None
        slice_230: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_229, 4, 0, 9223372036854775807);  slice_229 = None
        expand_52: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_230, [1, 8, 4, arg5_1, 128]);  slice_230 = None
        clone_30: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_52, memory_format = torch.contiguous_format);  expand_52 = None
        view_328: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_30, [1, 32, arg5_1, 128]);  clone_30 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_231: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_232: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_231, 1, 0, 9223372036854775807);  slice_231 = None
        slice_233: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_232, 2, 0, 9223372036854775807);  slice_232 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_31: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_3096, memory_format = torch.contiguous_format);  add_3096 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_18: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_19: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_9: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_233, scalar_tensor_19, scalar_tensor_18);  slice_233 = scalar_tensor_19 = scalar_tensor_18 = None
        expand_53: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_9, [1, 32, arg0_1, arg5_1]);  where_9 = None
        _scaled_dot_product_efficient_attention_9 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_31, view_327, view_328, expand_53, False, scale = 0.08838834764831845);  clone_31 = view_327 = view_328 = expand_53 = None
        getitem_74: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_9[0];  _scaled_dot_product_efficient_attention_9 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_106: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_74, [0, 2, 1, 3]);  getitem_74 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_329: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_106, [1, arg0_1, -1]);  permute_106 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_107: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg103_1, [1, 0]);  arg103_1 = None
        view_330: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_329, [arg0_1, 4096]);  view_329 = None
        mm_66: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_330, permute_107);  view_330 = permute_107 = None
        view_331: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_66, [1, arg0_1, 4096]);  mm_66 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_3278: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_3002, view_331);  add_3002 = view_331 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_332: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_3278, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_38: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_184: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_38);  empty_38 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_39: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_185: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_39);  empty_39 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_19 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 20, constant_args_idx = 19, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_184, 'X_ptr': view_332, 'W_ptr': arg104_1, 'RSTD_ptr': alias_185}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_184 = view_332 = arg104_1 = alias_185 = None
        getitem_78: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_19['Y_ptr'];  triton_kernel_wrapper_functional_proxy_19 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_108: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg105_1, [1, 0]);  arg105_1 = None
        alias_188: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_78)
        view_335: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_188, [1, arg0_1, 4096]);  alias_188 = None
        view_336: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_335, [arg0_1, 4096]);  view_335 = None
        mm_67: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_336, permute_108);  view_336 = permute_108 = None
        view_337: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_67, [1, arg0_1, 14336]);  mm_67 = None
        convert_element_type_157: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_337, torch.float32);  view_337 = None
        sigmoid_9: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_157)
        mul_4107: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_157, sigmoid_9);  convert_element_type_157 = sigmoid_9 = None
        convert_element_type_158: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_4107, torch.float16);  mul_4107 = None
        permute_109: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg106_1, [1, 0]);  arg106_1 = None
        alias_189: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_78);  getitem_78 = None
        view_339: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_189, [1, arg0_1, 4096]);  alias_189 = None
        view_340: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_339, [arg0_1, 4096]);  view_339 = None
        mm_68: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_340, permute_109);  view_340 = permute_109 = None
        view_341: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_68, [1, arg0_1, 14336]);  mm_68 = None
        mul_4124: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_158, view_341);  convert_element_type_158 = view_341 = None
        permute_110: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg107_1, [1, 0]);  arg107_1 = None
        view_342: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_4124, [arg0_1, 14336]);  mul_4124 = None
        mm_69: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_342, permute_110);  view_342 = permute_110 = None
        view_343: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_69, [1, arg0_1, 4096]);  mm_69 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_3331: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_3278, view_343);  add_3278 = view_343 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_344: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_3331, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_40: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_190: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_40);  empty_40 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_41: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_191: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_41);  empty_41 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_20 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 21, constant_args_idx = 20, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_190, 'X_ptr': view_344, 'W_ptr': arg108_1, 'RSTD_ptr': alias_191}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_190 = view_344 = arg108_1 = alias_191 = None
        getitem_80: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_20['Y_ptr'];  triton_kernel_wrapper_functional_proxy_20 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_111: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg109_1, [1, 0]);  arg109_1 = None
        alias_194: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_80)
        view_347: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_194, [1, arg0_1, 4096]);  alias_194 = None
        view_348: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_347, [arg0_1, 4096]);  view_347 = None
        mm_70: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_348, permute_111);  view_348 = permute_111 = None
        view_349: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_70, [1, arg0_1, 4096]);  mm_70 = None
        view_350: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_349, [1, arg0_1, -1, 128]);  view_349 = None
        permute_112: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_350, [0, 2, 1, 3]);  view_350 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_113: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg110_1, [1, 0]);  arg110_1 = None
        alias_195: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_80)
        view_352: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_195, [1, arg0_1, 4096]);  alias_195 = None
        view_353: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_352, [arg0_1, 4096]);  view_352 = None
        mm_71: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_353, permute_113);  view_353 = permute_113 = None
        view_354: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_71, [1, arg0_1, 1024]);  mm_71 = None
        view_355: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_354, [1, arg0_1, -1, 128]);  view_354 = None
        permute_114: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_355, [0, 2, 1, 3]);  view_355 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_115: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg111_1, [1, 0]);  arg111_1 = None
        alias_196: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_80);  getitem_80 = None
        view_357: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_196, [1, arg0_1, 4096]);  alias_196 = None
        view_358: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_357, [arg0_1, 4096]);  view_357 = None
        mm_72: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_358, permute_115);  view_358 = permute_115 = None
        view_359: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_72, [1, arg0_1, 1024]);  mm_72 = None
        view_360: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_359, [1, arg0_1, -1, 128]);  view_359 = None
        permute_116: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_360, [0, 2, 1, 3]);  view_360 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_64: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_65: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4226: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_112, unsqueeze_64)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_234: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_112, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_235: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_112, 3, 64, 9223372036854775807);  permute_112 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_20: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_235);  slice_235 = None
        cat_20: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_20, slice_234], -1);  neg_20 = slice_234 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4243: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_20, unsqueeze_65);  cat_20 = None
        add_3425: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_4226, mul_4243);  mul_4226 = mul_4243 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_4251: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_114, unsqueeze_64);  unsqueeze_64 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_236: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_114, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_237: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_114, 3, 64, 9223372036854775807);  permute_114 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_21: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_237);  slice_237 = None
        cat_21: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_21, slice_236], -1);  neg_21 = slice_236 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_4268: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_21, unsqueeze_65);  cat_21 = unsqueeze_65 = None
        add_3449: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_4251, mul_4268);  mul_4251 = mul_4268 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_20: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg112_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_197: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_20);  full_20 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_21: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg112_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg112_1 = None
        alias_198: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_21);  full_21 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_20: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_197, [None, None, arg3_1], add_3449);  alias_197 = add_3449 = None
        alias_199: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_20)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_21: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_198, [None, None, arg3_1], permute_116);  alias_198 = permute_116 = None
        alias_200: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_21)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_201: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_20);  index_put_20 = None
        slice_242: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_201, 0, 0, 9223372036854775807);  alias_201 = None
        slice_243: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_242, 1, 0, 9223372036854775807);  slice_242 = None
        unsqueeze_67: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_243, 2);  slice_243 = None
        slice_244: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_67, 3, 0, 9223372036854775807);  unsqueeze_67 = None
        slice_245: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_244, 4, 0, 9223372036854775807);  slice_244 = None
        expand_55: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_245, [1, 8, 4, arg5_1, 128]);  slice_245 = None
        clone_32: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_55, memory_format = torch.contiguous_format);  expand_55 = None
        view_361: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_32, [1, 32, arg5_1, 128]);  clone_32 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_202: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_21);  index_put_21 = None
        slice_250: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_202, 0, 0, 9223372036854775807);  alias_202 = None
        slice_251: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_250, 1, 0, 9223372036854775807);  slice_250 = None
        unsqueeze_69: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_251, 2);  slice_251 = None
        slice_252: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_69, 3, 0, 9223372036854775807);  unsqueeze_69 = None
        slice_253: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_252, 4, 0, 9223372036854775807);  slice_252 = None
        expand_57: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_253, [1, 8, 4, arg5_1, 128]);  slice_253 = None
        clone_33: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_57, memory_format = torch.contiguous_format);  expand_57 = None
        view_362: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_33, [1, 32, arg5_1, 128]);  clone_33 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_254: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_255: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_254, 1, 0, 9223372036854775807);  slice_254 = None
        slice_256: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_255, 2, 0, 9223372036854775807);  slice_255 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_34: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_3425, memory_format = torch.contiguous_format);  add_3425 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_20: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_21: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_10: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_256, scalar_tensor_21, scalar_tensor_20);  slice_256 = scalar_tensor_21 = scalar_tensor_20 = None
        expand_58: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_10, [1, 32, arg0_1, arg5_1]);  where_10 = None
        _scaled_dot_product_efficient_attention_10 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_34, view_361, view_362, expand_58, False, scale = 0.08838834764831845);  clone_34 = view_361 = view_362 = expand_58 = None
        getitem_82: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_10[0];  _scaled_dot_product_efficient_attention_10 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_117: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_82, [0, 2, 1, 3]);  getitem_82 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_363: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_117, [1, arg0_1, -1]);  permute_117 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_118: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg113_1, [1, 0]);  arg113_1 = None
        view_364: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_363, [arg0_1, 4096]);  view_363 = None
        mm_73: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_364, permute_118);  view_364 = permute_118 = None
        view_365: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_73, [1, arg0_1, 4096]);  mm_73 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_3607: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_3331, view_365);  add_3331 = view_365 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_366: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_3607, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_42: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_203: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_42);  empty_42 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_43: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_204: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_43);  empty_43 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_21 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 22, constant_args_idx = 21, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_203, 'X_ptr': view_366, 'W_ptr': arg114_1, 'RSTD_ptr': alias_204}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_203 = view_366 = arg114_1 = alias_204 = None
        getitem_86: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_21['Y_ptr'];  triton_kernel_wrapper_functional_proxy_21 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_119: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg115_1, [1, 0]);  arg115_1 = None
        alias_207: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_86)
        view_369: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_207, [1, arg0_1, 4096]);  alias_207 = None
        view_370: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_369, [arg0_1, 4096]);  view_369 = None
        mm_74: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_370, permute_119);  view_370 = permute_119 = None
        view_371: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_74, [1, arg0_1, 14336]);  mm_74 = None
        convert_element_type_173: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_371, torch.float32);  view_371 = None
        sigmoid_10: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_173)
        mul_4516: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_173, sigmoid_10);  convert_element_type_173 = sigmoid_10 = None
        convert_element_type_174: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_4516, torch.float16);  mul_4516 = None
        permute_120: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg116_1, [1, 0]);  arg116_1 = None
        alias_208: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_86);  getitem_86 = None
        view_373: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_208, [1, arg0_1, 4096]);  alias_208 = None
        view_374: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_373, [arg0_1, 4096]);  view_373 = None
        mm_75: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_374, permute_120);  view_374 = permute_120 = None
        view_375: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_75, [1, arg0_1, 14336]);  mm_75 = None
        mul_4533: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_174, view_375);  convert_element_type_174 = view_375 = None
        permute_121: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg117_1, [1, 0]);  arg117_1 = None
        view_376: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_4533, [arg0_1, 14336]);  mul_4533 = None
        mm_76: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_376, permute_121);  view_376 = permute_121 = None
        view_377: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_76, [1, arg0_1, 4096]);  mm_76 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_3660: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_3607, view_377);  add_3607 = view_377 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_378: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_3660, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_44: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_209: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_44);  empty_44 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_45: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_210: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_45);  empty_45 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_22 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 23, constant_args_idx = 22, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_209, 'X_ptr': view_378, 'W_ptr': arg118_1, 'RSTD_ptr': alias_210}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_209 = view_378 = arg118_1 = alias_210 = None
        getitem_88: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_22['Y_ptr'];  triton_kernel_wrapper_functional_proxy_22 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_122: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg119_1, [1, 0]);  arg119_1 = None
        alias_213: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_88)
        view_381: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_213, [1, arg0_1, 4096]);  alias_213 = None
        view_382: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_381, [arg0_1, 4096]);  view_381 = None
        mm_77: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_382, permute_122);  view_382 = permute_122 = None
        view_383: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_77, [1, arg0_1, 4096]);  mm_77 = None
        view_384: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_383, [1, arg0_1, -1, 128]);  view_383 = None
        permute_123: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_384, [0, 2, 1, 3]);  view_384 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_124: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg120_1, [1, 0]);  arg120_1 = None
        alias_214: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_88)
        view_386: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_214, [1, arg0_1, 4096]);  alias_214 = None
        view_387: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_386, [arg0_1, 4096]);  view_386 = None
        mm_78: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_387, permute_124);  view_387 = permute_124 = None
        view_388: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_78, [1, arg0_1, 1024]);  mm_78 = None
        view_389: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_388, [1, arg0_1, -1, 128]);  view_388 = None
        permute_125: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_389, [0, 2, 1, 3]);  view_389 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_126: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg121_1, [1, 0]);  arg121_1 = None
        alias_215: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_88);  getitem_88 = None
        view_391: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_215, [1, arg0_1, 4096]);  alias_215 = None
        view_392: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_391, [arg0_1, 4096]);  view_391 = None
        mm_79: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_392, permute_126);  view_392 = permute_126 = None
        view_393: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_79, [1, arg0_1, 1024]);  mm_79 = None
        view_394: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_393, [1, arg0_1, -1, 128]);  view_393 = None
        permute_127: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_394, [0, 2, 1, 3]);  view_394 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_70: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_71: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4635: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_123, unsqueeze_70)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_257: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_123, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_258: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_123, 3, 64, 9223372036854775807);  permute_123 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_22: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_258);  slice_258 = None
        cat_22: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_22, slice_257], -1);  neg_22 = slice_257 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4652: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_22, unsqueeze_71);  cat_22 = None
        add_3754: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_4635, mul_4652);  mul_4635 = mul_4652 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_4660: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_125, unsqueeze_70);  unsqueeze_70 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_259: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_125, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_260: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_125, 3, 64, 9223372036854775807);  permute_125 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_23: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_260);  slice_260 = None
        cat_23: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_23, slice_259], -1);  neg_23 = slice_259 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_4677: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_23, unsqueeze_71);  cat_23 = unsqueeze_71 = None
        add_3778: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_4660, mul_4677);  mul_4660 = mul_4677 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_22: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg122_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_216: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_22);  full_22 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_23: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg122_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg122_1 = None
        alias_217: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_23);  full_23 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_22: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_216, [None, None, arg3_1], add_3778);  alias_216 = add_3778 = None
        alias_218: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_22)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_23: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_217, [None, None, arg3_1], permute_127);  alias_217 = permute_127 = None
        alias_219: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_23)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_220: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_22);  index_put_22 = None
        slice_265: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_220, 0, 0, 9223372036854775807);  alias_220 = None
        slice_266: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_265, 1, 0, 9223372036854775807);  slice_265 = None
        unsqueeze_73: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_266, 2);  slice_266 = None
        slice_267: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_73, 3, 0, 9223372036854775807);  unsqueeze_73 = None
        slice_268: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_267, 4, 0, 9223372036854775807);  slice_267 = None
        expand_60: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_268, [1, 8, 4, arg5_1, 128]);  slice_268 = None
        clone_35: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_60, memory_format = torch.contiguous_format);  expand_60 = None
        view_395: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_35, [1, 32, arg5_1, 128]);  clone_35 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_221: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_23);  index_put_23 = None
        slice_273: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_221, 0, 0, 9223372036854775807);  alias_221 = None
        slice_274: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_273, 1, 0, 9223372036854775807);  slice_273 = None
        unsqueeze_75: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_274, 2);  slice_274 = None
        slice_275: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_75, 3, 0, 9223372036854775807);  unsqueeze_75 = None
        slice_276: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_275, 4, 0, 9223372036854775807);  slice_275 = None
        expand_62: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_276, [1, 8, 4, arg5_1, 128]);  slice_276 = None
        clone_36: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_62, memory_format = torch.contiguous_format);  expand_62 = None
        view_396: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_36, [1, 32, arg5_1, 128]);  clone_36 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_277: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_278: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_277, 1, 0, 9223372036854775807);  slice_277 = None
        slice_279: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_278, 2, 0, 9223372036854775807);  slice_278 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_37: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_3754, memory_format = torch.contiguous_format);  add_3754 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_22: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_23: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_11: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_279, scalar_tensor_23, scalar_tensor_22);  slice_279 = scalar_tensor_23 = scalar_tensor_22 = None
        expand_63: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_11, [1, 32, arg0_1, arg5_1]);  where_11 = None
        _scaled_dot_product_efficient_attention_11 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_37, view_395, view_396, expand_63, False, scale = 0.08838834764831845);  clone_37 = view_395 = view_396 = expand_63 = None
        getitem_90: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_11[0];  _scaled_dot_product_efficient_attention_11 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_128: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_90, [0, 2, 1, 3]);  getitem_90 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_397: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_128, [1, arg0_1, -1]);  permute_128 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_129: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg123_1, [1, 0]);  arg123_1 = None
        view_398: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_397, [arg0_1, 4096]);  view_397 = None
        mm_80: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_398, permute_129);  view_398 = permute_129 = None
        view_399: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_80, [1, arg0_1, 4096]);  mm_80 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_3936: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_3660, view_399);  add_3660 = view_399 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_400: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_3936, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_46: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_222: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_46);  empty_46 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_47: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_223: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_47);  empty_47 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_23 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 24, constant_args_idx = 23, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_222, 'X_ptr': view_400, 'W_ptr': arg124_1, 'RSTD_ptr': alias_223}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_222 = view_400 = arg124_1 = alias_223 = None
        getitem_94: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_23['Y_ptr'];  triton_kernel_wrapper_functional_proxy_23 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_130: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg125_1, [1, 0]);  arg125_1 = None
        alias_226: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_94)
        view_403: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_226, [1, arg0_1, 4096]);  alias_226 = None
        view_404: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_403, [arg0_1, 4096]);  view_403 = None
        mm_81: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_404, permute_130);  view_404 = permute_130 = None
        view_405: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_81, [1, arg0_1, 14336]);  mm_81 = None
        convert_element_type_189: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_405, torch.float32);  view_405 = None
        sigmoid_11: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_189)
        mul_4925: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_189, sigmoid_11);  convert_element_type_189 = sigmoid_11 = None
        convert_element_type_190: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_4925, torch.float16);  mul_4925 = None
        permute_131: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg126_1, [1, 0]);  arg126_1 = None
        alias_227: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_94);  getitem_94 = None
        view_407: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_227, [1, arg0_1, 4096]);  alias_227 = None
        view_408: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_407, [arg0_1, 4096]);  view_407 = None
        mm_82: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_408, permute_131);  view_408 = permute_131 = None
        view_409: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_82, [1, arg0_1, 14336]);  mm_82 = None
        mul_4942: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_190, view_409);  convert_element_type_190 = view_409 = None
        permute_132: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg127_1, [1, 0]);  arg127_1 = None
        view_410: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_4942, [arg0_1, 14336]);  mul_4942 = None
        mm_83: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_410, permute_132);  view_410 = permute_132 = None
        view_411: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_83, [1, arg0_1, 4096]);  mm_83 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_3989: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_3936, view_411);  add_3936 = view_411 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_412: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_3989, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_48: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_228: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_48);  empty_48 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_49: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_229: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_49);  empty_49 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_24 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 25, constant_args_idx = 24, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_228, 'X_ptr': view_412, 'W_ptr': arg128_1, 'RSTD_ptr': alias_229}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_228 = view_412 = arg128_1 = alias_229 = None
        getitem_96: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_24['Y_ptr'];  triton_kernel_wrapper_functional_proxy_24 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_133: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg129_1, [1, 0]);  arg129_1 = None
        alias_232: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_96)
        view_415: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_232, [1, arg0_1, 4096]);  alias_232 = None
        view_416: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_415, [arg0_1, 4096]);  view_415 = None
        mm_84: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_416, permute_133);  view_416 = permute_133 = None
        view_417: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_84, [1, arg0_1, 4096]);  mm_84 = None
        view_418: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_417, [1, arg0_1, -1, 128]);  view_417 = None
        permute_134: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_418, [0, 2, 1, 3]);  view_418 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_135: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg130_1, [1, 0]);  arg130_1 = None
        alias_233: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_96)
        view_420: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_233, [1, arg0_1, 4096]);  alias_233 = None
        view_421: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_420, [arg0_1, 4096]);  view_420 = None
        mm_85: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_421, permute_135);  view_421 = permute_135 = None
        view_422: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_85, [1, arg0_1, 1024]);  mm_85 = None
        view_423: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_422, [1, arg0_1, -1, 128]);  view_422 = None
        permute_136: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_423, [0, 2, 1, 3]);  view_423 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_137: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg131_1, [1, 0]);  arg131_1 = None
        alias_234: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_96);  getitem_96 = None
        view_425: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_234, [1, arg0_1, 4096]);  alias_234 = None
        view_426: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_425, [arg0_1, 4096]);  view_425 = None
        mm_86: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_426, permute_137);  view_426 = permute_137 = None
        view_427: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_86, [1, arg0_1, 1024]);  mm_86 = None
        view_428: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_427, [1, arg0_1, -1, 128]);  view_427 = None
        permute_138: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_428, [0, 2, 1, 3]);  view_428 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_76: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_77: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5044: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_134, unsqueeze_76)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_280: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_134, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_281: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_134, 3, 64, 9223372036854775807);  permute_134 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_24: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_281);  slice_281 = None
        cat_24: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_24, slice_280], -1);  neg_24 = slice_280 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5061: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_24, unsqueeze_77);  cat_24 = None
        add_4083: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_5044, mul_5061);  mul_5044 = mul_5061 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_5069: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_136, unsqueeze_76);  unsqueeze_76 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_282: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_136, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_283: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_136, 3, 64, 9223372036854775807);  permute_136 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_25: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_283);  slice_283 = None
        cat_25: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_25, slice_282], -1);  neg_25 = slice_282 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_5086: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_25, unsqueeze_77);  cat_25 = unsqueeze_77 = None
        add_4107: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_5069, mul_5086);  mul_5069 = mul_5086 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_24: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg132_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_235: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_24);  full_24 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_25: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg132_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg132_1 = None
        alias_236: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_25);  full_25 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_24: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_235, [None, None, arg3_1], add_4107);  alias_235 = add_4107 = None
        alias_237: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_24)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_25: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_236, [None, None, arg3_1], permute_138);  alias_236 = permute_138 = None
        alias_238: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_25)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_239: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_24);  index_put_24 = None
        slice_288: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_239, 0, 0, 9223372036854775807);  alias_239 = None
        slice_289: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_288, 1, 0, 9223372036854775807);  slice_288 = None
        unsqueeze_79: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_289, 2);  slice_289 = None
        slice_290: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_79, 3, 0, 9223372036854775807);  unsqueeze_79 = None
        slice_291: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_290, 4, 0, 9223372036854775807);  slice_290 = None
        expand_65: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_291, [1, 8, 4, arg5_1, 128]);  slice_291 = None
        clone_38: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_65, memory_format = torch.contiguous_format);  expand_65 = None
        view_429: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_38, [1, 32, arg5_1, 128]);  clone_38 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_240: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_25);  index_put_25 = None
        slice_296: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_240, 0, 0, 9223372036854775807);  alias_240 = None
        slice_297: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_296, 1, 0, 9223372036854775807);  slice_296 = None
        unsqueeze_81: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_297, 2);  slice_297 = None
        slice_298: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_81, 3, 0, 9223372036854775807);  unsqueeze_81 = None
        slice_299: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_298, 4, 0, 9223372036854775807);  slice_298 = None
        expand_67: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_299, [1, 8, 4, arg5_1, 128]);  slice_299 = None
        clone_39: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_67, memory_format = torch.contiguous_format);  expand_67 = None
        view_430: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_39, [1, 32, arg5_1, 128]);  clone_39 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_300: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_301: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_300, 1, 0, 9223372036854775807);  slice_300 = None
        slice_302: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_301, 2, 0, 9223372036854775807);  slice_301 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_40: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_4083, memory_format = torch.contiguous_format);  add_4083 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_24: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_25: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_12: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_302, scalar_tensor_25, scalar_tensor_24);  slice_302 = scalar_tensor_25 = scalar_tensor_24 = None
        expand_68: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_12, [1, 32, arg0_1, arg5_1]);  where_12 = None
        _scaled_dot_product_efficient_attention_12 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_40, view_429, view_430, expand_68, False, scale = 0.08838834764831845);  clone_40 = view_429 = view_430 = expand_68 = None
        getitem_98: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_12[0];  _scaled_dot_product_efficient_attention_12 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_139: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_98, [0, 2, 1, 3]);  getitem_98 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_431: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_139, [1, arg0_1, -1]);  permute_139 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_140: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg133_1, [1, 0]);  arg133_1 = None
        view_432: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_431, [arg0_1, 4096]);  view_431 = None
        mm_87: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_432, permute_140);  view_432 = permute_140 = None
        view_433: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_87, [1, arg0_1, 4096]);  mm_87 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_4265: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_3989, view_433);  add_3989 = view_433 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_434: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_4265, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_50: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_241: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_50);  empty_50 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_51: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_242: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_51);  empty_51 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_25 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 26, constant_args_idx = 25, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_241, 'X_ptr': view_434, 'W_ptr': arg134_1, 'RSTD_ptr': alias_242}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_241 = view_434 = arg134_1 = alias_242 = None
        getitem_102: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_25['Y_ptr'];  triton_kernel_wrapper_functional_proxy_25 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_141: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg135_1, [1, 0]);  arg135_1 = None
        alias_245: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_102)
        view_437: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_245, [1, arg0_1, 4096]);  alias_245 = None
        view_438: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_437, [arg0_1, 4096]);  view_437 = None
        mm_88: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_438, permute_141);  view_438 = permute_141 = None
        view_439: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_88, [1, arg0_1, 14336]);  mm_88 = None
        convert_element_type_205: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_439, torch.float32);  view_439 = None
        sigmoid_12: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_205)
        mul_5334: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_205, sigmoid_12);  convert_element_type_205 = sigmoid_12 = None
        convert_element_type_206: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_5334, torch.float16);  mul_5334 = None
        permute_142: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg136_1, [1, 0]);  arg136_1 = None
        alias_246: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_102);  getitem_102 = None
        view_441: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_246, [1, arg0_1, 4096]);  alias_246 = None
        view_442: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_441, [arg0_1, 4096]);  view_441 = None
        mm_89: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_442, permute_142);  view_442 = permute_142 = None
        view_443: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_89, [1, arg0_1, 14336]);  mm_89 = None
        mul_5351: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_206, view_443);  convert_element_type_206 = view_443 = None
        permute_143: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg137_1, [1, 0]);  arg137_1 = None
        view_444: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_5351, [arg0_1, 14336]);  mul_5351 = None
        mm_90: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_444, permute_143);  view_444 = permute_143 = None
        view_445: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_90, [1, arg0_1, 4096]);  mm_90 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_4318: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_4265, view_445);  add_4265 = view_445 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_446: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_4318, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_52: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_247: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_52);  empty_52 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_53: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_248: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_53);  empty_53 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_26 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 27, constant_args_idx = 26, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_247, 'X_ptr': view_446, 'W_ptr': arg138_1, 'RSTD_ptr': alias_248}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_247 = view_446 = arg138_1 = alias_248 = None
        getitem_104: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_26['Y_ptr'];  triton_kernel_wrapper_functional_proxy_26 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_144: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg139_1, [1, 0]);  arg139_1 = None
        alias_251: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_104)
        view_449: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_251, [1, arg0_1, 4096]);  alias_251 = None
        view_450: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_449, [arg0_1, 4096]);  view_449 = None
        mm_91: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_450, permute_144);  view_450 = permute_144 = None
        view_451: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_91, [1, arg0_1, 4096]);  mm_91 = None
        view_452: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_451, [1, arg0_1, -1, 128]);  view_451 = None
        permute_145: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_452, [0, 2, 1, 3]);  view_452 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_146: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg140_1, [1, 0]);  arg140_1 = None
        alias_252: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_104)
        view_454: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_252, [1, arg0_1, 4096]);  alias_252 = None
        view_455: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_454, [arg0_1, 4096]);  view_454 = None
        mm_92: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_455, permute_146);  view_455 = permute_146 = None
        view_456: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_92, [1, arg0_1, 1024]);  mm_92 = None
        view_457: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_456, [1, arg0_1, -1, 128]);  view_456 = None
        permute_147: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_457, [0, 2, 1, 3]);  view_457 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_148: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg141_1, [1, 0]);  arg141_1 = None
        alias_253: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_104);  getitem_104 = None
        view_459: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_253, [1, arg0_1, 4096]);  alias_253 = None
        view_460: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_459, [arg0_1, 4096]);  view_459 = None
        mm_93: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_460, permute_148);  view_460 = permute_148 = None
        view_461: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_93, [1, arg0_1, 1024]);  mm_93 = None
        view_462: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_461, [1, arg0_1, -1, 128]);  view_461 = None
        permute_149: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_462, [0, 2, 1, 3]);  view_462 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_82: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_83: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5453: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_145, unsqueeze_82)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_303: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_145, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_304: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_145, 3, 64, 9223372036854775807);  permute_145 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_26: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_304);  slice_304 = None
        cat_26: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_26, slice_303], -1);  neg_26 = slice_303 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5470: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_26, unsqueeze_83);  cat_26 = None
        add_4412: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_5453, mul_5470);  mul_5453 = mul_5470 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_5478: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_147, unsqueeze_82);  unsqueeze_82 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_305: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_147, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_306: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_147, 3, 64, 9223372036854775807);  permute_147 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_27: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_306);  slice_306 = None
        cat_27: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_27, slice_305], -1);  neg_27 = slice_305 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_5495: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_27, unsqueeze_83);  cat_27 = unsqueeze_83 = None
        add_4436: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_5478, mul_5495);  mul_5478 = mul_5495 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_26: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg142_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_254: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_26);  full_26 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_27: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg142_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg142_1 = None
        alias_255: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_27);  full_27 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_26: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_254, [None, None, arg3_1], add_4436);  alias_254 = add_4436 = None
        alias_256: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_26)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_27: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_255, [None, None, arg3_1], permute_149);  alias_255 = permute_149 = None
        alias_257: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_27)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_258: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_26);  index_put_26 = None
        slice_311: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_258, 0, 0, 9223372036854775807);  alias_258 = None
        slice_312: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_311, 1, 0, 9223372036854775807);  slice_311 = None
        unsqueeze_85: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_312, 2);  slice_312 = None
        slice_313: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_85, 3, 0, 9223372036854775807);  unsqueeze_85 = None
        slice_314: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_313, 4, 0, 9223372036854775807);  slice_313 = None
        expand_70: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_314, [1, 8, 4, arg5_1, 128]);  slice_314 = None
        clone_41: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_70, memory_format = torch.contiguous_format);  expand_70 = None
        view_463: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_41, [1, 32, arg5_1, 128]);  clone_41 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_259: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_27);  index_put_27 = None
        slice_319: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_259, 0, 0, 9223372036854775807);  alias_259 = None
        slice_320: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_319, 1, 0, 9223372036854775807);  slice_319 = None
        unsqueeze_87: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_320, 2);  slice_320 = None
        slice_321: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_87, 3, 0, 9223372036854775807);  unsqueeze_87 = None
        slice_322: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_321, 4, 0, 9223372036854775807);  slice_321 = None
        expand_72: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_322, [1, 8, 4, arg5_1, 128]);  slice_322 = None
        clone_42: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_72, memory_format = torch.contiguous_format);  expand_72 = None
        view_464: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_42, [1, 32, arg5_1, 128]);  clone_42 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_323: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_324: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_323, 1, 0, 9223372036854775807);  slice_323 = None
        slice_325: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_324, 2, 0, 9223372036854775807);  slice_324 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_43: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_4412, memory_format = torch.contiguous_format);  add_4412 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_26: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_27: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_13: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_325, scalar_tensor_27, scalar_tensor_26);  slice_325 = scalar_tensor_27 = scalar_tensor_26 = None
        expand_73: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_13, [1, 32, arg0_1, arg5_1]);  where_13 = None
        _scaled_dot_product_efficient_attention_13 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_43, view_463, view_464, expand_73, False, scale = 0.08838834764831845);  clone_43 = view_463 = view_464 = expand_73 = None
        getitem_106: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_13[0];  _scaled_dot_product_efficient_attention_13 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_150: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_106, [0, 2, 1, 3]);  getitem_106 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_465: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_150, [1, arg0_1, -1]);  permute_150 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_151: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg143_1, [1, 0]);  arg143_1 = None
        view_466: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_465, [arg0_1, 4096]);  view_465 = None
        mm_94: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_466, permute_151);  view_466 = permute_151 = None
        view_467: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_94, [1, arg0_1, 4096]);  mm_94 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_4594: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_4318, view_467);  add_4318 = view_467 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_468: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_4594, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_54: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_260: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_54);  empty_54 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_55: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_261: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_55);  empty_55 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_27 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 28, constant_args_idx = 27, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_260, 'X_ptr': view_468, 'W_ptr': arg144_1, 'RSTD_ptr': alias_261}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_260 = view_468 = arg144_1 = alias_261 = None
        getitem_110: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_27['Y_ptr'];  triton_kernel_wrapper_functional_proxy_27 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_152: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg145_1, [1, 0]);  arg145_1 = None
        alias_264: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_110)
        view_471: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_264, [1, arg0_1, 4096]);  alias_264 = None
        view_472: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_471, [arg0_1, 4096]);  view_471 = None
        mm_95: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_472, permute_152);  view_472 = permute_152 = None
        view_473: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_95, [1, arg0_1, 14336]);  mm_95 = None
        convert_element_type_221: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_473, torch.float32);  view_473 = None
        sigmoid_13: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_221)
        mul_5743: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_221, sigmoid_13);  convert_element_type_221 = sigmoid_13 = None
        convert_element_type_222: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_5743, torch.float16);  mul_5743 = None
        permute_153: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg146_1, [1, 0]);  arg146_1 = None
        alias_265: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_110);  getitem_110 = None
        view_475: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_265, [1, arg0_1, 4096]);  alias_265 = None
        view_476: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_475, [arg0_1, 4096]);  view_475 = None
        mm_96: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_476, permute_153);  view_476 = permute_153 = None
        view_477: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_96, [1, arg0_1, 14336]);  mm_96 = None
        mul_5760: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_222, view_477);  convert_element_type_222 = view_477 = None
        permute_154: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg147_1, [1, 0]);  arg147_1 = None
        view_478: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_5760, [arg0_1, 14336]);  mul_5760 = None
        mm_97: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_478, permute_154);  view_478 = permute_154 = None
        view_479: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_97, [1, arg0_1, 4096]);  mm_97 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_4647: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_4594, view_479);  add_4594 = view_479 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_480: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_4647, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_56: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_266: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_56);  empty_56 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_57: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_267: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_57);  empty_57 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_28 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 29, constant_args_idx = 28, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_266, 'X_ptr': view_480, 'W_ptr': arg148_1, 'RSTD_ptr': alias_267}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_266 = view_480 = arg148_1 = alias_267 = None
        getitem_112: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_28['Y_ptr'];  triton_kernel_wrapper_functional_proxy_28 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_155: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg149_1, [1, 0]);  arg149_1 = None
        alias_270: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_112)
        view_483: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_270, [1, arg0_1, 4096]);  alias_270 = None
        view_484: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_483, [arg0_1, 4096]);  view_483 = None
        mm_98: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_484, permute_155);  view_484 = permute_155 = None
        view_485: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_98, [1, arg0_1, 4096]);  mm_98 = None
        view_486: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_485, [1, arg0_1, -1, 128]);  view_485 = None
        permute_156: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_486, [0, 2, 1, 3]);  view_486 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_157: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg150_1, [1, 0]);  arg150_1 = None
        alias_271: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_112)
        view_488: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_271, [1, arg0_1, 4096]);  alias_271 = None
        view_489: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_488, [arg0_1, 4096]);  view_488 = None
        mm_99: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_489, permute_157);  view_489 = permute_157 = None
        view_490: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_99, [1, arg0_1, 1024]);  mm_99 = None
        view_491: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_490, [1, arg0_1, -1, 128]);  view_490 = None
        permute_158: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_491, [0, 2, 1, 3]);  view_491 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_159: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg151_1, [1, 0]);  arg151_1 = None
        alias_272: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_112);  getitem_112 = None
        view_493: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_272, [1, arg0_1, 4096]);  alias_272 = None
        view_494: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_493, [arg0_1, 4096]);  view_493 = None
        mm_100: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_494, permute_159);  view_494 = permute_159 = None
        view_495: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_100, [1, arg0_1, 1024]);  mm_100 = None
        view_496: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_495, [1, arg0_1, -1, 128]);  view_495 = None
        permute_160: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_496, [0, 2, 1, 3]);  view_496 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_88: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_89: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5862: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_156, unsqueeze_88)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_326: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_156, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_327: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_156, 3, 64, 9223372036854775807);  permute_156 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_28: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_327);  slice_327 = None
        cat_28: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_28, slice_326], -1);  neg_28 = slice_326 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5879: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_28, unsqueeze_89);  cat_28 = None
        add_4741: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_5862, mul_5879);  mul_5862 = mul_5879 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_5887: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_158, unsqueeze_88);  unsqueeze_88 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_328: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_158, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_329: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_158, 3, 64, 9223372036854775807);  permute_158 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_29: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_329);  slice_329 = None
        cat_29: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_29, slice_328], -1);  neg_29 = slice_328 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_5904: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_29, unsqueeze_89);  cat_29 = unsqueeze_89 = None
        add_4765: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_5887, mul_5904);  mul_5887 = mul_5904 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_28: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg152_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_273: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_28);  full_28 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_29: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg152_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg152_1 = None
        alias_274: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_29);  full_29 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_28: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_273, [None, None, arg3_1], add_4765);  alias_273 = add_4765 = None
        alias_275: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_28)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_29: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_274, [None, None, arg3_1], permute_160);  alias_274 = permute_160 = None
        alias_276: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_29)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_277: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_28);  index_put_28 = None
        slice_334: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_277, 0, 0, 9223372036854775807);  alias_277 = None
        slice_335: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_334, 1, 0, 9223372036854775807);  slice_334 = None
        unsqueeze_91: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_335, 2);  slice_335 = None
        slice_336: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_91, 3, 0, 9223372036854775807);  unsqueeze_91 = None
        slice_337: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_336, 4, 0, 9223372036854775807);  slice_336 = None
        expand_75: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_337, [1, 8, 4, arg5_1, 128]);  slice_337 = None
        clone_44: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_75, memory_format = torch.contiguous_format);  expand_75 = None
        view_497: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_44, [1, 32, arg5_1, 128]);  clone_44 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_278: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_29);  index_put_29 = None
        slice_342: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_278, 0, 0, 9223372036854775807);  alias_278 = None
        slice_343: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_342, 1, 0, 9223372036854775807);  slice_342 = None
        unsqueeze_93: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_343, 2);  slice_343 = None
        slice_344: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_93, 3, 0, 9223372036854775807);  unsqueeze_93 = None
        slice_345: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_344, 4, 0, 9223372036854775807);  slice_344 = None
        expand_77: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_345, [1, 8, 4, arg5_1, 128]);  slice_345 = None
        clone_45: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_77, memory_format = torch.contiguous_format);  expand_77 = None
        view_498: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_45, [1, 32, arg5_1, 128]);  clone_45 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_346: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_347: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_346, 1, 0, 9223372036854775807);  slice_346 = None
        slice_348: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_347, 2, 0, 9223372036854775807);  slice_347 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_46: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_4741, memory_format = torch.contiguous_format);  add_4741 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_28: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_29: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_14: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_348, scalar_tensor_29, scalar_tensor_28);  slice_348 = scalar_tensor_29 = scalar_tensor_28 = None
        expand_78: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_14, [1, 32, arg0_1, arg5_1]);  where_14 = None
        _scaled_dot_product_efficient_attention_14 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_46, view_497, view_498, expand_78, False, scale = 0.08838834764831845);  clone_46 = view_497 = view_498 = expand_78 = None
        getitem_114: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_14[0];  _scaled_dot_product_efficient_attention_14 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_161: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_114, [0, 2, 1, 3]);  getitem_114 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_499: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_161, [1, arg0_1, -1]);  permute_161 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_162: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg153_1, [1, 0]);  arg153_1 = None
        view_500: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_499, [arg0_1, 4096]);  view_499 = None
        mm_101: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_500, permute_162);  view_500 = permute_162 = None
        view_501: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_101, [1, arg0_1, 4096]);  mm_101 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_4923: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_4647, view_501);  add_4647 = view_501 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_502: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_4923, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_58: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_279: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_58);  empty_58 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_59: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_280: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_59);  empty_59 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_29 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 30, constant_args_idx = 29, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_279, 'X_ptr': view_502, 'W_ptr': arg154_1, 'RSTD_ptr': alias_280}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_279 = view_502 = arg154_1 = alias_280 = None
        getitem_118: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_29['Y_ptr'];  triton_kernel_wrapper_functional_proxy_29 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_163: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg155_1, [1, 0]);  arg155_1 = None
        alias_283: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_118)
        view_505: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_283, [1, arg0_1, 4096]);  alias_283 = None
        view_506: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_505, [arg0_1, 4096]);  view_505 = None
        mm_102: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_506, permute_163);  view_506 = permute_163 = None
        view_507: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_102, [1, arg0_1, 14336]);  mm_102 = None
        convert_element_type_237: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_507, torch.float32);  view_507 = None
        sigmoid_14: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_237)
        mul_6152: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_237, sigmoid_14);  convert_element_type_237 = sigmoid_14 = None
        convert_element_type_238: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_6152, torch.float16);  mul_6152 = None
        permute_164: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg156_1, [1, 0]);  arg156_1 = None
        alias_284: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_118);  getitem_118 = None
        view_509: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_284, [1, arg0_1, 4096]);  alias_284 = None
        view_510: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_509, [arg0_1, 4096]);  view_509 = None
        mm_103: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_510, permute_164);  view_510 = permute_164 = None
        view_511: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_103, [1, arg0_1, 14336]);  mm_103 = None
        mul_6169: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_238, view_511);  convert_element_type_238 = view_511 = None
        permute_165: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg157_1, [1, 0]);  arg157_1 = None
        view_512: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_6169, [arg0_1, 14336]);  mul_6169 = None
        mm_104: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_512, permute_165);  view_512 = permute_165 = None
        view_513: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_104, [1, arg0_1, 4096]);  mm_104 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_4976: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_4923, view_513);  add_4923 = view_513 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_514: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_4976, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_60: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_285: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_60);  empty_60 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_61: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_286: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_61);  empty_61 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_30 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 31, constant_args_idx = 30, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_285, 'X_ptr': view_514, 'W_ptr': arg158_1, 'RSTD_ptr': alias_286}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_285 = view_514 = arg158_1 = alias_286 = None
        getitem_120: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_30['Y_ptr'];  triton_kernel_wrapper_functional_proxy_30 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_166: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg159_1, [1, 0]);  arg159_1 = None
        alias_289: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_120)
        view_517: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_289, [1, arg0_1, 4096]);  alias_289 = None
        view_518: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_517, [arg0_1, 4096]);  view_517 = None
        mm_105: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_518, permute_166);  view_518 = permute_166 = None
        view_519: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_105, [1, arg0_1, 4096]);  mm_105 = None
        view_520: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_519, [1, arg0_1, -1, 128]);  view_519 = None
        permute_167: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_520, [0, 2, 1, 3]);  view_520 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_168: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg160_1, [1, 0]);  arg160_1 = None
        alias_290: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_120)
        view_522: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_290, [1, arg0_1, 4096]);  alias_290 = None
        view_523: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_522, [arg0_1, 4096]);  view_522 = None
        mm_106: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_523, permute_168);  view_523 = permute_168 = None
        view_524: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_106, [1, arg0_1, 1024]);  mm_106 = None
        view_525: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_524, [1, arg0_1, -1, 128]);  view_524 = None
        permute_169: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_525, [0, 2, 1, 3]);  view_525 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_170: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg161_1, [1, 0]);  arg161_1 = None
        alias_291: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_120);  getitem_120 = None
        view_527: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_291, [1, arg0_1, 4096]);  alias_291 = None
        view_528: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_527, [arg0_1, 4096]);  view_527 = None
        mm_107: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_528, permute_170);  view_528 = permute_170 = None
        view_529: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_107, [1, arg0_1, 1024]);  mm_107 = None
        view_530: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_529, [1, arg0_1, -1, 128]);  view_529 = None
        permute_171: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_530, [0, 2, 1, 3]);  view_530 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_94: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_95: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_6271: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_167, unsqueeze_94)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_349: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_167, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_350: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_167, 3, 64, 9223372036854775807);  permute_167 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_30: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_350);  slice_350 = None
        cat_30: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_30, slice_349], -1);  neg_30 = slice_349 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_6288: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_30, unsqueeze_95);  cat_30 = None
        add_5070: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_6271, mul_6288);  mul_6271 = mul_6288 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6296: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_169, unsqueeze_94);  unsqueeze_94 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_351: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_169, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_352: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_169, 3, 64, 9223372036854775807);  permute_169 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_31: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_352);  slice_352 = None
        cat_31: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_31, slice_351], -1);  neg_31 = slice_351 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6313: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_31, unsqueeze_95);  cat_31 = unsqueeze_95 = None
        add_5094: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_6296, mul_6313);  mul_6296 = mul_6313 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_30: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg162_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_292: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_30);  full_30 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_31: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg162_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg162_1 = None
        alias_293: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_31);  full_31 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_30: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_292, [None, None, arg3_1], add_5094);  alias_292 = add_5094 = None
        alias_294: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_30)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_31: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_293, [None, None, arg3_1], permute_171);  alias_293 = permute_171 = None
        alias_295: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_31)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_296: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_30);  index_put_30 = None
        slice_357: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_296, 0, 0, 9223372036854775807);  alias_296 = None
        slice_358: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_357, 1, 0, 9223372036854775807);  slice_357 = None
        unsqueeze_97: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_358, 2);  slice_358 = None
        slice_359: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_97, 3, 0, 9223372036854775807);  unsqueeze_97 = None
        slice_360: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_359, 4, 0, 9223372036854775807);  slice_359 = None
        expand_80: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_360, [1, 8, 4, arg5_1, 128]);  slice_360 = None
        clone_47: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_80, memory_format = torch.contiguous_format);  expand_80 = None
        view_531: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_47, [1, 32, arg5_1, 128]);  clone_47 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_297: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_31);  index_put_31 = None
        slice_365: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_297, 0, 0, 9223372036854775807);  alias_297 = None
        slice_366: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_365, 1, 0, 9223372036854775807);  slice_365 = None
        unsqueeze_99: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_366, 2);  slice_366 = None
        slice_367: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_99, 3, 0, 9223372036854775807);  unsqueeze_99 = None
        slice_368: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_367, 4, 0, 9223372036854775807);  slice_367 = None
        expand_82: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_368, [1, 8, 4, arg5_1, 128]);  slice_368 = None
        clone_48: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_82, memory_format = torch.contiguous_format);  expand_82 = None
        view_532: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_48, [1, 32, arg5_1, 128]);  clone_48 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_369: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_370: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_369, 1, 0, 9223372036854775807);  slice_369 = None
        slice_371: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_370, 2, 0, 9223372036854775807);  slice_370 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_49: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_5070, memory_format = torch.contiguous_format);  add_5070 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_30: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_31: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_15: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_371, scalar_tensor_31, scalar_tensor_30);  slice_371 = scalar_tensor_31 = scalar_tensor_30 = None
        expand_83: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_15, [1, 32, arg0_1, arg5_1]);  where_15 = None
        _scaled_dot_product_efficient_attention_15 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_49, view_531, view_532, expand_83, False, scale = 0.08838834764831845);  clone_49 = view_531 = view_532 = expand_83 = None
        getitem_122: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_15[0];  _scaled_dot_product_efficient_attention_15 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_172: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_122, [0, 2, 1, 3]);  getitem_122 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_533: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_172, [1, arg0_1, -1]);  permute_172 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_173: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg163_1, [1, 0]);  arg163_1 = None
        view_534: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_533, [arg0_1, 4096]);  view_533 = None
        mm_108: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_534, permute_173);  view_534 = permute_173 = None
        view_535: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_108, [1, arg0_1, 4096]);  mm_108 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_5252: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_4976, view_535);  add_4976 = view_535 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_536: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_5252, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_62: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_298: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_62);  empty_62 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_63: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_299: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_63);  empty_63 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_31 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 32, constant_args_idx = 31, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_298, 'X_ptr': view_536, 'W_ptr': arg164_1, 'RSTD_ptr': alias_299}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_298 = view_536 = arg164_1 = alias_299 = None
        getitem_126: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_31['Y_ptr'];  triton_kernel_wrapper_functional_proxy_31 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_174: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg165_1, [1, 0]);  arg165_1 = None
        alias_302: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_126)
        view_539: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_302, [1, arg0_1, 4096]);  alias_302 = None
        view_540: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_539, [arg0_1, 4096]);  view_539 = None
        mm_109: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_540, permute_174);  view_540 = permute_174 = None
        view_541: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_109, [1, arg0_1, 14336]);  mm_109 = None
        convert_element_type_253: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_541, torch.float32);  view_541 = None
        sigmoid_15: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_253)
        mul_6561: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_253, sigmoid_15);  convert_element_type_253 = sigmoid_15 = None
        convert_element_type_254: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_6561, torch.float16);  mul_6561 = None
        permute_175: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg166_1, [1, 0]);  arg166_1 = None
        alias_303: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_126);  getitem_126 = None
        view_543: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_303, [1, arg0_1, 4096]);  alias_303 = None
        view_544: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_543, [arg0_1, 4096]);  view_543 = None
        mm_110: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_544, permute_175);  view_544 = permute_175 = None
        view_545: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_110, [1, arg0_1, 14336]);  mm_110 = None
        mul_6578: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_254, view_545);  convert_element_type_254 = view_545 = None
        permute_176: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg167_1, [1, 0]);  arg167_1 = None
        view_546: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_6578, [arg0_1, 14336]);  mul_6578 = None
        mm_111: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_546, permute_176);  view_546 = permute_176 = None
        view_547: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_111, [1, arg0_1, 4096]);  mm_111 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_5305: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_5252, view_547);  add_5252 = view_547 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_548: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_5305, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_64: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_304: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_64);  empty_64 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_65: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_305: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_65);  empty_65 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_32 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 33, constant_args_idx = 32, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_304, 'X_ptr': view_548, 'W_ptr': arg168_1, 'RSTD_ptr': alias_305}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_304 = view_548 = arg168_1 = alias_305 = None
        getitem_128: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_32['Y_ptr'];  triton_kernel_wrapper_functional_proxy_32 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_177: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg169_1, [1, 0]);  arg169_1 = None
        alias_308: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_128)
        view_551: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_308, [1, arg0_1, 4096]);  alias_308 = None
        view_552: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_551, [arg0_1, 4096]);  view_551 = None
        mm_112: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_552, permute_177);  view_552 = permute_177 = None
        view_553: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_112, [1, arg0_1, 4096]);  mm_112 = None
        view_554: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_553, [1, arg0_1, -1, 128]);  view_553 = None
        permute_178: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_554, [0, 2, 1, 3]);  view_554 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_179: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg170_1, [1, 0]);  arg170_1 = None
        alias_309: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_128)
        view_556: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_309, [1, arg0_1, 4096]);  alias_309 = None
        view_557: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_556, [arg0_1, 4096]);  view_556 = None
        mm_113: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_557, permute_179);  view_557 = permute_179 = None
        view_558: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_113, [1, arg0_1, 1024]);  mm_113 = None
        view_559: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_558, [1, arg0_1, -1, 128]);  view_558 = None
        permute_180: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_559, [0, 2, 1, 3]);  view_559 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_181: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg171_1, [1, 0]);  arg171_1 = None
        alias_310: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_128);  getitem_128 = None
        view_561: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_310, [1, arg0_1, 4096]);  alias_310 = None
        view_562: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_561, [arg0_1, 4096]);  view_561 = None
        mm_114: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_562, permute_181);  view_562 = permute_181 = None
        view_563: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_114, [1, arg0_1, 1024]);  mm_114 = None
        view_564: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_563, [1, arg0_1, -1, 128]);  view_563 = None
        permute_182: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_564, [0, 2, 1, 3]);  view_564 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_100: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_101: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_6680: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_178, unsqueeze_100)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_372: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_178, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_373: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_178, 3, 64, 9223372036854775807);  permute_178 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_32: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_373);  slice_373 = None
        cat_32: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_32, slice_372], -1);  neg_32 = slice_372 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_6697: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_32, unsqueeze_101);  cat_32 = None
        add_5399: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_6680, mul_6697);  mul_6680 = mul_6697 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6705: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_180, unsqueeze_100);  unsqueeze_100 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_374: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_180, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_375: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_180, 3, 64, 9223372036854775807);  permute_180 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_33: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_375);  slice_375 = None
        cat_33: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_33, slice_374], -1);  neg_33 = slice_374 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6722: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_33, unsqueeze_101);  cat_33 = unsqueeze_101 = None
        add_5423: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_6705, mul_6722);  mul_6705 = mul_6722 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_32: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg172_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_311: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_32);  full_32 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_33: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg172_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg172_1 = None
        alias_312: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_33);  full_33 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_32: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_311, [None, None, arg3_1], add_5423);  alias_311 = add_5423 = None
        alias_313: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_32)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_33: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_312, [None, None, arg3_1], permute_182);  alias_312 = permute_182 = None
        alias_314: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_33)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_315: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_32);  index_put_32 = None
        slice_380: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_315, 0, 0, 9223372036854775807);  alias_315 = None
        slice_381: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_380, 1, 0, 9223372036854775807);  slice_380 = None
        unsqueeze_103: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_381, 2);  slice_381 = None
        slice_382: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_103, 3, 0, 9223372036854775807);  unsqueeze_103 = None
        slice_383: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_382, 4, 0, 9223372036854775807);  slice_382 = None
        expand_85: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_383, [1, 8, 4, arg5_1, 128]);  slice_383 = None
        clone_50: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_85, memory_format = torch.contiguous_format);  expand_85 = None
        view_565: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_50, [1, 32, arg5_1, 128]);  clone_50 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_316: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_33);  index_put_33 = None
        slice_388: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_316, 0, 0, 9223372036854775807);  alias_316 = None
        slice_389: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_388, 1, 0, 9223372036854775807);  slice_388 = None
        unsqueeze_105: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_389, 2);  slice_389 = None
        slice_390: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_105, 3, 0, 9223372036854775807);  unsqueeze_105 = None
        slice_391: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_390, 4, 0, 9223372036854775807);  slice_390 = None
        expand_87: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_391, [1, 8, 4, arg5_1, 128]);  slice_391 = None
        clone_51: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_87, memory_format = torch.contiguous_format);  expand_87 = None
        view_566: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_51, [1, 32, arg5_1, 128]);  clone_51 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_392: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_393: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_392, 1, 0, 9223372036854775807);  slice_392 = None
        slice_394: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_393, 2, 0, 9223372036854775807);  slice_393 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_52: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_5399, memory_format = torch.contiguous_format);  add_5399 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_32: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_33: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_16: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_394, scalar_tensor_33, scalar_tensor_32);  slice_394 = scalar_tensor_33 = scalar_tensor_32 = None
        expand_88: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_16, [1, 32, arg0_1, arg5_1]);  where_16 = None
        _scaled_dot_product_efficient_attention_16 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_52, view_565, view_566, expand_88, False, scale = 0.08838834764831845);  clone_52 = view_565 = view_566 = expand_88 = None
        getitem_130: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_16[0];  _scaled_dot_product_efficient_attention_16 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_183: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_130, [0, 2, 1, 3]);  getitem_130 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_567: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_183, [1, arg0_1, -1]);  permute_183 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_184: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg173_1, [1, 0]);  arg173_1 = None
        view_568: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_567, [arg0_1, 4096]);  view_567 = None
        mm_115: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_568, permute_184);  view_568 = permute_184 = None
        view_569: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_115, [1, arg0_1, 4096]);  mm_115 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_5581: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_5305, view_569);  add_5305 = view_569 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_570: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_5581, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_66: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_317: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_66);  empty_66 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_67: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_318: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_67);  empty_67 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_33 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 34, constant_args_idx = 33, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_317, 'X_ptr': view_570, 'W_ptr': arg174_1, 'RSTD_ptr': alias_318}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_317 = view_570 = arg174_1 = alias_318 = None
        getitem_134: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_33['Y_ptr'];  triton_kernel_wrapper_functional_proxy_33 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_185: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg175_1, [1, 0]);  arg175_1 = None
        alias_321: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_134)
        view_573: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_321, [1, arg0_1, 4096]);  alias_321 = None
        view_574: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_573, [arg0_1, 4096]);  view_573 = None
        mm_116: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_574, permute_185);  view_574 = permute_185 = None
        view_575: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_116, [1, arg0_1, 14336]);  mm_116 = None
        convert_element_type_269: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_575, torch.float32);  view_575 = None
        sigmoid_16: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_269)
        mul_6970: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_269, sigmoid_16);  convert_element_type_269 = sigmoid_16 = None
        convert_element_type_270: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_6970, torch.float16);  mul_6970 = None
        permute_186: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg176_1, [1, 0]);  arg176_1 = None
        alias_322: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_134);  getitem_134 = None
        view_577: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_322, [1, arg0_1, 4096]);  alias_322 = None
        view_578: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_577, [arg0_1, 4096]);  view_577 = None
        mm_117: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_578, permute_186);  view_578 = permute_186 = None
        view_579: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_117, [1, arg0_1, 14336]);  mm_117 = None
        mul_6987: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_270, view_579);  convert_element_type_270 = view_579 = None
        permute_187: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg177_1, [1, 0]);  arg177_1 = None
        view_580: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_6987, [arg0_1, 14336]);  mul_6987 = None
        mm_118: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_580, permute_187);  view_580 = permute_187 = None
        view_581: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_118, [1, arg0_1, 4096]);  mm_118 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_5634: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_5581, view_581);  add_5581 = view_581 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_582: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_5634, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_68: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_323: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_68);  empty_68 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_69: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_324: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_69);  empty_69 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_34 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 35, constant_args_idx = 34, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_323, 'X_ptr': view_582, 'W_ptr': arg178_1, 'RSTD_ptr': alias_324}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_323 = view_582 = arg178_1 = alias_324 = None
        getitem_136: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_34['Y_ptr'];  triton_kernel_wrapper_functional_proxy_34 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_188: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg179_1, [1, 0]);  arg179_1 = None
        alias_327: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_136)
        view_585: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_327, [1, arg0_1, 4096]);  alias_327 = None
        view_586: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_585, [arg0_1, 4096]);  view_585 = None
        mm_119: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_586, permute_188);  view_586 = permute_188 = None
        view_587: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_119, [1, arg0_1, 4096]);  mm_119 = None
        view_588: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_587, [1, arg0_1, -1, 128]);  view_587 = None
        permute_189: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_588, [0, 2, 1, 3]);  view_588 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_190: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg180_1, [1, 0]);  arg180_1 = None
        alias_328: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_136)
        view_590: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_328, [1, arg0_1, 4096]);  alias_328 = None
        view_591: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_590, [arg0_1, 4096]);  view_590 = None
        mm_120: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_591, permute_190);  view_591 = permute_190 = None
        view_592: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_120, [1, arg0_1, 1024]);  mm_120 = None
        view_593: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_592, [1, arg0_1, -1, 128]);  view_592 = None
        permute_191: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_593, [0, 2, 1, 3]);  view_593 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_192: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg181_1, [1, 0]);  arg181_1 = None
        alias_329: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_136);  getitem_136 = None
        view_595: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_329, [1, arg0_1, 4096]);  alias_329 = None
        view_596: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_595, [arg0_1, 4096]);  view_595 = None
        mm_121: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_596, permute_192);  view_596 = permute_192 = None
        view_597: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_121, [1, arg0_1, 1024]);  mm_121 = None
        view_598: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_597, [1, arg0_1, -1, 128]);  view_597 = None
        permute_193: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_598, [0, 2, 1, 3]);  view_598 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_106: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_107: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_7089: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_189, unsqueeze_106)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_395: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_189, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_396: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_189, 3, 64, 9223372036854775807);  permute_189 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_34: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_396);  slice_396 = None
        cat_34: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_34, slice_395], -1);  neg_34 = slice_395 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_7106: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_34, unsqueeze_107);  cat_34 = None
        add_5728: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_7089, mul_7106);  mul_7089 = mul_7106 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7114: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_191, unsqueeze_106);  unsqueeze_106 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_397: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_191, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_398: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_191, 3, 64, 9223372036854775807);  permute_191 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_35: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_398);  slice_398 = None
        cat_35: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_35, slice_397], -1);  neg_35 = slice_397 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7131: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_35, unsqueeze_107);  cat_35 = unsqueeze_107 = None
        add_5752: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_7114, mul_7131);  mul_7114 = mul_7131 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_34: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg182_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_330: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_34);  full_34 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_35: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg182_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg182_1 = None
        alias_331: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_35);  full_35 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_34: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_330, [None, None, arg3_1], add_5752);  alias_330 = add_5752 = None
        alias_332: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_34)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_35: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_331, [None, None, arg3_1], permute_193);  alias_331 = permute_193 = None
        alias_333: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_35)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_334: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_34);  index_put_34 = None
        slice_403: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_334, 0, 0, 9223372036854775807);  alias_334 = None
        slice_404: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_403, 1, 0, 9223372036854775807);  slice_403 = None
        unsqueeze_109: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_404, 2);  slice_404 = None
        slice_405: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_109, 3, 0, 9223372036854775807);  unsqueeze_109 = None
        slice_406: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_405, 4, 0, 9223372036854775807);  slice_405 = None
        expand_90: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_406, [1, 8, 4, arg5_1, 128]);  slice_406 = None
        clone_53: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_90, memory_format = torch.contiguous_format);  expand_90 = None
        view_599: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_53, [1, 32, arg5_1, 128]);  clone_53 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_335: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_35);  index_put_35 = None
        slice_411: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_335, 0, 0, 9223372036854775807);  alias_335 = None
        slice_412: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_411, 1, 0, 9223372036854775807);  slice_411 = None
        unsqueeze_111: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_412, 2);  slice_412 = None
        slice_413: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_111, 3, 0, 9223372036854775807);  unsqueeze_111 = None
        slice_414: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_413, 4, 0, 9223372036854775807);  slice_413 = None
        expand_92: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_414, [1, 8, 4, arg5_1, 128]);  slice_414 = None
        clone_54: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_92, memory_format = torch.contiguous_format);  expand_92 = None
        view_600: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_54, [1, 32, arg5_1, 128]);  clone_54 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_415: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_416: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_415, 1, 0, 9223372036854775807);  slice_415 = None
        slice_417: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_416, 2, 0, 9223372036854775807);  slice_416 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_55: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_5728, memory_format = torch.contiguous_format);  add_5728 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_34: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_35: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_17: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_417, scalar_tensor_35, scalar_tensor_34);  slice_417 = scalar_tensor_35 = scalar_tensor_34 = None
        expand_93: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_17, [1, 32, arg0_1, arg5_1]);  where_17 = None
        _scaled_dot_product_efficient_attention_17 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_55, view_599, view_600, expand_93, False, scale = 0.08838834764831845);  clone_55 = view_599 = view_600 = expand_93 = None
        getitem_138: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_17[0];  _scaled_dot_product_efficient_attention_17 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_194: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_138, [0, 2, 1, 3]);  getitem_138 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_601: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_194, [1, arg0_1, -1]);  permute_194 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_195: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg183_1, [1, 0]);  arg183_1 = None
        view_602: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_601, [arg0_1, 4096]);  view_601 = None
        mm_122: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_602, permute_195);  view_602 = permute_195 = None
        view_603: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_122, [1, arg0_1, 4096]);  mm_122 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_5910: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_5634, view_603);  add_5634 = view_603 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_604: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_5910, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_70: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_336: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_70);  empty_70 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_71: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_337: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_71);  empty_71 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_35 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 36, constant_args_idx = 35, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_336, 'X_ptr': view_604, 'W_ptr': arg184_1, 'RSTD_ptr': alias_337}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_336 = view_604 = arg184_1 = alias_337 = None
        getitem_142: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_35['Y_ptr'];  triton_kernel_wrapper_functional_proxy_35 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_196: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg185_1, [1, 0]);  arg185_1 = None
        alias_340: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_142)
        view_607: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_340, [1, arg0_1, 4096]);  alias_340 = None
        view_608: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_607, [arg0_1, 4096]);  view_607 = None
        mm_123: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_608, permute_196);  view_608 = permute_196 = None
        view_609: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_123, [1, arg0_1, 14336]);  mm_123 = None
        convert_element_type_285: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_609, torch.float32);  view_609 = None
        sigmoid_17: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_285)
        mul_7379: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_285, sigmoid_17);  convert_element_type_285 = sigmoid_17 = None
        convert_element_type_286: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_7379, torch.float16);  mul_7379 = None
        permute_197: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg186_1, [1, 0]);  arg186_1 = None
        alias_341: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_142);  getitem_142 = None
        view_611: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_341, [1, arg0_1, 4096]);  alias_341 = None
        view_612: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_611, [arg0_1, 4096]);  view_611 = None
        mm_124: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_612, permute_197);  view_612 = permute_197 = None
        view_613: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_124, [1, arg0_1, 14336]);  mm_124 = None
        mul_7396: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_286, view_613);  convert_element_type_286 = view_613 = None
        permute_198: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg187_1, [1, 0]);  arg187_1 = None
        view_614: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_7396, [arg0_1, 14336]);  mul_7396 = None
        mm_125: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_614, permute_198);  view_614 = permute_198 = None
        view_615: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_125, [1, arg0_1, 4096]);  mm_125 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_5963: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_5910, view_615);  add_5910 = view_615 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_616: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_5963, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_72: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_342: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_72);  empty_72 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_73: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_343: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_73);  empty_73 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_36 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 37, constant_args_idx = 36, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_342, 'X_ptr': view_616, 'W_ptr': arg188_1, 'RSTD_ptr': alias_343}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_342 = view_616 = arg188_1 = alias_343 = None
        getitem_144: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_36['Y_ptr'];  triton_kernel_wrapper_functional_proxy_36 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_199: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg189_1, [1, 0]);  arg189_1 = None
        alias_346: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_144)
        view_619: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_346, [1, arg0_1, 4096]);  alias_346 = None
        view_620: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_619, [arg0_1, 4096]);  view_619 = None
        mm_126: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_620, permute_199);  view_620 = permute_199 = None
        view_621: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_126, [1, arg0_1, 4096]);  mm_126 = None
        view_622: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_621, [1, arg0_1, -1, 128]);  view_621 = None
        permute_200: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_622, [0, 2, 1, 3]);  view_622 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_201: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg190_1, [1, 0]);  arg190_1 = None
        alias_347: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_144)
        view_624: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_347, [1, arg0_1, 4096]);  alias_347 = None
        view_625: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_624, [arg0_1, 4096]);  view_624 = None
        mm_127: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_625, permute_201);  view_625 = permute_201 = None
        view_626: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_127, [1, arg0_1, 1024]);  mm_127 = None
        view_627: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_626, [1, arg0_1, -1, 128]);  view_626 = None
        permute_202: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_627, [0, 2, 1, 3]);  view_627 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_203: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg191_1, [1, 0]);  arg191_1 = None
        alias_348: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_144);  getitem_144 = None
        view_629: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_348, [1, arg0_1, 4096]);  alias_348 = None
        view_630: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_629, [arg0_1, 4096]);  view_629 = None
        mm_128: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_630, permute_203);  view_630 = permute_203 = None
        view_631: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_128, [1, arg0_1, 1024]);  mm_128 = None
        view_632: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_631, [1, arg0_1, -1, 128]);  view_631 = None
        permute_204: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_632, [0, 2, 1, 3]);  view_632 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_112: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_113: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_7498: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_200, unsqueeze_112)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_418: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_200, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_419: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_200, 3, 64, 9223372036854775807);  permute_200 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_36: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_419);  slice_419 = None
        cat_36: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_36, slice_418], -1);  neg_36 = slice_418 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_7515: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_36, unsqueeze_113);  cat_36 = None
        add_6057: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_7498, mul_7515);  mul_7498 = mul_7515 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7523: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_202, unsqueeze_112);  unsqueeze_112 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_420: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_202, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_421: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_202, 3, 64, 9223372036854775807);  permute_202 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_37: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_421);  slice_421 = None
        cat_37: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_37, slice_420], -1);  neg_37 = slice_420 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7540: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_37, unsqueeze_113);  cat_37 = unsqueeze_113 = None
        add_6081: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_7523, mul_7540);  mul_7523 = mul_7540 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_36: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg192_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_349: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_36);  full_36 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_37: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg192_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg192_1 = None
        alias_350: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_37);  full_37 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_36: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_349, [None, None, arg3_1], add_6081);  alias_349 = add_6081 = None
        alias_351: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_36)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_37: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_350, [None, None, arg3_1], permute_204);  alias_350 = permute_204 = None
        alias_352: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_37)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_353: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_36);  index_put_36 = None
        slice_426: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_353, 0, 0, 9223372036854775807);  alias_353 = None
        slice_427: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_426, 1, 0, 9223372036854775807);  slice_426 = None
        unsqueeze_115: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_427, 2);  slice_427 = None
        slice_428: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_115, 3, 0, 9223372036854775807);  unsqueeze_115 = None
        slice_429: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_428, 4, 0, 9223372036854775807);  slice_428 = None
        expand_95: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_429, [1, 8, 4, arg5_1, 128]);  slice_429 = None
        clone_56: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_95, memory_format = torch.contiguous_format);  expand_95 = None
        view_633: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_56, [1, 32, arg5_1, 128]);  clone_56 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_354: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_37);  index_put_37 = None
        slice_434: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_354, 0, 0, 9223372036854775807);  alias_354 = None
        slice_435: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_434, 1, 0, 9223372036854775807);  slice_434 = None
        unsqueeze_117: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_435, 2);  slice_435 = None
        slice_436: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_117, 3, 0, 9223372036854775807);  unsqueeze_117 = None
        slice_437: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_436, 4, 0, 9223372036854775807);  slice_436 = None
        expand_97: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_437, [1, 8, 4, arg5_1, 128]);  slice_437 = None
        clone_57: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_97, memory_format = torch.contiguous_format);  expand_97 = None
        view_634: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_57, [1, 32, arg5_1, 128]);  clone_57 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_438: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_439: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_438, 1, 0, 9223372036854775807);  slice_438 = None
        slice_440: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_439, 2, 0, 9223372036854775807);  slice_439 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_58: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_6057, memory_format = torch.contiguous_format);  add_6057 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_36: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_37: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_18: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_440, scalar_tensor_37, scalar_tensor_36);  slice_440 = scalar_tensor_37 = scalar_tensor_36 = None
        expand_98: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_18, [1, 32, arg0_1, arg5_1]);  where_18 = None
        _scaled_dot_product_efficient_attention_18 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_58, view_633, view_634, expand_98, False, scale = 0.08838834764831845);  clone_58 = view_633 = view_634 = expand_98 = None
        getitem_146: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_18[0];  _scaled_dot_product_efficient_attention_18 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_205: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_146, [0, 2, 1, 3]);  getitem_146 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_635: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_205, [1, arg0_1, -1]);  permute_205 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_206: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg193_1, [1, 0]);  arg193_1 = None
        view_636: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_635, [arg0_1, 4096]);  view_635 = None
        mm_129: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_636, permute_206);  view_636 = permute_206 = None
        view_637: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_129, [1, arg0_1, 4096]);  mm_129 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_6239: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_5963, view_637);  add_5963 = view_637 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_638: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_6239, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_74: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_355: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_74);  empty_74 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_75: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_356: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_75);  empty_75 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_37 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 38, constant_args_idx = 37, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_355, 'X_ptr': view_638, 'W_ptr': arg194_1, 'RSTD_ptr': alias_356}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_355 = view_638 = arg194_1 = alias_356 = None
        getitem_150: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_37['Y_ptr'];  triton_kernel_wrapper_functional_proxy_37 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_207: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg195_1, [1, 0]);  arg195_1 = None
        alias_359: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_150)
        view_641: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_359, [1, arg0_1, 4096]);  alias_359 = None
        view_642: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_641, [arg0_1, 4096]);  view_641 = None
        mm_130: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_642, permute_207);  view_642 = permute_207 = None
        view_643: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_130, [1, arg0_1, 14336]);  mm_130 = None
        convert_element_type_301: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_643, torch.float32);  view_643 = None
        sigmoid_18: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_301)
        mul_7788: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_301, sigmoid_18);  convert_element_type_301 = sigmoid_18 = None
        convert_element_type_302: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_7788, torch.float16);  mul_7788 = None
        permute_208: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg196_1, [1, 0]);  arg196_1 = None
        alias_360: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_150);  getitem_150 = None
        view_645: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_360, [1, arg0_1, 4096]);  alias_360 = None
        view_646: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_645, [arg0_1, 4096]);  view_645 = None
        mm_131: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_646, permute_208);  view_646 = permute_208 = None
        view_647: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_131, [1, arg0_1, 14336]);  mm_131 = None
        mul_7805: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_302, view_647);  convert_element_type_302 = view_647 = None
        permute_209: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg197_1, [1, 0]);  arg197_1 = None
        view_648: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_7805, [arg0_1, 14336]);  mul_7805 = None
        mm_132: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_648, permute_209);  view_648 = permute_209 = None
        view_649: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_132, [1, arg0_1, 4096]);  mm_132 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_6292: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_6239, view_649);  add_6239 = view_649 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_650: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_6292, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_76: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_361: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_76);  empty_76 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_77: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_362: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_77);  empty_77 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_38 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 39, constant_args_idx = 38, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_361, 'X_ptr': view_650, 'W_ptr': arg198_1, 'RSTD_ptr': alias_362}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_361 = view_650 = arg198_1 = alias_362 = None
        getitem_152: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_38['Y_ptr'];  triton_kernel_wrapper_functional_proxy_38 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_210: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg199_1, [1, 0]);  arg199_1 = None
        alias_365: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_152)
        view_653: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_365, [1, arg0_1, 4096]);  alias_365 = None
        view_654: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_653, [arg0_1, 4096]);  view_653 = None
        mm_133: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_654, permute_210);  view_654 = permute_210 = None
        view_655: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_133, [1, arg0_1, 4096]);  mm_133 = None
        view_656: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_655, [1, arg0_1, -1, 128]);  view_655 = None
        permute_211: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_656, [0, 2, 1, 3]);  view_656 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_212: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg200_1, [1, 0]);  arg200_1 = None
        alias_366: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_152)
        view_658: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_366, [1, arg0_1, 4096]);  alias_366 = None
        view_659: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_658, [arg0_1, 4096]);  view_658 = None
        mm_134: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_659, permute_212);  view_659 = permute_212 = None
        view_660: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_134, [1, arg0_1, 1024]);  mm_134 = None
        view_661: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_660, [1, arg0_1, -1, 128]);  view_660 = None
        permute_213: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_661, [0, 2, 1, 3]);  view_661 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_214: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg201_1, [1, 0]);  arg201_1 = None
        alias_367: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_152);  getitem_152 = None
        view_663: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_367, [1, arg0_1, 4096]);  alias_367 = None
        view_664: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_663, [arg0_1, 4096]);  view_663 = None
        mm_135: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_664, permute_214);  view_664 = permute_214 = None
        view_665: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_135, [1, arg0_1, 1024]);  mm_135 = None
        view_666: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_665, [1, arg0_1, -1, 128]);  view_665 = None
        permute_215: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_666, [0, 2, 1, 3]);  view_666 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_118: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_119: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_7907: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_211, unsqueeze_118)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_441: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_211, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_442: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_211, 3, 64, 9223372036854775807);  permute_211 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_38: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_442);  slice_442 = None
        cat_38: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_38, slice_441], -1);  neg_38 = slice_441 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_7924: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_38, unsqueeze_119);  cat_38 = None
        add_6386: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_7907, mul_7924);  mul_7907 = mul_7924 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7932: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_213, unsqueeze_118);  unsqueeze_118 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_443: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_213, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_444: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_213, 3, 64, 9223372036854775807);  permute_213 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_39: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_444);  slice_444 = None
        cat_39: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_39, slice_443], -1);  neg_39 = slice_443 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7949: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_39, unsqueeze_119);  cat_39 = unsqueeze_119 = None
        add_6410: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_7932, mul_7949);  mul_7932 = mul_7949 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_38: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg202_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_368: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_38);  full_38 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_39: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg202_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg202_1 = None
        alias_369: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_39);  full_39 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_38: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_368, [None, None, arg3_1], add_6410);  alias_368 = add_6410 = None
        alias_370: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_38)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_39: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_369, [None, None, arg3_1], permute_215);  alias_369 = permute_215 = None
        alias_371: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_39)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_372: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_38);  index_put_38 = None
        slice_449: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_372, 0, 0, 9223372036854775807);  alias_372 = None
        slice_450: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_449, 1, 0, 9223372036854775807);  slice_449 = None
        unsqueeze_121: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_450, 2);  slice_450 = None
        slice_451: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_121, 3, 0, 9223372036854775807);  unsqueeze_121 = None
        slice_452: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_451, 4, 0, 9223372036854775807);  slice_451 = None
        expand_100: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_452, [1, 8, 4, arg5_1, 128]);  slice_452 = None
        clone_59: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_100, memory_format = torch.contiguous_format);  expand_100 = None
        view_667: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_59, [1, 32, arg5_1, 128]);  clone_59 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_373: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_39);  index_put_39 = None
        slice_457: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_373, 0, 0, 9223372036854775807);  alias_373 = None
        slice_458: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_457, 1, 0, 9223372036854775807);  slice_457 = None
        unsqueeze_123: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_458, 2);  slice_458 = None
        slice_459: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_123, 3, 0, 9223372036854775807);  unsqueeze_123 = None
        slice_460: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_459, 4, 0, 9223372036854775807);  slice_459 = None
        expand_102: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_460, [1, 8, 4, arg5_1, 128]);  slice_460 = None
        clone_60: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_102, memory_format = torch.contiguous_format);  expand_102 = None
        view_668: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_60, [1, 32, arg5_1, 128]);  clone_60 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_461: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_462: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_461, 1, 0, 9223372036854775807);  slice_461 = None
        slice_463: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_462, 2, 0, 9223372036854775807);  slice_462 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_61: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_6386, memory_format = torch.contiguous_format);  add_6386 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_38: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_39: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_19: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_463, scalar_tensor_39, scalar_tensor_38);  slice_463 = scalar_tensor_39 = scalar_tensor_38 = None
        expand_103: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_19, [1, 32, arg0_1, arg5_1]);  where_19 = None
        _scaled_dot_product_efficient_attention_19 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_61, view_667, view_668, expand_103, False, scale = 0.08838834764831845);  clone_61 = view_667 = view_668 = expand_103 = None
        getitem_154: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_19[0];  _scaled_dot_product_efficient_attention_19 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_216: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_154, [0, 2, 1, 3]);  getitem_154 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_669: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_216, [1, arg0_1, -1]);  permute_216 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_217: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg203_1, [1, 0]);  arg203_1 = None
        view_670: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_669, [arg0_1, 4096]);  view_669 = None
        mm_136: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_670, permute_217);  view_670 = permute_217 = None
        view_671: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_136, [1, arg0_1, 4096]);  mm_136 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_6568: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_6292, view_671);  add_6292 = view_671 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_672: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_6568, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_78: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_374: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_78);  empty_78 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_79: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_375: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_79);  empty_79 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_39 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 40, constant_args_idx = 39, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_374, 'X_ptr': view_672, 'W_ptr': arg204_1, 'RSTD_ptr': alias_375}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_374 = view_672 = arg204_1 = alias_375 = None
        getitem_158: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_39['Y_ptr'];  triton_kernel_wrapper_functional_proxy_39 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_218: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg205_1, [1, 0]);  arg205_1 = None
        alias_378: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_158)
        view_675: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_378, [1, arg0_1, 4096]);  alias_378 = None
        view_676: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_675, [arg0_1, 4096]);  view_675 = None
        mm_137: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_676, permute_218);  view_676 = permute_218 = None
        view_677: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_137, [1, arg0_1, 14336]);  mm_137 = None
        convert_element_type_317: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_677, torch.float32);  view_677 = None
        sigmoid_19: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_317)
        mul_8197: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_317, sigmoid_19);  convert_element_type_317 = sigmoid_19 = None
        convert_element_type_318: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_8197, torch.float16);  mul_8197 = None
        permute_219: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg206_1, [1, 0]);  arg206_1 = None
        alias_379: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_158);  getitem_158 = None
        view_679: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_379, [1, arg0_1, 4096]);  alias_379 = None
        view_680: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_679, [arg0_1, 4096]);  view_679 = None
        mm_138: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_680, permute_219);  view_680 = permute_219 = None
        view_681: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_138, [1, arg0_1, 14336]);  mm_138 = None
        mul_8214: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_318, view_681);  convert_element_type_318 = view_681 = None
        permute_220: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg207_1, [1, 0]);  arg207_1 = None
        view_682: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_8214, [arg0_1, 14336]);  mul_8214 = None
        mm_139: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_682, permute_220);  view_682 = permute_220 = None
        view_683: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_139, [1, arg0_1, 4096]);  mm_139 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_6621: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_6568, view_683);  add_6568 = view_683 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_684: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_6621, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_80: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_380: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_80);  empty_80 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_81: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_381: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_81);  empty_81 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_40 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 41, constant_args_idx = 40, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_380, 'X_ptr': view_684, 'W_ptr': arg208_1, 'RSTD_ptr': alias_381}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_380 = view_684 = arg208_1 = alias_381 = None
        getitem_160: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_40['Y_ptr'];  triton_kernel_wrapper_functional_proxy_40 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_221: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg209_1, [1, 0]);  arg209_1 = None
        alias_384: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_160)
        view_687: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_384, [1, arg0_1, 4096]);  alias_384 = None
        view_688: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_687, [arg0_1, 4096]);  view_687 = None
        mm_140: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_688, permute_221);  view_688 = permute_221 = None
        view_689: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_140, [1, arg0_1, 4096]);  mm_140 = None
        view_690: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_689, [1, arg0_1, -1, 128]);  view_689 = None
        permute_222: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_690, [0, 2, 1, 3]);  view_690 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_223: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg210_1, [1, 0]);  arg210_1 = None
        alias_385: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_160)
        view_692: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_385, [1, arg0_1, 4096]);  alias_385 = None
        view_693: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_692, [arg0_1, 4096]);  view_692 = None
        mm_141: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_693, permute_223);  view_693 = permute_223 = None
        view_694: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_141, [1, arg0_1, 1024]);  mm_141 = None
        view_695: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_694, [1, arg0_1, -1, 128]);  view_694 = None
        permute_224: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_695, [0, 2, 1, 3]);  view_695 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_225: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg211_1, [1, 0]);  arg211_1 = None
        alias_386: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_160);  getitem_160 = None
        view_697: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_386, [1, arg0_1, 4096]);  alias_386 = None
        view_698: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_697, [arg0_1, 4096]);  view_697 = None
        mm_142: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_698, permute_225);  view_698 = permute_225 = None
        view_699: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_142, [1, arg0_1, 1024]);  mm_142 = None
        view_700: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_699, [1, arg0_1, -1, 128]);  view_699 = None
        permute_226: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_700, [0, 2, 1, 3]);  view_700 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_124: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_125: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_8316: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_222, unsqueeze_124)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_464: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_222, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_465: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_222, 3, 64, 9223372036854775807);  permute_222 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_40: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_465);  slice_465 = None
        cat_40: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_40, slice_464], -1);  neg_40 = slice_464 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_8333: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_40, unsqueeze_125);  cat_40 = None
        add_6715: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_8316, mul_8333);  mul_8316 = mul_8333 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_8341: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_224, unsqueeze_124);  unsqueeze_124 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_466: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_224, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_467: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_224, 3, 64, 9223372036854775807);  permute_224 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_41: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_467);  slice_467 = None
        cat_41: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_41, slice_466], -1);  neg_41 = slice_466 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_8358: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_41, unsqueeze_125);  cat_41 = unsqueeze_125 = None
        add_6739: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_8341, mul_8358);  mul_8341 = mul_8358 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_40: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg212_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_387: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_40);  full_40 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_41: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg212_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg212_1 = None
        alias_388: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_41);  full_41 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_40: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_387, [None, None, arg3_1], add_6739);  alias_387 = add_6739 = None
        alias_389: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_40)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_41: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_388, [None, None, arg3_1], permute_226);  alias_388 = permute_226 = None
        alias_390: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_41)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_391: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_40);  index_put_40 = None
        slice_472: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_391, 0, 0, 9223372036854775807);  alias_391 = None
        slice_473: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_472, 1, 0, 9223372036854775807);  slice_472 = None
        unsqueeze_127: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_473, 2);  slice_473 = None
        slice_474: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_127, 3, 0, 9223372036854775807);  unsqueeze_127 = None
        slice_475: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_474, 4, 0, 9223372036854775807);  slice_474 = None
        expand_105: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_475, [1, 8, 4, arg5_1, 128]);  slice_475 = None
        clone_62: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_105, memory_format = torch.contiguous_format);  expand_105 = None
        view_701: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_62, [1, 32, arg5_1, 128]);  clone_62 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_392: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_41);  index_put_41 = None
        slice_480: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_392, 0, 0, 9223372036854775807);  alias_392 = None
        slice_481: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_480, 1, 0, 9223372036854775807);  slice_480 = None
        unsqueeze_129: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_481, 2);  slice_481 = None
        slice_482: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_129, 3, 0, 9223372036854775807);  unsqueeze_129 = None
        slice_483: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_482, 4, 0, 9223372036854775807);  slice_482 = None
        expand_107: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_483, [1, 8, 4, arg5_1, 128]);  slice_483 = None
        clone_63: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_107, memory_format = torch.contiguous_format);  expand_107 = None
        view_702: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_63, [1, 32, arg5_1, 128]);  clone_63 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_484: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_485: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_484, 1, 0, 9223372036854775807);  slice_484 = None
        slice_486: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_485, 2, 0, 9223372036854775807);  slice_485 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_64: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_6715, memory_format = torch.contiguous_format);  add_6715 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_40: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_41: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_20: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_486, scalar_tensor_41, scalar_tensor_40);  slice_486 = scalar_tensor_41 = scalar_tensor_40 = None
        expand_108: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_20, [1, 32, arg0_1, arg5_1]);  where_20 = None
        _scaled_dot_product_efficient_attention_20 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_64, view_701, view_702, expand_108, False, scale = 0.08838834764831845);  clone_64 = view_701 = view_702 = expand_108 = None
        getitem_162: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_20[0];  _scaled_dot_product_efficient_attention_20 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_227: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_162, [0, 2, 1, 3]);  getitem_162 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_703: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_227, [1, arg0_1, -1]);  permute_227 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_228: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg213_1, [1, 0]);  arg213_1 = None
        view_704: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_703, [arg0_1, 4096]);  view_703 = None
        mm_143: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_704, permute_228);  view_704 = permute_228 = None
        view_705: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_143, [1, arg0_1, 4096]);  mm_143 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_6897: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_6621, view_705);  add_6621 = view_705 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_706: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_6897, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_82: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_393: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_82);  empty_82 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_83: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_394: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_83);  empty_83 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_41 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 42, constant_args_idx = 41, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_393, 'X_ptr': view_706, 'W_ptr': arg214_1, 'RSTD_ptr': alias_394}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_393 = view_706 = arg214_1 = alias_394 = None
        getitem_166: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_41['Y_ptr'];  triton_kernel_wrapper_functional_proxy_41 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_229: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg215_1, [1, 0]);  arg215_1 = None
        alias_397: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_166)
        view_709: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_397, [1, arg0_1, 4096]);  alias_397 = None
        view_710: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_709, [arg0_1, 4096]);  view_709 = None
        mm_144: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_710, permute_229);  view_710 = permute_229 = None
        view_711: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_144, [1, arg0_1, 14336]);  mm_144 = None
        convert_element_type_333: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_711, torch.float32);  view_711 = None
        sigmoid_20: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_333)
        mul_8606: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_333, sigmoid_20);  convert_element_type_333 = sigmoid_20 = None
        convert_element_type_334: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_8606, torch.float16);  mul_8606 = None
        permute_230: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg216_1, [1, 0]);  arg216_1 = None
        alias_398: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_166);  getitem_166 = None
        view_713: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_398, [1, arg0_1, 4096]);  alias_398 = None
        view_714: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_713, [arg0_1, 4096]);  view_713 = None
        mm_145: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_714, permute_230);  view_714 = permute_230 = None
        view_715: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_145, [1, arg0_1, 14336]);  mm_145 = None
        mul_8623: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_334, view_715);  convert_element_type_334 = view_715 = None
        permute_231: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg217_1, [1, 0]);  arg217_1 = None
        view_716: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_8623, [arg0_1, 14336]);  mul_8623 = None
        mm_146: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_716, permute_231);  view_716 = permute_231 = None
        view_717: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_146, [1, arg0_1, 4096]);  mm_146 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_6950: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_6897, view_717);  add_6897 = view_717 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_718: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_6950, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_84: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_399: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_84);  empty_84 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_85: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_400: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_85);  empty_85 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_42 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 43, constant_args_idx = 42, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_399, 'X_ptr': view_718, 'W_ptr': arg218_1, 'RSTD_ptr': alias_400}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_399 = view_718 = arg218_1 = alias_400 = None
        getitem_168: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_42['Y_ptr'];  triton_kernel_wrapper_functional_proxy_42 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_232: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg219_1, [1, 0]);  arg219_1 = None
        alias_403: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_168)
        view_721: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_403, [1, arg0_1, 4096]);  alias_403 = None
        view_722: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_721, [arg0_1, 4096]);  view_721 = None
        mm_147: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_722, permute_232);  view_722 = permute_232 = None
        view_723: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_147, [1, arg0_1, 4096]);  mm_147 = None
        view_724: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_723, [1, arg0_1, -1, 128]);  view_723 = None
        permute_233: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_724, [0, 2, 1, 3]);  view_724 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_234: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg220_1, [1, 0]);  arg220_1 = None
        alias_404: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_168)
        view_726: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_404, [1, arg0_1, 4096]);  alias_404 = None
        view_727: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_726, [arg0_1, 4096]);  view_726 = None
        mm_148: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_727, permute_234);  view_727 = permute_234 = None
        view_728: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_148, [1, arg0_1, 1024]);  mm_148 = None
        view_729: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_728, [1, arg0_1, -1, 128]);  view_728 = None
        permute_235: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_729, [0, 2, 1, 3]);  view_729 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_236: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg221_1, [1, 0]);  arg221_1 = None
        alias_405: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_168);  getitem_168 = None
        view_731: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_405, [1, arg0_1, 4096]);  alias_405 = None
        view_732: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_731, [arg0_1, 4096]);  view_731 = None
        mm_149: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_732, permute_236);  view_732 = permute_236 = None
        view_733: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_149, [1, arg0_1, 1024]);  mm_149 = None
        view_734: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_733, [1, arg0_1, -1, 128]);  view_733 = None
        permute_237: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_734, [0, 2, 1, 3]);  view_734 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_130: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_131: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_8725: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_233, unsqueeze_130)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_487: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_233, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_488: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_233, 3, 64, 9223372036854775807);  permute_233 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_42: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_488);  slice_488 = None
        cat_42: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_42, slice_487], -1);  neg_42 = slice_487 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_8742: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_42, unsqueeze_131);  cat_42 = None
        add_7044: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_8725, mul_8742);  mul_8725 = mul_8742 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_8750: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_235, unsqueeze_130);  unsqueeze_130 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_489: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_235, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_490: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_235, 3, 64, 9223372036854775807);  permute_235 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_43: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_490);  slice_490 = None
        cat_43: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_43, slice_489], -1);  neg_43 = slice_489 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_8767: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_43, unsqueeze_131);  cat_43 = unsqueeze_131 = None
        add_7068: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_8750, mul_8767);  mul_8750 = mul_8767 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_42: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg222_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_406: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_42);  full_42 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_43: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg222_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg222_1 = None
        alias_407: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_43);  full_43 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_42: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_406, [None, None, arg3_1], add_7068);  alias_406 = add_7068 = None
        alias_408: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_42)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_43: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_407, [None, None, arg3_1], permute_237);  alias_407 = permute_237 = None
        alias_409: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_43)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_410: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_42);  index_put_42 = None
        slice_495: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_410, 0, 0, 9223372036854775807);  alias_410 = None
        slice_496: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_495, 1, 0, 9223372036854775807);  slice_495 = None
        unsqueeze_133: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_496, 2);  slice_496 = None
        slice_497: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_133, 3, 0, 9223372036854775807);  unsqueeze_133 = None
        slice_498: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_497, 4, 0, 9223372036854775807);  slice_497 = None
        expand_110: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_498, [1, 8, 4, arg5_1, 128]);  slice_498 = None
        clone_65: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_110, memory_format = torch.contiguous_format);  expand_110 = None
        view_735: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_65, [1, 32, arg5_1, 128]);  clone_65 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_411: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_43);  index_put_43 = None
        slice_503: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_411, 0, 0, 9223372036854775807);  alias_411 = None
        slice_504: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_503, 1, 0, 9223372036854775807);  slice_503 = None
        unsqueeze_135: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_504, 2);  slice_504 = None
        slice_505: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_135, 3, 0, 9223372036854775807);  unsqueeze_135 = None
        slice_506: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_505, 4, 0, 9223372036854775807);  slice_505 = None
        expand_112: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_506, [1, 8, 4, arg5_1, 128]);  slice_506 = None
        clone_66: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_112, memory_format = torch.contiguous_format);  expand_112 = None
        view_736: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_66, [1, 32, arg5_1, 128]);  clone_66 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_507: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_508: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_507, 1, 0, 9223372036854775807);  slice_507 = None
        slice_509: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_508, 2, 0, 9223372036854775807);  slice_508 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_67: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_7044, memory_format = torch.contiguous_format);  add_7044 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_42: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_43: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_21: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_509, scalar_tensor_43, scalar_tensor_42);  slice_509 = scalar_tensor_43 = scalar_tensor_42 = None
        expand_113: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_21, [1, 32, arg0_1, arg5_1]);  where_21 = None
        _scaled_dot_product_efficient_attention_21 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_67, view_735, view_736, expand_113, False, scale = 0.08838834764831845);  clone_67 = view_735 = view_736 = expand_113 = None
        getitem_170: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_21[0];  _scaled_dot_product_efficient_attention_21 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_238: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_170, [0, 2, 1, 3]);  getitem_170 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_737: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_238, [1, arg0_1, -1]);  permute_238 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_239: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg223_1, [1, 0]);  arg223_1 = None
        view_738: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_737, [arg0_1, 4096]);  view_737 = None
        mm_150: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_738, permute_239);  view_738 = permute_239 = None
        view_739: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_150, [1, arg0_1, 4096]);  mm_150 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_7226: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_6950, view_739);  add_6950 = view_739 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_740: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_7226, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_86: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_412: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_86);  empty_86 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_87: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_413: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_87);  empty_87 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_43 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 44, constant_args_idx = 43, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_412, 'X_ptr': view_740, 'W_ptr': arg224_1, 'RSTD_ptr': alias_413}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_412 = view_740 = arg224_1 = alias_413 = None
        getitem_174: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_43['Y_ptr'];  triton_kernel_wrapper_functional_proxy_43 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_240: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg225_1, [1, 0]);  arg225_1 = None
        alias_416: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_174)
        view_743: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_416, [1, arg0_1, 4096]);  alias_416 = None
        view_744: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_743, [arg0_1, 4096]);  view_743 = None
        mm_151: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_744, permute_240);  view_744 = permute_240 = None
        view_745: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_151, [1, arg0_1, 14336]);  mm_151 = None
        convert_element_type_349: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_745, torch.float32);  view_745 = None
        sigmoid_21: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_349)
        mul_9015: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_349, sigmoid_21);  convert_element_type_349 = sigmoid_21 = None
        convert_element_type_350: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_9015, torch.float16);  mul_9015 = None
        permute_241: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg226_1, [1, 0]);  arg226_1 = None
        alias_417: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_174);  getitem_174 = None
        view_747: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_417, [1, arg0_1, 4096]);  alias_417 = None
        view_748: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_747, [arg0_1, 4096]);  view_747 = None
        mm_152: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_748, permute_241);  view_748 = permute_241 = None
        view_749: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_152, [1, arg0_1, 14336]);  mm_152 = None
        mul_9032: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_350, view_749);  convert_element_type_350 = view_749 = None
        permute_242: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg227_1, [1, 0]);  arg227_1 = None
        view_750: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_9032, [arg0_1, 14336]);  mul_9032 = None
        mm_153: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_750, permute_242);  view_750 = permute_242 = None
        view_751: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_153, [1, arg0_1, 4096]);  mm_153 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_7279: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_7226, view_751);  add_7226 = view_751 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_752: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_7279, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_88: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_418: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_88);  empty_88 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_89: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_419: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_89);  empty_89 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_44 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 45, constant_args_idx = 44, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_418, 'X_ptr': view_752, 'W_ptr': arg228_1, 'RSTD_ptr': alias_419}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_418 = view_752 = arg228_1 = alias_419 = None
        getitem_176: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_44['Y_ptr'];  triton_kernel_wrapper_functional_proxy_44 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_243: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg229_1, [1, 0]);  arg229_1 = None
        alias_422: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_176)
        view_755: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_422, [1, arg0_1, 4096]);  alias_422 = None
        view_756: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_755, [arg0_1, 4096]);  view_755 = None
        mm_154: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_756, permute_243);  view_756 = permute_243 = None
        view_757: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_154, [1, arg0_1, 4096]);  mm_154 = None
        view_758: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_757, [1, arg0_1, -1, 128]);  view_757 = None
        permute_244: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_758, [0, 2, 1, 3]);  view_758 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_245: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg230_1, [1, 0]);  arg230_1 = None
        alias_423: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_176)
        view_760: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_423, [1, arg0_1, 4096]);  alias_423 = None
        view_761: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_760, [arg0_1, 4096]);  view_760 = None
        mm_155: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_761, permute_245);  view_761 = permute_245 = None
        view_762: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_155, [1, arg0_1, 1024]);  mm_155 = None
        view_763: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_762, [1, arg0_1, -1, 128]);  view_762 = None
        permute_246: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_763, [0, 2, 1, 3]);  view_763 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_247: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg231_1, [1, 0]);  arg231_1 = None
        alias_424: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_176);  getitem_176 = None
        view_765: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_424, [1, arg0_1, 4096]);  alias_424 = None
        view_766: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_765, [arg0_1, 4096]);  view_765 = None
        mm_156: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_766, permute_247);  view_766 = permute_247 = None
        view_767: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_156, [1, arg0_1, 1024]);  mm_156 = None
        view_768: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_767, [1, arg0_1, -1, 128]);  view_767 = None
        permute_248: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_768, [0, 2, 1, 3]);  view_768 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_136: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_137: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_9134: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_244, unsqueeze_136)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_510: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_244, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_511: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_244, 3, 64, 9223372036854775807);  permute_244 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_44: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_511);  slice_511 = None
        cat_44: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_44, slice_510], -1);  neg_44 = slice_510 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_9151: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_44, unsqueeze_137);  cat_44 = None
        add_7373: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_9134, mul_9151);  mul_9134 = mul_9151 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_9159: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_246, unsqueeze_136);  unsqueeze_136 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_512: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_246, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_513: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_246, 3, 64, 9223372036854775807);  permute_246 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_45: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_513);  slice_513 = None
        cat_45: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_45, slice_512], -1);  neg_45 = slice_512 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_9176: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_45, unsqueeze_137);  cat_45 = unsqueeze_137 = None
        add_7397: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_9159, mul_9176);  mul_9159 = mul_9176 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_44: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg232_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_425: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_44);  full_44 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_45: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg232_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg232_1 = None
        alias_426: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_45);  full_45 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_44: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_425, [None, None, arg3_1], add_7397);  alias_425 = add_7397 = None
        alias_427: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_44)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_45: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_426, [None, None, arg3_1], permute_248);  alias_426 = permute_248 = None
        alias_428: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_45)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_429: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_44);  index_put_44 = None
        slice_518: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_429, 0, 0, 9223372036854775807);  alias_429 = None
        slice_519: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_518, 1, 0, 9223372036854775807);  slice_518 = None
        unsqueeze_139: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_519, 2);  slice_519 = None
        slice_520: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_139, 3, 0, 9223372036854775807);  unsqueeze_139 = None
        slice_521: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_520, 4, 0, 9223372036854775807);  slice_520 = None
        expand_115: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_521, [1, 8, 4, arg5_1, 128]);  slice_521 = None
        clone_68: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_115, memory_format = torch.contiguous_format);  expand_115 = None
        view_769: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_68, [1, 32, arg5_1, 128]);  clone_68 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_430: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_45);  index_put_45 = None
        slice_526: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_430, 0, 0, 9223372036854775807);  alias_430 = None
        slice_527: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_526, 1, 0, 9223372036854775807);  slice_526 = None
        unsqueeze_141: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_527, 2);  slice_527 = None
        slice_528: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_141, 3, 0, 9223372036854775807);  unsqueeze_141 = None
        slice_529: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_528, 4, 0, 9223372036854775807);  slice_528 = None
        expand_117: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_529, [1, 8, 4, arg5_1, 128]);  slice_529 = None
        clone_69: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_117, memory_format = torch.contiguous_format);  expand_117 = None
        view_770: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_69, [1, 32, arg5_1, 128]);  clone_69 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_530: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_531: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_530, 1, 0, 9223372036854775807);  slice_530 = None
        slice_532: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_531, 2, 0, 9223372036854775807);  slice_531 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_70: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_7373, memory_format = torch.contiguous_format);  add_7373 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_44: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_45: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_22: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_532, scalar_tensor_45, scalar_tensor_44);  slice_532 = scalar_tensor_45 = scalar_tensor_44 = None
        expand_118: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_22, [1, 32, arg0_1, arg5_1]);  where_22 = None
        _scaled_dot_product_efficient_attention_22 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_70, view_769, view_770, expand_118, False, scale = 0.08838834764831845);  clone_70 = view_769 = view_770 = expand_118 = None
        getitem_178: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_22[0];  _scaled_dot_product_efficient_attention_22 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_249: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_178, [0, 2, 1, 3]);  getitem_178 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_771: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_249, [1, arg0_1, -1]);  permute_249 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_250: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg233_1, [1, 0]);  arg233_1 = None
        view_772: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_771, [arg0_1, 4096]);  view_771 = None
        mm_157: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_772, permute_250);  view_772 = permute_250 = None
        view_773: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_157, [1, arg0_1, 4096]);  mm_157 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_7555: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_7279, view_773);  add_7279 = view_773 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_774: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_7555, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_90: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_431: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_90);  empty_90 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_91: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_432: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_91);  empty_91 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_45 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 46, constant_args_idx = 45, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_431, 'X_ptr': view_774, 'W_ptr': arg234_1, 'RSTD_ptr': alias_432}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_431 = view_774 = arg234_1 = alias_432 = None
        getitem_182: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_45['Y_ptr'];  triton_kernel_wrapper_functional_proxy_45 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_251: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg235_1, [1, 0]);  arg235_1 = None
        alias_435: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_182)
        view_777: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_435, [1, arg0_1, 4096]);  alias_435 = None
        view_778: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_777, [arg0_1, 4096]);  view_777 = None
        mm_158: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_778, permute_251);  view_778 = permute_251 = None
        view_779: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_158, [1, arg0_1, 14336]);  mm_158 = None
        convert_element_type_365: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_779, torch.float32);  view_779 = None
        sigmoid_22: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_365)
        mul_9424: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_365, sigmoid_22);  convert_element_type_365 = sigmoid_22 = None
        convert_element_type_366: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_9424, torch.float16);  mul_9424 = None
        permute_252: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg236_1, [1, 0]);  arg236_1 = None
        alias_436: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_182);  getitem_182 = None
        view_781: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_436, [1, arg0_1, 4096]);  alias_436 = None
        view_782: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_781, [arg0_1, 4096]);  view_781 = None
        mm_159: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_782, permute_252);  view_782 = permute_252 = None
        view_783: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_159, [1, arg0_1, 14336]);  mm_159 = None
        mul_9441: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_366, view_783);  convert_element_type_366 = view_783 = None
        permute_253: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg237_1, [1, 0]);  arg237_1 = None
        view_784: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_9441, [arg0_1, 14336]);  mul_9441 = None
        mm_160: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_784, permute_253);  view_784 = permute_253 = None
        view_785: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_160, [1, arg0_1, 4096]);  mm_160 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_7608: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_7555, view_785);  add_7555 = view_785 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_786: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_7608, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_92: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_437: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_92);  empty_92 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_93: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_438: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_93);  empty_93 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_46 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 47, constant_args_idx = 46, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_437, 'X_ptr': view_786, 'W_ptr': arg238_1, 'RSTD_ptr': alias_438}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_437 = view_786 = arg238_1 = alias_438 = None
        getitem_184: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_46['Y_ptr'];  triton_kernel_wrapper_functional_proxy_46 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_254: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg239_1, [1, 0]);  arg239_1 = None
        alias_441: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_184)
        view_789: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_441, [1, arg0_1, 4096]);  alias_441 = None
        view_790: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_789, [arg0_1, 4096]);  view_789 = None
        mm_161: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_790, permute_254);  view_790 = permute_254 = None
        view_791: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_161, [1, arg0_1, 4096]);  mm_161 = None
        view_792: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_791, [1, arg0_1, -1, 128]);  view_791 = None
        permute_255: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_792, [0, 2, 1, 3]);  view_792 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_256: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg240_1, [1, 0]);  arg240_1 = None
        alias_442: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_184)
        view_794: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_442, [1, arg0_1, 4096]);  alias_442 = None
        view_795: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_794, [arg0_1, 4096]);  view_794 = None
        mm_162: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_795, permute_256);  view_795 = permute_256 = None
        view_796: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_162, [1, arg0_1, 1024]);  mm_162 = None
        view_797: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_796, [1, arg0_1, -1, 128]);  view_796 = None
        permute_257: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_797, [0, 2, 1, 3]);  view_797 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_258: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg241_1, [1, 0]);  arg241_1 = None
        alias_443: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_184);  getitem_184 = None
        view_799: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_443, [1, arg0_1, 4096]);  alias_443 = None
        view_800: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_799, [arg0_1, 4096]);  view_799 = None
        mm_163: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_800, permute_258);  view_800 = permute_258 = None
        view_801: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_163, [1, arg0_1, 1024]);  mm_163 = None
        view_802: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_801, [1, arg0_1, -1, 128]);  view_801 = None
        permute_259: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_802, [0, 2, 1, 3]);  view_802 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_142: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_143: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_9543: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_255, unsqueeze_142)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_533: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_255, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_534: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_255, 3, 64, 9223372036854775807);  permute_255 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_46: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_534);  slice_534 = None
        cat_46: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_46, slice_533], -1);  neg_46 = slice_533 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_9560: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_46, unsqueeze_143);  cat_46 = None
        add_7702: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_9543, mul_9560);  mul_9543 = mul_9560 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_9568: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_257, unsqueeze_142);  unsqueeze_142 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_535: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_257, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_536: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_257, 3, 64, 9223372036854775807);  permute_257 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_47: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_536);  slice_536 = None
        cat_47: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_47, slice_535], -1);  neg_47 = slice_535 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_9585: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_47, unsqueeze_143);  cat_47 = unsqueeze_143 = None
        add_7726: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_9568, mul_9585);  mul_9568 = mul_9585 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_46: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg242_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_444: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_46);  full_46 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_47: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg242_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg242_1 = None
        alias_445: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_47);  full_47 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_46: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_444, [None, None, arg3_1], add_7726);  alias_444 = add_7726 = None
        alias_446: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_46)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_47: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_445, [None, None, arg3_1], permute_259);  alias_445 = permute_259 = None
        alias_447: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_47)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_448: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_46);  index_put_46 = None
        slice_541: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_448, 0, 0, 9223372036854775807);  alias_448 = None
        slice_542: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_541, 1, 0, 9223372036854775807);  slice_541 = None
        unsqueeze_145: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_542, 2);  slice_542 = None
        slice_543: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_145, 3, 0, 9223372036854775807);  unsqueeze_145 = None
        slice_544: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_543, 4, 0, 9223372036854775807);  slice_543 = None
        expand_120: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_544, [1, 8, 4, arg5_1, 128]);  slice_544 = None
        clone_71: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_120, memory_format = torch.contiguous_format);  expand_120 = None
        view_803: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_71, [1, 32, arg5_1, 128]);  clone_71 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_449: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_47);  index_put_47 = None
        slice_549: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_449, 0, 0, 9223372036854775807);  alias_449 = None
        slice_550: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_549, 1, 0, 9223372036854775807);  slice_549 = None
        unsqueeze_147: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_550, 2);  slice_550 = None
        slice_551: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_147, 3, 0, 9223372036854775807);  unsqueeze_147 = None
        slice_552: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_551, 4, 0, 9223372036854775807);  slice_551 = None
        expand_122: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_552, [1, 8, 4, arg5_1, 128]);  slice_552 = None
        clone_72: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_122, memory_format = torch.contiguous_format);  expand_122 = None
        view_804: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_72, [1, 32, arg5_1, 128]);  clone_72 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_553: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_554: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_553, 1, 0, 9223372036854775807);  slice_553 = None
        slice_555: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_554, 2, 0, 9223372036854775807);  slice_554 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_73: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_7702, memory_format = torch.contiguous_format);  add_7702 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_46: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_47: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_23: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_555, scalar_tensor_47, scalar_tensor_46);  slice_555 = scalar_tensor_47 = scalar_tensor_46 = None
        expand_123: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_23, [1, 32, arg0_1, arg5_1]);  where_23 = None
        _scaled_dot_product_efficient_attention_23 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_73, view_803, view_804, expand_123, False, scale = 0.08838834764831845);  clone_73 = view_803 = view_804 = expand_123 = None
        getitem_186: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_23[0];  _scaled_dot_product_efficient_attention_23 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_260: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_186, [0, 2, 1, 3]);  getitem_186 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_805: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_260, [1, arg0_1, -1]);  permute_260 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_261: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg243_1, [1, 0]);  arg243_1 = None
        view_806: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_805, [arg0_1, 4096]);  view_805 = None
        mm_164: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_806, permute_261);  view_806 = permute_261 = None
        view_807: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_164, [1, arg0_1, 4096]);  mm_164 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_7884: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_7608, view_807);  add_7608 = view_807 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_808: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_7884, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_94: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_450: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_94);  empty_94 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_95: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_451: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_95);  empty_95 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_47 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 48, constant_args_idx = 47, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_450, 'X_ptr': view_808, 'W_ptr': arg244_1, 'RSTD_ptr': alias_451}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_450 = view_808 = arg244_1 = alias_451 = None
        getitem_190: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_47['Y_ptr'];  triton_kernel_wrapper_functional_proxy_47 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_262: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg245_1, [1, 0]);  arg245_1 = None
        alias_454: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_190)
        view_811: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_454, [1, arg0_1, 4096]);  alias_454 = None
        view_812: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_811, [arg0_1, 4096]);  view_811 = None
        mm_165: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_812, permute_262);  view_812 = permute_262 = None
        view_813: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_165, [1, arg0_1, 14336]);  mm_165 = None
        convert_element_type_381: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_813, torch.float32);  view_813 = None
        sigmoid_23: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_381)
        mul_9833: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_381, sigmoid_23);  convert_element_type_381 = sigmoid_23 = None
        convert_element_type_382: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_9833, torch.float16);  mul_9833 = None
        permute_263: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg246_1, [1, 0]);  arg246_1 = None
        alias_455: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_190);  getitem_190 = None
        view_815: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_455, [1, arg0_1, 4096]);  alias_455 = None
        view_816: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_815, [arg0_1, 4096]);  view_815 = None
        mm_166: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_816, permute_263);  view_816 = permute_263 = None
        view_817: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_166, [1, arg0_1, 14336]);  mm_166 = None
        mul_9850: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_382, view_817);  convert_element_type_382 = view_817 = None
        permute_264: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg247_1, [1, 0]);  arg247_1 = None
        view_818: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_9850, [arg0_1, 14336]);  mul_9850 = None
        mm_167: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_818, permute_264);  view_818 = permute_264 = None
        view_819: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_167, [1, arg0_1, 4096]);  mm_167 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_7937: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_7884, view_819);  add_7884 = view_819 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_820: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_7937, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_96: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_456: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_96);  empty_96 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_97: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_457: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_97);  empty_97 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_48 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 49, constant_args_idx = 48, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_456, 'X_ptr': view_820, 'W_ptr': arg248_1, 'RSTD_ptr': alias_457}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_456 = view_820 = arg248_1 = alias_457 = None
        getitem_192: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_48['Y_ptr'];  triton_kernel_wrapper_functional_proxy_48 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_265: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg249_1, [1, 0]);  arg249_1 = None
        alias_460: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_192)
        view_823: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_460, [1, arg0_1, 4096]);  alias_460 = None
        view_824: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_823, [arg0_1, 4096]);  view_823 = None
        mm_168: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_824, permute_265);  view_824 = permute_265 = None
        view_825: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_168, [1, arg0_1, 4096]);  mm_168 = None
        view_826: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_825, [1, arg0_1, -1, 128]);  view_825 = None
        permute_266: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_826, [0, 2, 1, 3]);  view_826 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_267: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg250_1, [1, 0]);  arg250_1 = None
        alias_461: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_192)
        view_828: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_461, [1, arg0_1, 4096]);  alias_461 = None
        view_829: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_828, [arg0_1, 4096]);  view_828 = None
        mm_169: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_829, permute_267);  view_829 = permute_267 = None
        view_830: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_169, [1, arg0_1, 1024]);  mm_169 = None
        view_831: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_830, [1, arg0_1, -1, 128]);  view_830 = None
        permute_268: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_831, [0, 2, 1, 3]);  view_831 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_269: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg251_1, [1, 0]);  arg251_1 = None
        alias_462: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_192);  getitem_192 = None
        view_833: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_462, [1, arg0_1, 4096]);  alias_462 = None
        view_834: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_833, [arg0_1, 4096]);  view_833 = None
        mm_170: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_834, permute_269);  view_834 = permute_269 = None
        view_835: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_170, [1, arg0_1, 1024]);  mm_170 = None
        view_836: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_835, [1, arg0_1, -1, 128]);  view_835 = None
        permute_270: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_836, [0, 2, 1, 3]);  view_836 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_148: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_149: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_9952: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_266, unsqueeze_148)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_556: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_266, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_557: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_266, 3, 64, 9223372036854775807);  permute_266 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_48: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_557);  slice_557 = None
        cat_48: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_48, slice_556], -1);  neg_48 = slice_556 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_9969: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_48, unsqueeze_149);  cat_48 = None
        add_8031: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_9952, mul_9969);  mul_9952 = mul_9969 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_9977: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_268, unsqueeze_148);  unsqueeze_148 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_558: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_268, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_559: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_268, 3, 64, 9223372036854775807);  permute_268 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_49: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_559);  slice_559 = None
        cat_49: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_49, slice_558], -1);  neg_49 = slice_558 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_9994: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_49, unsqueeze_149);  cat_49 = unsqueeze_149 = None
        add_8055: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_9977, mul_9994);  mul_9977 = mul_9994 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_48: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg252_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_463: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_48);  full_48 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_49: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg252_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg252_1 = None
        alias_464: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_49);  full_49 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_48: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_463, [None, None, arg3_1], add_8055);  alias_463 = add_8055 = None
        alias_465: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_48)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_49: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_464, [None, None, arg3_1], permute_270);  alias_464 = permute_270 = None
        alias_466: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_49)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_467: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_48);  index_put_48 = None
        slice_564: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_467, 0, 0, 9223372036854775807);  alias_467 = None
        slice_565: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_564, 1, 0, 9223372036854775807);  slice_564 = None
        unsqueeze_151: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_565, 2);  slice_565 = None
        slice_566: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_151, 3, 0, 9223372036854775807);  unsqueeze_151 = None
        slice_567: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_566, 4, 0, 9223372036854775807);  slice_566 = None
        expand_125: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_567, [1, 8, 4, arg5_1, 128]);  slice_567 = None
        clone_74: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_125, memory_format = torch.contiguous_format);  expand_125 = None
        view_837: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_74, [1, 32, arg5_1, 128]);  clone_74 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_468: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_49);  index_put_49 = None
        slice_572: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_468, 0, 0, 9223372036854775807);  alias_468 = None
        slice_573: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_572, 1, 0, 9223372036854775807);  slice_572 = None
        unsqueeze_153: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_573, 2);  slice_573 = None
        slice_574: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_153, 3, 0, 9223372036854775807);  unsqueeze_153 = None
        slice_575: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_574, 4, 0, 9223372036854775807);  slice_574 = None
        expand_127: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_575, [1, 8, 4, arg5_1, 128]);  slice_575 = None
        clone_75: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_127, memory_format = torch.contiguous_format);  expand_127 = None
        view_838: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_75, [1, 32, arg5_1, 128]);  clone_75 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_576: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_577: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_576, 1, 0, 9223372036854775807);  slice_576 = None
        slice_578: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_577, 2, 0, 9223372036854775807);  slice_577 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_76: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_8031, memory_format = torch.contiguous_format);  add_8031 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_48: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_49: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_24: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_578, scalar_tensor_49, scalar_tensor_48);  slice_578 = scalar_tensor_49 = scalar_tensor_48 = None
        expand_128: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_24, [1, 32, arg0_1, arg5_1]);  where_24 = None
        _scaled_dot_product_efficient_attention_24 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_76, view_837, view_838, expand_128, False, scale = 0.08838834764831845);  clone_76 = view_837 = view_838 = expand_128 = None
        getitem_194: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_24[0];  _scaled_dot_product_efficient_attention_24 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_271: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_194, [0, 2, 1, 3]);  getitem_194 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_839: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_271, [1, arg0_1, -1]);  permute_271 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_272: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg253_1, [1, 0]);  arg253_1 = None
        view_840: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_839, [arg0_1, 4096]);  view_839 = None
        mm_171: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_840, permute_272);  view_840 = permute_272 = None
        view_841: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_171, [1, arg0_1, 4096]);  mm_171 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_8213: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_7937, view_841);  add_7937 = view_841 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_842: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_8213, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_98: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_469: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_98);  empty_98 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_99: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_470: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_99);  empty_99 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_49 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 50, constant_args_idx = 49, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_469, 'X_ptr': view_842, 'W_ptr': arg254_1, 'RSTD_ptr': alias_470}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_469 = view_842 = arg254_1 = alias_470 = None
        getitem_198: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_49['Y_ptr'];  triton_kernel_wrapper_functional_proxy_49 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_273: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg255_1, [1, 0]);  arg255_1 = None
        alias_473: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_198)
        view_845: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_473, [1, arg0_1, 4096]);  alias_473 = None
        view_846: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_845, [arg0_1, 4096]);  view_845 = None
        mm_172: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_846, permute_273);  view_846 = permute_273 = None
        view_847: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_172, [1, arg0_1, 14336]);  mm_172 = None
        convert_element_type_397: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_847, torch.float32);  view_847 = None
        sigmoid_24: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_397)
        mul_10242: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_397, sigmoid_24);  convert_element_type_397 = sigmoid_24 = None
        convert_element_type_398: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_10242, torch.float16);  mul_10242 = None
        permute_274: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg256_1, [1, 0]);  arg256_1 = None
        alias_474: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_198);  getitem_198 = None
        view_849: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_474, [1, arg0_1, 4096]);  alias_474 = None
        view_850: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_849, [arg0_1, 4096]);  view_849 = None
        mm_173: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_850, permute_274);  view_850 = permute_274 = None
        view_851: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_173, [1, arg0_1, 14336]);  mm_173 = None
        mul_10259: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_398, view_851);  convert_element_type_398 = view_851 = None
        permute_275: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg257_1, [1, 0]);  arg257_1 = None
        view_852: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_10259, [arg0_1, 14336]);  mul_10259 = None
        mm_174: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_852, permute_275);  view_852 = permute_275 = None
        view_853: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_174, [1, arg0_1, 4096]);  mm_174 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_8266: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_8213, view_853);  add_8213 = view_853 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_854: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_8266, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_100: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_475: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_100);  empty_100 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_101: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_476: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_101);  empty_101 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_50 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 51, constant_args_idx = 50, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_475, 'X_ptr': view_854, 'W_ptr': arg258_1, 'RSTD_ptr': alias_476}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_475 = view_854 = arg258_1 = alias_476 = None
        getitem_200: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_50['Y_ptr'];  triton_kernel_wrapper_functional_proxy_50 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_276: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg259_1, [1, 0]);  arg259_1 = None
        alias_479: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_200)
        view_857: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_479, [1, arg0_1, 4096]);  alias_479 = None
        view_858: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_857, [arg0_1, 4096]);  view_857 = None
        mm_175: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_858, permute_276);  view_858 = permute_276 = None
        view_859: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_175, [1, arg0_1, 4096]);  mm_175 = None
        view_860: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_859, [1, arg0_1, -1, 128]);  view_859 = None
        permute_277: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_860, [0, 2, 1, 3]);  view_860 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_278: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg260_1, [1, 0]);  arg260_1 = None
        alias_480: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_200)
        view_862: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_480, [1, arg0_1, 4096]);  alias_480 = None
        view_863: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_862, [arg0_1, 4096]);  view_862 = None
        mm_176: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_863, permute_278);  view_863 = permute_278 = None
        view_864: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_176, [1, arg0_1, 1024]);  mm_176 = None
        view_865: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_864, [1, arg0_1, -1, 128]);  view_864 = None
        permute_279: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_865, [0, 2, 1, 3]);  view_865 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_280: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg261_1, [1, 0]);  arg261_1 = None
        alias_481: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_200);  getitem_200 = None
        view_867: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_481, [1, arg0_1, 4096]);  alias_481 = None
        view_868: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_867, [arg0_1, 4096]);  view_867 = None
        mm_177: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_868, permute_280);  view_868 = permute_280 = None
        view_869: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_177, [1, arg0_1, 1024]);  mm_177 = None
        view_870: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_869, [1, arg0_1, -1, 128]);  view_869 = None
        permute_281: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_870, [0, 2, 1, 3]);  view_870 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_154: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_155: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_10361: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_277, unsqueeze_154)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_579: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_277, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_580: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_277, 3, 64, 9223372036854775807);  permute_277 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_50: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_580);  slice_580 = None
        cat_50: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_50, slice_579], -1);  neg_50 = slice_579 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_10378: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_50, unsqueeze_155);  cat_50 = None
        add_8360: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_10361, mul_10378);  mul_10361 = mul_10378 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_10386: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_279, unsqueeze_154);  unsqueeze_154 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_581: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_279, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_582: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_279, 3, 64, 9223372036854775807);  permute_279 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_51: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_582);  slice_582 = None
        cat_51: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_51, slice_581], -1);  neg_51 = slice_581 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_10403: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_51, unsqueeze_155);  cat_51 = unsqueeze_155 = None
        add_8384: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_10386, mul_10403);  mul_10386 = mul_10403 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_50: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg262_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_482: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_50);  full_50 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_51: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg262_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg262_1 = None
        alias_483: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_51);  full_51 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_50: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_482, [None, None, arg3_1], add_8384);  alias_482 = add_8384 = None
        alias_484: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_50)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_51: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_483, [None, None, arg3_1], permute_281);  alias_483 = permute_281 = None
        alias_485: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_51)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_486: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_50);  index_put_50 = None
        slice_587: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_486, 0, 0, 9223372036854775807);  alias_486 = None
        slice_588: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_587, 1, 0, 9223372036854775807);  slice_587 = None
        unsqueeze_157: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_588, 2);  slice_588 = None
        slice_589: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_157, 3, 0, 9223372036854775807);  unsqueeze_157 = None
        slice_590: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_589, 4, 0, 9223372036854775807);  slice_589 = None
        expand_130: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_590, [1, 8, 4, arg5_1, 128]);  slice_590 = None
        clone_77: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_130, memory_format = torch.contiguous_format);  expand_130 = None
        view_871: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_77, [1, 32, arg5_1, 128]);  clone_77 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_487: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_51);  index_put_51 = None
        slice_595: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_487, 0, 0, 9223372036854775807);  alias_487 = None
        slice_596: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_595, 1, 0, 9223372036854775807);  slice_595 = None
        unsqueeze_159: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_596, 2);  slice_596 = None
        slice_597: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_159, 3, 0, 9223372036854775807);  unsqueeze_159 = None
        slice_598: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_597, 4, 0, 9223372036854775807);  slice_597 = None
        expand_132: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_598, [1, 8, 4, arg5_1, 128]);  slice_598 = None
        clone_78: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_132, memory_format = torch.contiguous_format);  expand_132 = None
        view_872: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_78, [1, 32, arg5_1, 128]);  clone_78 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_599: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_600: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_599, 1, 0, 9223372036854775807);  slice_599 = None
        slice_601: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_600, 2, 0, 9223372036854775807);  slice_600 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_79: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_8360, memory_format = torch.contiguous_format);  add_8360 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_50: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_51: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_25: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_601, scalar_tensor_51, scalar_tensor_50);  slice_601 = scalar_tensor_51 = scalar_tensor_50 = None
        expand_133: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_25, [1, 32, arg0_1, arg5_1]);  where_25 = None
        _scaled_dot_product_efficient_attention_25 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_79, view_871, view_872, expand_133, False, scale = 0.08838834764831845);  clone_79 = view_871 = view_872 = expand_133 = None
        getitem_202: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_25[0];  _scaled_dot_product_efficient_attention_25 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_282: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_202, [0, 2, 1, 3]);  getitem_202 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_873: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_282, [1, arg0_1, -1]);  permute_282 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_283: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg263_1, [1, 0]);  arg263_1 = None
        view_874: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_873, [arg0_1, 4096]);  view_873 = None
        mm_178: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_874, permute_283);  view_874 = permute_283 = None
        view_875: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_178, [1, arg0_1, 4096]);  mm_178 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_8542: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_8266, view_875);  add_8266 = view_875 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_876: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_8542, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_102: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_488: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_102);  empty_102 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_103: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_489: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_103);  empty_103 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_51 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 52, constant_args_idx = 51, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_488, 'X_ptr': view_876, 'W_ptr': arg264_1, 'RSTD_ptr': alias_489}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_488 = view_876 = arg264_1 = alias_489 = None
        getitem_206: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_51['Y_ptr'];  triton_kernel_wrapper_functional_proxy_51 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_284: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg265_1, [1, 0]);  arg265_1 = None
        alias_492: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_206)
        view_879: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_492, [1, arg0_1, 4096]);  alias_492 = None
        view_880: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_879, [arg0_1, 4096]);  view_879 = None
        mm_179: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_880, permute_284);  view_880 = permute_284 = None
        view_881: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_179, [1, arg0_1, 14336]);  mm_179 = None
        convert_element_type_413: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_881, torch.float32);  view_881 = None
        sigmoid_25: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_413)
        mul_10651: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_413, sigmoid_25);  convert_element_type_413 = sigmoid_25 = None
        convert_element_type_414: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_10651, torch.float16);  mul_10651 = None
        permute_285: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg266_1, [1, 0]);  arg266_1 = None
        alias_493: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_206);  getitem_206 = None
        view_883: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_493, [1, arg0_1, 4096]);  alias_493 = None
        view_884: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_883, [arg0_1, 4096]);  view_883 = None
        mm_180: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_884, permute_285);  view_884 = permute_285 = None
        view_885: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_180, [1, arg0_1, 14336]);  mm_180 = None
        mul_10668: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_414, view_885);  convert_element_type_414 = view_885 = None
        permute_286: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg267_1, [1, 0]);  arg267_1 = None
        view_886: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_10668, [arg0_1, 14336]);  mul_10668 = None
        mm_181: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_886, permute_286);  view_886 = permute_286 = None
        view_887: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_181, [1, arg0_1, 4096]);  mm_181 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_8595: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_8542, view_887);  add_8542 = view_887 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_888: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_8595, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_104: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_494: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_104);  empty_104 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_105: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_495: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_105);  empty_105 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_52 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 53, constant_args_idx = 52, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_494, 'X_ptr': view_888, 'W_ptr': arg268_1, 'RSTD_ptr': alias_495}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_494 = view_888 = arg268_1 = alias_495 = None
        getitem_208: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_52['Y_ptr'];  triton_kernel_wrapper_functional_proxy_52 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_287: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg269_1, [1, 0]);  arg269_1 = None
        alias_498: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_208)
        view_891: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_498, [1, arg0_1, 4096]);  alias_498 = None
        view_892: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_891, [arg0_1, 4096]);  view_891 = None
        mm_182: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_892, permute_287);  view_892 = permute_287 = None
        view_893: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_182, [1, arg0_1, 4096]);  mm_182 = None
        view_894: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_893, [1, arg0_1, -1, 128]);  view_893 = None
        permute_288: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_894, [0, 2, 1, 3]);  view_894 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_289: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg270_1, [1, 0]);  arg270_1 = None
        alias_499: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_208)
        view_896: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_499, [1, arg0_1, 4096]);  alias_499 = None
        view_897: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_896, [arg0_1, 4096]);  view_896 = None
        mm_183: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_897, permute_289);  view_897 = permute_289 = None
        view_898: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_183, [1, arg0_1, 1024]);  mm_183 = None
        view_899: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_898, [1, arg0_1, -1, 128]);  view_898 = None
        permute_290: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_899, [0, 2, 1, 3]);  view_899 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_291: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg271_1, [1, 0]);  arg271_1 = None
        alias_500: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_208);  getitem_208 = None
        view_901: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_500, [1, arg0_1, 4096]);  alias_500 = None
        view_902: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_901, [arg0_1, 4096]);  view_901 = None
        mm_184: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_902, permute_291);  view_902 = permute_291 = None
        view_903: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_184, [1, arg0_1, 1024]);  mm_184 = None
        view_904: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_903, [1, arg0_1, -1, 128]);  view_903 = None
        permute_292: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_904, [0, 2, 1, 3]);  view_904 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_160: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_161: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_10770: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_288, unsqueeze_160)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_602: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_288, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_603: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_288, 3, 64, 9223372036854775807);  permute_288 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_52: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_603);  slice_603 = None
        cat_52: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_52, slice_602], -1);  neg_52 = slice_602 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_10787: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_52, unsqueeze_161);  cat_52 = None
        add_8689: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_10770, mul_10787);  mul_10770 = mul_10787 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_10795: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_290, unsqueeze_160);  unsqueeze_160 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_604: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_290, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_605: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_290, 3, 64, 9223372036854775807);  permute_290 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_53: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_605);  slice_605 = None
        cat_53: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_53, slice_604], -1);  neg_53 = slice_604 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_10812: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_53, unsqueeze_161);  cat_53 = unsqueeze_161 = None
        add_8713: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_10795, mul_10812);  mul_10795 = mul_10812 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_52: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg272_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_501: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_52);  full_52 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_53: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg272_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg272_1 = None
        alias_502: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_53);  full_53 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_52: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_501, [None, None, arg3_1], add_8713);  alias_501 = add_8713 = None
        alias_503: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_52)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_53: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_502, [None, None, arg3_1], permute_292);  alias_502 = permute_292 = None
        alias_504: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_53)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_505: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_52);  index_put_52 = None
        slice_610: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_505, 0, 0, 9223372036854775807);  alias_505 = None
        slice_611: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_610, 1, 0, 9223372036854775807);  slice_610 = None
        unsqueeze_163: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_611, 2);  slice_611 = None
        slice_612: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_163, 3, 0, 9223372036854775807);  unsqueeze_163 = None
        slice_613: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_612, 4, 0, 9223372036854775807);  slice_612 = None
        expand_135: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_613, [1, 8, 4, arg5_1, 128]);  slice_613 = None
        clone_80: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_135, memory_format = torch.contiguous_format);  expand_135 = None
        view_905: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_80, [1, 32, arg5_1, 128]);  clone_80 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_506: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_53);  index_put_53 = None
        slice_618: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_506, 0, 0, 9223372036854775807);  alias_506 = None
        slice_619: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_618, 1, 0, 9223372036854775807);  slice_618 = None
        unsqueeze_165: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_619, 2);  slice_619 = None
        slice_620: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_165, 3, 0, 9223372036854775807);  unsqueeze_165 = None
        slice_621: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_620, 4, 0, 9223372036854775807);  slice_620 = None
        expand_137: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_621, [1, 8, 4, arg5_1, 128]);  slice_621 = None
        clone_81: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_137, memory_format = torch.contiguous_format);  expand_137 = None
        view_906: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_81, [1, 32, arg5_1, 128]);  clone_81 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_622: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_623: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_622, 1, 0, 9223372036854775807);  slice_622 = None
        slice_624: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_623, 2, 0, 9223372036854775807);  slice_623 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_82: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_8689, memory_format = torch.contiguous_format);  add_8689 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_52: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_53: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_26: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_624, scalar_tensor_53, scalar_tensor_52);  slice_624 = scalar_tensor_53 = scalar_tensor_52 = None
        expand_138: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_26, [1, 32, arg0_1, arg5_1]);  where_26 = None
        _scaled_dot_product_efficient_attention_26 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_82, view_905, view_906, expand_138, False, scale = 0.08838834764831845);  clone_82 = view_905 = view_906 = expand_138 = None
        getitem_210: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_26[0];  _scaled_dot_product_efficient_attention_26 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_293: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_210, [0, 2, 1, 3]);  getitem_210 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_907: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_293, [1, arg0_1, -1]);  permute_293 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_294: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg273_1, [1, 0]);  arg273_1 = None
        view_908: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_907, [arg0_1, 4096]);  view_907 = None
        mm_185: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_908, permute_294);  view_908 = permute_294 = None
        view_909: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_185, [1, arg0_1, 4096]);  mm_185 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_8871: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_8595, view_909);  add_8595 = view_909 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_910: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_8871, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_106: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_507: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_106);  empty_106 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_107: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_508: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_107);  empty_107 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_53 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 54, constant_args_idx = 53, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_507, 'X_ptr': view_910, 'W_ptr': arg274_1, 'RSTD_ptr': alias_508}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_507 = view_910 = arg274_1 = alias_508 = None
        getitem_214: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_53['Y_ptr'];  triton_kernel_wrapper_functional_proxy_53 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_295: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg275_1, [1, 0]);  arg275_1 = None
        alias_511: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_214)
        view_913: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_511, [1, arg0_1, 4096]);  alias_511 = None
        view_914: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_913, [arg0_1, 4096]);  view_913 = None
        mm_186: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_914, permute_295);  view_914 = permute_295 = None
        view_915: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_186, [1, arg0_1, 14336]);  mm_186 = None
        convert_element_type_429: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_915, torch.float32);  view_915 = None
        sigmoid_26: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_429)
        mul_11060: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_429, sigmoid_26);  convert_element_type_429 = sigmoid_26 = None
        convert_element_type_430: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_11060, torch.float16);  mul_11060 = None
        permute_296: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg276_1, [1, 0]);  arg276_1 = None
        alias_512: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_214);  getitem_214 = None
        view_917: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_512, [1, arg0_1, 4096]);  alias_512 = None
        view_918: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_917, [arg0_1, 4096]);  view_917 = None
        mm_187: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_918, permute_296);  view_918 = permute_296 = None
        view_919: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_187, [1, arg0_1, 14336]);  mm_187 = None
        mul_11077: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_430, view_919);  convert_element_type_430 = view_919 = None
        permute_297: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg277_1, [1, 0]);  arg277_1 = None
        view_920: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_11077, [arg0_1, 14336]);  mul_11077 = None
        mm_188: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_920, permute_297);  view_920 = permute_297 = None
        view_921: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_188, [1, arg0_1, 4096]);  mm_188 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_8924: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_8871, view_921);  add_8871 = view_921 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_922: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_8924, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_108: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_513: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_108);  empty_108 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_109: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_514: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_109);  empty_109 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_54 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 55, constant_args_idx = 54, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_513, 'X_ptr': view_922, 'W_ptr': arg278_1, 'RSTD_ptr': alias_514}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_513 = view_922 = arg278_1 = alias_514 = None
        getitem_216: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_54['Y_ptr'];  triton_kernel_wrapper_functional_proxy_54 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_298: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg279_1, [1, 0]);  arg279_1 = None
        alias_517: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_216)
        view_925: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_517, [1, arg0_1, 4096]);  alias_517 = None
        view_926: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_925, [arg0_1, 4096]);  view_925 = None
        mm_189: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_926, permute_298);  view_926 = permute_298 = None
        view_927: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_189, [1, arg0_1, 4096]);  mm_189 = None
        view_928: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_927, [1, arg0_1, -1, 128]);  view_927 = None
        permute_299: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_928, [0, 2, 1, 3]);  view_928 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_300: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg280_1, [1, 0]);  arg280_1 = None
        alias_518: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_216)
        view_930: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_518, [1, arg0_1, 4096]);  alias_518 = None
        view_931: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_930, [arg0_1, 4096]);  view_930 = None
        mm_190: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_931, permute_300);  view_931 = permute_300 = None
        view_932: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_190, [1, arg0_1, 1024]);  mm_190 = None
        view_933: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_932, [1, arg0_1, -1, 128]);  view_932 = None
        permute_301: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_933, [0, 2, 1, 3]);  view_933 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_302: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg281_1, [1, 0]);  arg281_1 = None
        alias_519: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_216);  getitem_216 = None
        view_935: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_519, [1, arg0_1, 4096]);  alias_519 = None
        view_936: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_935, [arg0_1, 4096]);  view_935 = None
        mm_191: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_936, permute_302);  view_936 = permute_302 = None
        view_937: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_191, [1, arg0_1, 1024]);  mm_191 = None
        view_938: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_937, [1, arg0_1, -1, 128]);  view_937 = None
        permute_303: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_938, [0, 2, 1, 3]);  view_938 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_166: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_167: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_11179: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_299, unsqueeze_166)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_625: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_299, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_626: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_299, 3, 64, 9223372036854775807);  permute_299 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_54: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_626);  slice_626 = None
        cat_54: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_54, slice_625], -1);  neg_54 = slice_625 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_11196: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_54, unsqueeze_167);  cat_54 = None
        add_9018: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_11179, mul_11196);  mul_11179 = mul_11196 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_11204: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_301, unsqueeze_166);  unsqueeze_166 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_627: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_301, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_628: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_301, 3, 64, 9223372036854775807);  permute_301 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_55: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_628);  slice_628 = None
        cat_55: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_55, slice_627], -1);  neg_55 = slice_627 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_11221: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_55, unsqueeze_167);  cat_55 = unsqueeze_167 = None
        add_9042: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_11204, mul_11221);  mul_11204 = mul_11221 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_54: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg282_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_520: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_54);  full_54 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_55: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg282_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg282_1 = None
        alias_521: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_55);  full_55 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_54: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_520, [None, None, arg3_1], add_9042);  alias_520 = add_9042 = None
        alias_522: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_54)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_55: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_521, [None, None, arg3_1], permute_303);  alias_521 = permute_303 = None
        alias_523: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_55)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_524: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_54);  index_put_54 = None
        slice_633: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_524, 0, 0, 9223372036854775807);  alias_524 = None
        slice_634: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_633, 1, 0, 9223372036854775807);  slice_633 = None
        unsqueeze_169: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_634, 2);  slice_634 = None
        slice_635: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_169, 3, 0, 9223372036854775807);  unsqueeze_169 = None
        slice_636: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_635, 4, 0, 9223372036854775807);  slice_635 = None
        expand_140: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_636, [1, 8, 4, arg5_1, 128]);  slice_636 = None
        clone_83: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_140, memory_format = torch.contiguous_format);  expand_140 = None
        view_939: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_83, [1, 32, arg5_1, 128]);  clone_83 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_525: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_55);  index_put_55 = None
        slice_641: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_525, 0, 0, 9223372036854775807);  alias_525 = None
        slice_642: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_641, 1, 0, 9223372036854775807);  slice_641 = None
        unsqueeze_171: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_642, 2);  slice_642 = None
        slice_643: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_171, 3, 0, 9223372036854775807);  unsqueeze_171 = None
        slice_644: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_643, 4, 0, 9223372036854775807);  slice_643 = None
        expand_142: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_644, [1, 8, 4, arg5_1, 128]);  slice_644 = None
        clone_84: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_142, memory_format = torch.contiguous_format);  expand_142 = None
        view_940: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_84, [1, 32, arg5_1, 128]);  clone_84 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_645: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_646: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_645, 1, 0, 9223372036854775807);  slice_645 = None
        slice_647: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_646, 2, 0, 9223372036854775807);  slice_646 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_85: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_9018, memory_format = torch.contiguous_format);  add_9018 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_54: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_55: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_27: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_647, scalar_tensor_55, scalar_tensor_54);  slice_647 = scalar_tensor_55 = scalar_tensor_54 = None
        expand_143: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_27, [1, 32, arg0_1, arg5_1]);  where_27 = None
        _scaled_dot_product_efficient_attention_27 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_85, view_939, view_940, expand_143, False, scale = 0.08838834764831845);  clone_85 = view_939 = view_940 = expand_143 = None
        getitem_218: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_27[0];  _scaled_dot_product_efficient_attention_27 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_304: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_218, [0, 2, 1, 3]);  getitem_218 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_941: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_304, [1, arg0_1, -1]);  permute_304 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_305: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg283_1, [1, 0]);  arg283_1 = None
        view_942: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_941, [arg0_1, 4096]);  view_941 = None
        mm_192: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_942, permute_305);  view_942 = permute_305 = None
        view_943: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_192, [1, arg0_1, 4096]);  mm_192 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_9200: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_8924, view_943);  add_8924 = view_943 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_944: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_9200, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_110: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_526: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_110);  empty_110 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_111: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_527: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_111);  empty_111 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_55 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 56, constant_args_idx = 55, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_526, 'X_ptr': view_944, 'W_ptr': arg284_1, 'RSTD_ptr': alias_527}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_526 = view_944 = arg284_1 = alias_527 = None
        getitem_222: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_55['Y_ptr'];  triton_kernel_wrapper_functional_proxy_55 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_306: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg285_1, [1, 0]);  arg285_1 = None
        alias_530: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_222)
        view_947: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_530, [1, arg0_1, 4096]);  alias_530 = None
        view_948: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_947, [arg0_1, 4096]);  view_947 = None
        mm_193: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_948, permute_306);  view_948 = permute_306 = None
        view_949: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_193, [1, arg0_1, 14336]);  mm_193 = None
        convert_element_type_445: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_949, torch.float32);  view_949 = None
        sigmoid_27: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_445)
        mul_11469: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_445, sigmoid_27);  convert_element_type_445 = sigmoid_27 = None
        convert_element_type_446: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_11469, torch.float16);  mul_11469 = None
        permute_307: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg286_1, [1, 0]);  arg286_1 = None
        alias_531: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_222);  getitem_222 = None
        view_951: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_531, [1, arg0_1, 4096]);  alias_531 = None
        view_952: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_951, [arg0_1, 4096]);  view_951 = None
        mm_194: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_952, permute_307);  view_952 = permute_307 = None
        view_953: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_194, [1, arg0_1, 14336]);  mm_194 = None
        mul_11486: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_446, view_953);  convert_element_type_446 = view_953 = None
        permute_308: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg287_1, [1, 0]);  arg287_1 = None
        view_954: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_11486, [arg0_1, 14336]);  mul_11486 = None
        mm_195: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_954, permute_308);  view_954 = permute_308 = None
        view_955: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_195, [1, arg0_1, 4096]);  mm_195 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_9253: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_9200, view_955);  add_9200 = view_955 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_956: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_9253, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_112: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_532: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_112);  empty_112 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_113: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_533: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_113);  empty_113 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_56 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 57, constant_args_idx = 56, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_532, 'X_ptr': view_956, 'W_ptr': arg288_1, 'RSTD_ptr': alias_533}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_532 = view_956 = arg288_1 = alias_533 = None
        getitem_224: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_56['Y_ptr'];  triton_kernel_wrapper_functional_proxy_56 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_309: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg289_1, [1, 0]);  arg289_1 = None
        alias_536: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_224)
        view_959: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_536, [1, arg0_1, 4096]);  alias_536 = None
        view_960: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_959, [arg0_1, 4096]);  view_959 = None
        mm_196: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_960, permute_309);  view_960 = permute_309 = None
        view_961: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_196, [1, arg0_1, 4096]);  mm_196 = None
        view_962: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_961, [1, arg0_1, -1, 128]);  view_961 = None
        permute_310: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_962, [0, 2, 1, 3]);  view_962 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_311: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg290_1, [1, 0]);  arg290_1 = None
        alias_537: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_224)
        view_964: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_537, [1, arg0_1, 4096]);  alias_537 = None
        view_965: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_964, [arg0_1, 4096]);  view_964 = None
        mm_197: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_965, permute_311);  view_965 = permute_311 = None
        view_966: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_197, [1, arg0_1, 1024]);  mm_197 = None
        view_967: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_966, [1, arg0_1, -1, 128]);  view_966 = None
        permute_312: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_967, [0, 2, 1, 3]);  view_967 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_313: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg291_1, [1, 0]);  arg291_1 = None
        alias_538: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_224);  getitem_224 = None
        view_969: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_538, [1, arg0_1, 4096]);  alias_538 = None
        view_970: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_969, [arg0_1, 4096]);  view_969 = None
        mm_198: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_970, permute_313);  view_970 = permute_313 = None
        view_971: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_198, [1, arg0_1, 1024]);  mm_198 = None
        view_972: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_971, [1, arg0_1, -1, 128]);  view_971 = None
        permute_314: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_972, [0, 2, 1, 3]);  view_972 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_172: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_173: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_11588: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_310, unsqueeze_172)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_648: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_310, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_649: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_310, 3, 64, 9223372036854775807);  permute_310 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_56: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_649);  slice_649 = None
        cat_56: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_56, slice_648], -1);  neg_56 = slice_648 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_11605: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_56, unsqueeze_173);  cat_56 = None
        add_9347: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_11588, mul_11605);  mul_11588 = mul_11605 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_11613: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_312, unsqueeze_172);  unsqueeze_172 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_650: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_312, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_651: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_312, 3, 64, 9223372036854775807);  permute_312 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_57: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_651);  slice_651 = None
        cat_57: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_57, slice_650], -1);  neg_57 = slice_650 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_11630: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_57, unsqueeze_173);  cat_57 = unsqueeze_173 = None
        add_9371: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_11613, mul_11630);  mul_11613 = mul_11630 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_56: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg292_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_539: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_56);  full_56 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_57: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg292_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg292_1 = None
        alias_540: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_57);  full_57 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_56: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_539, [None, None, arg3_1], add_9371);  alias_539 = add_9371 = None
        alias_541: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_56)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_57: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_540, [None, None, arg3_1], permute_314);  alias_540 = permute_314 = None
        alias_542: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_57)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_543: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_56);  index_put_56 = None
        slice_656: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_543, 0, 0, 9223372036854775807);  alias_543 = None
        slice_657: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_656, 1, 0, 9223372036854775807);  slice_656 = None
        unsqueeze_175: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_657, 2);  slice_657 = None
        slice_658: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_175, 3, 0, 9223372036854775807);  unsqueeze_175 = None
        slice_659: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_658, 4, 0, 9223372036854775807);  slice_658 = None
        expand_145: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_659, [1, 8, 4, arg5_1, 128]);  slice_659 = None
        clone_86: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_145, memory_format = torch.contiguous_format);  expand_145 = None
        view_973: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_86, [1, 32, arg5_1, 128]);  clone_86 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_544: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_57);  index_put_57 = None
        slice_664: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_544, 0, 0, 9223372036854775807);  alias_544 = None
        slice_665: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_664, 1, 0, 9223372036854775807);  slice_664 = None
        unsqueeze_177: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_665, 2);  slice_665 = None
        slice_666: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_177, 3, 0, 9223372036854775807);  unsqueeze_177 = None
        slice_667: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_666, 4, 0, 9223372036854775807);  slice_666 = None
        expand_147: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_667, [1, 8, 4, arg5_1, 128]);  slice_667 = None
        clone_87: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_147, memory_format = torch.contiguous_format);  expand_147 = None
        view_974: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_87, [1, 32, arg5_1, 128]);  clone_87 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_668: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_669: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_668, 1, 0, 9223372036854775807);  slice_668 = None
        slice_670: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_669, 2, 0, 9223372036854775807);  slice_669 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_88: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_9347, memory_format = torch.contiguous_format);  add_9347 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_56: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_57: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_28: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_670, scalar_tensor_57, scalar_tensor_56);  slice_670 = scalar_tensor_57 = scalar_tensor_56 = None
        expand_148: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_28, [1, 32, arg0_1, arg5_1]);  where_28 = None
        _scaled_dot_product_efficient_attention_28 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_88, view_973, view_974, expand_148, False, scale = 0.08838834764831845);  clone_88 = view_973 = view_974 = expand_148 = None
        getitem_226: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_28[0];  _scaled_dot_product_efficient_attention_28 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_315: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_226, [0, 2, 1, 3]);  getitem_226 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_975: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_315, [1, arg0_1, -1]);  permute_315 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_316: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg293_1, [1, 0]);  arg293_1 = None
        view_976: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_975, [arg0_1, 4096]);  view_975 = None
        mm_199: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_976, permute_316);  view_976 = permute_316 = None
        view_977: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_199, [1, arg0_1, 4096]);  mm_199 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_9529: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_9253, view_977);  add_9253 = view_977 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_978: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_9529, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_114: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_545: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_114);  empty_114 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_115: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_546: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_115);  empty_115 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_57 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 58, constant_args_idx = 57, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_545, 'X_ptr': view_978, 'W_ptr': arg294_1, 'RSTD_ptr': alias_546}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_545 = view_978 = arg294_1 = alias_546 = None
        getitem_230: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_57['Y_ptr'];  triton_kernel_wrapper_functional_proxy_57 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_317: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg295_1, [1, 0]);  arg295_1 = None
        alias_549: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_230)
        view_981: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_549, [1, arg0_1, 4096]);  alias_549 = None
        view_982: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_981, [arg0_1, 4096]);  view_981 = None
        mm_200: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_982, permute_317);  view_982 = permute_317 = None
        view_983: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_200, [1, arg0_1, 14336]);  mm_200 = None
        convert_element_type_461: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_983, torch.float32);  view_983 = None
        sigmoid_28: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_461)
        mul_11878: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_461, sigmoid_28);  convert_element_type_461 = sigmoid_28 = None
        convert_element_type_462: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_11878, torch.float16);  mul_11878 = None
        permute_318: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg296_1, [1, 0]);  arg296_1 = None
        alias_550: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_230);  getitem_230 = None
        view_985: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_550, [1, arg0_1, 4096]);  alias_550 = None
        view_986: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_985, [arg0_1, 4096]);  view_985 = None
        mm_201: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_986, permute_318);  view_986 = permute_318 = None
        view_987: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_201, [1, arg0_1, 14336]);  mm_201 = None
        mul_11895: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_462, view_987);  convert_element_type_462 = view_987 = None
        permute_319: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg297_1, [1, 0]);  arg297_1 = None
        view_988: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_11895, [arg0_1, 14336]);  mul_11895 = None
        mm_202: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_988, permute_319);  view_988 = permute_319 = None
        view_989: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_202, [1, arg0_1, 4096]);  mm_202 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_9582: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_9529, view_989);  add_9529 = view_989 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_990: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_9582, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_116: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_551: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_116);  empty_116 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_117: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_552: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_117);  empty_117 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_58 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 59, constant_args_idx = 58, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_551, 'X_ptr': view_990, 'W_ptr': arg298_1, 'RSTD_ptr': alias_552}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_551 = view_990 = arg298_1 = alias_552 = None
        getitem_232: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_58['Y_ptr'];  triton_kernel_wrapper_functional_proxy_58 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_320: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg299_1, [1, 0]);  arg299_1 = None
        alias_555: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_232)
        view_993: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_555, [1, arg0_1, 4096]);  alias_555 = None
        view_994: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_993, [arg0_1, 4096]);  view_993 = None
        mm_203: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_994, permute_320);  view_994 = permute_320 = None
        view_995: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_203, [1, arg0_1, 4096]);  mm_203 = None
        view_996: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_995, [1, arg0_1, -1, 128]);  view_995 = None
        permute_321: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_996, [0, 2, 1, 3]);  view_996 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_322: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg300_1, [1, 0]);  arg300_1 = None
        alias_556: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_232)
        view_998: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_556, [1, arg0_1, 4096]);  alias_556 = None
        view_999: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_998, [arg0_1, 4096]);  view_998 = None
        mm_204: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_999, permute_322);  view_999 = permute_322 = None
        view_1000: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_204, [1, arg0_1, 1024]);  mm_204 = None
        view_1001: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_1000, [1, arg0_1, -1, 128]);  view_1000 = None
        permute_323: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_1001, [0, 2, 1, 3]);  view_1001 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_324: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg301_1, [1, 0]);  arg301_1 = None
        alias_557: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_232);  getitem_232 = None
        view_1003: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_557, [1, arg0_1, 4096]);  alias_557 = None
        view_1004: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1003, [arg0_1, 4096]);  view_1003 = None
        mm_205: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_1004, permute_324);  view_1004 = permute_324 = None
        view_1005: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_205, [1, arg0_1, 1024]);  mm_205 = None
        view_1006: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_1005, [1, arg0_1, -1, 128]);  view_1005 = None
        permute_325: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_1006, [0, 2, 1, 3]);  view_1006 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_178: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_179: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_11997: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_321, unsqueeze_178)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_671: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_321, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_672: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_321, 3, 64, 9223372036854775807);  permute_321 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_58: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_672);  slice_672 = None
        cat_58: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_58, slice_671], -1);  neg_58 = slice_671 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_12014: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_58, unsqueeze_179);  cat_58 = None
        add_9676: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_11997, mul_12014);  mul_11997 = mul_12014 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_12022: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_323, unsqueeze_178);  unsqueeze_178 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_673: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_323, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_674: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_323, 3, 64, 9223372036854775807);  permute_323 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_59: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_674);  slice_674 = None
        cat_59: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_59, slice_673], -1);  neg_59 = slice_673 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_12039: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_59, unsqueeze_179);  cat_59 = unsqueeze_179 = None
        add_9700: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_12022, mul_12039);  mul_12022 = mul_12039 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_58: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg302_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_558: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_58);  full_58 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_59: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg302_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg302_1 = None
        alias_559: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_59);  full_59 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_58: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_558, [None, None, arg3_1], add_9700);  alias_558 = add_9700 = None
        alias_560: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_58)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_59: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_559, [None, None, arg3_1], permute_325);  alias_559 = permute_325 = None
        alias_561: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_59)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_562: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_58);  index_put_58 = None
        slice_679: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_562, 0, 0, 9223372036854775807);  alias_562 = None
        slice_680: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_679, 1, 0, 9223372036854775807);  slice_679 = None
        unsqueeze_181: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_680, 2);  slice_680 = None
        slice_681: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_181, 3, 0, 9223372036854775807);  unsqueeze_181 = None
        slice_682: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_681, 4, 0, 9223372036854775807);  slice_681 = None
        expand_150: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_682, [1, 8, 4, arg5_1, 128]);  slice_682 = None
        clone_89: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_150, memory_format = torch.contiguous_format);  expand_150 = None
        view_1007: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_89, [1, 32, arg5_1, 128]);  clone_89 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_563: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_59);  index_put_59 = None
        slice_687: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_563, 0, 0, 9223372036854775807);  alias_563 = None
        slice_688: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_687, 1, 0, 9223372036854775807);  slice_687 = None
        unsqueeze_183: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_688, 2);  slice_688 = None
        slice_689: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_183, 3, 0, 9223372036854775807);  unsqueeze_183 = None
        slice_690: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_689, 4, 0, 9223372036854775807);  slice_689 = None
        expand_152: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_690, [1, 8, 4, arg5_1, 128]);  slice_690 = None
        clone_90: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_152, memory_format = torch.contiguous_format);  expand_152 = None
        view_1008: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_90, [1, 32, arg5_1, 128]);  clone_90 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_691: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_692: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_691, 1, 0, 9223372036854775807);  slice_691 = None
        slice_693: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_692, 2, 0, 9223372036854775807);  slice_692 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_91: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_9676, memory_format = torch.contiguous_format);  add_9676 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_58: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_59: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_29: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_693, scalar_tensor_59, scalar_tensor_58);  slice_693 = scalar_tensor_59 = scalar_tensor_58 = None
        expand_153: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_29, [1, 32, arg0_1, arg5_1]);  where_29 = None
        _scaled_dot_product_efficient_attention_29 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_91, view_1007, view_1008, expand_153, False, scale = 0.08838834764831845);  clone_91 = view_1007 = view_1008 = expand_153 = None
        getitem_234: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_29[0];  _scaled_dot_product_efficient_attention_29 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_326: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_234, [0, 2, 1, 3]);  getitem_234 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_1009: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_326, [1, arg0_1, -1]);  permute_326 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_327: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg303_1, [1, 0]);  arg303_1 = None
        view_1010: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1009, [arg0_1, 4096]);  view_1009 = None
        mm_206: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_1010, permute_327);  view_1010 = permute_327 = None
        view_1011: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_206, [1, arg0_1, 4096]);  mm_206 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_9858: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_9582, view_1011);  add_9582 = view_1011 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_1012: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_9858, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_118: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_564: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_118);  empty_118 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_119: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_565: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_119);  empty_119 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_59 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 60, constant_args_idx = 59, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_564, 'X_ptr': view_1012, 'W_ptr': arg304_1, 'RSTD_ptr': alias_565}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_564 = view_1012 = arg304_1 = alias_565 = None
        getitem_238: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_59['Y_ptr'];  triton_kernel_wrapper_functional_proxy_59 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_328: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg305_1, [1, 0]);  arg305_1 = None
        alias_568: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_238)
        view_1015: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_568, [1, arg0_1, 4096]);  alias_568 = None
        view_1016: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1015, [arg0_1, 4096]);  view_1015 = None
        mm_207: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_1016, permute_328);  view_1016 = permute_328 = None
        view_1017: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_207, [1, arg0_1, 14336]);  mm_207 = None
        convert_element_type_477: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1017, torch.float32);  view_1017 = None
        sigmoid_29: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_477)
        mul_12287: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_477, sigmoid_29);  convert_element_type_477 = sigmoid_29 = None
        convert_element_type_478: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_12287, torch.float16);  mul_12287 = None
        permute_329: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg306_1, [1, 0]);  arg306_1 = None
        alias_569: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_238);  getitem_238 = None
        view_1019: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_569, [1, arg0_1, 4096]);  alias_569 = None
        view_1020: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1019, [arg0_1, 4096]);  view_1019 = None
        mm_208: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_1020, permute_329);  view_1020 = permute_329 = None
        view_1021: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_208, [1, arg0_1, 14336]);  mm_208 = None
        mul_12304: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_478, view_1021);  convert_element_type_478 = view_1021 = None
        permute_330: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg307_1, [1, 0]);  arg307_1 = None
        view_1022: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_12304, [arg0_1, 14336]);  mul_12304 = None
        mm_209: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_1022, permute_330);  view_1022 = permute_330 = None
        view_1023: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_209, [1, arg0_1, 4096]);  mm_209 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_9911: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_9858, view_1023);  add_9858 = view_1023 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_1024: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_9911, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_120: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_570: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_120);  empty_120 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_121: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_571: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_121);  empty_121 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_60 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 61, constant_args_idx = 60, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_570, 'X_ptr': view_1024, 'W_ptr': arg308_1, 'RSTD_ptr': alias_571}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_570 = view_1024 = arg308_1 = alias_571 = None
        getitem_240: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_60['Y_ptr'];  triton_kernel_wrapper_functional_proxy_60 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_331: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg309_1, [1, 0]);  arg309_1 = None
        alias_574: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_240)
        view_1027: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_574, [1, arg0_1, 4096]);  alias_574 = None
        view_1028: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1027, [arg0_1, 4096]);  view_1027 = None
        mm_210: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_1028, permute_331);  view_1028 = permute_331 = None
        view_1029: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_210, [1, arg0_1, 4096]);  mm_210 = None
        view_1030: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_1029, [1, arg0_1, -1, 128]);  view_1029 = None
        permute_332: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_1030, [0, 2, 1, 3]);  view_1030 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_333: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg310_1, [1, 0]);  arg310_1 = None
        alias_575: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_240)
        view_1032: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_575, [1, arg0_1, 4096]);  alias_575 = None
        view_1033: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1032, [arg0_1, 4096]);  view_1032 = None
        mm_211: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_1033, permute_333);  view_1033 = permute_333 = None
        view_1034: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_211, [1, arg0_1, 1024]);  mm_211 = None
        view_1035: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_1034, [1, arg0_1, -1, 128]);  view_1034 = None
        permute_334: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_1035, [0, 2, 1, 3]);  view_1035 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_335: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg311_1, [1, 0]);  arg311_1 = None
        alias_576: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_240);  getitem_240 = None
        view_1037: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_576, [1, arg0_1, 4096]);  alias_576 = None
        view_1038: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1037, [arg0_1, 4096]);  view_1037 = None
        mm_212: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_1038, permute_335);  view_1038 = permute_335 = None
        view_1039: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_212, [1, arg0_1, 1024]);  mm_212 = None
        view_1040: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_1039, [1, arg0_1, -1, 128]);  view_1039 = None
        permute_336: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_1040, [0, 2, 1, 3]);  view_1040 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_184: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_185: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_12406: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_332, unsqueeze_184)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_694: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_332, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_695: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_332, 3, 64, 9223372036854775807);  permute_332 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_60: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_695);  slice_695 = None
        cat_60: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_60, slice_694], -1);  neg_60 = slice_694 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_12423: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_60, unsqueeze_185);  cat_60 = None
        add_10005: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_12406, mul_12423);  mul_12406 = mul_12423 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_12431: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_334, unsqueeze_184);  unsqueeze_184 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_696: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_334, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_697: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_334, 3, 64, 9223372036854775807);  permute_334 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_61: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_697);  slice_697 = None
        cat_61: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_61, slice_696], -1);  neg_61 = slice_696 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_12448: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_61, unsqueeze_185);  cat_61 = unsqueeze_185 = None
        add_10029: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_12431, mul_12448);  mul_12431 = mul_12448 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_60: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg312_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_577: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_60);  full_60 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_61: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg312_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg312_1 = None
        alias_578: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_61);  full_61 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_60: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_577, [None, None, arg3_1], add_10029);  alias_577 = add_10029 = None
        alias_579: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_60)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_61: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_578, [None, None, arg3_1], permute_336);  alias_578 = permute_336 = None
        alias_580: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_61)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_581: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_60);  index_put_60 = None
        slice_702: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_581, 0, 0, 9223372036854775807);  alias_581 = None
        slice_703: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_702, 1, 0, 9223372036854775807);  slice_702 = None
        unsqueeze_187: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_703, 2);  slice_703 = None
        slice_704: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_187, 3, 0, 9223372036854775807);  unsqueeze_187 = None
        slice_705: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_704, 4, 0, 9223372036854775807);  slice_704 = None
        expand_155: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_705, [1, 8, 4, arg5_1, 128]);  slice_705 = None
        clone_92: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_155, memory_format = torch.contiguous_format);  expand_155 = None
        view_1041: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_92, [1, 32, arg5_1, 128]);  clone_92 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_582: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_61);  index_put_61 = None
        slice_710: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_582, 0, 0, 9223372036854775807);  alias_582 = None
        slice_711: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_710, 1, 0, 9223372036854775807);  slice_710 = None
        unsqueeze_189: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_711, 2);  slice_711 = None
        slice_712: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_189, 3, 0, 9223372036854775807);  unsqueeze_189 = None
        slice_713: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_712, 4, 0, 9223372036854775807);  slice_712 = None
        expand_157: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_713, [1, 8, 4, arg5_1, 128]);  slice_713 = None
        clone_93: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_157, memory_format = torch.contiguous_format);  expand_157 = None
        view_1042: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_93, [1, 32, arg5_1, 128]);  clone_93 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_714: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807)
        slice_715: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_714, 1, 0, 9223372036854775807);  slice_714 = None
        slice_716: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_715, 2, 0, 9223372036854775807);  slice_715 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_94: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_10005, memory_format = torch.contiguous_format);  add_10005 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_60: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_61: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_30: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_716, scalar_tensor_61, scalar_tensor_60);  slice_716 = scalar_tensor_61 = scalar_tensor_60 = None
        expand_158: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_30, [1, 32, arg0_1, arg5_1]);  where_30 = None
        _scaled_dot_product_efficient_attention_30 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_94, view_1041, view_1042, expand_158, False, scale = 0.08838834764831845);  clone_94 = view_1041 = view_1042 = expand_158 = None
        getitem_242: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_30[0];  _scaled_dot_product_efficient_attention_30 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_337: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_242, [0, 2, 1, 3]);  getitem_242 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_1043: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_337, [1, arg0_1, -1]);  permute_337 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_338: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg313_1, [1, 0]);  arg313_1 = None
        view_1044: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1043, [arg0_1, 4096]);  view_1043 = None
        mm_213: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_1044, permute_338);  view_1044 = permute_338 = None
        view_1045: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_213, [1, arg0_1, 4096]);  mm_213 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_10187: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_9911, view_1045);  add_9911 = view_1045 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_1046: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_10187, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_122: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_583: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_122);  empty_122 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_123: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_584: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_123);  empty_123 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_61 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 62, constant_args_idx = 61, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_583, 'X_ptr': view_1046, 'W_ptr': arg314_1, 'RSTD_ptr': alias_584}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_583 = view_1046 = arg314_1 = alias_584 = None
        getitem_246: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_61['Y_ptr'];  triton_kernel_wrapper_functional_proxy_61 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_339: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg315_1, [1, 0]);  arg315_1 = None
        alias_587: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_246)
        view_1049: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_587, [1, arg0_1, 4096]);  alias_587 = None
        view_1050: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1049, [arg0_1, 4096]);  view_1049 = None
        mm_214: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_1050, permute_339);  view_1050 = permute_339 = None
        view_1051: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_214, [1, arg0_1, 14336]);  mm_214 = None
        convert_element_type_493: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1051, torch.float32);  view_1051 = None
        sigmoid_30: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_493)
        mul_12696: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_493, sigmoid_30);  convert_element_type_493 = sigmoid_30 = None
        convert_element_type_494: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_12696, torch.float16);  mul_12696 = None
        permute_340: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg316_1, [1, 0]);  arg316_1 = None
        alias_588: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_246);  getitem_246 = None
        view_1053: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_588, [1, arg0_1, 4096]);  alias_588 = None
        view_1054: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1053, [arg0_1, 4096]);  view_1053 = None
        mm_215: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_1054, permute_340);  view_1054 = permute_340 = None
        view_1055: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_215, [1, arg0_1, 14336]);  mm_215 = None
        mul_12713: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_494, view_1055);  convert_element_type_494 = view_1055 = None
        permute_341: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg317_1, [1, 0]);  arg317_1 = None
        view_1056: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_12713, [arg0_1, 14336]);  mul_12713 = None
        mm_216: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_1056, permute_341);  view_1056 = permute_341 = None
        view_1057: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_216, [1, arg0_1, 4096]);  mm_216 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_10240: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_10187, view_1057);  add_10187 = view_1057 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_1058: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_10240, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_124: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_589: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_124);  empty_124 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_125: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_590: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_125);  empty_125 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_62 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 63, constant_args_idx = 62, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_589, 'X_ptr': view_1058, 'W_ptr': arg318_1, 'RSTD_ptr': alias_590}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_589 = view_1058 = arg318_1 = alias_590 = None
        getitem_248: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_62['Y_ptr'];  triton_kernel_wrapper_functional_proxy_62 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_342: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg319_1, [1, 0]);  arg319_1 = None
        alias_593: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_248)
        view_1061: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_593, [1, arg0_1, 4096]);  alias_593 = None
        view_1062: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1061, [arg0_1, 4096]);  view_1061 = None
        mm_217: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_1062, permute_342);  view_1062 = permute_342 = None
        view_1063: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_217, [1, arg0_1, 4096]);  mm_217 = None
        view_1064: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.view.default(view_1063, [1, arg0_1, -1, 128]);  view_1063 = None
        permute_343: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_1064, [0, 2, 1, 3]);  view_1064 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_344: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg320_1, [1, 0]);  arg320_1 = None
        alias_594: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_248)
        view_1066: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_594, [1, arg0_1, 4096]);  alias_594 = None
        view_1067: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1066, [arg0_1, 4096]);  view_1066 = None
        mm_218: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_1067, permute_344);  view_1067 = permute_344 = None
        view_1068: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_218, [1, arg0_1, 1024]);  mm_218 = None
        view_1069: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_1068, [1, arg0_1, -1, 128]);  view_1068 = None
        permute_345: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_1069, [0, 2, 1, 3]);  view_1069 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_346: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg321_1, [1, 0]);  arg321_1 = None
        alias_595: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_248);  getitem_248 = None
        view_1071: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_595, [1, arg0_1, 4096]);  alias_595 = None
        view_1072: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1071, [arg0_1, 4096]);  view_1071 = None
        mm_219: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_1072, permute_346);  view_1072 = permute_346 = None
        view_1073: "f16[1, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.ops.aten.view.default(mm_219, [1, arg0_1, 1024]);  mm_219 = None
        view_1074: "f16[1, s0, 8, 128][1024*s0, 1024, 128, 1]cuda:0" = torch.ops.aten.view.default(view_1073, [1, arg0_1, -1, 128]);  view_1073 = None
        permute_347: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_1074, [0, 2, 1, 3]);  view_1074 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_190: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1);  convert_element_type_1 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_191: "f16[1, 1, s0, 128][128*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1);  convert_element_type_2 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_12815: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_343, unsqueeze_190)
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_717: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_343, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_718: "f16[1, 32, s0, 64][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_343, 3, 64, 9223372036854775807);  permute_343 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_62: "f16[1, 32, s0, 64][2048*s0, 64, 2048, 1]cuda:0" = torch.ops.aten.neg.default(slice_718);  slice_718 = None
        cat_62: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_62, slice_717], -1);  neg_62 = slice_717 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_12832: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_62, unsqueeze_191);  cat_62 = None
        add_10334: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_12815, mul_12832);  mul_12815 = mul_12832 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_12840: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_345, unsqueeze_190);  unsqueeze_190 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_719: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_345, 3, 0, 64)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_720: "f16[1, 8, s0, 64][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_345, 3, 64, 9223372036854775807);  permute_345 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_63: "f16[1, 8, s0, 64][512*s0, 64, 512, 1]cuda:0" = torch.ops.aten.neg.default(slice_720);  slice_720 = None
        cat_63: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_63, slice_719], -1);  neg_63 = slice_719 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_12857: "f16[1, 8, s0, 128][1024*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_63, unsqueeze_191);  cat_63 = unsqueeze_191 = None
        add_10358: "f16[1, 8, s0, 128][1024*s0, 128, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_12840, mul_12857);  mul_12840 = mul_12857 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:109 in lazy_initialization, code: self.keys = torch.zeros(
        full_62: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg322_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        alias_596: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_62);  full_62 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:114 in lazy_initialization, code: self.values = torch.zeros(
        full_63: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.full.default([1, 8, arg322_1, 128], 0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  arg322_1 = None
        alias_597: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(full_63);  full_63 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:150 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_62: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_596, [None, None, arg3_1], add_10358);  alias_596 = add_10358 = None
        alias_598: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_62)
        
         # File: /app/low_bit_inference/optims/cache_optim.py:151 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_63: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.index_put.default(alias_597, [None, None, arg3_1], permute_347);  alias_597 = arg3_1 = permute_347 = None
        alias_599: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_63)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_600: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_62);  index_put_62 = None
        slice_725: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_600, 0, 0, 9223372036854775807);  alias_600 = None
        slice_726: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_725, 1, 0, 9223372036854775807);  slice_725 = None
        unsqueeze_193: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_726, 2);  slice_726 = None
        slice_727: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_193, 3, 0, 9223372036854775807);  unsqueeze_193 = None
        slice_728: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_727, 4, 0, 9223372036854775807);  slice_727 = None
        expand_160: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_728, [1, 8, 4, arg5_1, 128]);  slice_728 = None
        clone_95: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_160, memory_format = torch.contiguous_format);  expand_160 = None
        view_1075: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_95, [1, 32, arg5_1, 128]);  clone_95 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        alias_601: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.alias.default(index_put_63);  index_put_63 = None
        slice_733: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(alias_601, 0, 0, 9223372036854775807);  alias_601 = None
        slice_734: "f16[1, 8, s1, 128][1024*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_733, 1, 0, 9223372036854775807);  slice_733 = None
        unsqueeze_195: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(slice_734, 2);  slice_734 = None
        slice_735: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(unsqueeze_195, 3, 0, 9223372036854775807);  unsqueeze_195 = None
        slice_736: "f16[1, 8, 1, s1, 128][1024*s1, 128*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_735, 4, 0, 9223372036854775807);  slice_735 = None
        expand_162: "f16[1, 8, 4, s1, 128][1024*s1, 128*s1, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(slice_736, [1, 8, 4, arg5_1, 128]);  slice_736 = None
        clone_96: "f16[1, 8, 4, s1, 128][4096*s1, 512*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_162, memory_format = torch.contiguous_format);  expand_162 = None
        view_1076: "f16[1, 32, s1, 128][4096*s1, 128*s1, 128, 1]cuda:0" = torch.ops.aten.view.default(clone_96, [1, 32, arg5_1, 128]);  clone_96 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35 in sdpa_attention_forward, code: causal_mask = causal_mask[:, :, :, : key.shape[-2]]
        slice_737: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(arg6_1, 0, 0, 9223372036854775807);  arg6_1 = None
        slice_738: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_737, 1, 0, 9223372036854775807);  slice_737 = None
        slice_739: "b8[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_738, 2, 0, 9223372036854775807);  slice_738 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:39 in sdpa_attention_forward, code: query = query.contiguous()
        clone_97: "f16[1, 32, s0, 128][4096*s0, 128*s0, 128, 1]cuda:0" = torch.ops.aten.clone.default(add_10334, memory_format = torch.contiguous_format);  add_10334 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        scalar_tensor_62: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(-inf, dtype = torch.float16, device = device(type='cuda', index=0))
        scalar_tensor_63: "f16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0))
        where_31: "f16[1, 1, s0, s1][s0*s1, s0*s1, s1, 1]cuda:0" = torch.ops.aten.where.self(slice_739, scalar_tensor_63, scalar_tensor_62);  slice_739 = scalar_tensor_63 = scalar_tensor_62 = None
        expand_163: "f16[1, 32, s0, s1][s0*s1, 0, s1, 1]cuda:0" = torch.ops.aten.expand.default(where_31, [1, 32, arg0_1, arg5_1]);  where_31 = arg5_1 = None
        _scaled_dot_product_efficient_attention_31 = torch.ops.aten._scaled_dot_product_efficient_attention.default(clone_97, view_1075, view_1076, expand_163, False, scale = 0.08838834764831845);  clone_97 = view_1075 = view_1076 = expand_163 = None
        getitem_250: "f16[1, 32, s0, 128][4096*s0, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_31[0];  _scaled_dot_product_efficient_attention_31 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_348: "f16[1, s0, 32, 128][4096*s0, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_250, [0, 2, 1, 3]);  getitem_250 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_1077: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(permute_348, [1, arg0_1, -1]);  permute_348 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        permute_349: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg323_1, [1, 0]);  arg323_1 = None
        view_1078: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1077, [arg0_1, 4096]);  view_1077 = None
        mm_220: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_1078, permute_349);  view_1078 = permute_349 = None
        view_1079: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_220, [1, arg0_1, 4096]);  mm_220 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_10516: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_10240, view_1079);  add_10240 = view_1079 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_1080: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_10516, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_126: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_602: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_126);  empty_126 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_127: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_603: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_127);  empty_127 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_63 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 64, constant_args_idx = 63, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_602, 'X_ptr': view_1080, 'W_ptr': arg324_1, 'RSTD_ptr': alias_603}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_602 = view_1080 = arg324_1 = alias_603 = None
        getitem_254: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_63['Y_ptr'];  triton_kernel_wrapper_functional_proxy_63 = None
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_350: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg325_1, [1, 0]);  arg325_1 = None
        alias_606: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_254)
        view_1083: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_606, [1, arg0_1, 4096]);  alias_606 = None
        view_1084: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1083, [arg0_1, 4096]);  view_1083 = None
        mm_221: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_1084, permute_350);  view_1084 = permute_350 = None
        view_1085: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_221, [1, arg0_1, 14336]);  mm_221 = None
        convert_element_type_509: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1085, torch.float32);  view_1085 = None
        sigmoid_31: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_509)
        mul_13105: "f32[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_509, sigmoid_31);  convert_element_type_509 = sigmoid_31 = None
        convert_element_type_510: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_13105, torch.float16);  mul_13105 = None
        permute_351: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg326_1, [1, 0]);  arg326_1 = None
        alias_607: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_254);  getitem_254 = None
        view_1087: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_607, [1, arg0_1, 4096]);  alias_607 = None
        view_1088: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1087, [arg0_1, 4096]);  view_1087 = None
        mm_222: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.mm.default(view_1088, permute_351);  view_1088 = permute_351 = None
        view_1089: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.view.default(mm_222, [1, arg0_1, 14336]);  mm_222 = None
        mul_13122: "f16[1, s0, 14336][14336*s0, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_510, view_1089);  convert_element_type_510 = view_1089 = None
        permute_352: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg327_1, [1, 0]);  arg327_1 = None
        view_1090: "f16[s0, 14336][14336, 1]cuda:0" = torch.ops.aten.view.default(mul_13122, [arg0_1, 14336]);  mul_13122 = None
        mm_223: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_1090, permute_352);  view_1090 = permute_352 = None
        view_1091: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(mm_223, [1, arg0_1, 4096]);  mm_223 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_10569: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_10516, view_1091);  add_10516 = view_1091 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_1092: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_10569, [-1, 4096]);  add_10569 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_128: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        alias_608: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(empty_128);  empty_128 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_129: "f32[s0][1]cuda:0" = torch.ops.aten.empty.memory_format([arg0_1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        alias_609: "f32[s0][1]cuda:0" = torch.ops.aten.alias.default(empty_129);  empty_129 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_64 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 65, constant_args_idx = 64, grid = [(arg0_1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': alias_608, 'X_ptr': view_1092, 'W_ptr': arg328_1, 'RSTD_ptr': alias_609}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  alias_608 = view_1092 = arg328_1 = alias_609 = None
        getitem_256: "f16[s0, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_64['Y_ptr'];  triton_kernel_wrapper_functional_proxy_64 = None
        
         # File: /app/low_bit_inference/model.py:453 in forward, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
        permute_353: "f16[4096, 128256][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg330_1, [1, 0]);  arg330_1 = None
        alias_612: "f16[s0, 4096][4096, 1]cuda:0" = torch.ops.aten.alias.default(getitem_256);  getitem_256 = None
        view_1095: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.view.default(alias_612, [1, arg0_1, 4096]);  alias_612 = arg0_1 = None
        slice_743: "f16[1, s0, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1095, 0, 0, 9223372036854775807);  view_1095 = None
        slice_744: "f16[1, 1, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_743, 1, -1, 9223372036854775807);  slice_743 = None
        slice_745: "f16[1, 1, 4096][4096*s0, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_744, 2, 0, 9223372036854775807);  slice_744 = None
        view_1096: "f16[1, 4096][4096*s0, 1]cuda:0" = torch.ops.aten.view.default(slice_745, [1, 4096]);  slice_745 = None
        convert_element_type_515: "f32[1, 4096][4096*s0, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1096, torch.float32);  view_1096 = None
        convert_element_type_516: "f32[4096, 128256][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_353, torch.float32);  permute_353 = None
        unsqueeze_196: "f32[1, 4096, 1][4096*s0, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_515, 2);  convert_element_type_515 = None
        unsqueeze_197: "f32[1, 4096, 128256][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_516, 0);  convert_element_type_516 = None
        mul_13166: "f32[1, 4096, 128256][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_196, unsqueeze_197);  unsqueeze_196 = unsqueeze_197 = None
        sum_1: "f32[1, 128256][128256, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_13166, [1]);  mul_13166 = None
        convert_element_type_517: "f16[1, 128256][128256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_1, torch.float16);  sum_1 = None
        view_1097: "f16[1, 1, 128256][128256, 128256, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_517, [1, 1, 128256]);  convert_element_type_517 = None
        return (alias_10, alias_9, alias_29, alias_28, alias_48, alias_47, alias_67, alias_66, alias_86, alias_85, alias_105, alias_104, alias_124, alias_123, alias_143, alias_142, alias_162, alias_161, alias_181, alias_180, alias_200, alias_199, alias_219, alias_218, alias_238, alias_237, alias_257, alias_256, alias_276, alias_275, alias_295, alias_294, alias_314, alias_313, alias_333, alias_332, alias_352, alias_351, alias_371, alias_370, alias_390, alias_389, alias_409, alias_408, alias_428, alias_427, alias_447, alias_446, alias_466, alias_465, alias_485, alias_484, alias_504, alias_503, alias_523, alias_522, alias_542, alias_541, alias_561, alias_560, alias_580, alias_579, alias_599, alias_598, view_1097)
        