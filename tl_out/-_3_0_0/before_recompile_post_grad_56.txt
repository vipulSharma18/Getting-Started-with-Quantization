class <lambda>(torch.nn.Module):
    def forward(self, arg0_1: "i64[1, 1][1, 1]cuda:0", arg1_1: "f16[128256, 4096][4096, 1]cuda:0", arg2_1: "i64[1][1]cuda:0", arg3_1: "i64[1, 1][1, 1]cuda:0", arg4_1: "b8[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0", arg5_1: "f32[64][1]cuda:0", arg6_1: "f16[4096][1]cuda:0", arg7_1: "f16[4096, 4096][4096, 1]cuda:0", arg8_1: "f16[1024, 4096][4096, 1]cuda:0", arg9_1: "f16[1024, 4096][4096, 1]cuda:0", arg10_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg11_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg12_1: "f16[4096, 4096][4096, 1]cuda:0", arg13_1: "f16[4096][1]cuda:0", arg14_1: "f16[14336, 4096][4096, 1]cuda:0", arg15_1: "f16[14336, 4096][4096, 1]cuda:0", arg16_1: "f16[4096, 14336][14336, 1]cuda:0", arg17_1: "f16[4096][1]cuda:0", arg18_1: "f16[4096, 4096][4096, 1]cuda:0", arg19_1: "f16[1024, 4096][4096, 1]cuda:0", arg20_1: "f16[1024, 4096][4096, 1]cuda:0", arg21_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg22_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg23_1: "f16[4096, 4096][4096, 1]cuda:0", arg24_1: "f16[4096][1]cuda:0", arg25_1: "f16[14336, 4096][4096, 1]cuda:0", arg26_1: "f16[14336, 4096][4096, 1]cuda:0", arg27_1: "f16[4096, 14336][14336, 1]cuda:0", arg28_1: "f16[4096][1]cuda:0", arg29_1: "f16[4096, 4096][4096, 1]cuda:0", arg30_1: "f16[1024, 4096][4096, 1]cuda:0", arg31_1: "f16[1024, 4096][4096, 1]cuda:0", arg32_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg33_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg34_1: "f16[4096, 4096][4096, 1]cuda:0", arg35_1: "f16[4096][1]cuda:0", arg36_1: "f16[14336, 4096][4096, 1]cuda:0", arg37_1: "f16[14336, 4096][4096, 1]cuda:0", arg38_1: "f16[4096, 14336][14336, 1]cuda:0", arg39_1: "f16[4096][1]cuda:0", arg40_1: "f16[4096, 4096][4096, 1]cuda:0", arg41_1: "f16[1024, 4096][4096, 1]cuda:0", arg42_1: "f16[1024, 4096][4096, 1]cuda:0", arg43_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg44_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg45_1: "f16[4096, 4096][4096, 1]cuda:0", arg46_1: "f16[4096][1]cuda:0", arg47_1: "f16[14336, 4096][4096, 1]cuda:0", arg48_1: "f16[14336, 4096][4096, 1]cuda:0", arg49_1: "f16[4096, 14336][14336, 1]cuda:0", arg50_1: "f16[4096][1]cuda:0", arg51_1: "f16[4096, 4096][4096, 1]cuda:0", arg52_1: "f16[1024, 4096][4096, 1]cuda:0", arg53_1: "f16[1024, 4096][4096, 1]cuda:0", arg54_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg55_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg56_1: "f16[4096, 4096][4096, 1]cuda:0", arg57_1: "f16[4096][1]cuda:0", arg58_1: "f16[14336, 4096][4096, 1]cuda:0", arg59_1: "f16[14336, 4096][4096, 1]cuda:0", arg60_1: "f16[4096, 14336][14336, 1]cuda:0", arg61_1: "f16[4096][1]cuda:0", arg62_1: "f16[4096, 4096][4096, 1]cuda:0", arg63_1: "f16[1024, 4096][4096, 1]cuda:0", arg64_1: "f16[1024, 4096][4096, 1]cuda:0", arg65_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg66_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg67_1: "f16[4096, 4096][4096, 1]cuda:0", arg68_1: "f16[4096][1]cuda:0", arg69_1: "f16[14336, 4096][4096, 1]cuda:0", arg70_1: "f16[14336, 4096][4096, 1]cuda:0", arg71_1: "f16[4096, 14336][14336, 1]cuda:0", arg72_1: "f16[4096][1]cuda:0", arg73_1: "f16[4096, 4096][4096, 1]cuda:0", arg74_1: "f16[1024, 4096][4096, 1]cuda:0", arg75_1: "f16[1024, 4096][4096, 1]cuda:0", arg76_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg77_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg78_1: "f16[4096, 4096][4096, 1]cuda:0", arg79_1: "f16[4096][1]cuda:0", arg80_1: "f16[14336, 4096][4096, 1]cuda:0", arg81_1: "f16[14336, 4096][4096, 1]cuda:0", arg82_1: "f16[4096, 14336][14336, 1]cuda:0", arg83_1: "f16[4096][1]cuda:0", arg84_1: "f16[4096, 4096][4096, 1]cuda:0", arg85_1: "f16[1024, 4096][4096, 1]cuda:0", arg86_1: "f16[1024, 4096][4096, 1]cuda:0", arg87_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg88_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg89_1: "f16[4096, 4096][4096, 1]cuda:0", arg90_1: "f16[4096][1]cuda:0", arg91_1: "f16[14336, 4096][4096, 1]cuda:0", arg92_1: "f16[14336, 4096][4096, 1]cuda:0", arg93_1: "f16[4096, 14336][14336, 1]cuda:0", arg94_1: "f16[4096][1]cuda:0", arg95_1: "f16[4096, 4096][4096, 1]cuda:0", arg96_1: "f16[1024, 4096][4096, 1]cuda:0", arg97_1: "f16[1024, 4096][4096, 1]cuda:0", arg98_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg99_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg100_1: "f16[4096, 4096][4096, 1]cuda:0", arg101_1: "f16[4096][1]cuda:0", arg102_1: "f16[14336, 4096][4096, 1]cuda:0", arg103_1: "f16[14336, 4096][4096, 1]cuda:0", arg104_1: "f16[4096, 14336][14336, 1]cuda:0", arg105_1: "f16[4096][1]cuda:0", arg106_1: "f16[4096, 4096][4096, 1]cuda:0", arg107_1: "f16[1024, 4096][4096, 1]cuda:0", arg108_1: "f16[1024, 4096][4096, 1]cuda:0", arg109_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg110_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg111_1: "f16[4096, 4096][4096, 1]cuda:0", arg112_1: "f16[4096][1]cuda:0", arg113_1: "f16[14336, 4096][4096, 1]cuda:0", arg114_1: "f16[14336, 4096][4096, 1]cuda:0", arg115_1: "f16[4096, 14336][14336, 1]cuda:0", arg116_1: "f16[4096][1]cuda:0", arg117_1: "f16[4096, 4096][4096, 1]cuda:0", arg118_1: "f16[1024, 4096][4096, 1]cuda:0", arg119_1: "f16[1024, 4096][4096, 1]cuda:0", arg120_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg121_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg122_1: "f16[4096, 4096][4096, 1]cuda:0", arg123_1: "f16[4096][1]cuda:0", arg124_1: "f16[14336, 4096][4096, 1]cuda:0", arg125_1: "f16[14336, 4096][4096, 1]cuda:0", arg126_1: "f16[4096, 14336][14336, 1]cuda:0", arg127_1: "f16[4096][1]cuda:0", arg128_1: "f16[4096, 4096][4096, 1]cuda:0", arg129_1: "f16[1024, 4096][4096, 1]cuda:0", arg130_1: "f16[1024, 4096][4096, 1]cuda:0", arg131_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg132_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg133_1: "f16[4096, 4096][4096, 1]cuda:0", arg134_1: "f16[4096][1]cuda:0", arg135_1: "f16[14336, 4096][4096, 1]cuda:0", arg136_1: "f16[14336, 4096][4096, 1]cuda:0", arg137_1: "f16[4096, 14336][14336, 1]cuda:0", arg138_1: "f16[4096][1]cuda:0", arg139_1: "f16[4096, 4096][4096, 1]cuda:0", arg140_1: "f16[1024, 4096][4096, 1]cuda:0", arg141_1: "f16[1024, 4096][4096, 1]cuda:0", arg142_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg143_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg144_1: "f16[4096, 4096][4096, 1]cuda:0", arg145_1: "f16[4096][1]cuda:0", arg146_1: "f16[14336, 4096][4096, 1]cuda:0", arg147_1: "f16[14336, 4096][4096, 1]cuda:0", arg148_1: "f16[4096, 14336][14336, 1]cuda:0", arg149_1: "f16[4096][1]cuda:0", arg150_1: "f16[4096, 4096][4096, 1]cuda:0", arg151_1: "f16[1024, 4096][4096, 1]cuda:0", arg152_1: "f16[1024, 4096][4096, 1]cuda:0", arg153_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg154_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg155_1: "f16[4096, 4096][4096, 1]cuda:0", arg156_1: "f16[4096][1]cuda:0", arg157_1: "f16[14336, 4096][4096, 1]cuda:0", arg158_1: "f16[14336, 4096][4096, 1]cuda:0", arg159_1: "f16[4096, 14336][14336, 1]cuda:0", arg160_1: "f16[4096][1]cuda:0", arg161_1: "f16[4096, 4096][4096, 1]cuda:0", arg162_1: "f16[1024, 4096][4096, 1]cuda:0", arg163_1: "f16[1024, 4096][4096, 1]cuda:0", arg164_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg165_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg166_1: "f16[4096, 4096][4096, 1]cuda:0", arg167_1: "f16[4096][1]cuda:0", arg168_1: "f16[14336, 4096][4096, 1]cuda:0", arg169_1: "f16[14336, 4096][4096, 1]cuda:0", arg170_1: "f16[4096, 14336][14336, 1]cuda:0", arg171_1: "f16[4096][1]cuda:0", arg172_1: "f16[4096, 4096][4096, 1]cuda:0", arg173_1: "f16[1024, 4096][4096, 1]cuda:0", arg174_1: "f16[1024, 4096][4096, 1]cuda:0", arg175_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg176_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg177_1: "f16[4096, 4096][4096, 1]cuda:0", arg178_1: "f16[4096][1]cuda:0", arg179_1: "f16[14336, 4096][4096, 1]cuda:0", arg180_1: "f16[14336, 4096][4096, 1]cuda:0", arg181_1: "f16[4096, 14336][14336, 1]cuda:0", arg182_1: "f16[4096][1]cuda:0", arg183_1: "f16[4096, 4096][4096, 1]cuda:0", arg184_1: "f16[1024, 4096][4096, 1]cuda:0", arg185_1: "f16[1024, 4096][4096, 1]cuda:0", arg186_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg187_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg188_1: "f16[4096, 4096][4096, 1]cuda:0", arg189_1: "f16[4096][1]cuda:0", arg190_1: "f16[14336, 4096][4096, 1]cuda:0", arg191_1: "f16[14336, 4096][4096, 1]cuda:0", arg192_1: "f16[4096, 14336][14336, 1]cuda:0", arg193_1: "f16[4096][1]cuda:0", arg194_1: "f16[4096, 4096][4096, 1]cuda:0", arg195_1: "f16[1024, 4096][4096, 1]cuda:0", arg196_1: "f16[1024, 4096][4096, 1]cuda:0", arg197_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg198_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg199_1: "f16[4096, 4096][4096, 1]cuda:0", arg200_1: "f16[4096][1]cuda:0", arg201_1: "f16[14336, 4096][4096, 1]cuda:0", arg202_1: "f16[14336, 4096][4096, 1]cuda:0", arg203_1: "f16[4096, 14336][14336, 1]cuda:0", arg204_1: "f16[4096][1]cuda:0", arg205_1: "f16[4096, 4096][4096, 1]cuda:0", arg206_1: "f16[1024, 4096][4096, 1]cuda:0", arg207_1: "f16[1024, 4096][4096, 1]cuda:0", arg208_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg209_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg210_1: "f16[4096, 4096][4096, 1]cuda:0", arg211_1: "f16[4096][1]cuda:0", arg212_1: "f16[14336, 4096][4096, 1]cuda:0", arg213_1: "f16[14336, 4096][4096, 1]cuda:0", arg214_1: "f16[4096, 14336][14336, 1]cuda:0", arg215_1: "f16[4096][1]cuda:0", arg216_1: "f16[4096, 4096][4096, 1]cuda:0", arg217_1: "f16[1024, 4096][4096, 1]cuda:0", arg218_1: "f16[1024, 4096][4096, 1]cuda:0", arg219_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg220_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg221_1: "f16[4096, 4096][4096, 1]cuda:0", arg222_1: "f16[4096][1]cuda:0", arg223_1: "f16[14336, 4096][4096, 1]cuda:0", arg224_1: "f16[14336, 4096][4096, 1]cuda:0", arg225_1: "f16[4096, 14336][14336, 1]cuda:0", arg226_1: "f16[4096][1]cuda:0", arg227_1: "f16[4096, 4096][4096, 1]cuda:0", arg228_1: "f16[1024, 4096][4096, 1]cuda:0", arg229_1: "f16[1024, 4096][4096, 1]cuda:0", arg230_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg231_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg232_1: "f16[4096, 4096][4096, 1]cuda:0", arg233_1: "f16[4096][1]cuda:0", arg234_1: "f16[14336, 4096][4096, 1]cuda:0", arg235_1: "f16[14336, 4096][4096, 1]cuda:0", arg236_1: "f16[4096, 14336][14336, 1]cuda:0", arg237_1: "f16[4096][1]cuda:0", arg238_1: "f16[4096, 4096][4096, 1]cuda:0", arg239_1: "f16[1024, 4096][4096, 1]cuda:0", arg240_1: "f16[1024, 4096][4096, 1]cuda:0", arg241_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg242_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg243_1: "f16[4096, 4096][4096, 1]cuda:0", arg244_1: "f16[4096][1]cuda:0", arg245_1: "f16[14336, 4096][4096, 1]cuda:0", arg246_1: "f16[14336, 4096][4096, 1]cuda:0", arg247_1: "f16[4096, 14336][14336, 1]cuda:0", arg248_1: "f16[4096][1]cuda:0", arg249_1: "f16[4096, 4096][4096, 1]cuda:0", arg250_1: "f16[1024, 4096][4096, 1]cuda:0", arg251_1: "f16[1024, 4096][4096, 1]cuda:0", arg252_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg253_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg254_1: "f16[4096, 4096][4096, 1]cuda:0", arg255_1: "f16[4096][1]cuda:0", arg256_1: "f16[14336, 4096][4096, 1]cuda:0", arg257_1: "f16[14336, 4096][4096, 1]cuda:0", arg258_1: "f16[4096, 14336][14336, 1]cuda:0", arg259_1: "f16[4096][1]cuda:0", arg260_1: "f16[4096, 4096][4096, 1]cuda:0", arg261_1: "f16[1024, 4096][4096, 1]cuda:0", arg262_1: "f16[1024, 4096][4096, 1]cuda:0", arg263_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg264_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg265_1: "f16[4096, 4096][4096, 1]cuda:0", arg266_1: "f16[4096][1]cuda:0", arg267_1: "f16[14336, 4096][4096, 1]cuda:0", arg268_1: "f16[14336, 4096][4096, 1]cuda:0", arg269_1: "f16[4096, 14336][14336, 1]cuda:0", arg270_1: "f16[4096][1]cuda:0", arg271_1: "f16[4096, 4096][4096, 1]cuda:0", arg272_1: "f16[1024, 4096][4096, 1]cuda:0", arg273_1: "f16[1024, 4096][4096, 1]cuda:0", arg274_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg275_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg276_1: "f16[4096, 4096][4096, 1]cuda:0", arg277_1: "f16[4096][1]cuda:0", arg278_1: "f16[14336, 4096][4096, 1]cuda:0", arg279_1: "f16[14336, 4096][4096, 1]cuda:0", arg280_1: "f16[4096, 14336][14336, 1]cuda:0", arg281_1: "f16[4096][1]cuda:0", arg282_1: "f16[4096, 4096][4096, 1]cuda:0", arg283_1: "f16[1024, 4096][4096, 1]cuda:0", arg284_1: "f16[1024, 4096][4096, 1]cuda:0", arg285_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg286_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg287_1: "f16[4096, 4096][4096, 1]cuda:0", arg288_1: "f16[4096][1]cuda:0", arg289_1: "f16[14336, 4096][4096, 1]cuda:0", arg290_1: "f16[14336, 4096][4096, 1]cuda:0", arg291_1: "f16[4096, 14336][14336, 1]cuda:0", arg292_1: "f16[4096][1]cuda:0", arg293_1: "f16[4096, 4096][4096, 1]cuda:0", arg294_1: "f16[1024, 4096][4096, 1]cuda:0", arg295_1: "f16[1024, 4096][4096, 1]cuda:0", arg296_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg297_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg298_1: "f16[4096, 4096][4096, 1]cuda:0", arg299_1: "f16[4096][1]cuda:0", arg300_1: "f16[14336, 4096][4096, 1]cuda:0", arg301_1: "f16[14336, 4096][4096, 1]cuda:0", arg302_1: "f16[4096, 14336][14336, 1]cuda:0", arg303_1: "f16[4096][1]cuda:0", arg304_1: "f16[4096, 4096][4096, 1]cuda:0", arg305_1: "f16[1024, 4096][4096, 1]cuda:0", arg306_1: "f16[1024, 4096][4096, 1]cuda:0", arg307_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg308_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg309_1: "f16[4096, 4096][4096, 1]cuda:0", arg310_1: "f16[4096][1]cuda:0", arg311_1: "f16[14336, 4096][4096, 1]cuda:0", arg312_1: "f16[14336, 4096][4096, 1]cuda:0", arg313_1: "f16[4096, 14336][14336, 1]cuda:0", arg314_1: "f16[4096][1]cuda:0", arg315_1: "f16[4096, 4096][4096, 1]cuda:0", arg316_1: "f16[1024, 4096][4096, 1]cuda:0", arg317_1: "f16[1024, 4096][4096, 1]cuda:0", arg318_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg319_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg320_1: "f16[4096, 4096][4096, 1]cuda:0", arg321_1: "f16[4096][1]cuda:0", arg322_1: "f16[14336, 4096][4096, 1]cuda:0", arg323_1: "f16[14336, 4096][4096, 1]cuda:0", arg324_1: "f16[4096, 14336][14336, 1]cuda:0", arg325_1: "f16[4096][1]cuda:0", arg326_1: "f16[4096, 4096][4096, 1]cuda:0", arg327_1: "f16[1024, 4096][4096, 1]cuda:0", arg328_1: "f16[1024, 4096][4096, 1]cuda:0", arg329_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg330_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg331_1: "f16[4096, 4096][4096, 1]cuda:0", arg332_1: "f16[4096][1]cuda:0", arg333_1: "f16[14336, 4096][4096, 1]cuda:0", arg334_1: "f16[14336, 4096][4096, 1]cuda:0", arg335_1: "f16[4096, 14336][14336, 1]cuda:0", arg336_1: "f16[4096][1]cuda:0", arg337_1: "f16[4096, 4096][4096, 1]cuda:0", arg338_1: "f16[1024, 4096][4096, 1]cuda:0", arg339_1: "f16[1024, 4096][4096, 1]cuda:0", arg340_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg341_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg342_1: "f16[4096, 4096][4096, 1]cuda:0", arg343_1: "f16[4096][1]cuda:0", arg344_1: "f16[14336, 4096][4096, 1]cuda:0", arg345_1: "f16[14336, 4096][4096, 1]cuda:0", arg346_1: "f16[4096, 14336][14336, 1]cuda:0", arg347_1: "f16[4096][1]cuda:0", arg348_1: "f16[4096, 4096][4096, 1]cuda:0", arg349_1: "f16[1024, 4096][4096, 1]cuda:0", arg350_1: "f16[1024, 4096][4096, 1]cuda:0", arg351_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg352_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0", arg353_1: "f16[4096, 4096][4096, 1]cuda:0", arg354_1: "f16[4096][1]cuda:0", arg355_1: "f16[14336, 4096][4096, 1]cuda:0", arg356_1: "f16[14336, 4096][4096, 1]cuda:0", arg357_1: "f16[4096, 14336][14336, 1]cuda:0", arg358_1: "f16[4096][1]cuda:0", arg359_1: "f16[128256, 4096][4096, 1]cuda:0"):
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:346 in forward, code: inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)
        embedding: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.embedding.default(arg1_1, arg0_1, 128004);  arg1_1 = arg0_1 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_4: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(embedding, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_1: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 196, constant_args_idx = 195, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty, 'X_ptr': view_4, 'W_ptr': arg6_1, 'RSTD_ptr': empty_1}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty = view_4 = arg6_1 = empty_1 = None
        getitem: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy['Y_ptr'];  triton_kernel_wrapper_functional_proxy = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_3: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem, torch.float32)
        unsqueeze_6: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_3, 2);  convert_element_type_3 = None
        permute_1: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg7_1, [1, 0]);  arg7_1 = None
        convert_element_type_4: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_1, torch.float32);  permute_1 = None
        unsqueeze_7: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_4, 0);  convert_element_type_4 = None
        mul_3: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_6, unsqueeze_7);  unsqueeze_6 = unsqueeze_7 = None
        sum_2: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_3, [1]);  mul_3 = None
        convert_element_type_5: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_2, torch.float16);  sum_2 = None
        view_9: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_5, [1, 1, 4096]);  convert_element_type_5 = None
        view_10: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_9, [1, 1, -1, 128]);  view_9 = None
        permute_2: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_10, [0, 2, 1, 3]);  view_10 = None
        
         # File: /app/low_bit_inference/model.py:65 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        unsqueeze: "f32[1, 64][64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(arg5_1, 0);  arg5_1 = None
        unsqueeze_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(unsqueeze, 2);  unsqueeze = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_1, [1, -1, 1]);  unsqueeze_1 = None
        
         # File: /app/low_bit_inference/model.py:70 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        expand_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = torch.ops.aten.expand.default(expand, [1, 64, 1]);  expand = None
        unsqueeze_3: "f32[1, 64, 1, 1][64, 1, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(expand_1, -1);  expand_1 = None
        
         # File: /app/low_bit_inference/model.py:66 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        unsqueeze_2: "i64[1, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(arg3_1, 1);  arg3_1 = None
        convert_element_type: "f32[1, 1, 1][1, 1, 1]cuda:0" = torch.ops.prims.convert_element_type.default(unsqueeze_2, torch.float32);  unsqueeze_2 = None
        
         # File: /app/low_bit_inference/model.py:70 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        expand_2: "f32[1, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.expand.default(convert_element_type, [1, 1, 1]);  convert_element_type = None
        unsqueeze_4: "f32[1, 1, 1, 1][1, 1, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(expand_2, 1);  expand_2 = None
        mul: "f32[1, 64, 1, 1][64, 1, 1, 1]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_3, unsqueeze_4);  unsqueeze_3 = unsqueeze_4 = None
        sum_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul, [2]);  mul = None
        permute: "f32[1, 1, 64][64, 1, 1]cuda:0" = torch.ops.aten.permute.default(sum_1, [0, 2, 1]);  sum_1 = None
        
         # File: /app/low_bit_inference/model.py:71 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        unsqueeze_5: "f32[1, 1, 1, 64][64, 1, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute, 2);  permute = None
        expand_3: "f32[1, 1, 2, 64][64, 1, 0, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_5, [1, 1, 2, 64]);  unsqueeze_5 = None
        clone: "f32[1, 1, 2, 64][128, 128, 64, 1]cuda:0" = torch.ops.aten.clone.default(expand_3, memory_format = torch.contiguous_format);  expand_3 = None
        view_3: "f32[1, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone, [1, 1, 128]);  clone = None
        
         # File: /app/low_bit_inference/model.py:72 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.cos.default(view_3)
        mul_1: "f32[1, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cos, 1.0);  cos = None
        
         # File: /app/low_bit_inference/model.py:75 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        convert_element_type_1: "f16[1, 1, 128][128, 128, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1, torch.float16);  mul_1 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_12: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_6: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_2, unsqueeze_12)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_5: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_2, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_5);  slice_5 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_4: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_2, 3, 0, 64);  permute_2 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg, slice_4], -1);  neg = slice_4 = None
        
         # File: /app/low_bit_inference/model.py:73 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.sin.default(view_3);  view_3 = None
        mul_2: "f32[1, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(sin, 1.0);  sin = None
        
         # File: /app/low_bit_inference/model.py:75 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        convert_element_type_2: "f16[1, 1, 128][128, 128, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_2, torch.float16);  mul_2 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_13: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_7: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat, unsqueeze_13);  cat = None
        add: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_6, mul_7);  mul_6 = mul_7 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_6: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem, torch.float32)
        unsqueeze_8: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_6, 2);  convert_element_type_6 = None
        permute_3: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg8_1, [1, 0]);  arg8_1 = None
        convert_element_type_7: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_3, torch.float32);  permute_3 = None
        unsqueeze_9: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_7, 0);  convert_element_type_7 = None
        mul_4: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_8, unsqueeze_9);  unsqueeze_8 = unsqueeze_9 = None
        sum_3: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_4, [1]);  mul_4 = None
        convert_element_type_8: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_3, torch.float16);  sum_3 = None
        view_14: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_8, [1, 1, 1024]);  convert_element_type_8 = None
        view_15: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_14, [1, 1, -1, 128]);  view_14 = None
        permute_4: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_15, [0, 2, 1, 3]);  view_15 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_8: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_4, unsqueeze_12);  unsqueeze_12 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_7: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_4, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_7);  slice_7 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_6: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_4, 3, 0, 64);  permute_4 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_1: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_1, slice_6], -1);  neg_1 = slice_6 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_9: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_1, unsqueeze_13);  cat_1 = unsqueeze_13 = None
        add_1: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_8, mul_9);  mul_8 = mul_9 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg10_1, [None, None, arg2_1], add_1);  add_1 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_15: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put, 2)
        expand_5: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_15, [1, 8, 4, 2048, 128]);  unsqueeze_15 = None
        clone_2: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_5, memory_format = torch.contiguous_format);  expand_5 = None
        view_21: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_2, [1, 32, 2048, 128]);  clone_2 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_9: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem, torch.float32);  getitem = None
        unsqueeze_10: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_9, 2);  convert_element_type_9 = None
        permute_5: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg9_1, [1, 0]);  arg9_1 = None
        convert_element_type_10: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_5, torch.float32);  permute_5 = None
        unsqueeze_11: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_10, 0);  convert_element_type_10 = None
        mul_5: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_10, unsqueeze_11);  unsqueeze_10 = unsqueeze_11 = None
        sum_4: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_5, [1]);  mul_5 = None
        convert_element_type_11: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_4, torch.float16);  sum_4 = None
        view_19: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_11, [1, 1, 1024]);  convert_element_type_11 = None
        view_20: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_19, [1, 1, -1, 128]);  view_19 = None
        permute_6: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_20, [0, 2, 1, 3]);  view_20 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg11_1, [None, None, arg2_1], permute_6);  permute_6 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_17: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_1, 2)
        expand_7: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_17, [1, 8, 4, 2048, 128]);  unsqueeze_17 = None
        clone_3: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_7, memory_format = torch.contiguous_format);  expand_7 = None
        view_22: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_3, [1, 32, 2048, 128]);  clone_3 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_1: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_1, full_default);  full_default_1 = full_default = None
        expand_8: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where, [1, 32, 1, 2048]);  where = None
        _scaled_dot_product_efficient_attention = torch.ops.aten._scaled_dot_product_efficient_attention.default(add, view_21, view_22, expand_8, False, scale = 0.08838834764831845);  add = view_21 = view_22 = expand_8 = None
        getitem_2: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention[0];  _scaled_dot_product_efficient_attention = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_2: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_7: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_2, [0, 2, 1, 3]);  getitem_2 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_23: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_7, [1, 1, -1]);  permute_7 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_24: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_23, [1, 4096]);  view_23 = None
        convert_element_type_12: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_24, torch.float32);  view_24 = None
        unsqueeze_18: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_12, 2);  convert_element_type_12 = None
        permute_8: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg12_1, [1, 0]);  arg12_1 = None
        convert_element_type_13: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_8, torch.float32);  permute_8 = None
        unsqueeze_19: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_13, 0);  convert_element_type_13 = None
        mul_10: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_18, unsqueeze_19);  unsqueeze_18 = unsqueeze_19 = None
        sum_5: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_10, [1]);  mul_10 = None
        convert_element_type_14: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_5, torch.float16);  sum_5 = None
        view_25: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_14, [1, 1, 4096]);  convert_element_type_14 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_2: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(embedding, view_25);  embedding = view_25 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_26: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_2, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_3: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_1 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 197, constant_args_idx = 196, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_2, 'X_ptr': view_26, 'W_ptr': arg13_1, 'RSTD_ptr': empty_3}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_2 = view_26 = arg13_1 = empty_3 = None
        getitem_6: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_1['Y_ptr'];  triton_kernel_wrapper_functional_proxy_1 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_4: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_15: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_6, torch.float32)
        unsqueeze_20: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_15, 2);  convert_element_type_15 = None
        permute_9: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg14_1, [1, 0]);  arg14_1 = None
        convert_element_type_16: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_9, torch.float32);  permute_9 = None
        unsqueeze_21: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_16, 0);  convert_element_type_16 = None
        mul_11: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_20, unsqueeze_21);  unsqueeze_20 = unsqueeze_21 = None
        sum_6: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_11, [1]);  mul_11 = None
        convert_element_type_17: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_6, torch.float16);  sum_6 = None
        view_31: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_17, [1, 1, 14336]);  convert_element_type_17 = None
        convert_element_type_18: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_31, torch.float32);  view_31 = None
        sigmoid: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_18)
        mul_12: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_18, sigmoid);  convert_element_type_18 = sigmoid = None
        convert_element_type_19: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_12, torch.float16);  mul_12 = None
        convert_element_type_20: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_6, torch.float32);  getitem_6 = None
        unsqueeze_22: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_20, 2);  convert_element_type_20 = None
        permute_10: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg15_1, [1, 0]);  arg15_1 = None
        convert_element_type_21: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_10, torch.float32);  permute_10 = None
        unsqueeze_23: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_21, 0);  convert_element_type_21 = None
        mul_13: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_22, unsqueeze_23);  unsqueeze_22 = unsqueeze_23 = None
        sum_7: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_13, [1]);  mul_13 = None
        convert_element_type_22: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_7, torch.float16);  sum_7 = None
        view_35: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_22, [1, 1, 14336]);  convert_element_type_22 = None
        mul_14: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_19, view_35);  convert_element_type_19 = view_35 = None
        view_36: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_14, [1, 14336]);  mul_14 = None
        convert_element_type_23: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_36, torch.float32);  view_36 = None
        unsqueeze_24: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_23, 2);  convert_element_type_23 = None
        permute_11: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg16_1, [1, 0]);  arg16_1 = None
        convert_element_type_24: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_11, torch.float32);  permute_11 = None
        unsqueeze_25: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_24, 0);  convert_element_type_24 = None
        mul_15: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_24, unsqueeze_25);  unsqueeze_24 = unsqueeze_25 = None
        sum_8: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_15, [1]);  mul_15 = None
        convert_element_type_25: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_8, torch.float16);  sum_8 = None
        view_37: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_25, [1, 1, 4096]);  convert_element_type_25 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_3: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_2, view_37);  add_2 = view_37 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_38: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_3, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_5: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_2 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 198, constant_args_idx = 197, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_4, 'X_ptr': view_38, 'W_ptr': arg17_1, 'RSTD_ptr': empty_5}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_4 = view_38 = arg17_1 = empty_5 = None
        getitem_8: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_2['Y_ptr'];  triton_kernel_wrapper_functional_proxy_2 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_26: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_8, torch.float32)
        unsqueeze_26: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_26, 2);  convert_element_type_26 = None
        permute_12: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg18_1, [1, 0]);  arg18_1 = None
        convert_element_type_27: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_12, torch.float32);  permute_12 = None
        unsqueeze_27: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_27, 0);  convert_element_type_27 = None
        mul_16: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_26, unsqueeze_27);  unsqueeze_26 = unsqueeze_27 = None
        sum_9: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_16, [1]);  mul_16 = None
        convert_element_type_28: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_9, torch.float16);  sum_9 = None
        view_43: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_28, [1, 1, 4096]);  convert_element_type_28 = None
        view_44: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_43, [1, 1, -1, 128]);  view_43 = None
        permute_13: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_44, [0, 2, 1, 3]);  view_44 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_32: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_19: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_13, unsqueeze_32)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_28: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_13, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_2: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_28);  slice_28 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_27: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_13, 3, 0, 64);  permute_13 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_2: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_2, slice_27], -1);  neg_2 = slice_27 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_33: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_20: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_2, unsqueeze_33);  cat_2 = None
        add_4: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_19, mul_20);  mul_19 = mul_20 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_29: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_8, torch.float32)
        unsqueeze_28: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_29, 2);  convert_element_type_29 = None
        permute_14: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg19_1, [1, 0]);  arg19_1 = None
        convert_element_type_30: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_14, torch.float32);  permute_14 = None
        unsqueeze_29: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_30, 0);  convert_element_type_30 = None
        mul_17: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_28, unsqueeze_29);  unsqueeze_28 = unsqueeze_29 = None
        sum_10: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_17, [1]);  mul_17 = None
        convert_element_type_31: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_10, torch.float16);  sum_10 = None
        view_48: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_31, [1, 1, 1024]);  convert_element_type_31 = None
        view_49: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_48, [1, 1, -1, 128]);  view_48 = None
        permute_15: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_49, [0, 2, 1, 3]);  view_49 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_21: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_15, unsqueeze_32);  unsqueeze_32 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_30: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_15, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_3: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_30);  slice_30 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_29: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_15, 3, 0, 64);  permute_15 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_3: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_3, slice_29], -1);  neg_3 = slice_29 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_22: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_3, unsqueeze_33);  cat_3 = unsqueeze_33 = None
        add_5: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_21, mul_22);  mul_21 = mul_22 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_2: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg21_1, [None, None, arg2_1], add_5);  add_5 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_35: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_2, 2)
        expand_10: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_35, [1, 8, 4, 2048, 128]);  unsqueeze_35 = None
        clone_4: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_10, memory_format = torch.contiguous_format);  expand_10 = None
        view_55: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_4, [1, 32, 2048, 128]);  clone_4 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_32: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_8, torch.float32);  getitem_8 = None
        unsqueeze_30: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_32, 2);  convert_element_type_32 = None
        permute_16: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg20_1, [1, 0]);  arg20_1 = None
        convert_element_type_33: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_16, torch.float32);  permute_16 = None
        unsqueeze_31: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_33, 0);  convert_element_type_33 = None
        mul_18: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_30, unsqueeze_31);  unsqueeze_30 = unsqueeze_31 = None
        sum_11: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_18, [1]);  mul_18 = None
        convert_element_type_34: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_11, torch.float16);  sum_11 = None
        view_53: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_34, [1, 1, 1024]);  convert_element_type_34 = None
        view_54: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_53, [1, 1, -1, 128]);  view_53 = None
        permute_17: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_54, [0, 2, 1, 3]);  view_54 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_3: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg22_1, [None, None, arg2_1], permute_17);  permute_17 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_37: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_3, 2)
        expand_12: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_37, [1, 8, 4, 2048, 128]);  unsqueeze_37 = None
        clone_5: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_12, memory_format = torch.contiguous_format);  expand_12 = None
        view_56: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_5, [1, 32, 2048, 128]);  clone_5 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_3: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_2: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_1: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_3, full_default_2);  full_default_3 = full_default_2 = None
        expand_13: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_1, [1, 32, 1, 2048]);  where_1 = None
        _scaled_dot_product_efficient_attention_1 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_4, view_55, view_56, expand_13, False, scale = 0.08838834764831845);  add_4 = view_55 = view_56 = expand_13 = None
        getitem_10: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_1[0];  _scaled_dot_product_efficient_attention_1 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_6: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_18: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_10, [0, 2, 1, 3]);  getitem_10 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_57: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_18, [1, 1, -1]);  permute_18 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_58: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_57, [1, 4096]);  view_57 = None
        convert_element_type_35: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_58, torch.float32);  view_58 = None
        unsqueeze_38: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_35, 2);  convert_element_type_35 = None
        permute_19: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg23_1, [1, 0]);  arg23_1 = None
        convert_element_type_36: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_19, torch.float32);  permute_19 = None
        unsqueeze_39: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_36, 0);  convert_element_type_36 = None
        mul_23: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_38, unsqueeze_39);  unsqueeze_38 = unsqueeze_39 = None
        sum_12: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_23, [1]);  mul_23 = None
        convert_element_type_37: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_12, torch.float16);  sum_12 = None
        view_59: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_37, [1, 1, 4096]);  convert_element_type_37 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_6: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_3, view_59);  add_3 = view_59 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_60: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_6, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_7: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_3 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 199, constant_args_idx = 198, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_6, 'X_ptr': view_60, 'W_ptr': arg24_1, 'RSTD_ptr': empty_7}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_6 = view_60 = arg24_1 = empty_7 = None
        getitem_14: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_3['Y_ptr'];  triton_kernel_wrapper_functional_proxy_3 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_8: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_38: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_14, torch.float32)
        unsqueeze_40: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_38, 2);  convert_element_type_38 = None
        permute_20: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg25_1, [1, 0]);  arg25_1 = None
        convert_element_type_39: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_20, torch.float32);  permute_20 = None
        unsqueeze_41: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_39, 0);  convert_element_type_39 = None
        mul_24: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_40, unsqueeze_41);  unsqueeze_40 = unsqueeze_41 = None
        sum_13: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_24, [1]);  mul_24 = None
        convert_element_type_40: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_13, torch.float16);  sum_13 = None
        view_65: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_40, [1, 1, 14336]);  convert_element_type_40 = None
        convert_element_type_41: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_65, torch.float32);  view_65 = None
        sigmoid_1: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_41)
        mul_25: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_41, sigmoid_1);  convert_element_type_41 = sigmoid_1 = None
        convert_element_type_42: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_25, torch.float16);  mul_25 = None
        convert_element_type_43: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_14, torch.float32);  getitem_14 = None
        unsqueeze_42: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_43, 2);  convert_element_type_43 = None
        permute_21: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg26_1, [1, 0]);  arg26_1 = None
        convert_element_type_44: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_21, torch.float32);  permute_21 = None
        unsqueeze_43: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_44, 0);  convert_element_type_44 = None
        mul_26: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_42, unsqueeze_43);  unsqueeze_42 = unsqueeze_43 = None
        sum_14: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_26, [1]);  mul_26 = None
        convert_element_type_45: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_14, torch.float16);  sum_14 = None
        view_69: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_45, [1, 1, 14336]);  convert_element_type_45 = None
        mul_27: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_42, view_69);  convert_element_type_42 = view_69 = None
        view_70: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_27, [1, 14336]);  mul_27 = None
        convert_element_type_46: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_70, torch.float32);  view_70 = None
        unsqueeze_44: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_46, 2);  convert_element_type_46 = None
        permute_22: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg27_1, [1, 0]);  arg27_1 = None
        convert_element_type_47: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_22, torch.float32);  permute_22 = None
        unsqueeze_45: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_47, 0);  convert_element_type_47 = None
        mul_28: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_44, unsqueeze_45);  unsqueeze_44 = unsqueeze_45 = None
        sum_15: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_28, [1]);  mul_28 = None
        convert_element_type_48: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_15, torch.float16);  sum_15 = None
        view_71: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_48, [1, 1, 4096]);  convert_element_type_48 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_7: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_6, view_71);  add_6 = view_71 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_72: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_7, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_9: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_4 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 200, constant_args_idx = 199, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_8, 'X_ptr': view_72, 'W_ptr': arg28_1, 'RSTD_ptr': empty_9}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_8 = view_72 = arg28_1 = empty_9 = None
        getitem_16: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_4['Y_ptr'];  triton_kernel_wrapper_functional_proxy_4 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_49: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_16, torch.float32)
        unsqueeze_46: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_49, 2);  convert_element_type_49 = None
        permute_23: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg29_1, [1, 0]);  arg29_1 = None
        convert_element_type_50: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_23, torch.float32);  permute_23 = None
        unsqueeze_47: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_50, 0);  convert_element_type_50 = None
        mul_29: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_46, unsqueeze_47);  unsqueeze_46 = unsqueeze_47 = None
        sum_16: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_29, [1]);  mul_29 = None
        convert_element_type_51: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_16, torch.float16);  sum_16 = None
        view_77: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_51, [1, 1, 4096]);  convert_element_type_51 = None
        view_78: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_77, [1, 1, -1, 128]);  view_77 = None
        permute_24: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_78, [0, 2, 1, 3]);  view_78 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_52: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_32: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_24, unsqueeze_52)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_51: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_24, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_4: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_51);  slice_51 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_50: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_24, 3, 0, 64);  permute_24 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_4: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_4, slice_50], -1);  neg_4 = slice_50 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_53: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_33: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_4, unsqueeze_53);  cat_4 = None
        add_8: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_32, mul_33);  mul_32 = mul_33 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_52: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_16, torch.float32)
        unsqueeze_48: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_52, 2);  convert_element_type_52 = None
        permute_25: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg30_1, [1, 0]);  arg30_1 = None
        convert_element_type_53: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_25, torch.float32);  permute_25 = None
        unsqueeze_49: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_53, 0);  convert_element_type_53 = None
        mul_30: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_48, unsqueeze_49);  unsqueeze_48 = unsqueeze_49 = None
        sum_17: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_30, [1]);  mul_30 = None
        convert_element_type_54: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_17, torch.float16);  sum_17 = None
        view_82: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_54, [1, 1, 1024]);  convert_element_type_54 = None
        view_83: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_82, [1, 1, -1, 128]);  view_82 = None
        permute_26: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_83, [0, 2, 1, 3]);  view_83 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_34: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_26, unsqueeze_52);  unsqueeze_52 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_53: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_26, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_5: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_53);  slice_53 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_52: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_26, 3, 0, 64);  permute_26 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_5: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_5, slice_52], -1);  neg_5 = slice_52 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_35: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_5, unsqueeze_53);  cat_5 = unsqueeze_53 = None
        add_9: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_34, mul_35);  mul_34 = mul_35 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_4: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg32_1, [None, None, arg2_1], add_9);  add_9 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_55: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_4, 2)
        expand_15: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_55, [1, 8, 4, 2048, 128]);  unsqueeze_55 = None
        clone_6: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_15, memory_format = torch.contiguous_format);  expand_15 = None
        view_89: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_6, [1, 32, 2048, 128]);  clone_6 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_55: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_16, torch.float32);  getitem_16 = None
        unsqueeze_50: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_55, 2);  convert_element_type_55 = None
        permute_27: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg31_1, [1, 0]);  arg31_1 = None
        convert_element_type_56: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_27, torch.float32);  permute_27 = None
        unsqueeze_51: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_56, 0);  convert_element_type_56 = None
        mul_31: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_50, unsqueeze_51);  unsqueeze_50 = unsqueeze_51 = None
        sum_18: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_31, [1]);  mul_31 = None
        convert_element_type_57: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_18, torch.float16);  sum_18 = None
        view_87: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_57, [1, 1, 1024]);  convert_element_type_57 = None
        view_88: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_87, [1, 1, -1, 128]);  view_87 = None
        permute_28: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_88, [0, 2, 1, 3]);  view_88 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_5: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg33_1, [None, None, arg2_1], permute_28);  permute_28 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_57: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_5, 2)
        expand_17: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_57, [1, 8, 4, 2048, 128]);  unsqueeze_57 = None
        clone_7: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_17, memory_format = torch.contiguous_format);  expand_17 = None
        view_90: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_7, [1, 32, 2048, 128]);  clone_7 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_5: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_4: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_2: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_5, full_default_4);  full_default_5 = full_default_4 = None
        expand_18: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_2, [1, 32, 1, 2048]);  where_2 = None
        _scaled_dot_product_efficient_attention_2 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_8, view_89, view_90, expand_18, False, scale = 0.08838834764831845);  add_8 = view_89 = view_90 = expand_18 = None
        getitem_18: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_2[0];  _scaled_dot_product_efficient_attention_2 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_10: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_29: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_18, [0, 2, 1, 3]);  getitem_18 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_91: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_29, [1, 1, -1]);  permute_29 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_92: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_91, [1, 4096]);  view_91 = None
        convert_element_type_58: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_92, torch.float32);  view_92 = None
        unsqueeze_58: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_58, 2);  convert_element_type_58 = None
        permute_30: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg34_1, [1, 0]);  arg34_1 = None
        convert_element_type_59: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_30, torch.float32);  permute_30 = None
        unsqueeze_59: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_59, 0);  convert_element_type_59 = None
        mul_36: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_58, unsqueeze_59);  unsqueeze_58 = unsqueeze_59 = None
        sum_19: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_36, [1]);  mul_36 = None
        convert_element_type_60: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_19, torch.float16);  sum_19 = None
        view_93: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_60, [1, 1, 4096]);  convert_element_type_60 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_10: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_7, view_93);  add_7 = view_93 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_94: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_10, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_11: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_5 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 201, constant_args_idx = 200, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_10, 'X_ptr': view_94, 'W_ptr': arg35_1, 'RSTD_ptr': empty_11}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_10 = view_94 = arg35_1 = empty_11 = None
        getitem_22: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_5['Y_ptr'];  triton_kernel_wrapper_functional_proxy_5 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_12: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_61: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_22, torch.float32)
        unsqueeze_60: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_61, 2);  convert_element_type_61 = None
        permute_31: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg36_1, [1, 0]);  arg36_1 = None
        convert_element_type_62: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_31, torch.float32);  permute_31 = None
        unsqueeze_61: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_62, 0);  convert_element_type_62 = None
        mul_37: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_60, unsqueeze_61);  unsqueeze_60 = unsqueeze_61 = None
        sum_20: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_37, [1]);  mul_37 = None
        convert_element_type_63: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_20, torch.float16);  sum_20 = None
        view_99: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_63, [1, 1, 14336]);  convert_element_type_63 = None
        convert_element_type_64: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_99, torch.float32);  view_99 = None
        sigmoid_2: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_64)
        mul_38: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_64, sigmoid_2);  convert_element_type_64 = sigmoid_2 = None
        convert_element_type_65: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_38, torch.float16);  mul_38 = None
        convert_element_type_66: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_22, torch.float32);  getitem_22 = None
        unsqueeze_62: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_66, 2);  convert_element_type_66 = None
        permute_32: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg37_1, [1, 0]);  arg37_1 = None
        convert_element_type_67: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_32, torch.float32);  permute_32 = None
        unsqueeze_63: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_67, 0);  convert_element_type_67 = None
        mul_39: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_62, unsqueeze_63);  unsqueeze_62 = unsqueeze_63 = None
        sum_21: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_39, [1]);  mul_39 = None
        convert_element_type_68: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_21, torch.float16);  sum_21 = None
        view_103: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_68, [1, 1, 14336]);  convert_element_type_68 = None
        mul_40: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_65, view_103);  convert_element_type_65 = view_103 = None
        view_104: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_40, [1, 14336]);  mul_40 = None
        convert_element_type_69: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_104, torch.float32);  view_104 = None
        unsqueeze_64: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_69, 2);  convert_element_type_69 = None
        permute_33: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg38_1, [1, 0]);  arg38_1 = None
        convert_element_type_70: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_33, torch.float32);  permute_33 = None
        unsqueeze_65: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_70, 0);  convert_element_type_70 = None
        mul_41: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_64, unsqueeze_65);  unsqueeze_64 = unsqueeze_65 = None
        sum_22: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_41, [1]);  mul_41 = None
        convert_element_type_71: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_22, torch.float16);  sum_22 = None
        view_105: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_71, [1, 1, 4096]);  convert_element_type_71 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_11: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_10, view_105);  add_10 = view_105 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_106: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_11, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_13: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_6 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 202, constant_args_idx = 201, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_12, 'X_ptr': view_106, 'W_ptr': arg39_1, 'RSTD_ptr': empty_13}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_12 = view_106 = arg39_1 = empty_13 = None
        getitem_24: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_6['Y_ptr'];  triton_kernel_wrapper_functional_proxy_6 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_72: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_24, torch.float32)
        unsqueeze_66: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_72, 2);  convert_element_type_72 = None
        permute_34: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg40_1, [1, 0]);  arg40_1 = None
        convert_element_type_73: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_34, torch.float32);  permute_34 = None
        unsqueeze_67: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_73, 0);  convert_element_type_73 = None
        mul_42: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_66, unsqueeze_67);  unsqueeze_66 = unsqueeze_67 = None
        sum_23: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_42, [1]);  mul_42 = None
        convert_element_type_74: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_23, torch.float16);  sum_23 = None
        view_111: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_74, [1, 1, 4096]);  convert_element_type_74 = None
        view_112: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_111, [1, 1, -1, 128]);  view_111 = None
        permute_35: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_112, [0, 2, 1, 3]);  view_112 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_72: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_45: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_35, unsqueeze_72)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_74: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_35, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_6: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_74);  slice_74 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_73: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_35, 3, 0, 64);  permute_35 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_6: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_6, slice_73], -1);  neg_6 = slice_73 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_73: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_46: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_6, unsqueeze_73);  cat_6 = None
        add_12: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_45, mul_46);  mul_45 = mul_46 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_75: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_24, torch.float32)
        unsqueeze_68: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_75, 2);  convert_element_type_75 = None
        permute_36: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg41_1, [1, 0]);  arg41_1 = None
        convert_element_type_76: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_36, torch.float32);  permute_36 = None
        unsqueeze_69: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_76, 0);  convert_element_type_76 = None
        mul_43: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_68, unsqueeze_69);  unsqueeze_68 = unsqueeze_69 = None
        sum_24: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_43, [1]);  mul_43 = None
        convert_element_type_77: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_24, torch.float16);  sum_24 = None
        view_116: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_77, [1, 1, 1024]);  convert_element_type_77 = None
        view_117: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_116, [1, 1, -1, 128]);  view_116 = None
        permute_37: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_117, [0, 2, 1, 3]);  view_117 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_47: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_37, unsqueeze_72);  unsqueeze_72 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_76: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_37, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_7: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_76);  slice_76 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_75: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_37, 3, 0, 64);  permute_37 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_7: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_7, slice_75], -1);  neg_7 = slice_75 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_48: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_7, unsqueeze_73);  cat_7 = unsqueeze_73 = None
        add_13: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_47, mul_48);  mul_47 = mul_48 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_6: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg43_1, [None, None, arg2_1], add_13);  add_13 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_75: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_6, 2)
        expand_20: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_75, [1, 8, 4, 2048, 128]);  unsqueeze_75 = None
        clone_8: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_20, memory_format = torch.contiguous_format);  expand_20 = None
        view_123: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_8, [1, 32, 2048, 128]);  clone_8 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_78: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_24, torch.float32);  getitem_24 = None
        unsqueeze_70: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_78, 2);  convert_element_type_78 = None
        permute_38: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg42_1, [1, 0]);  arg42_1 = None
        convert_element_type_79: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_38, torch.float32);  permute_38 = None
        unsqueeze_71: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_79, 0);  convert_element_type_79 = None
        mul_44: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_70, unsqueeze_71);  unsqueeze_70 = unsqueeze_71 = None
        sum_25: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_44, [1]);  mul_44 = None
        convert_element_type_80: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_25, torch.float16);  sum_25 = None
        view_121: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_80, [1, 1, 1024]);  convert_element_type_80 = None
        view_122: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_121, [1, 1, -1, 128]);  view_121 = None
        permute_39: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_122, [0, 2, 1, 3]);  view_122 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_7: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg44_1, [None, None, arg2_1], permute_39);  permute_39 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_77: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_7, 2)
        expand_22: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_77, [1, 8, 4, 2048, 128]);  unsqueeze_77 = None
        clone_9: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_22, memory_format = torch.contiguous_format);  expand_22 = None
        view_124: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_9, [1, 32, 2048, 128]);  clone_9 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_7: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_6: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_3: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_7, full_default_6);  full_default_7 = full_default_6 = None
        expand_23: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_3, [1, 32, 1, 2048]);  where_3 = None
        _scaled_dot_product_efficient_attention_3 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_12, view_123, view_124, expand_23, False, scale = 0.08838834764831845);  add_12 = view_123 = view_124 = expand_23 = None
        getitem_26: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_3[0];  _scaled_dot_product_efficient_attention_3 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_14: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_40: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_26, [0, 2, 1, 3]);  getitem_26 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_125: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_40, [1, 1, -1]);  permute_40 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_126: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_125, [1, 4096]);  view_125 = None
        convert_element_type_81: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_126, torch.float32);  view_126 = None
        unsqueeze_78: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_81, 2);  convert_element_type_81 = None
        permute_41: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg45_1, [1, 0]);  arg45_1 = None
        convert_element_type_82: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_41, torch.float32);  permute_41 = None
        unsqueeze_79: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_82, 0);  convert_element_type_82 = None
        mul_49: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_78, unsqueeze_79);  unsqueeze_78 = unsqueeze_79 = None
        sum_26: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_49, [1]);  mul_49 = None
        convert_element_type_83: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_26, torch.float16);  sum_26 = None
        view_127: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_83, [1, 1, 4096]);  convert_element_type_83 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_14: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_11, view_127);  add_11 = view_127 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_128: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_14, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_15: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_7 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 203, constant_args_idx = 202, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_14, 'X_ptr': view_128, 'W_ptr': arg46_1, 'RSTD_ptr': empty_15}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_14 = view_128 = arg46_1 = empty_15 = None
        getitem_30: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_7['Y_ptr'];  triton_kernel_wrapper_functional_proxy_7 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_16: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_84: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_30, torch.float32)
        unsqueeze_80: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_84, 2);  convert_element_type_84 = None
        permute_42: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg47_1, [1, 0]);  arg47_1 = None
        convert_element_type_85: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_42, torch.float32);  permute_42 = None
        unsqueeze_81: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_85, 0);  convert_element_type_85 = None
        mul_50: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_80, unsqueeze_81);  unsqueeze_80 = unsqueeze_81 = None
        sum_27: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_50, [1]);  mul_50 = None
        convert_element_type_86: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_27, torch.float16);  sum_27 = None
        view_133: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_86, [1, 1, 14336]);  convert_element_type_86 = None
        convert_element_type_87: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_133, torch.float32);  view_133 = None
        sigmoid_3: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_87)
        mul_51: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_87, sigmoid_3);  convert_element_type_87 = sigmoid_3 = None
        convert_element_type_88: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_51, torch.float16);  mul_51 = None
        convert_element_type_89: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_30, torch.float32);  getitem_30 = None
        unsqueeze_82: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_89, 2);  convert_element_type_89 = None
        permute_43: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg48_1, [1, 0]);  arg48_1 = None
        convert_element_type_90: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_43, torch.float32);  permute_43 = None
        unsqueeze_83: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_90, 0);  convert_element_type_90 = None
        mul_52: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_82, unsqueeze_83);  unsqueeze_82 = unsqueeze_83 = None
        sum_28: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_52, [1]);  mul_52 = None
        convert_element_type_91: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_28, torch.float16);  sum_28 = None
        view_137: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_91, [1, 1, 14336]);  convert_element_type_91 = None
        mul_53: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_88, view_137);  convert_element_type_88 = view_137 = None
        view_138: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_53, [1, 14336]);  mul_53 = None
        convert_element_type_92: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_138, torch.float32);  view_138 = None
        unsqueeze_84: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_92, 2);  convert_element_type_92 = None
        permute_44: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg49_1, [1, 0]);  arg49_1 = None
        convert_element_type_93: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_44, torch.float32);  permute_44 = None
        unsqueeze_85: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_93, 0);  convert_element_type_93 = None
        mul_54: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_84, unsqueeze_85);  unsqueeze_84 = unsqueeze_85 = None
        sum_29: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_54, [1]);  mul_54 = None
        convert_element_type_94: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_29, torch.float16);  sum_29 = None
        view_139: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_94, [1, 1, 4096]);  convert_element_type_94 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_15: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_14, view_139);  add_14 = view_139 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_140: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_15, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_17: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_8 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 204, constant_args_idx = 203, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_16, 'X_ptr': view_140, 'W_ptr': arg50_1, 'RSTD_ptr': empty_17}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_16 = view_140 = arg50_1 = empty_17 = None
        getitem_32: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_8['Y_ptr'];  triton_kernel_wrapper_functional_proxy_8 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_95: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_32, torch.float32)
        unsqueeze_86: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_95, 2);  convert_element_type_95 = None
        permute_45: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg51_1, [1, 0]);  arg51_1 = None
        convert_element_type_96: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_45, torch.float32);  permute_45 = None
        unsqueeze_87: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_96, 0);  convert_element_type_96 = None
        mul_55: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_86, unsqueeze_87);  unsqueeze_86 = unsqueeze_87 = None
        sum_30: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_55, [1]);  mul_55 = None
        convert_element_type_97: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_30, torch.float16);  sum_30 = None
        view_145: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_97, [1, 1, 4096]);  convert_element_type_97 = None
        view_146: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_145, [1, 1, -1, 128]);  view_145 = None
        permute_46: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_146, [0, 2, 1, 3]);  view_146 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_92: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_58: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_46, unsqueeze_92)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_97: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_46, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_8: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_97);  slice_97 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_96: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_46, 3, 0, 64);  permute_46 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_8: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_8, slice_96], -1);  neg_8 = slice_96 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_93: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_59: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_8, unsqueeze_93);  cat_8 = None
        add_16: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_58, mul_59);  mul_58 = mul_59 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_98: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_32, torch.float32)
        unsqueeze_88: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_98, 2);  convert_element_type_98 = None
        permute_47: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg52_1, [1, 0]);  arg52_1 = None
        convert_element_type_99: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_47, torch.float32);  permute_47 = None
        unsqueeze_89: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_99, 0);  convert_element_type_99 = None
        mul_56: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_88, unsqueeze_89);  unsqueeze_88 = unsqueeze_89 = None
        sum_31: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_56, [1]);  mul_56 = None
        convert_element_type_100: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_31, torch.float16);  sum_31 = None
        view_150: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_100, [1, 1, 1024]);  convert_element_type_100 = None
        view_151: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_150, [1, 1, -1, 128]);  view_150 = None
        permute_48: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_151, [0, 2, 1, 3]);  view_151 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_60: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_48, unsqueeze_92);  unsqueeze_92 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_99: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_48, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_9: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_99);  slice_99 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_98: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_48, 3, 0, 64);  permute_48 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_9: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_9, slice_98], -1);  neg_9 = slice_98 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_61: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_9, unsqueeze_93);  cat_9 = unsqueeze_93 = None
        add_17: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_60, mul_61);  mul_60 = mul_61 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_8: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg54_1, [None, None, arg2_1], add_17);  add_17 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_95: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_8, 2)
        expand_25: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_95, [1, 8, 4, 2048, 128]);  unsqueeze_95 = None
        clone_10: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_25, memory_format = torch.contiguous_format);  expand_25 = None
        view_157: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_10, [1, 32, 2048, 128]);  clone_10 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_101: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_32, torch.float32);  getitem_32 = None
        unsqueeze_90: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_101, 2);  convert_element_type_101 = None
        permute_49: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg53_1, [1, 0]);  arg53_1 = None
        convert_element_type_102: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_49, torch.float32);  permute_49 = None
        unsqueeze_91: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_102, 0);  convert_element_type_102 = None
        mul_57: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_90, unsqueeze_91);  unsqueeze_90 = unsqueeze_91 = None
        sum_32: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_57, [1]);  mul_57 = None
        convert_element_type_103: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_32, torch.float16);  sum_32 = None
        view_155: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_103, [1, 1, 1024]);  convert_element_type_103 = None
        view_156: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_155, [1, 1, -1, 128]);  view_155 = None
        permute_50: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_156, [0, 2, 1, 3]);  view_156 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_9: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg55_1, [None, None, arg2_1], permute_50);  permute_50 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_97: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_9, 2)
        expand_27: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_97, [1, 8, 4, 2048, 128]);  unsqueeze_97 = None
        clone_11: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_27, memory_format = torch.contiguous_format);  expand_27 = None
        view_158: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_11, [1, 32, 2048, 128]);  clone_11 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_9: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_8: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_4: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_9, full_default_8);  full_default_9 = full_default_8 = None
        expand_28: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_4, [1, 32, 1, 2048]);  where_4 = None
        _scaled_dot_product_efficient_attention_4 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_16, view_157, view_158, expand_28, False, scale = 0.08838834764831845);  add_16 = view_157 = view_158 = expand_28 = None
        getitem_34: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_4[0];  _scaled_dot_product_efficient_attention_4 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_18: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_51: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_34, [0, 2, 1, 3]);  getitem_34 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_159: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_51, [1, 1, -1]);  permute_51 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_160: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_159, [1, 4096]);  view_159 = None
        convert_element_type_104: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_160, torch.float32);  view_160 = None
        unsqueeze_98: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_104, 2);  convert_element_type_104 = None
        permute_52: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg56_1, [1, 0]);  arg56_1 = None
        convert_element_type_105: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_52, torch.float32);  permute_52 = None
        unsqueeze_99: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_105, 0);  convert_element_type_105 = None
        mul_62: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_98, unsqueeze_99);  unsqueeze_98 = unsqueeze_99 = None
        sum_33: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_62, [1]);  mul_62 = None
        convert_element_type_106: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_33, torch.float16);  sum_33 = None
        view_161: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_106, [1, 1, 4096]);  convert_element_type_106 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_18: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_15, view_161);  add_15 = view_161 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_162: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_18, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_19: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_9 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 205, constant_args_idx = 204, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_18, 'X_ptr': view_162, 'W_ptr': arg57_1, 'RSTD_ptr': empty_19}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_18 = view_162 = arg57_1 = empty_19 = None
        getitem_38: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_9['Y_ptr'];  triton_kernel_wrapper_functional_proxy_9 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_20: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_107: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_38, torch.float32)
        unsqueeze_100: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_107, 2);  convert_element_type_107 = None
        permute_53: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg58_1, [1, 0]);  arg58_1 = None
        convert_element_type_108: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_53, torch.float32);  permute_53 = None
        unsqueeze_101: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_108, 0);  convert_element_type_108 = None
        mul_63: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_100, unsqueeze_101);  unsqueeze_100 = unsqueeze_101 = None
        sum_34: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_63, [1]);  mul_63 = None
        convert_element_type_109: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_34, torch.float16);  sum_34 = None
        view_167: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_109, [1, 1, 14336]);  convert_element_type_109 = None
        convert_element_type_110: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_167, torch.float32);  view_167 = None
        sigmoid_4: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_110)
        mul_64: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_110, sigmoid_4);  convert_element_type_110 = sigmoid_4 = None
        convert_element_type_111: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_64, torch.float16);  mul_64 = None
        convert_element_type_112: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_38, torch.float32);  getitem_38 = None
        unsqueeze_102: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_112, 2);  convert_element_type_112 = None
        permute_54: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg59_1, [1, 0]);  arg59_1 = None
        convert_element_type_113: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_54, torch.float32);  permute_54 = None
        unsqueeze_103: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_113, 0);  convert_element_type_113 = None
        mul_65: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_102, unsqueeze_103);  unsqueeze_102 = unsqueeze_103 = None
        sum_35: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_65, [1]);  mul_65 = None
        convert_element_type_114: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_35, torch.float16);  sum_35 = None
        view_171: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_114, [1, 1, 14336]);  convert_element_type_114 = None
        mul_66: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_111, view_171);  convert_element_type_111 = view_171 = None
        view_172: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_66, [1, 14336]);  mul_66 = None
        convert_element_type_115: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_172, torch.float32);  view_172 = None
        unsqueeze_104: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_115, 2);  convert_element_type_115 = None
        permute_55: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg60_1, [1, 0]);  arg60_1 = None
        convert_element_type_116: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_55, torch.float32);  permute_55 = None
        unsqueeze_105: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_116, 0);  convert_element_type_116 = None
        mul_67: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_104, unsqueeze_105);  unsqueeze_104 = unsqueeze_105 = None
        sum_36: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_67, [1]);  mul_67 = None
        convert_element_type_117: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_36, torch.float16);  sum_36 = None
        view_173: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_117, [1, 1, 4096]);  convert_element_type_117 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_19: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_18, view_173);  add_18 = view_173 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_174: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_19, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_21: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_10 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 206, constant_args_idx = 205, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_20, 'X_ptr': view_174, 'W_ptr': arg61_1, 'RSTD_ptr': empty_21}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_20 = view_174 = arg61_1 = empty_21 = None
        getitem_40: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_10['Y_ptr'];  triton_kernel_wrapper_functional_proxy_10 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_118: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_40, torch.float32)
        unsqueeze_106: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_118, 2);  convert_element_type_118 = None
        permute_56: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg62_1, [1, 0]);  arg62_1 = None
        convert_element_type_119: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_56, torch.float32);  permute_56 = None
        unsqueeze_107: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_119, 0);  convert_element_type_119 = None
        mul_68: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_106, unsqueeze_107);  unsqueeze_106 = unsqueeze_107 = None
        sum_37: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_68, [1]);  mul_68 = None
        convert_element_type_120: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_37, torch.float16);  sum_37 = None
        view_179: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_120, [1, 1, 4096]);  convert_element_type_120 = None
        view_180: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_179, [1, 1, -1, 128]);  view_179 = None
        permute_57: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_180, [0, 2, 1, 3]);  view_180 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_112: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_71: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_57, unsqueeze_112)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_120: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_57, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_10: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_120);  slice_120 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_119: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_57, 3, 0, 64);  permute_57 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_10: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_10, slice_119], -1);  neg_10 = slice_119 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_113: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_72: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_10, unsqueeze_113);  cat_10 = None
        add_20: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_71, mul_72);  mul_71 = mul_72 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_121: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_40, torch.float32)
        unsqueeze_108: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_121, 2);  convert_element_type_121 = None
        permute_58: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg63_1, [1, 0]);  arg63_1 = None
        convert_element_type_122: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_58, torch.float32);  permute_58 = None
        unsqueeze_109: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_122, 0);  convert_element_type_122 = None
        mul_69: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_108, unsqueeze_109);  unsqueeze_108 = unsqueeze_109 = None
        sum_38: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_69, [1]);  mul_69 = None
        convert_element_type_123: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_38, torch.float16);  sum_38 = None
        view_184: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_123, [1, 1, 1024]);  convert_element_type_123 = None
        view_185: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_184, [1, 1, -1, 128]);  view_184 = None
        permute_59: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_185, [0, 2, 1, 3]);  view_185 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_73: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_59, unsqueeze_112);  unsqueeze_112 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_122: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_59, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_11: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_122);  slice_122 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_121: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_59, 3, 0, 64);  permute_59 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_11: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_11, slice_121], -1);  neg_11 = slice_121 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_74: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_11, unsqueeze_113);  cat_11 = unsqueeze_113 = None
        add_21: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_73, mul_74);  mul_73 = mul_74 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_10: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg65_1, [None, None, arg2_1], add_21);  add_21 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_115: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_10, 2)
        expand_30: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_115, [1, 8, 4, 2048, 128]);  unsqueeze_115 = None
        clone_12: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_30, memory_format = torch.contiguous_format);  expand_30 = None
        view_191: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_12, [1, 32, 2048, 128]);  clone_12 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_124: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_40, torch.float32);  getitem_40 = None
        unsqueeze_110: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_124, 2);  convert_element_type_124 = None
        permute_60: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg64_1, [1, 0]);  arg64_1 = None
        convert_element_type_125: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_60, torch.float32);  permute_60 = None
        unsqueeze_111: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_125, 0);  convert_element_type_125 = None
        mul_70: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_110, unsqueeze_111);  unsqueeze_110 = unsqueeze_111 = None
        sum_39: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_70, [1]);  mul_70 = None
        convert_element_type_126: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_39, torch.float16);  sum_39 = None
        view_189: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_126, [1, 1, 1024]);  convert_element_type_126 = None
        view_190: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_189, [1, 1, -1, 128]);  view_189 = None
        permute_61: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_190, [0, 2, 1, 3]);  view_190 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_11: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg66_1, [None, None, arg2_1], permute_61);  permute_61 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_117: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_11, 2)
        expand_32: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_117, [1, 8, 4, 2048, 128]);  unsqueeze_117 = None
        clone_13: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_32, memory_format = torch.contiguous_format);  expand_32 = None
        view_192: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_13, [1, 32, 2048, 128]);  clone_13 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_11: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_10: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_5: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_11, full_default_10);  full_default_11 = full_default_10 = None
        expand_33: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_5, [1, 32, 1, 2048]);  where_5 = None
        _scaled_dot_product_efficient_attention_5 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_20, view_191, view_192, expand_33, False, scale = 0.08838834764831845);  add_20 = view_191 = view_192 = expand_33 = None
        getitem_42: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_5[0];  _scaled_dot_product_efficient_attention_5 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_22: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_62: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_42, [0, 2, 1, 3]);  getitem_42 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_193: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_62, [1, 1, -1]);  permute_62 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_194: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_193, [1, 4096]);  view_193 = None
        convert_element_type_127: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_194, torch.float32);  view_194 = None
        unsqueeze_118: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_127, 2);  convert_element_type_127 = None
        permute_63: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg67_1, [1, 0]);  arg67_1 = None
        convert_element_type_128: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_63, torch.float32);  permute_63 = None
        unsqueeze_119: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_128, 0);  convert_element_type_128 = None
        mul_75: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_118, unsqueeze_119);  unsqueeze_118 = unsqueeze_119 = None
        sum_40: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_75, [1]);  mul_75 = None
        convert_element_type_129: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_40, torch.float16);  sum_40 = None
        view_195: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_129, [1, 1, 4096]);  convert_element_type_129 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_22: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_19, view_195);  add_19 = view_195 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_196: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_22, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_23: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_11 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 207, constant_args_idx = 206, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_22, 'X_ptr': view_196, 'W_ptr': arg68_1, 'RSTD_ptr': empty_23}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_22 = view_196 = arg68_1 = empty_23 = None
        getitem_46: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_11['Y_ptr'];  triton_kernel_wrapper_functional_proxy_11 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_24: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_130: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_46, torch.float32)
        unsqueeze_120: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_130, 2);  convert_element_type_130 = None
        permute_64: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg69_1, [1, 0]);  arg69_1 = None
        convert_element_type_131: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_64, torch.float32);  permute_64 = None
        unsqueeze_121: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_131, 0);  convert_element_type_131 = None
        mul_76: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_120, unsqueeze_121);  unsqueeze_120 = unsqueeze_121 = None
        sum_41: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_76, [1]);  mul_76 = None
        convert_element_type_132: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_41, torch.float16);  sum_41 = None
        view_201: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_132, [1, 1, 14336]);  convert_element_type_132 = None
        convert_element_type_133: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_201, torch.float32);  view_201 = None
        sigmoid_5: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_133)
        mul_77: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_133, sigmoid_5);  convert_element_type_133 = sigmoid_5 = None
        convert_element_type_134: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_77, torch.float16);  mul_77 = None
        convert_element_type_135: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_46, torch.float32);  getitem_46 = None
        unsqueeze_122: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_135, 2);  convert_element_type_135 = None
        permute_65: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg70_1, [1, 0]);  arg70_1 = None
        convert_element_type_136: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_65, torch.float32);  permute_65 = None
        unsqueeze_123: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_136, 0);  convert_element_type_136 = None
        mul_78: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_122, unsqueeze_123);  unsqueeze_122 = unsqueeze_123 = None
        sum_42: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_78, [1]);  mul_78 = None
        convert_element_type_137: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_42, torch.float16);  sum_42 = None
        view_205: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_137, [1, 1, 14336]);  convert_element_type_137 = None
        mul_79: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_134, view_205);  convert_element_type_134 = view_205 = None
        view_206: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_79, [1, 14336]);  mul_79 = None
        convert_element_type_138: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_206, torch.float32);  view_206 = None
        unsqueeze_124: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_138, 2);  convert_element_type_138 = None
        permute_66: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg71_1, [1, 0]);  arg71_1 = None
        convert_element_type_139: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_66, torch.float32);  permute_66 = None
        unsqueeze_125: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_139, 0);  convert_element_type_139 = None
        mul_80: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_124, unsqueeze_125);  unsqueeze_124 = unsqueeze_125 = None
        sum_43: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_80, [1]);  mul_80 = None
        convert_element_type_140: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_43, torch.float16);  sum_43 = None
        view_207: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_140, [1, 1, 4096]);  convert_element_type_140 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_23: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_22, view_207);  add_22 = view_207 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_208: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_23, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_25: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_12 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 208, constant_args_idx = 207, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_24, 'X_ptr': view_208, 'W_ptr': arg72_1, 'RSTD_ptr': empty_25}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_24 = view_208 = arg72_1 = empty_25 = None
        getitem_48: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_12['Y_ptr'];  triton_kernel_wrapper_functional_proxy_12 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_141: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_48, torch.float32)
        unsqueeze_126: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_141, 2);  convert_element_type_141 = None
        permute_67: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg73_1, [1, 0]);  arg73_1 = None
        convert_element_type_142: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_67, torch.float32);  permute_67 = None
        unsqueeze_127: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_142, 0);  convert_element_type_142 = None
        mul_81: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_126, unsqueeze_127);  unsqueeze_126 = unsqueeze_127 = None
        sum_44: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_81, [1]);  mul_81 = None
        convert_element_type_143: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_44, torch.float16);  sum_44 = None
        view_213: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_143, [1, 1, 4096]);  convert_element_type_143 = None
        view_214: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_213, [1, 1, -1, 128]);  view_213 = None
        permute_68: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_214, [0, 2, 1, 3]);  view_214 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_132: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_84: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_68, unsqueeze_132)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_143: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_68, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_12: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_143);  slice_143 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_142: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_68, 3, 0, 64);  permute_68 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_12: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_12, slice_142], -1);  neg_12 = slice_142 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_133: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_85: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_12, unsqueeze_133);  cat_12 = None
        add_24: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_84, mul_85);  mul_84 = mul_85 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_144: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_48, torch.float32)
        unsqueeze_128: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_144, 2);  convert_element_type_144 = None
        permute_69: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg74_1, [1, 0]);  arg74_1 = None
        convert_element_type_145: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_69, torch.float32);  permute_69 = None
        unsqueeze_129: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_145, 0);  convert_element_type_145 = None
        mul_82: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_128, unsqueeze_129);  unsqueeze_128 = unsqueeze_129 = None
        sum_45: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_82, [1]);  mul_82 = None
        convert_element_type_146: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_45, torch.float16);  sum_45 = None
        view_218: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_146, [1, 1, 1024]);  convert_element_type_146 = None
        view_219: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_218, [1, 1, -1, 128]);  view_218 = None
        permute_70: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_219, [0, 2, 1, 3]);  view_219 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_86: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_70, unsqueeze_132);  unsqueeze_132 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_145: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_70, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_13: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_145);  slice_145 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_144: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_70, 3, 0, 64);  permute_70 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_13: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_13, slice_144], -1);  neg_13 = slice_144 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_87: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_13, unsqueeze_133);  cat_13 = unsqueeze_133 = None
        add_25: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_86, mul_87);  mul_86 = mul_87 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_12: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg76_1, [None, None, arg2_1], add_25);  add_25 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_135: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_12, 2)
        expand_35: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_135, [1, 8, 4, 2048, 128]);  unsqueeze_135 = None
        clone_14: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_35, memory_format = torch.contiguous_format);  expand_35 = None
        view_225: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_14, [1, 32, 2048, 128]);  clone_14 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_147: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_48, torch.float32);  getitem_48 = None
        unsqueeze_130: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_147, 2);  convert_element_type_147 = None
        permute_71: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg75_1, [1, 0]);  arg75_1 = None
        convert_element_type_148: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_71, torch.float32);  permute_71 = None
        unsqueeze_131: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_148, 0);  convert_element_type_148 = None
        mul_83: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_130, unsqueeze_131);  unsqueeze_130 = unsqueeze_131 = None
        sum_46: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_83, [1]);  mul_83 = None
        convert_element_type_149: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_46, torch.float16);  sum_46 = None
        view_223: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_149, [1, 1, 1024]);  convert_element_type_149 = None
        view_224: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_223, [1, 1, -1, 128]);  view_223 = None
        permute_72: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_224, [0, 2, 1, 3]);  view_224 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_13: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg77_1, [None, None, arg2_1], permute_72);  permute_72 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_137: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_13, 2)
        expand_37: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_137, [1, 8, 4, 2048, 128]);  unsqueeze_137 = None
        clone_15: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_37, memory_format = torch.contiguous_format);  expand_37 = None
        view_226: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_15, [1, 32, 2048, 128]);  clone_15 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_13: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_12: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_6: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_13, full_default_12);  full_default_13 = full_default_12 = None
        expand_38: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_6, [1, 32, 1, 2048]);  where_6 = None
        _scaled_dot_product_efficient_attention_6 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_24, view_225, view_226, expand_38, False, scale = 0.08838834764831845);  add_24 = view_225 = view_226 = expand_38 = None
        getitem_50: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_6[0];  _scaled_dot_product_efficient_attention_6 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_26: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_73: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_50, [0, 2, 1, 3]);  getitem_50 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_227: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_73, [1, 1, -1]);  permute_73 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_228: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_227, [1, 4096]);  view_227 = None
        convert_element_type_150: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_228, torch.float32);  view_228 = None
        unsqueeze_138: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_150, 2);  convert_element_type_150 = None
        permute_74: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg78_1, [1, 0]);  arg78_1 = None
        convert_element_type_151: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_74, torch.float32);  permute_74 = None
        unsqueeze_139: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_151, 0);  convert_element_type_151 = None
        mul_88: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_138, unsqueeze_139);  unsqueeze_138 = unsqueeze_139 = None
        sum_47: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_88, [1]);  mul_88 = None
        convert_element_type_152: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_47, torch.float16);  sum_47 = None
        view_229: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_152, [1, 1, 4096]);  convert_element_type_152 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_26: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_23, view_229);  add_23 = view_229 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_230: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_26, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_27: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_13 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 209, constant_args_idx = 208, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_26, 'X_ptr': view_230, 'W_ptr': arg79_1, 'RSTD_ptr': empty_27}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_26 = view_230 = arg79_1 = empty_27 = None
        getitem_54: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_13['Y_ptr'];  triton_kernel_wrapper_functional_proxy_13 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_28: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_153: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_54, torch.float32)
        unsqueeze_140: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_153, 2);  convert_element_type_153 = None
        permute_75: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg80_1, [1, 0]);  arg80_1 = None
        convert_element_type_154: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_75, torch.float32);  permute_75 = None
        unsqueeze_141: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_154, 0);  convert_element_type_154 = None
        mul_89: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_140, unsqueeze_141);  unsqueeze_140 = unsqueeze_141 = None
        sum_48: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_89, [1]);  mul_89 = None
        convert_element_type_155: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_48, torch.float16);  sum_48 = None
        view_235: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_155, [1, 1, 14336]);  convert_element_type_155 = None
        convert_element_type_156: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_235, torch.float32);  view_235 = None
        sigmoid_6: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_156)
        mul_90: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_156, sigmoid_6);  convert_element_type_156 = sigmoid_6 = None
        convert_element_type_157: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_90, torch.float16);  mul_90 = None
        convert_element_type_158: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_54, torch.float32);  getitem_54 = None
        unsqueeze_142: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_158, 2);  convert_element_type_158 = None
        permute_76: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg81_1, [1, 0]);  arg81_1 = None
        convert_element_type_159: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_76, torch.float32);  permute_76 = None
        unsqueeze_143: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_159, 0);  convert_element_type_159 = None
        mul_91: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_142, unsqueeze_143);  unsqueeze_142 = unsqueeze_143 = None
        sum_49: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_91, [1]);  mul_91 = None
        convert_element_type_160: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_49, torch.float16);  sum_49 = None
        view_239: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_160, [1, 1, 14336]);  convert_element_type_160 = None
        mul_92: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_157, view_239);  convert_element_type_157 = view_239 = None
        view_240: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_92, [1, 14336]);  mul_92 = None
        convert_element_type_161: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_240, torch.float32);  view_240 = None
        unsqueeze_144: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_161, 2);  convert_element_type_161 = None
        permute_77: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg82_1, [1, 0]);  arg82_1 = None
        convert_element_type_162: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_77, torch.float32);  permute_77 = None
        unsqueeze_145: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_162, 0);  convert_element_type_162 = None
        mul_93: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_144, unsqueeze_145);  unsqueeze_144 = unsqueeze_145 = None
        sum_50: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_93, [1]);  mul_93 = None
        convert_element_type_163: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_50, torch.float16);  sum_50 = None
        view_241: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_163, [1, 1, 4096]);  convert_element_type_163 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_27: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_26, view_241);  add_26 = view_241 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_242: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_27, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_29: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_14 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 210, constant_args_idx = 209, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_28, 'X_ptr': view_242, 'W_ptr': arg83_1, 'RSTD_ptr': empty_29}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_28 = view_242 = arg83_1 = empty_29 = None
        getitem_56: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_14['Y_ptr'];  triton_kernel_wrapper_functional_proxy_14 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_164: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_56, torch.float32)
        unsqueeze_146: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_164, 2);  convert_element_type_164 = None
        permute_78: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg84_1, [1, 0]);  arg84_1 = None
        convert_element_type_165: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_78, torch.float32);  permute_78 = None
        unsqueeze_147: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_165, 0);  convert_element_type_165 = None
        mul_94: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_146, unsqueeze_147);  unsqueeze_146 = unsqueeze_147 = None
        sum_51: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_94, [1]);  mul_94 = None
        convert_element_type_166: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_51, torch.float16);  sum_51 = None
        view_247: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_166, [1, 1, 4096]);  convert_element_type_166 = None
        view_248: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_247, [1, 1, -1, 128]);  view_247 = None
        permute_79: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_248, [0, 2, 1, 3]);  view_248 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_152: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_97: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_79, unsqueeze_152)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_166: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_79, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_14: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_166);  slice_166 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_165: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_79, 3, 0, 64);  permute_79 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_14: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_14, slice_165], -1);  neg_14 = slice_165 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_153: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_98: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_14, unsqueeze_153);  cat_14 = None
        add_28: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_97, mul_98);  mul_97 = mul_98 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_167: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_56, torch.float32)
        unsqueeze_148: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_167, 2);  convert_element_type_167 = None
        permute_80: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg85_1, [1, 0]);  arg85_1 = None
        convert_element_type_168: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_80, torch.float32);  permute_80 = None
        unsqueeze_149: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_168, 0);  convert_element_type_168 = None
        mul_95: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_148, unsqueeze_149);  unsqueeze_148 = unsqueeze_149 = None
        sum_52: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_95, [1]);  mul_95 = None
        convert_element_type_169: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_52, torch.float16);  sum_52 = None
        view_252: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_169, [1, 1, 1024]);  convert_element_type_169 = None
        view_253: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_252, [1, 1, -1, 128]);  view_252 = None
        permute_81: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_253, [0, 2, 1, 3]);  view_253 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_99: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_81, unsqueeze_152);  unsqueeze_152 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_168: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_81, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_15: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_168);  slice_168 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_167: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_81, 3, 0, 64);  permute_81 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_15: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_15, slice_167], -1);  neg_15 = slice_167 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_100: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_15, unsqueeze_153);  cat_15 = unsqueeze_153 = None
        add_29: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_99, mul_100);  mul_99 = mul_100 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_14: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg87_1, [None, None, arg2_1], add_29);  add_29 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_155: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_14, 2)
        expand_40: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_155, [1, 8, 4, 2048, 128]);  unsqueeze_155 = None
        clone_16: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_40, memory_format = torch.contiguous_format);  expand_40 = None
        view_259: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_16, [1, 32, 2048, 128]);  clone_16 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_170: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_56, torch.float32);  getitem_56 = None
        unsqueeze_150: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_170, 2);  convert_element_type_170 = None
        permute_82: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg86_1, [1, 0]);  arg86_1 = None
        convert_element_type_171: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_82, torch.float32);  permute_82 = None
        unsqueeze_151: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_171, 0);  convert_element_type_171 = None
        mul_96: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_150, unsqueeze_151);  unsqueeze_150 = unsqueeze_151 = None
        sum_53: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_96, [1]);  mul_96 = None
        convert_element_type_172: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_53, torch.float16);  sum_53 = None
        view_257: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_172, [1, 1, 1024]);  convert_element_type_172 = None
        view_258: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_257, [1, 1, -1, 128]);  view_257 = None
        permute_83: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_258, [0, 2, 1, 3]);  view_258 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_15: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg88_1, [None, None, arg2_1], permute_83);  permute_83 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_157: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_15, 2)
        expand_42: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_157, [1, 8, 4, 2048, 128]);  unsqueeze_157 = None
        clone_17: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_42, memory_format = torch.contiguous_format);  expand_42 = None
        view_260: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_17, [1, 32, 2048, 128]);  clone_17 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_15: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_14: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_7: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_15, full_default_14);  full_default_15 = full_default_14 = None
        expand_43: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_7, [1, 32, 1, 2048]);  where_7 = None
        _scaled_dot_product_efficient_attention_7 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_28, view_259, view_260, expand_43, False, scale = 0.08838834764831845);  add_28 = view_259 = view_260 = expand_43 = None
        getitem_58: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_7[0];  _scaled_dot_product_efficient_attention_7 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_30: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_84: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_58, [0, 2, 1, 3]);  getitem_58 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_261: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_84, [1, 1, -1]);  permute_84 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_262: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_261, [1, 4096]);  view_261 = None
        convert_element_type_173: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_262, torch.float32);  view_262 = None
        unsqueeze_158: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_173, 2);  convert_element_type_173 = None
        permute_85: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg89_1, [1, 0]);  arg89_1 = None
        convert_element_type_174: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_85, torch.float32);  permute_85 = None
        unsqueeze_159: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_174, 0);  convert_element_type_174 = None
        mul_101: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_158, unsqueeze_159);  unsqueeze_158 = unsqueeze_159 = None
        sum_54: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_101, [1]);  mul_101 = None
        convert_element_type_175: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_54, torch.float16);  sum_54 = None
        view_263: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_175, [1, 1, 4096]);  convert_element_type_175 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_30: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_27, view_263);  add_27 = view_263 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_264: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_30, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_31: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_15 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 211, constant_args_idx = 210, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_30, 'X_ptr': view_264, 'W_ptr': arg90_1, 'RSTD_ptr': empty_31}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_30 = view_264 = arg90_1 = empty_31 = None
        getitem_62: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_15['Y_ptr'];  triton_kernel_wrapper_functional_proxy_15 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_32: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_176: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_62, torch.float32)
        unsqueeze_160: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_176, 2);  convert_element_type_176 = None
        permute_86: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg91_1, [1, 0]);  arg91_1 = None
        convert_element_type_177: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_86, torch.float32);  permute_86 = None
        unsqueeze_161: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_177, 0);  convert_element_type_177 = None
        mul_102: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_160, unsqueeze_161);  unsqueeze_160 = unsqueeze_161 = None
        sum_55: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_102, [1]);  mul_102 = None
        convert_element_type_178: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_55, torch.float16);  sum_55 = None
        view_269: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_178, [1, 1, 14336]);  convert_element_type_178 = None
        convert_element_type_179: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_269, torch.float32);  view_269 = None
        sigmoid_7: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_179)
        mul_103: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_179, sigmoid_7);  convert_element_type_179 = sigmoid_7 = None
        convert_element_type_180: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_103, torch.float16);  mul_103 = None
        convert_element_type_181: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_62, torch.float32);  getitem_62 = None
        unsqueeze_162: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_181, 2);  convert_element_type_181 = None
        permute_87: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg92_1, [1, 0]);  arg92_1 = None
        convert_element_type_182: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_87, torch.float32);  permute_87 = None
        unsqueeze_163: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_182, 0);  convert_element_type_182 = None
        mul_104: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_162, unsqueeze_163);  unsqueeze_162 = unsqueeze_163 = None
        sum_56: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_104, [1]);  mul_104 = None
        convert_element_type_183: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_56, torch.float16);  sum_56 = None
        view_273: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_183, [1, 1, 14336]);  convert_element_type_183 = None
        mul_105: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_180, view_273);  convert_element_type_180 = view_273 = None
        view_274: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_105, [1, 14336]);  mul_105 = None
        convert_element_type_184: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_274, torch.float32);  view_274 = None
        unsqueeze_164: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_184, 2);  convert_element_type_184 = None
        permute_88: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg93_1, [1, 0]);  arg93_1 = None
        convert_element_type_185: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_88, torch.float32);  permute_88 = None
        unsqueeze_165: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_185, 0);  convert_element_type_185 = None
        mul_106: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_164, unsqueeze_165);  unsqueeze_164 = unsqueeze_165 = None
        sum_57: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_106, [1]);  mul_106 = None
        convert_element_type_186: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_57, torch.float16);  sum_57 = None
        view_275: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_186, [1, 1, 4096]);  convert_element_type_186 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_31: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_30, view_275);  add_30 = view_275 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_276: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_31, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_33: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_16 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 212, constant_args_idx = 211, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_32, 'X_ptr': view_276, 'W_ptr': arg94_1, 'RSTD_ptr': empty_33}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_32 = view_276 = arg94_1 = empty_33 = None
        getitem_64: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_16['Y_ptr'];  triton_kernel_wrapper_functional_proxy_16 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_187: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_64, torch.float32)
        unsqueeze_166: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_187, 2);  convert_element_type_187 = None
        permute_89: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg95_1, [1, 0]);  arg95_1 = None
        convert_element_type_188: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_89, torch.float32);  permute_89 = None
        unsqueeze_167: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_188, 0);  convert_element_type_188 = None
        mul_107: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_166, unsqueeze_167);  unsqueeze_166 = unsqueeze_167 = None
        sum_58: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_107, [1]);  mul_107 = None
        convert_element_type_189: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_58, torch.float16);  sum_58 = None
        view_281: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_189, [1, 1, 4096]);  convert_element_type_189 = None
        view_282: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_281, [1, 1, -1, 128]);  view_281 = None
        permute_90: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_282, [0, 2, 1, 3]);  view_282 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_172: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_110: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_90, unsqueeze_172)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_189: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_90, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_16: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_189);  slice_189 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_188: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_90, 3, 0, 64);  permute_90 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_16: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_16, slice_188], -1);  neg_16 = slice_188 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_173: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_111: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_16, unsqueeze_173);  cat_16 = None
        add_32: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_110, mul_111);  mul_110 = mul_111 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_190: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_64, torch.float32)
        unsqueeze_168: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_190, 2);  convert_element_type_190 = None
        permute_91: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg96_1, [1, 0]);  arg96_1 = None
        convert_element_type_191: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_91, torch.float32);  permute_91 = None
        unsqueeze_169: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_191, 0);  convert_element_type_191 = None
        mul_108: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_168, unsqueeze_169);  unsqueeze_168 = unsqueeze_169 = None
        sum_59: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_108, [1]);  mul_108 = None
        convert_element_type_192: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_59, torch.float16);  sum_59 = None
        view_286: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_192, [1, 1, 1024]);  convert_element_type_192 = None
        view_287: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_286, [1, 1, -1, 128]);  view_286 = None
        permute_92: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_287, [0, 2, 1, 3]);  view_287 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_112: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_92, unsqueeze_172);  unsqueeze_172 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_191: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_92, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_17: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_191);  slice_191 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_190: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_92, 3, 0, 64);  permute_92 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_17: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_17, slice_190], -1);  neg_17 = slice_190 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_113: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_17, unsqueeze_173);  cat_17 = unsqueeze_173 = None
        add_33: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_112, mul_113);  mul_112 = mul_113 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_16: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg98_1, [None, None, arg2_1], add_33);  add_33 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_175: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_16, 2)
        expand_45: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_175, [1, 8, 4, 2048, 128]);  unsqueeze_175 = None
        clone_18: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_45, memory_format = torch.contiguous_format);  expand_45 = None
        view_293: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_18, [1, 32, 2048, 128]);  clone_18 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_193: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_64, torch.float32);  getitem_64 = None
        unsqueeze_170: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_193, 2);  convert_element_type_193 = None
        permute_93: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg97_1, [1, 0]);  arg97_1 = None
        convert_element_type_194: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_93, torch.float32);  permute_93 = None
        unsqueeze_171: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_194, 0);  convert_element_type_194 = None
        mul_109: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_170, unsqueeze_171);  unsqueeze_170 = unsqueeze_171 = None
        sum_60: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_109, [1]);  mul_109 = None
        convert_element_type_195: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_60, torch.float16);  sum_60 = None
        view_291: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_195, [1, 1, 1024]);  convert_element_type_195 = None
        view_292: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_291, [1, 1, -1, 128]);  view_291 = None
        permute_94: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_292, [0, 2, 1, 3]);  view_292 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_17: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg99_1, [None, None, arg2_1], permute_94);  permute_94 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_177: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_17, 2)
        expand_47: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_177, [1, 8, 4, 2048, 128]);  unsqueeze_177 = None
        clone_19: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_47, memory_format = torch.contiguous_format);  expand_47 = None
        view_294: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_19, [1, 32, 2048, 128]);  clone_19 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_17: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_16: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_8: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_17, full_default_16);  full_default_17 = full_default_16 = None
        expand_48: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_8, [1, 32, 1, 2048]);  where_8 = None
        _scaled_dot_product_efficient_attention_8 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_32, view_293, view_294, expand_48, False, scale = 0.08838834764831845);  add_32 = view_293 = view_294 = expand_48 = None
        getitem_66: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_8[0];  _scaled_dot_product_efficient_attention_8 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_34: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_95: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_66, [0, 2, 1, 3]);  getitem_66 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_295: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_95, [1, 1, -1]);  permute_95 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_296: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_295, [1, 4096]);  view_295 = None
        convert_element_type_196: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_296, torch.float32);  view_296 = None
        unsqueeze_178: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_196, 2);  convert_element_type_196 = None
        permute_96: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg100_1, [1, 0]);  arg100_1 = None
        convert_element_type_197: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_96, torch.float32);  permute_96 = None
        unsqueeze_179: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_197, 0);  convert_element_type_197 = None
        mul_114: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_178, unsqueeze_179);  unsqueeze_178 = unsqueeze_179 = None
        sum_61: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_114, [1]);  mul_114 = None
        convert_element_type_198: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_61, torch.float16);  sum_61 = None
        view_297: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_198, [1, 1, 4096]);  convert_element_type_198 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_34: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_31, view_297);  add_31 = view_297 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_298: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_34, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_35: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_17 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 213, constant_args_idx = 212, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_34, 'X_ptr': view_298, 'W_ptr': arg101_1, 'RSTD_ptr': empty_35}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_34 = view_298 = arg101_1 = empty_35 = None
        getitem_70: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_17['Y_ptr'];  triton_kernel_wrapper_functional_proxy_17 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_36: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_199: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_70, torch.float32)
        unsqueeze_180: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_199, 2);  convert_element_type_199 = None
        permute_97: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg102_1, [1, 0]);  arg102_1 = None
        convert_element_type_200: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_97, torch.float32);  permute_97 = None
        unsqueeze_181: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_200, 0);  convert_element_type_200 = None
        mul_115: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_180, unsqueeze_181);  unsqueeze_180 = unsqueeze_181 = None
        sum_62: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_115, [1]);  mul_115 = None
        convert_element_type_201: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_62, torch.float16);  sum_62 = None
        view_303: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_201, [1, 1, 14336]);  convert_element_type_201 = None
        convert_element_type_202: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_303, torch.float32);  view_303 = None
        sigmoid_8: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_202)
        mul_116: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_202, sigmoid_8);  convert_element_type_202 = sigmoid_8 = None
        convert_element_type_203: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_116, torch.float16);  mul_116 = None
        convert_element_type_204: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_70, torch.float32);  getitem_70 = None
        unsqueeze_182: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_204, 2);  convert_element_type_204 = None
        permute_98: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg103_1, [1, 0]);  arg103_1 = None
        convert_element_type_205: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_98, torch.float32);  permute_98 = None
        unsqueeze_183: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_205, 0);  convert_element_type_205 = None
        mul_117: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_182, unsqueeze_183);  unsqueeze_182 = unsqueeze_183 = None
        sum_63: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_117, [1]);  mul_117 = None
        convert_element_type_206: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_63, torch.float16);  sum_63 = None
        view_307: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_206, [1, 1, 14336]);  convert_element_type_206 = None
        mul_118: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_203, view_307);  convert_element_type_203 = view_307 = None
        view_308: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_118, [1, 14336]);  mul_118 = None
        convert_element_type_207: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_308, torch.float32);  view_308 = None
        unsqueeze_184: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_207, 2);  convert_element_type_207 = None
        permute_99: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg104_1, [1, 0]);  arg104_1 = None
        convert_element_type_208: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_99, torch.float32);  permute_99 = None
        unsqueeze_185: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_208, 0);  convert_element_type_208 = None
        mul_119: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_184, unsqueeze_185);  unsqueeze_184 = unsqueeze_185 = None
        sum_64: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_119, [1]);  mul_119 = None
        convert_element_type_209: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_64, torch.float16);  sum_64 = None
        view_309: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_209, [1, 1, 4096]);  convert_element_type_209 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_35: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_34, view_309);  add_34 = view_309 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_310: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_35, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_37: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_18 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 214, constant_args_idx = 213, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_36, 'X_ptr': view_310, 'W_ptr': arg105_1, 'RSTD_ptr': empty_37}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_36 = view_310 = arg105_1 = empty_37 = None
        getitem_72: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_18['Y_ptr'];  triton_kernel_wrapper_functional_proxy_18 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_210: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_72, torch.float32)
        unsqueeze_186: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_210, 2);  convert_element_type_210 = None
        permute_100: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg106_1, [1, 0]);  arg106_1 = None
        convert_element_type_211: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_100, torch.float32);  permute_100 = None
        unsqueeze_187: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_211, 0);  convert_element_type_211 = None
        mul_120: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_186, unsqueeze_187);  unsqueeze_186 = unsqueeze_187 = None
        sum_65: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_120, [1]);  mul_120 = None
        convert_element_type_212: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_65, torch.float16);  sum_65 = None
        view_315: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_212, [1, 1, 4096]);  convert_element_type_212 = None
        view_316: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_315, [1, 1, -1, 128]);  view_315 = None
        permute_101: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_316, [0, 2, 1, 3]);  view_316 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_192: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_123: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_101, unsqueeze_192)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_212: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_101, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_18: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_212);  slice_212 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_211: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_101, 3, 0, 64);  permute_101 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_18: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_18, slice_211], -1);  neg_18 = slice_211 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_193: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_124: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_18, unsqueeze_193);  cat_18 = None
        add_36: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_123, mul_124);  mul_123 = mul_124 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_213: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_72, torch.float32)
        unsqueeze_188: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_213, 2);  convert_element_type_213 = None
        permute_102: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg107_1, [1, 0]);  arg107_1 = None
        convert_element_type_214: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_102, torch.float32);  permute_102 = None
        unsqueeze_189: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_214, 0);  convert_element_type_214 = None
        mul_121: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_188, unsqueeze_189);  unsqueeze_188 = unsqueeze_189 = None
        sum_66: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_121, [1]);  mul_121 = None
        convert_element_type_215: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_66, torch.float16);  sum_66 = None
        view_320: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_215, [1, 1, 1024]);  convert_element_type_215 = None
        view_321: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_320, [1, 1, -1, 128]);  view_320 = None
        permute_103: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_321, [0, 2, 1, 3]);  view_321 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_125: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_103, unsqueeze_192);  unsqueeze_192 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_214: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_103, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_19: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_214);  slice_214 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_213: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_103, 3, 0, 64);  permute_103 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_19: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_19, slice_213], -1);  neg_19 = slice_213 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_126: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_19, unsqueeze_193);  cat_19 = unsqueeze_193 = None
        add_37: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_125, mul_126);  mul_125 = mul_126 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_18: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg109_1, [None, None, arg2_1], add_37);  add_37 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_195: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_18, 2)
        expand_50: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_195, [1, 8, 4, 2048, 128]);  unsqueeze_195 = None
        clone_20: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_50, memory_format = torch.contiguous_format);  expand_50 = None
        view_327: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_20, [1, 32, 2048, 128]);  clone_20 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_216: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_72, torch.float32);  getitem_72 = None
        unsqueeze_190: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_216, 2);  convert_element_type_216 = None
        permute_104: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg108_1, [1, 0]);  arg108_1 = None
        convert_element_type_217: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_104, torch.float32);  permute_104 = None
        unsqueeze_191: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_217, 0);  convert_element_type_217 = None
        mul_122: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_190, unsqueeze_191);  unsqueeze_190 = unsqueeze_191 = None
        sum_67: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_122, [1]);  mul_122 = None
        convert_element_type_218: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_67, torch.float16);  sum_67 = None
        view_325: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_218, [1, 1, 1024]);  convert_element_type_218 = None
        view_326: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_325, [1, 1, -1, 128]);  view_325 = None
        permute_105: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_326, [0, 2, 1, 3]);  view_326 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_19: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg110_1, [None, None, arg2_1], permute_105);  permute_105 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_197: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_19, 2)
        expand_52: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_197, [1, 8, 4, 2048, 128]);  unsqueeze_197 = None
        clone_21: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_52, memory_format = torch.contiguous_format);  expand_52 = None
        view_328: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_21, [1, 32, 2048, 128]);  clone_21 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_19: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_18: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_9: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_19, full_default_18);  full_default_19 = full_default_18 = None
        expand_53: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_9, [1, 32, 1, 2048]);  where_9 = None
        _scaled_dot_product_efficient_attention_9 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_36, view_327, view_328, expand_53, False, scale = 0.08838834764831845);  add_36 = view_327 = view_328 = expand_53 = None
        getitem_74: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_9[0];  _scaled_dot_product_efficient_attention_9 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_38: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_106: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_74, [0, 2, 1, 3]);  getitem_74 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_329: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_106, [1, 1, -1]);  permute_106 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_330: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_329, [1, 4096]);  view_329 = None
        convert_element_type_219: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_330, torch.float32);  view_330 = None
        unsqueeze_198: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_219, 2);  convert_element_type_219 = None
        permute_107: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg111_1, [1, 0]);  arg111_1 = None
        convert_element_type_220: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_107, torch.float32);  permute_107 = None
        unsqueeze_199: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_220, 0);  convert_element_type_220 = None
        mul_127: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_198, unsqueeze_199);  unsqueeze_198 = unsqueeze_199 = None
        sum_68: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_127, [1]);  mul_127 = None
        convert_element_type_221: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_68, torch.float16);  sum_68 = None
        view_331: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_221, [1, 1, 4096]);  convert_element_type_221 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_38: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_35, view_331);  add_35 = view_331 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_332: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_38, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_39: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_19 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 215, constant_args_idx = 214, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_38, 'X_ptr': view_332, 'W_ptr': arg112_1, 'RSTD_ptr': empty_39}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_38 = view_332 = arg112_1 = empty_39 = None
        getitem_78: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_19['Y_ptr'];  triton_kernel_wrapper_functional_proxy_19 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_40: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_222: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_78, torch.float32)
        unsqueeze_200: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_222, 2);  convert_element_type_222 = None
        permute_108: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg113_1, [1, 0]);  arg113_1 = None
        convert_element_type_223: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_108, torch.float32);  permute_108 = None
        unsqueeze_201: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_223, 0);  convert_element_type_223 = None
        mul_128: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_200, unsqueeze_201);  unsqueeze_200 = unsqueeze_201 = None
        sum_69: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_128, [1]);  mul_128 = None
        convert_element_type_224: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_69, torch.float16);  sum_69 = None
        view_337: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_224, [1, 1, 14336]);  convert_element_type_224 = None
        convert_element_type_225: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_337, torch.float32);  view_337 = None
        sigmoid_9: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_225)
        mul_129: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_225, sigmoid_9);  convert_element_type_225 = sigmoid_9 = None
        convert_element_type_226: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_129, torch.float16);  mul_129 = None
        convert_element_type_227: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_78, torch.float32);  getitem_78 = None
        unsqueeze_202: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_227, 2);  convert_element_type_227 = None
        permute_109: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg114_1, [1, 0]);  arg114_1 = None
        convert_element_type_228: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_109, torch.float32);  permute_109 = None
        unsqueeze_203: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_228, 0);  convert_element_type_228 = None
        mul_130: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_202, unsqueeze_203);  unsqueeze_202 = unsqueeze_203 = None
        sum_70: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_130, [1]);  mul_130 = None
        convert_element_type_229: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_70, torch.float16);  sum_70 = None
        view_341: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_229, [1, 1, 14336]);  convert_element_type_229 = None
        mul_131: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_226, view_341);  convert_element_type_226 = view_341 = None
        view_342: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_131, [1, 14336]);  mul_131 = None
        convert_element_type_230: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_342, torch.float32);  view_342 = None
        unsqueeze_204: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_230, 2);  convert_element_type_230 = None
        permute_110: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg115_1, [1, 0]);  arg115_1 = None
        convert_element_type_231: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_110, torch.float32);  permute_110 = None
        unsqueeze_205: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_231, 0);  convert_element_type_231 = None
        mul_132: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_204, unsqueeze_205);  unsqueeze_204 = unsqueeze_205 = None
        sum_71: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_132, [1]);  mul_132 = None
        convert_element_type_232: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_71, torch.float16);  sum_71 = None
        view_343: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_232, [1, 1, 4096]);  convert_element_type_232 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_39: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_38, view_343);  add_38 = view_343 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_344: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_39, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_41: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_20 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 216, constant_args_idx = 215, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_40, 'X_ptr': view_344, 'W_ptr': arg116_1, 'RSTD_ptr': empty_41}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_40 = view_344 = arg116_1 = empty_41 = None
        getitem_80: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_20['Y_ptr'];  triton_kernel_wrapper_functional_proxy_20 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_233: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_80, torch.float32)
        unsqueeze_206: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_233, 2);  convert_element_type_233 = None
        permute_111: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg117_1, [1, 0]);  arg117_1 = None
        convert_element_type_234: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_111, torch.float32);  permute_111 = None
        unsqueeze_207: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_234, 0);  convert_element_type_234 = None
        mul_133: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_206, unsqueeze_207);  unsqueeze_206 = unsqueeze_207 = None
        sum_72: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_133, [1]);  mul_133 = None
        convert_element_type_235: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_72, torch.float16);  sum_72 = None
        view_349: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_235, [1, 1, 4096]);  convert_element_type_235 = None
        view_350: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_349, [1, 1, -1, 128]);  view_349 = None
        permute_112: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_350, [0, 2, 1, 3]);  view_350 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_212: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_136: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_112, unsqueeze_212)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_235: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_112, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_20: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_235);  slice_235 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_234: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_112, 3, 0, 64);  permute_112 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_20: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_20, slice_234], -1);  neg_20 = slice_234 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_213: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_137: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_20, unsqueeze_213);  cat_20 = None
        add_40: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_136, mul_137);  mul_136 = mul_137 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_236: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_80, torch.float32)
        unsqueeze_208: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_236, 2);  convert_element_type_236 = None
        permute_113: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg118_1, [1, 0]);  arg118_1 = None
        convert_element_type_237: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_113, torch.float32);  permute_113 = None
        unsqueeze_209: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_237, 0);  convert_element_type_237 = None
        mul_134: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_208, unsqueeze_209);  unsqueeze_208 = unsqueeze_209 = None
        sum_73: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_134, [1]);  mul_134 = None
        convert_element_type_238: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_73, torch.float16);  sum_73 = None
        view_354: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_238, [1, 1, 1024]);  convert_element_type_238 = None
        view_355: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_354, [1, 1, -1, 128]);  view_354 = None
        permute_114: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_355, [0, 2, 1, 3]);  view_355 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_138: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_114, unsqueeze_212);  unsqueeze_212 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_237: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_114, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_21: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_237);  slice_237 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_236: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_114, 3, 0, 64);  permute_114 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_21: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_21, slice_236], -1);  neg_21 = slice_236 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_139: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_21, unsqueeze_213);  cat_21 = unsqueeze_213 = None
        add_41: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_138, mul_139);  mul_138 = mul_139 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_20: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg120_1, [None, None, arg2_1], add_41);  add_41 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_215: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_20, 2)
        expand_55: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_215, [1, 8, 4, 2048, 128]);  unsqueeze_215 = None
        clone_22: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_55, memory_format = torch.contiguous_format);  expand_55 = None
        view_361: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_22, [1, 32, 2048, 128]);  clone_22 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_239: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_80, torch.float32);  getitem_80 = None
        unsqueeze_210: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_239, 2);  convert_element_type_239 = None
        permute_115: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg119_1, [1, 0]);  arg119_1 = None
        convert_element_type_240: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_115, torch.float32);  permute_115 = None
        unsqueeze_211: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_240, 0);  convert_element_type_240 = None
        mul_135: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_210, unsqueeze_211);  unsqueeze_210 = unsqueeze_211 = None
        sum_74: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_135, [1]);  mul_135 = None
        convert_element_type_241: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_74, torch.float16);  sum_74 = None
        view_359: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_241, [1, 1, 1024]);  convert_element_type_241 = None
        view_360: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_359, [1, 1, -1, 128]);  view_359 = None
        permute_116: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_360, [0, 2, 1, 3]);  view_360 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_21: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg121_1, [None, None, arg2_1], permute_116);  permute_116 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_217: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_21, 2)
        expand_57: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_217, [1, 8, 4, 2048, 128]);  unsqueeze_217 = None
        clone_23: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_57, memory_format = torch.contiguous_format);  expand_57 = None
        view_362: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_23, [1, 32, 2048, 128]);  clone_23 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_21: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_20: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_10: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_21, full_default_20);  full_default_21 = full_default_20 = None
        expand_58: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_10, [1, 32, 1, 2048]);  where_10 = None
        _scaled_dot_product_efficient_attention_10 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_40, view_361, view_362, expand_58, False, scale = 0.08838834764831845);  add_40 = view_361 = view_362 = expand_58 = None
        getitem_82: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_10[0];  _scaled_dot_product_efficient_attention_10 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_42: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_117: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_82, [0, 2, 1, 3]);  getitem_82 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_363: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_117, [1, 1, -1]);  permute_117 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_364: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_363, [1, 4096]);  view_363 = None
        convert_element_type_242: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_364, torch.float32);  view_364 = None
        unsqueeze_218: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_242, 2);  convert_element_type_242 = None
        permute_118: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg122_1, [1, 0]);  arg122_1 = None
        convert_element_type_243: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_118, torch.float32);  permute_118 = None
        unsqueeze_219: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_243, 0);  convert_element_type_243 = None
        mul_140: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_218, unsqueeze_219);  unsqueeze_218 = unsqueeze_219 = None
        sum_75: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_140, [1]);  mul_140 = None
        convert_element_type_244: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_75, torch.float16);  sum_75 = None
        view_365: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_244, [1, 1, 4096]);  convert_element_type_244 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_42: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_39, view_365);  add_39 = view_365 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_366: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_42, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_43: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_21 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 217, constant_args_idx = 216, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_42, 'X_ptr': view_366, 'W_ptr': arg123_1, 'RSTD_ptr': empty_43}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_42 = view_366 = arg123_1 = empty_43 = None
        getitem_86: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_21['Y_ptr'];  triton_kernel_wrapper_functional_proxy_21 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_44: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_245: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_86, torch.float32)
        unsqueeze_220: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_245, 2);  convert_element_type_245 = None
        permute_119: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg124_1, [1, 0]);  arg124_1 = None
        convert_element_type_246: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_119, torch.float32);  permute_119 = None
        unsqueeze_221: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_246, 0);  convert_element_type_246 = None
        mul_141: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_220, unsqueeze_221);  unsqueeze_220 = unsqueeze_221 = None
        sum_76: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_141, [1]);  mul_141 = None
        convert_element_type_247: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_76, torch.float16);  sum_76 = None
        view_371: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_247, [1, 1, 14336]);  convert_element_type_247 = None
        convert_element_type_248: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_371, torch.float32);  view_371 = None
        sigmoid_10: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_248)
        mul_142: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_248, sigmoid_10);  convert_element_type_248 = sigmoid_10 = None
        convert_element_type_249: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_142, torch.float16);  mul_142 = None
        convert_element_type_250: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_86, torch.float32);  getitem_86 = None
        unsqueeze_222: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_250, 2);  convert_element_type_250 = None
        permute_120: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg125_1, [1, 0]);  arg125_1 = None
        convert_element_type_251: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_120, torch.float32);  permute_120 = None
        unsqueeze_223: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_251, 0);  convert_element_type_251 = None
        mul_143: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_222, unsqueeze_223);  unsqueeze_222 = unsqueeze_223 = None
        sum_77: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_143, [1]);  mul_143 = None
        convert_element_type_252: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_77, torch.float16);  sum_77 = None
        view_375: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_252, [1, 1, 14336]);  convert_element_type_252 = None
        mul_144: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_249, view_375);  convert_element_type_249 = view_375 = None
        view_376: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_144, [1, 14336]);  mul_144 = None
        convert_element_type_253: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_376, torch.float32);  view_376 = None
        unsqueeze_224: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_253, 2);  convert_element_type_253 = None
        permute_121: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg126_1, [1, 0]);  arg126_1 = None
        convert_element_type_254: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_121, torch.float32);  permute_121 = None
        unsqueeze_225: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_254, 0);  convert_element_type_254 = None
        mul_145: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_224, unsqueeze_225);  unsqueeze_224 = unsqueeze_225 = None
        sum_78: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_145, [1]);  mul_145 = None
        convert_element_type_255: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_78, torch.float16);  sum_78 = None
        view_377: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_255, [1, 1, 4096]);  convert_element_type_255 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_43: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_42, view_377);  add_42 = view_377 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_378: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_43, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_45: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_22 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 218, constant_args_idx = 217, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_44, 'X_ptr': view_378, 'W_ptr': arg127_1, 'RSTD_ptr': empty_45}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_44 = view_378 = arg127_1 = empty_45 = None
        getitem_88: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_22['Y_ptr'];  triton_kernel_wrapper_functional_proxy_22 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_256: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_88, torch.float32)
        unsqueeze_226: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_256, 2);  convert_element_type_256 = None
        permute_122: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg128_1, [1, 0]);  arg128_1 = None
        convert_element_type_257: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_122, torch.float32);  permute_122 = None
        unsqueeze_227: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_257, 0);  convert_element_type_257 = None
        mul_146: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_226, unsqueeze_227);  unsqueeze_226 = unsqueeze_227 = None
        sum_79: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_146, [1]);  mul_146 = None
        convert_element_type_258: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_79, torch.float16);  sum_79 = None
        view_383: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_258, [1, 1, 4096]);  convert_element_type_258 = None
        view_384: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_383, [1, 1, -1, 128]);  view_383 = None
        permute_123: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_384, [0, 2, 1, 3]);  view_384 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_232: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_149: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_123, unsqueeze_232)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_258: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_123, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_22: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_258);  slice_258 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_257: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_123, 3, 0, 64);  permute_123 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_22: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_22, slice_257], -1);  neg_22 = slice_257 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_233: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_150: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_22, unsqueeze_233);  cat_22 = None
        add_44: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_149, mul_150);  mul_149 = mul_150 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_259: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_88, torch.float32)
        unsqueeze_228: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_259, 2);  convert_element_type_259 = None
        permute_124: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg129_1, [1, 0]);  arg129_1 = None
        convert_element_type_260: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_124, torch.float32);  permute_124 = None
        unsqueeze_229: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_260, 0);  convert_element_type_260 = None
        mul_147: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_228, unsqueeze_229);  unsqueeze_228 = unsqueeze_229 = None
        sum_80: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_147, [1]);  mul_147 = None
        convert_element_type_261: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_80, torch.float16);  sum_80 = None
        view_388: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_261, [1, 1, 1024]);  convert_element_type_261 = None
        view_389: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_388, [1, 1, -1, 128]);  view_388 = None
        permute_125: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_389, [0, 2, 1, 3]);  view_389 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_151: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_125, unsqueeze_232);  unsqueeze_232 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_260: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_125, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_23: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_260);  slice_260 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_259: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_125, 3, 0, 64);  permute_125 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_23: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_23, slice_259], -1);  neg_23 = slice_259 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_152: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_23, unsqueeze_233);  cat_23 = unsqueeze_233 = None
        add_45: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_151, mul_152);  mul_151 = mul_152 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_22: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg131_1, [None, None, arg2_1], add_45);  add_45 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_235: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_22, 2)
        expand_60: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_235, [1, 8, 4, 2048, 128]);  unsqueeze_235 = None
        clone_24: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_60, memory_format = torch.contiguous_format);  expand_60 = None
        view_395: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_24, [1, 32, 2048, 128]);  clone_24 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_262: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_88, torch.float32);  getitem_88 = None
        unsqueeze_230: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_262, 2);  convert_element_type_262 = None
        permute_126: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg130_1, [1, 0]);  arg130_1 = None
        convert_element_type_263: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_126, torch.float32);  permute_126 = None
        unsqueeze_231: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_263, 0);  convert_element_type_263 = None
        mul_148: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_230, unsqueeze_231);  unsqueeze_230 = unsqueeze_231 = None
        sum_81: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_148, [1]);  mul_148 = None
        convert_element_type_264: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_81, torch.float16);  sum_81 = None
        view_393: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_264, [1, 1, 1024]);  convert_element_type_264 = None
        view_394: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_393, [1, 1, -1, 128]);  view_393 = None
        permute_127: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_394, [0, 2, 1, 3]);  view_394 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_23: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg132_1, [None, None, arg2_1], permute_127);  permute_127 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_237: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_23, 2)
        expand_62: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_237, [1, 8, 4, 2048, 128]);  unsqueeze_237 = None
        clone_25: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_62, memory_format = torch.contiguous_format);  expand_62 = None
        view_396: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_25, [1, 32, 2048, 128]);  clone_25 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_23: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_22: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_11: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_23, full_default_22);  full_default_23 = full_default_22 = None
        expand_63: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_11, [1, 32, 1, 2048]);  where_11 = None
        _scaled_dot_product_efficient_attention_11 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_44, view_395, view_396, expand_63, False, scale = 0.08838834764831845);  add_44 = view_395 = view_396 = expand_63 = None
        getitem_90: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_11[0];  _scaled_dot_product_efficient_attention_11 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_46: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_128: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_90, [0, 2, 1, 3]);  getitem_90 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_397: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_128, [1, 1, -1]);  permute_128 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_398: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_397, [1, 4096]);  view_397 = None
        convert_element_type_265: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_398, torch.float32);  view_398 = None
        unsqueeze_238: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_265, 2);  convert_element_type_265 = None
        permute_129: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg133_1, [1, 0]);  arg133_1 = None
        convert_element_type_266: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_129, torch.float32);  permute_129 = None
        unsqueeze_239: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_266, 0);  convert_element_type_266 = None
        mul_153: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_238, unsqueeze_239);  unsqueeze_238 = unsqueeze_239 = None
        sum_82: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_153, [1]);  mul_153 = None
        convert_element_type_267: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_82, torch.float16);  sum_82 = None
        view_399: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_267, [1, 1, 4096]);  convert_element_type_267 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_46: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_43, view_399);  add_43 = view_399 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_400: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_46, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_47: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_23 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 219, constant_args_idx = 218, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_46, 'X_ptr': view_400, 'W_ptr': arg134_1, 'RSTD_ptr': empty_47}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_46 = view_400 = arg134_1 = empty_47 = None
        getitem_94: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_23['Y_ptr'];  triton_kernel_wrapper_functional_proxy_23 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_48: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_268: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_94, torch.float32)
        unsqueeze_240: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_268, 2);  convert_element_type_268 = None
        permute_130: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg135_1, [1, 0]);  arg135_1 = None
        convert_element_type_269: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_130, torch.float32);  permute_130 = None
        unsqueeze_241: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_269, 0);  convert_element_type_269 = None
        mul_154: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_240, unsqueeze_241);  unsqueeze_240 = unsqueeze_241 = None
        sum_83: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_154, [1]);  mul_154 = None
        convert_element_type_270: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_83, torch.float16);  sum_83 = None
        view_405: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_270, [1, 1, 14336]);  convert_element_type_270 = None
        convert_element_type_271: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_405, torch.float32);  view_405 = None
        sigmoid_11: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_271)
        mul_155: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_271, sigmoid_11);  convert_element_type_271 = sigmoid_11 = None
        convert_element_type_272: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_155, torch.float16);  mul_155 = None
        convert_element_type_273: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_94, torch.float32);  getitem_94 = None
        unsqueeze_242: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_273, 2);  convert_element_type_273 = None
        permute_131: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg136_1, [1, 0]);  arg136_1 = None
        convert_element_type_274: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_131, torch.float32);  permute_131 = None
        unsqueeze_243: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_274, 0);  convert_element_type_274 = None
        mul_156: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_242, unsqueeze_243);  unsqueeze_242 = unsqueeze_243 = None
        sum_84: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_156, [1]);  mul_156 = None
        convert_element_type_275: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_84, torch.float16);  sum_84 = None
        view_409: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_275, [1, 1, 14336]);  convert_element_type_275 = None
        mul_157: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_272, view_409);  convert_element_type_272 = view_409 = None
        view_410: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_157, [1, 14336]);  mul_157 = None
        convert_element_type_276: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_410, torch.float32);  view_410 = None
        unsqueeze_244: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_276, 2);  convert_element_type_276 = None
        permute_132: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg137_1, [1, 0]);  arg137_1 = None
        convert_element_type_277: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_132, torch.float32);  permute_132 = None
        unsqueeze_245: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_277, 0);  convert_element_type_277 = None
        mul_158: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_244, unsqueeze_245);  unsqueeze_244 = unsqueeze_245 = None
        sum_85: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_158, [1]);  mul_158 = None
        convert_element_type_278: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_85, torch.float16);  sum_85 = None
        view_411: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_278, [1, 1, 4096]);  convert_element_type_278 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_47: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_46, view_411);  add_46 = view_411 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_412: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_47, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_49: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_24 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 220, constant_args_idx = 219, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_48, 'X_ptr': view_412, 'W_ptr': arg138_1, 'RSTD_ptr': empty_49}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_48 = view_412 = arg138_1 = empty_49 = None
        getitem_96: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_24['Y_ptr'];  triton_kernel_wrapper_functional_proxy_24 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_279: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_96, torch.float32)
        unsqueeze_246: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_279, 2);  convert_element_type_279 = None
        permute_133: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg139_1, [1, 0]);  arg139_1 = None
        convert_element_type_280: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_133, torch.float32);  permute_133 = None
        unsqueeze_247: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_280, 0);  convert_element_type_280 = None
        mul_159: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_246, unsqueeze_247);  unsqueeze_246 = unsqueeze_247 = None
        sum_86: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_159, [1]);  mul_159 = None
        convert_element_type_281: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_86, torch.float16);  sum_86 = None
        view_417: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_281, [1, 1, 4096]);  convert_element_type_281 = None
        view_418: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_417, [1, 1, -1, 128]);  view_417 = None
        permute_134: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_418, [0, 2, 1, 3]);  view_418 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_252: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_162: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_134, unsqueeze_252)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_281: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_134, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_24: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_281);  slice_281 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_280: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_134, 3, 0, 64);  permute_134 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_24: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_24, slice_280], -1);  neg_24 = slice_280 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_253: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_163: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_24, unsqueeze_253);  cat_24 = None
        add_48: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_162, mul_163);  mul_162 = mul_163 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_282: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_96, torch.float32)
        unsqueeze_248: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_282, 2);  convert_element_type_282 = None
        permute_135: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg140_1, [1, 0]);  arg140_1 = None
        convert_element_type_283: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_135, torch.float32);  permute_135 = None
        unsqueeze_249: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_283, 0);  convert_element_type_283 = None
        mul_160: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_248, unsqueeze_249);  unsqueeze_248 = unsqueeze_249 = None
        sum_87: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_160, [1]);  mul_160 = None
        convert_element_type_284: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_87, torch.float16);  sum_87 = None
        view_422: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_284, [1, 1, 1024]);  convert_element_type_284 = None
        view_423: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_422, [1, 1, -1, 128]);  view_422 = None
        permute_136: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_423, [0, 2, 1, 3]);  view_423 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_164: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_136, unsqueeze_252);  unsqueeze_252 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_283: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_136, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_25: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_283);  slice_283 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_282: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_136, 3, 0, 64);  permute_136 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_25: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_25, slice_282], -1);  neg_25 = slice_282 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_165: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_25, unsqueeze_253);  cat_25 = unsqueeze_253 = None
        add_49: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_164, mul_165);  mul_164 = mul_165 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_24: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg142_1, [None, None, arg2_1], add_49);  add_49 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_255: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_24, 2)
        expand_65: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_255, [1, 8, 4, 2048, 128]);  unsqueeze_255 = None
        clone_26: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_65, memory_format = torch.contiguous_format);  expand_65 = None
        view_429: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_26, [1, 32, 2048, 128]);  clone_26 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_285: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_96, torch.float32);  getitem_96 = None
        unsqueeze_250: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_285, 2);  convert_element_type_285 = None
        permute_137: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg141_1, [1, 0]);  arg141_1 = None
        convert_element_type_286: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_137, torch.float32);  permute_137 = None
        unsqueeze_251: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_286, 0);  convert_element_type_286 = None
        mul_161: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_250, unsqueeze_251);  unsqueeze_250 = unsqueeze_251 = None
        sum_88: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_161, [1]);  mul_161 = None
        convert_element_type_287: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_88, torch.float16);  sum_88 = None
        view_427: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_287, [1, 1, 1024]);  convert_element_type_287 = None
        view_428: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_427, [1, 1, -1, 128]);  view_427 = None
        permute_138: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_428, [0, 2, 1, 3]);  view_428 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_25: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg143_1, [None, None, arg2_1], permute_138);  permute_138 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_257: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_25, 2)
        expand_67: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_257, [1, 8, 4, 2048, 128]);  unsqueeze_257 = None
        clone_27: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_67, memory_format = torch.contiguous_format);  expand_67 = None
        view_430: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_27, [1, 32, 2048, 128]);  clone_27 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_25: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_24: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_12: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_25, full_default_24);  full_default_25 = full_default_24 = None
        expand_68: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_12, [1, 32, 1, 2048]);  where_12 = None
        _scaled_dot_product_efficient_attention_12 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_48, view_429, view_430, expand_68, False, scale = 0.08838834764831845);  add_48 = view_429 = view_430 = expand_68 = None
        getitem_98: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_12[0];  _scaled_dot_product_efficient_attention_12 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_50: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_139: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_98, [0, 2, 1, 3]);  getitem_98 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_431: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_139, [1, 1, -1]);  permute_139 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_432: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_431, [1, 4096]);  view_431 = None
        convert_element_type_288: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_432, torch.float32);  view_432 = None
        unsqueeze_258: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_288, 2);  convert_element_type_288 = None
        permute_140: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg144_1, [1, 0]);  arg144_1 = None
        convert_element_type_289: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_140, torch.float32);  permute_140 = None
        unsqueeze_259: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_289, 0);  convert_element_type_289 = None
        mul_166: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_258, unsqueeze_259);  unsqueeze_258 = unsqueeze_259 = None
        sum_89: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_166, [1]);  mul_166 = None
        convert_element_type_290: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_89, torch.float16);  sum_89 = None
        view_433: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_290, [1, 1, 4096]);  convert_element_type_290 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_50: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_47, view_433);  add_47 = view_433 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_434: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_50, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_51: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_25 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 221, constant_args_idx = 220, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_50, 'X_ptr': view_434, 'W_ptr': arg145_1, 'RSTD_ptr': empty_51}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_50 = view_434 = arg145_1 = empty_51 = None
        getitem_102: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_25['Y_ptr'];  triton_kernel_wrapper_functional_proxy_25 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_52: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_291: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_102, torch.float32)
        unsqueeze_260: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_291, 2);  convert_element_type_291 = None
        permute_141: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg146_1, [1, 0]);  arg146_1 = None
        convert_element_type_292: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_141, torch.float32);  permute_141 = None
        unsqueeze_261: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_292, 0);  convert_element_type_292 = None
        mul_167: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_260, unsqueeze_261);  unsqueeze_260 = unsqueeze_261 = None
        sum_90: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_167, [1]);  mul_167 = None
        convert_element_type_293: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_90, torch.float16);  sum_90 = None
        view_439: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_293, [1, 1, 14336]);  convert_element_type_293 = None
        convert_element_type_294: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_439, torch.float32);  view_439 = None
        sigmoid_12: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_294)
        mul_168: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_294, sigmoid_12);  convert_element_type_294 = sigmoid_12 = None
        convert_element_type_295: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_168, torch.float16);  mul_168 = None
        convert_element_type_296: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_102, torch.float32);  getitem_102 = None
        unsqueeze_262: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_296, 2);  convert_element_type_296 = None
        permute_142: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg147_1, [1, 0]);  arg147_1 = None
        convert_element_type_297: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_142, torch.float32);  permute_142 = None
        unsqueeze_263: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_297, 0);  convert_element_type_297 = None
        mul_169: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_262, unsqueeze_263);  unsqueeze_262 = unsqueeze_263 = None
        sum_91: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_169, [1]);  mul_169 = None
        convert_element_type_298: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_91, torch.float16);  sum_91 = None
        view_443: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_298, [1, 1, 14336]);  convert_element_type_298 = None
        mul_170: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_295, view_443);  convert_element_type_295 = view_443 = None
        view_444: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_170, [1, 14336]);  mul_170 = None
        convert_element_type_299: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_444, torch.float32);  view_444 = None
        unsqueeze_264: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_299, 2);  convert_element_type_299 = None
        permute_143: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg148_1, [1, 0]);  arg148_1 = None
        convert_element_type_300: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_143, torch.float32);  permute_143 = None
        unsqueeze_265: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_300, 0);  convert_element_type_300 = None
        mul_171: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_264, unsqueeze_265);  unsqueeze_264 = unsqueeze_265 = None
        sum_92: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_171, [1]);  mul_171 = None
        convert_element_type_301: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_92, torch.float16);  sum_92 = None
        view_445: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_301, [1, 1, 4096]);  convert_element_type_301 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_51: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_50, view_445);  add_50 = view_445 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_446: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_51, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_53: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_26 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 222, constant_args_idx = 221, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_52, 'X_ptr': view_446, 'W_ptr': arg149_1, 'RSTD_ptr': empty_53}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_52 = view_446 = arg149_1 = empty_53 = None
        getitem_104: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_26['Y_ptr'];  triton_kernel_wrapper_functional_proxy_26 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_302: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_104, torch.float32)
        unsqueeze_266: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_302, 2);  convert_element_type_302 = None
        permute_144: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg150_1, [1, 0]);  arg150_1 = None
        convert_element_type_303: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_144, torch.float32);  permute_144 = None
        unsqueeze_267: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_303, 0);  convert_element_type_303 = None
        mul_172: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_266, unsqueeze_267);  unsqueeze_266 = unsqueeze_267 = None
        sum_93: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_172, [1]);  mul_172 = None
        convert_element_type_304: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_93, torch.float16);  sum_93 = None
        view_451: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_304, [1, 1, 4096]);  convert_element_type_304 = None
        view_452: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_451, [1, 1, -1, 128]);  view_451 = None
        permute_145: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_452, [0, 2, 1, 3]);  view_452 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_272: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_175: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_145, unsqueeze_272)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_304: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_145, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_26: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_304);  slice_304 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_303: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_145, 3, 0, 64);  permute_145 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_26: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_26, slice_303], -1);  neg_26 = slice_303 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_273: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_176: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_26, unsqueeze_273);  cat_26 = None
        add_52: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_175, mul_176);  mul_175 = mul_176 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_305: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_104, torch.float32)
        unsqueeze_268: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_305, 2);  convert_element_type_305 = None
        permute_146: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg151_1, [1, 0]);  arg151_1 = None
        convert_element_type_306: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_146, torch.float32);  permute_146 = None
        unsqueeze_269: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_306, 0);  convert_element_type_306 = None
        mul_173: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_268, unsqueeze_269);  unsqueeze_268 = unsqueeze_269 = None
        sum_94: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_173, [1]);  mul_173 = None
        convert_element_type_307: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_94, torch.float16);  sum_94 = None
        view_456: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_307, [1, 1, 1024]);  convert_element_type_307 = None
        view_457: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_456, [1, 1, -1, 128]);  view_456 = None
        permute_147: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_457, [0, 2, 1, 3]);  view_457 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_177: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_147, unsqueeze_272);  unsqueeze_272 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_306: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_147, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_27: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_306);  slice_306 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_305: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_147, 3, 0, 64);  permute_147 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_27: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_27, slice_305], -1);  neg_27 = slice_305 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_178: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_27, unsqueeze_273);  cat_27 = unsqueeze_273 = None
        add_53: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_177, mul_178);  mul_177 = mul_178 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_26: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg153_1, [None, None, arg2_1], add_53);  add_53 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_275: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_26, 2)
        expand_70: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_275, [1, 8, 4, 2048, 128]);  unsqueeze_275 = None
        clone_28: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_70, memory_format = torch.contiguous_format);  expand_70 = None
        view_463: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_28, [1, 32, 2048, 128]);  clone_28 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_308: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_104, torch.float32);  getitem_104 = None
        unsqueeze_270: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_308, 2);  convert_element_type_308 = None
        permute_148: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg152_1, [1, 0]);  arg152_1 = None
        convert_element_type_309: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_148, torch.float32);  permute_148 = None
        unsqueeze_271: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_309, 0);  convert_element_type_309 = None
        mul_174: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_270, unsqueeze_271);  unsqueeze_270 = unsqueeze_271 = None
        sum_95: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_174, [1]);  mul_174 = None
        convert_element_type_310: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_95, torch.float16);  sum_95 = None
        view_461: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_310, [1, 1, 1024]);  convert_element_type_310 = None
        view_462: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_461, [1, 1, -1, 128]);  view_461 = None
        permute_149: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_462, [0, 2, 1, 3]);  view_462 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_27: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg154_1, [None, None, arg2_1], permute_149);  permute_149 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_277: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_27, 2)
        expand_72: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_277, [1, 8, 4, 2048, 128]);  unsqueeze_277 = None
        clone_29: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_72, memory_format = torch.contiguous_format);  expand_72 = None
        view_464: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_29, [1, 32, 2048, 128]);  clone_29 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_27: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_26: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_13: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_27, full_default_26);  full_default_27 = full_default_26 = None
        expand_73: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_13, [1, 32, 1, 2048]);  where_13 = None
        _scaled_dot_product_efficient_attention_13 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_52, view_463, view_464, expand_73, False, scale = 0.08838834764831845);  add_52 = view_463 = view_464 = expand_73 = None
        getitem_106: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_13[0];  _scaled_dot_product_efficient_attention_13 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_54: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_150: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_106, [0, 2, 1, 3]);  getitem_106 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_465: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_150, [1, 1, -1]);  permute_150 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_466: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_465, [1, 4096]);  view_465 = None
        convert_element_type_311: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_466, torch.float32);  view_466 = None
        unsqueeze_278: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_311, 2);  convert_element_type_311 = None
        permute_151: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg155_1, [1, 0]);  arg155_1 = None
        convert_element_type_312: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_151, torch.float32);  permute_151 = None
        unsqueeze_279: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_312, 0);  convert_element_type_312 = None
        mul_179: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_278, unsqueeze_279);  unsqueeze_278 = unsqueeze_279 = None
        sum_96: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_179, [1]);  mul_179 = None
        convert_element_type_313: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_96, torch.float16);  sum_96 = None
        view_467: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_313, [1, 1, 4096]);  convert_element_type_313 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_54: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_51, view_467);  add_51 = view_467 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_468: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_54, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_55: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_27 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 223, constant_args_idx = 222, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_54, 'X_ptr': view_468, 'W_ptr': arg156_1, 'RSTD_ptr': empty_55}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_54 = view_468 = arg156_1 = empty_55 = None
        getitem_110: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_27['Y_ptr'];  triton_kernel_wrapper_functional_proxy_27 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_56: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_314: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_110, torch.float32)
        unsqueeze_280: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_314, 2);  convert_element_type_314 = None
        permute_152: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg157_1, [1, 0]);  arg157_1 = None
        convert_element_type_315: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_152, torch.float32);  permute_152 = None
        unsqueeze_281: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_315, 0);  convert_element_type_315 = None
        mul_180: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_280, unsqueeze_281);  unsqueeze_280 = unsqueeze_281 = None
        sum_97: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_180, [1]);  mul_180 = None
        convert_element_type_316: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_97, torch.float16);  sum_97 = None
        view_473: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_316, [1, 1, 14336]);  convert_element_type_316 = None
        convert_element_type_317: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_473, torch.float32);  view_473 = None
        sigmoid_13: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_317)
        mul_181: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_317, sigmoid_13);  convert_element_type_317 = sigmoid_13 = None
        convert_element_type_318: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_181, torch.float16);  mul_181 = None
        convert_element_type_319: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_110, torch.float32);  getitem_110 = None
        unsqueeze_282: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_319, 2);  convert_element_type_319 = None
        permute_153: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg158_1, [1, 0]);  arg158_1 = None
        convert_element_type_320: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_153, torch.float32);  permute_153 = None
        unsqueeze_283: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_320, 0);  convert_element_type_320 = None
        mul_182: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_282, unsqueeze_283);  unsqueeze_282 = unsqueeze_283 = None
        sum_98: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_182, [1]);  mul_182 = None
        convert_element_type_321: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_98, torch.float16);  sum_98 = None
        view_477: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_321, [1, 1, 14336]);  convert_element_type_321 = None
        mul_183: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_318, view_477);  convert_element_type_318 = view_477 = None
        view_478: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_183, [1, 14336]);  mul_183 = None
        convert_element_type_322: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_478, torch.float32);  view_478 = None
        unsqueeze_284: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_322, 2);  convert_element_type_322 = None
        permute_154: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg159_1, [1, 0]);  arg159_1 = None
        convert_element_type_323: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_154, torch.float32);  permute_154 = None
        unsqueeze_285: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_323, 0);  convert_element_type_323 = None
        mul_184: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_284, unsqueeze_285);  unsqueeze_284 = unsqueeze_285 = None
        sum_99: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_184, [1]);  mul_184 = None
        convert_element_type_324: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_99, torch.float16);  sum_99 = None
        view_479: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_324, [1, 1, 4096]);  convert_element_type_324 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_55: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_54, view_479);  add_54 = view_479 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_480: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_55, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_57: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_28 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 224, constant_args_idx = 223, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_56, 'X_ptr': view_480, 'W_ptr': arg160_1, 'RSTD_ptr': empty_57}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_56 = view_480 = arg160_1 = empty_57 = None
        getitem_112: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_28['Y_ptr'];  triton_kernel_wrapper_functional_proxy_28 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_325: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_112, torch.float32)
        unsqueeze_286: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_325, 2);  convert_element_type_325 = None
        permute_155: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg161_1, [1, 0]);  arg161_1 = None
        convert_element_type_326: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_155, torch.float32);  permute_155 = None
        unsqueeze_287: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_326, 0);  convert_element_type_326 = None
        mul_185: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_286, unsqueeze_287);  unsqueeze_286 = unsqueeze_287 = None
        sum_100: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_185, [1]);  mul_185 = None
        convert_element_type_327: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_100, torch.float16);  sum_100 = None
        view_485: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_327, [1, 1, 4096]);  convert_element_type_327 = None
        view_486: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_485, [1, 1, -1, 128]);  view_485 = None
        permute_156: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_486, [0, 2, 1, 3]);  view_486 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_292: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_188: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_156, unsqueeze_292)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_327: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_156, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_28: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_327);  slice_327 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_326: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_156, 3, 0, 64);  permute_156 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_28: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_28, slice_326], -1);  neg_28 = slice_326 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_293: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_189: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_28, unsqueeze_293);  cat_28 = None
        add_56: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_188, mul_189);  mul_188 = mul_189 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_328: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_112, torch.float32)
        unsqueeze_288: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_328, 2);  convert_element_type_328 = None
        permute_157: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg162_1, [1, 0]);  arg162_1 = None
        convert_element_type_329: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_157, torch.float32);  permute_157 = None
        unsqueeze_289: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_329, 0);  convert_element_type_329 = None
        mul_186: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_288, unsqueeze_289);  unsqueeze_288 = unsqueeze_289 = None
        sum_101: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_186, [1]);  mul_186 = None
        convert_element_type_330: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_101, torch.float16);  sum_101 = None
        view_490: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_330, [1, 1, 1024]);  convert_element_type_330 = None
        view_491: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_490, [1, 1, -1, 128]);  view_490 = None
        permute_158: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_491, [0, 2, 1, 3]);  view_491 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_190: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_158, unsqueeze_292);  unsqueeze_292 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_329: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_158, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_29: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_329);  slice_329 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_328: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_158, 3, 0, 64);  permute_158 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_29: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_29, slice_328], -1);  neg_29 = slice_328 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_191: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_29, unsqueeze_293);  cat_29 = unsqueeze_293 = None
        add_57: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_190, mul_191);  mul_190 = mul_191 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_28: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg164_1, [None, None, arg2_1], add_57);  add_57 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_295: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_28, 2)
        expand_75: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_295, [1, 8, 4, 2048, 128]);  unsqueeze_295 = None
        clone_30: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_75, memory_format = torch.contiguous_format);  expand_75 = None
        view_497: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_30, [1, 32, 2048, 128]);  clone_30 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_331: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_112, torch.float32);  getitem_112 = None
        unsqueeze_290: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_331, 2);  convert_element_type_331 = None
        permute_159: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg163_1, [1, 0]);  arg163_1 = None
        convert_element_type_332: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_159, torch.float32);  permute_159 = None
        unsqueeze_291: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_332, 0);  convert_element_type_332 = None
        mul_187: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_290, unsqueeze_291);  unsqueeze_290 = unsqueeze_291 = None
        sum_102: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_187, [1]);  mul_187 = None
        convert_element_type_333: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_102, torch.float16);  sum_102 = None
        view_495: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_333, [1, 1, 1024]);  convert_element_type_333 = None
        view_496: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_495, [1, 1, -1, 128]);  view_495 = None
        permute_160: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_496, [0, 2, 1, 3]);  view_496 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_29: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg165_1, [None, None, arg2_1], permute_160);  permute_160 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_297: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_29, 2)
        expand_77: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_297, [1, 8, 4, 2048, 128]);  unsqueeze_297 = None
        clone_31: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_77, memory_format = torch.contiguous_format);  expand_77 = None
        view_498: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_31, [1, 32, 2048, 128]);  clone_31 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_29: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_28: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_14: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_29, full_default_28);  full_default_29 = full_default_28 = None
        expand_78: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_14, [1, 32, 1, 2048]);  where_14 = None
        _scaled_dot_product_efficient_attention_14 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_56, view_497, view_498, expand_78, False, scale = 0.08838834764831845);  add_56 = view_497 = view_498 = expand_78 = None
        getitem_114: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_14[0];  _scaled_dot_product_efficient_attention_14 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_58: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_161: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_114, [0, 2, 1, 3]);  getitem_114 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_499: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_161, [1, 1, -1]);  permute_161 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_500: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_499, [1, 4096]);  view_499 = None
        convert_element_type_334: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_500, torch.float32);  view_500 = None
        unsqueeze_298: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_334, 2);  convert_element_type_334 = None
        permute_162: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg166_1, [1, 0]);  arg166_1 = None
        convert_element_type_335: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_162, torch.float32);  permute_162 = None
        unsqueeze_299: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_335, 0);  convert_element_type_335 = None
        mul_192: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_298, unsqueeze_299);  unsqueeze_298 = unsqueeze_299 = None
        sum_103: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_192, [1]);  mul_192 = None
        convert_element_type_336: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_103, torch.float16);  sum_103 = None
        view_501: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_336, [1, 1, 4096]);  convert_element_type_336 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_58: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_55, view_501);  add_55 = view_501 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_502: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_58, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_59: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_29 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 225, constant_args_idx = 224, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_58, 'X_ptr': view_502, 'W_ptr': arg167_1, 'RSTD_ptr': empty_59}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_58 = view_502 = arg167_1 = empty_59 = None
        getitem_118: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_29['Y_ptr'];  triton_kernel_wrapper_functional_proxy_29 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_60: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_337: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_118, torch.float32)
        unsqueeze_300: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_337, 2);  convert_element_type_337 = None
        permute_163: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg168_1, [1, 0]);  arg168_1 = None
        convert_element_type_338: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_163, torch.float32);  permute_163 = None
        unsqueeze_301: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_338, 0);  convert_element_type_338 = None
        mul_193: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_300, unsqueeze_301);  unsqueeze_300 = unsqueeze_301 = None
        sum_104: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_193, [1]);  mul_193 = None
        convert_element_type_339: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_104, torch.float16);  sum_104 = None
        view_507: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_339, [1, 1, 14336]);  convert_element_type_339 = None
        convert_element_type_340: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_507, torch.float32);  view_507 = None
        sigmoid_14: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_340)
        mul_194: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_340, sigmoid_14);  convert_element_type_340 = sigmoid_14 = None
        convert_element_type_341: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_194, torch.float16);  mul_194 = None
        convert_element_type_342: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_118, torch.float32);  getitem_118 = None
        unsqueeze_302: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_342, 2);  convert_element_type_342 = None
        permute_164: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg169_1, [1, 0]);  arg169_1 = None
        convert_element_type_343: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_164, torch.float32);  permute_164 = None
        unsqueeze_303: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_343, 0);  convert_element_type_343 = None
        mul_195: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_302, unsqueeze_303);  unsqueeze_302 = unsqueeze_303 = None
        sum_105: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_195, [1]);  mul_195 = None
        convert_element_type_344: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_105, torch.float16);  sum_105 = None
        view_511: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_344, [1, 1, 14336]);  convert_element_type_344 = None
        mul_196: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_341, view_511);  convert_element_type_341 = view_511 = None
        view_512: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_196, [1, 14336]);  mul_196 = None
        convert_element_type_345: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_512, torch.float32);  view_512 = None
        unsqueeze_304: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_345, 2);  convert_element_type_345 = None
        permute_165: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg170_1, [1, 0]);  arg170_1 = None
        convert_element_type_346: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_165, torch.float32);  permute_165 = None
        unsqueeze_305: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_346, 0);  convert_element_type_346 = None
        mul_197: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_304, unsqueeze_305);  unsqueeze_304 = unsqueeze_305 = None
        sum_106: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_197, [1]);  mul_197 = None
        convert_element_type_347: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_106, torch.float16);  sum_106 = None
        view_513: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_347, [1, 1, 4096]);  convert_element_type_347 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_59: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_58, view_513);  add_58 = view_513 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_514: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_59, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_61: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_30 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 226, constant_args_idx = 225, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_60, 'X_ptr': view_514, 'W_ptr': arg171_1, 'RSTD_ptr': empty_61}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_60 = view_514 = arg171_1 = empty_61 = None
        getitem_120: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_30['Y_ptr'];  triton_kernel_wrapper_functional_proxy_30 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_348: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_120, torch.float32)
        unsqueeze_306: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_348, 2);  convert_element_type_348 = None
        permute_166: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg172_1, [1, 0]);  arg172_1 = None
        convert_element_type_349: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_166, torch.float32);  permute_166 = None
        unsqueeze_307: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_349, 0);  convert_element_type_349 = None
        mul_198: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_306, unsqueeze_307);  unsqueeze_306 = unsqueeze_307 = None
        sum_107: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_198, [1]);  mul_198 = None
        convert_element_type_350: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_107, torch.float16);  sum_107 = None
        view_519: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_350, [1, 1, 4096]);  convert_element_type_350 = None
        view_520: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_519, [1, 1, -1, 128]);  view_519 = None
        permute_167: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_520, [0, 2, 1, 3]);  view_520 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_312: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_201: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_167, unsqueeze_312)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_350: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_167, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_30: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_350);  slice_350 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_349: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_167, 3, 0, 64);  permute_167 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_30: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_30, slice_349], -1);  neg_30 = slice_349 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_313: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_202: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_30, unsqueeze_313);  cat_30 = None
        add_60: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_201, mul_202);  mul_201 = mul_202 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_351: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_120, torch.float32)
        unsqueeze_308: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_351, 2);  convert_element_type_351 = None
        permute_168: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg173_1, [1, 0]);  arg173_1 = None
        convert_element_type_352: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_168, torch.float32);  permute_168 = None
        unsqueeze_309: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_352, 0);  convert_element_type_352 = None
        mul_199: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_308, unsqueeze_309);  unsqueeze_308 = unsqueeze_309 = None
        sum_108: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_199, [1]);  mul_199 = None
        convert_element_type_353: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_108, torch.float16);  sum_108 = None
        view_524: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_353, [1, 1, 1024]);  convert_element_type_353 = None
        view_525: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_524, [1, 1, -1, 128]);  view_524 = None
        permute_169: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_525, [0, 2, 1, 3]);  view_525 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_203: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_169, unsqueeze_312);  unsqueeze_312 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_352: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_169, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_31: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_352);  slice_352 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_351: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_169, 3, 0, 64);  permute_169 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_31: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_31, slice_351], -1);  neg_31 = slice_351 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_204: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_31, unsqueeze_313);  cat_31 = unsqueeze_313 = None
        add_61: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_203, mul_204);  mul_203 = mul_204 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_30: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg175_1, [None, None, arg2_1], add_61);  add_61 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_315: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_30, 2)
        expand_80: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_315, [1, 8, 4, 2048, 128]);  unsqueeze_315 = None
        clone_32: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_80, memory_format = torch.contiguous_format);  expand_80 = None
        view_531: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_32, [1, 32, 2048, 128]);  clone_32 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_354: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_120, torch.float32);  getitem_120 = None
        unsqueeze_310: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_354, 2);  convert_element_type_354 = None
        permute_170: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg174_1, [1, 0]);  arg174_1 = None
        convert_element_type_355: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_170, torch.float32);  permute_170 = None
        unsqueeze_311: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_355, 0);  convert_element_type_355 = None
        mul_200: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_310, unsqueeze_311);  unsqueeze_310 = unsqueeze_311 = None
        sum_109: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_200, [1]);  mul_200 = None
        convert_element_type_356: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_109, torch.float16);  sum_109 = None
        view_529: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_356, [1, 1, 1024]);  convert_element_type_356 = None
        view_530: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_529, [1, 1, -1, 128]);  view_529 = None
        permute_171: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_530, [0, 2, 1, 3]);  view_530 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_31: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg176_1, [None, None, arg2_1], permute_171);  permute_171 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_317: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_31, 2)
        expand_82: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_317, [1, 8, 4, 2048, 128]);  unsqueeze_317 = None
        clone_33: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_82, memory_format = torch.contiguous_format);  expand_82 = None
        view_532: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_33, [1, 32, 2048, 128]);  clone_33 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_31: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_30: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_15: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_31, full_default_30);  full_default_31 = full_default_30 = None
        expand_83: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_15, [1, 32, 1, 2048]);  where_15 = None
        _scaled_dot_product_efficient_attention_15 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_60, view_531, view_532, expand_83, False, scale = 0.08838834764831845);  add_60 = view_531 = view_532 = expand_83 = None
        getitem_122: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_15[0];  _scaled_dot_product_efficient_attention_15 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_62: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_172: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_122, [0, 2, 1, 3]);  getitem_122 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_533: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_172, [1, 1, -1]);  permute_172 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_534: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_533, [1, 4096]);  view_533 = None
        convert_element_type_357: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_534, torch.float32);  view_534 = None
        unsqueeze_318: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_357, 2);  convert_element_type_357 = None
        permute_173: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg177_1, [1, 0]);  arg177_1 = None
        convert_element_type_358: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_173, torch.float32);  permute_173 = None
        unsqueeze_319: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_358, 0);  convert_element_type_358 = None
        mul_205: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_318, unsqueeze_319);  unsqueeze_318 = unsqueeze_319 = None
        sum_110: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_205, [1]);  mul_205 = None
        convert_element_type_359: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_110, torch.float16);  sum_110 = None
        view_535: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_359, [1, 1, 4096]);  convert_element_type_359 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_62: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_59, view_535);  add_59 = view_535 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_536: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_62, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_63: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_31 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 227, constant_args_idx = 226, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_62, 'X_ptr': view_536, 'W_ptr': arg178_1, 'RSTD_ptr': empty_63}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_62 = view_536 = arg178_1 = empty_63 = None
        getitem_126: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_31['Y_ptr'];  triton_kernel_wrapper_functional_proxy_31 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_64: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_360: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_126, torch.float32)
        unsqueeze_320: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_360, 2);  convert_element_type_360 = None
        permute_174: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg179_1, [1, 0]);  arg179_1 = None
        convert_element_type_361: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_174, torch.float32);  permute_174 = None
        unsqueeze_321: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_361, 0);  convert_element_type_361 = None
        mul_206: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_320, unsqueeze_321);  unsqueeze_320 = unsqueeze_321 = None
        sum_111: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_206, [1]);  mul_206 = None
        convert_element_type_362: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_111, torch.float16);  sum_111 = None
        view_541: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_362, [1, 1, 14336]);  convert_element_type_362 = None
        convert_element_type_363: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_541, torch.float32);  view_541 = None
        sigmoid_15: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_363)
        mul_207: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_363, sigmoid_15);  convert_element_type_363 = sigmoid_15 = None
        convert_element_type_364: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_207, torch.float16);  mul_207 = None
        convert_element_type_365: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_126, torch.float32);  getitem_126 = None
        unsqueeze_322: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_365, 2);  convert_element_type_365 = None
        permute_175: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg180_1, [1, 0]);  arg180_1 = None
        convert_element_type_366: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_175, torch.float32);  permute_175 = None
        unsqueeze_323: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_366, 0);  convert_element_type_366 = None
        mul_208: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_322, unsqueeze_323);  unsqueeze_322 = unsqueeze_323 = None
        sum_112: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_208, [1]);  mul_208 = None
        convert_element_type_367: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_112, torch.float16);  sum_112 = None
        view_545: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_367, [1, 1, 14336]);  convert_element_type_367 = None
        mul_209: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_364, view_545);  convert_element_type_364 = view_545 = None
        view_546: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_209, [1, 14336]);  mul_209 = None
        convert_element_type_368: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_546, torch.float32);  view_546 = None
        unsqueeze_324: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_368, 2);  convert_element_type_368 = None
        permute_176: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg181_1, [1, 0]);  arg181_1 = None
        convert_element_type_369: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_176, torch.float32);  permute_176 = None
        unsqueeze_325: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_369, 0);  convert_element_type_369 = None
        mul_210: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_324, unsqueeze_325);  unsqueeze_324 = unsqueeze_325 = None
        sum_113: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_210, [1]);  mul_210 = None
        convert_element_type_370: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_113, torch.float16);  sum_113 = None
        view_547: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_370, [1, 1, 4096]);  convert_element_type_370 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_63: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_62, view_547);  add_62 = view_547 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_548: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_63, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_65: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_32 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 228, constant_args_idx = 227, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_64, 'X_ptr': view_548, 'W_ptr': arg182_1, 'RSTD_ptr': empty_65}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_64 = view_548 = arg182_1 = empty_65 = None
        getitem_128: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_32['Y_ptr'];  triton_kernel_wrapper_functional_proxy_32 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_371: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_128, torch.float32)
        unsqueeze_326: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_371, 2);  convert_element_type_371 = None
        permute_177: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg183_1, [1, 0]);  arg183_1 = None
        convert_element_type_372: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_177, torch.float32);  permute_177 = None
        unsqueeze_327: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_372, 0);  convert_element_type_372 = None
        mul_211: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_326, unsqueeze_327);  unsqueeze_326 = unsqueeze_327 = None
        sum_114: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_211, [1]);  mul_211 = None
        convert_element_type_373: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_114, torch.float16);  sum_114 = None
        view_553: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_373, [1, 1, 4096]);  convert_element_type_373 = None
        view_554: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_553, [1, 1, -1, 128]);  view_553 = None
        permute_178: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_554, [0, 2, 1, 3]);  view_554 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_332: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_214: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_178, unsqueeze_332)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_373: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_178, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_32: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_373);  slice_373 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_372: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_178, 3, 0, 64);  permute_178 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_32: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_32, slice_372], -1);  neg_32 = slice_372 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_333: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_215: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_32, unsqueeze_333);  cat_32 = None
        add_64: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_214, mul_215);  mul_214 = mul_215 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_374: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_128, torch.float32)
        unsqueeze_328: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_374, 2);  convert_element_type_374 = None
        permute_179: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg184_1, [1, 0]);  arg184_1 = None
        convert_element_type_375: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_179, torch.float32);  permute_179 = None
        unsqueeze_329: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_375, 0);  convert_element_type_375 = None
        mul_212: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_328, unsqueeze_329);  unsqueeze_328 = unsqueeze_329 = None
        sum_115: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_212, [1]);  mul_212 = None
        convert_element_type_376: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_115, torch.float16);  sum_115 = None
        view_558: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_376, [1, 1, 1024]);  convert_element_type_376 = None
        view_559: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_558, [1, 1, -1, 128]);  view_558 = None
        permute_180: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_559, [0, 2, 1, 3]);  view_559 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_216: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_180, unsqueeze_332);  unsqueeze_332 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_375: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_180, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_33: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_375);  slice_375 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_374: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_180, 3, 0, 64);  permute_180 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_33: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_33, slice_374], -1);  neg_33 = slice_374 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_217: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_33, unsqueeze_333);  cat_33 = unsqueeze_333 = None
        add_65: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_216, mul_217);  mul_216 = mul_217 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_32: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg186_1, [None, None, arg2_1], add_65);  add_65 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_335: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_32, 2)
        expand_85: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_335, [1, 8, 4, 2048, 128]);  unsqueeze_335 = None
        clone_34: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_85, memory_format = torch.contiguous_format);  expand_85 = None
        view_565: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_34, [1, 32, 2048, 128]);  clone_34 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_377: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_128, torch.float32);  getitem_128 = None
        unsqueeze_330: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_377, 2);  convert_element_type_377 = None
        permute_181: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg185_1, [1, 0]);  arg185_1 = None
        convert_element_type_378: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_181, torch.float32);  permute_181 = None
        unsqueeze_331: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_378, 0);  convert_element_type_378 = None
        mul_213: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_330, unsqueeze_331);  unsqueeze_330 = unsqueeze_331 = None
        sum_116: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_213, [1]);  mul_213 = None
        convert_element_type_379: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_116, torch.float16);  sum_116 = None
        view_563: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_379, [1, 1, 1024]);  convert_element_type_379 = None
        view_564: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_563, [1, 1, -1, 128]);  view_563 = None
        permute_182: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_564, [0, 2, 1, 3]);  view_564 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_33: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg187_1, [None, None, arg2_1], permute_182);  permute_182 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_337: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_33, 2)
        expand_87: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_337, [1, 8, 4, 2048, 128]);  unsqueeze_337 = None
        clone_35: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_87, memory_format = torch.contiguous_format);  expand_87 = None
        view_566: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_35, [1, 32, 2048, 128]);  clone_35 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_33: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_32: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_16: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_33, full_default_32);  full_default_33 = full_default_32 = None
        expand_88: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_16, [1, 32, 1, 2048]);  where_16 = None
        _scaled_dot_product_efficient_attention_16 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_64, view_565, view_566, expand_88, False, scale = 0.08838834764831845);  add_64 = view_565 = view_566 = expand_88 = None
        getitem_130: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_16[0];  _scaled_dot_product_efficient_attention_16 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_66: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_183: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_130, [0, 2, 1, 3]);  getitem_130 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_567: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_183, [1, 1, -1]);  permute_183 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_568: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_567, [1, 4096]);  view_567 = None
        convert_element_type_380: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_568, torch.float32);  view_568 = None
        unsqueeze_338: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_380, 2);  convert_element_type_380 = None
        permute_184: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg188_1, [1, 0]);  arg188_1 = None
        convert_element_type_381: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_184, torch.float32);  permute_184 = None
        unsqueeze_339: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_381, 0);  convert_element_type_381 = None
        mul_218: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_338, unsqueeze_339);  unsqueeze_338 = unsqueeze_339 = None
        sum_117: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_218, [1]);  mul_218 = None
        convert_element_type_382: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_117, torch.float16);  sum_117 = None
        view_569: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_382, [1, 1, 4096]);  convert_element_type_382 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_66: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_63, view_569);  add_63 = view_569 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_570: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_66, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_67: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_33 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 229, constant_args_idx = 228, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_66, 'X_ptr': view_570, 'W_ptr': arg189_1, 'RSTD_ptr': empty_67}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_66 = view_570 = arg189_1 = empty_67 = None
        getitem_134: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_33['Y_ptr'];  triton_kernel_wrapper_functional_proxy_33 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_68: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_383: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_134, torch.float32)
        unsqueeze_340: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_383, 2);  convert_element_type_383 = None
        permute_185: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg190_1, [1, 0]);  arg190_1 = None
        convert_element_type_384: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_185, torch.float32);  permute_185 = None
        unsqueeze_341: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_384, 0);  convert_element_type_384 = None
        mul_219: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_340, unsqueeze_341);  unsqueeze_340 = unsqueeze_341 = None
        sum_118: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_219, [1]);  mul_219 = None
        convert_element_type_385: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_118, torch.float16);  sum_118 = None
        view_575: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_385, [1, 1, 14336]);  convert_element_type_385 = None
        convert_element_type_386: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_575, torch.float32);  view_575 = None
        sigmoid_16: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_386)
        mul_220: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_386, sigmoid_16);  convert_element_type_386 = sigmoid_16 = None
        convert_element_type_387: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_220, torch.float16);  mul_220 = None
        convert_element_type_388: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_134, torch.float32);  getitem_134 = None
        unsqueeze_342: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_388, 2);  convert_element_type_388 = None
        permute_186: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg191_1, [1, 0]);  arg191_1 = None
        convert_element_type_389: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_186, torch.float32);  permute_186 = None
        unsqueeze_343: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_389, 0);  convert_element_type_389 = None
        mul_221: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_342, unsqueeze_343);  unsqueeze_342 = unsqueeze_343 = None
        sum_119: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_221, [1]);  mul_221 = None
        convert_element_type_390: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_119, torch.float16);  sum_119 = None
        view_579: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_390, [1, 1, 14336]);  convert_element_type_390 = None
        mul_222: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_387, view_579);  convert_element_type_387 = view_579 = None
        view_580: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_222, [1, 14336]);  mul_222 = None
        convert_element_type_391: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_580, torch.float32);  view_580 = None
        unsqueeze_344: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_391, 2);  convert_element_type_391 = None
        permute_187: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg192_1, [1, 0]);  arg192_1 = None
        convert_element_type_392: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_187, torch.float32);  permute_187 = None
        unsqueeze_345: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_392, 0);  convert_element_type_392 = None
        mul_223: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_344, unsqueeze_345);  unsqueeze_344 = unsqueeze_345 = None
        sum_120: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_223, [1]);  mul_223 = None
        convert_element_type_393: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_120, torch.float16);  sum_120 = None
        view_581: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_393, [1, 1, 4096]);  convert_element_type_393 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_67: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_66, view_581);  add_66 = view_581 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_582: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_67, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_69: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_34 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 230, constant_args_idx = 229, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_68, 'X_ptr': view_582, 'W_ptr': arg193_1, 'RSTD_ptr': empty_69}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_68 = view_582 = arg193_1 = empty_69 = None
        getitem_136: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_34['Y_ptr'];  triton_kernel_wrapper_functional_proxy_34 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_394: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_136, torch.float32)
        unsqueeze_346: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_394, 2);  convert_element_type_394 = None
        permute_188: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg194_1, [1, 0]);  arg194_1 = None
        convert_element_type_395: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_188, torch.float32);  permute_188 = None
        unsqueeze_347: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_395, 0);  convert_element_type_395 = None
        mul_224: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_346, unsqueeze_347);  unsqueeze_346 = unsqueeze_347 = None
        sum_121: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_224, [1]);  mul_224 = None
        convert_element_type_396: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_121, torch.float16);  sum_121 = None
        view_587: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_396, [1, 1, 4096]);  convert_element_type_396 = None
        view_588: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_587, [1, 1, -1, 128]);  view_587 = None
        permute_189: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_588, [0, 2, 1, 3]);  view_588 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_352: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_227: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_189, unsqueeze_352)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_396: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_189, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_34: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_396);  slice_396 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_395: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_189, 3, 0, 64);  permute_189 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_34: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_34, slice_395], -1);  neg_34 = slice_395 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_353: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_228: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_34, unsqueeze_353);  cat_34 = None
        add_68: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_227, mul_228);  mul_227 = mul_228 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_397: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_136, torch.float32)
        unsqueeze_348: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_397, 2);  convert_element_type_397 = None
        permute_190: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg195_1, [1, 0]);  arg195_1 = None
        convert_element_type_398: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_190, torch.float32);  permute_190 = None
        unsqueeze_349: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_398, 0);  convert_element_type_398 = None
        mul_225: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_348, unsqueeze_349);  unsqueeze_348 = unsqueeze_349 = None
        sum_122: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_225, [1]);  mul_225 = None
        convert_element_type_399: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_122, torch.float16);  sum_122 = None
        view_592: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_399, [1, 1, 1024]);  convert_element_type_399 = None
        view_593: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_592, [1, 1, -1, 128]);  view_592 = None
        permute_191: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_593, [0, 2, 1, 3]);  view_593 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_229: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_191, unsqueeze_352);  unsqueeze_352 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_398: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_191, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_35: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_398);  slice_398 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_397: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_191, 3, 0, 64);  permute_191 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_35: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_35, slice_397], -1);  neg_35 = slice_397 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_230: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_35, unsqueeze_353);  cat_35 = unsqueeze_353 = None
        add_69: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_229, mul_230);  mul_229 = mul_230 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_34: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg197_1, [None, None, arg2_1], add_69);  add_69 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_355: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_34, 2)
        expand_90: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_355, [1, 8, 4, 2048, 128]);  unsqueeze_355 = None
        clone_36: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_90, memory_format = torch.contiguous_format);  expand_90 = None
        view_599: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_36, [1, 32, 2048, 128]);  clone_36 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_400: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_136, torch.float32);  getitem_136 = None
        unsqueeze_350: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_400, 2);  convert_element_type_400 = None
        permute_192: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg196_1, [1, 0]);  arg196_1 = None
        convert_element_type_401: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_192, torch.float32);  permute_192 = None
        unsqueeze_351: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_401, 0);  convert_element_type_401 = None
        mul_226: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_350, unsqueeze_351);  unsqueeze_350 = unsqueeze_351 = None
        sum_123: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_226, [1]);  mul_226 = None
        convert_element_type_402: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_123, torch.float16);  sum_123 = None
        view_597: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_402, [1, 1, 1024]);  convert_element_type_402 = None
        view_598: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_597, [1, 1, -1, 128]);  view_597 = None
        permute_193: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_598, [0, 2, 1, 3]);  view_598 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_35: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg198_1, [None, None, arg2_1], permute_193);  permute_193 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_357: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_35, 2)
        expand_92: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_357, [1, 8, 4, 2048, 128]);  unsqueeze_357 = None
        clone_37: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_92, memory_format = torch.contiguous_format);  expand_92 = None
        view_600: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_37, [1, 32, 2048, 128]);  clone_37 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_35: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_34: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_17: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_35, full_default_34);  full_default_35 = full_default_34 = None
        expand_93: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_17, [1, 32, 1, 2048]);  where_17 = None
        _scaled_dot_product_efficient_attention_17 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_68, view_599, view_600, expand_93, False, scale = 0.08838834764831845);  add_68 = view_599 = view_600 = expand_93 = None
        getitem_138: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_17[0];  _scaled_dot_product_efficient_attention_17 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_70: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_194: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_138, [0, 2, 1, 3]);  getitem_138 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_601: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_194, [1, 1, -1]);  permute_194 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_602: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_601, [1, 4096]);  view_601 = None
        convert_element_type_403: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_602, torch.float32);  view_602 = None
        unsqueeze_358: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_403, 2);  convert_element_type_403 = None
        permute_195: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg199_1, [1, 0]);  arg199_1 = None
        convert_element_type_404: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_195, torch.float32);  permute_195 = None
        unsqueeze_359: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_404, 0);  convert_element_type_404 = None
        mul_231: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_358, unsqueeze_359);  unsqueeze_358 = unsqueeze_359 = None
        sum_124: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_231, [1]);  mul_231 = None
        convert_element_type_405: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_124, torch.float16);  sum_124 = None
        view_603: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_405, [1, 1, 4096]);  convert_element_type_405 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_70: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_67, view_603);  add_67 = view_603 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_604: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_70, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_71: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_35 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 231, constant_args_idx = 230, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_70, 'X_ptr': view_604, 'W_ptr': arg200_1, 'RSTD_ptr': empty_71}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_70 = view_604 = arg200_1 = empty_71 = None
        getitem_142: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_35['Y_ptr'];  triton_kernel_wrapper_functional_proxy_35 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_72: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_406: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_142, torch.float32)
        unsqueeze_360: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_406, 2);  convert_element_type_406 = None
        permute_196: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg201_1, [1, 0]);  arg201_1 = None
        convert_element_type_407: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_196, torch.float32);  permute_196 = None
        unsqueeze_361: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_407, 0);  convert_element_type_407 = None
        mul_232: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_360, unsqueeze_361);  unsqueeze_360 = unsqueeze_361 = None
        sum_125: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_232, [1]);  mul_232 = None
        convert_element_type_408: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_125, torch.float16);  sum_125 = None
        view_609: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_408, [1, 1, 14336]);  convert_element_type_408 = None
        convert_element_type_409: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_609, torch.float32);  view_609 = None
        sigmoid_17: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_409)
        mul_233: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_409, sigmoid_17);  convert_element_type_409 = sigmoid_17 = None
        convert_element_type_410: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_233, torch.float16);  mul_233 = None
        convert_element_type_411: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_142, torch.float32);  getitem_142 = None
        unsqueeze_362: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_411, 2);  convert_element_type_411 = None
        permute_197: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg202_1, [1, 0]);  arg202_1 = None
        convert_element_type_412: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_197, torch.float32);  permute_197 = None
        unsqueeze_363: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_412, 0);  convert_element_type_412 = None
        mul_234: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_362, unsqueeze_363);  unsqueeze_362 = unsqueeze_363 = None
        sum_126: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_234, [1]);  mul_234 = None
        convert_element_type_413: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_126, torch.float16);  sum_126 = None
        view_613: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_413, [1, 1, 14336]);  convert_element_type_413 = None
        mul_235: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_410, view_613);  convert_element_type_410 = view_613 = None
        view_614: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_235, [1, 14336]);  mul_235 = None
        convert_element_type_414: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_614, torch.float32);  view_614 = None
        unsqueeze_364: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_414, 2);  convert_element_type_414 = None
        permute_198: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg203_1, [1, 0]);  arg203_1 = None
        convert_element_type_415: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_198, torch.float32);  permute_198 = None
        unsqueeze_365: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_415, 0);  convert_element_type_415 = None
        mul_236: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_364, unsqueeze_365);  unsqueeze_364 = unsqueeze_365 = None
        sum_127: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_236, [1]);  mul_236 = None
        convert_element_type_416: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_127, torch.float16);  sum_127 = None
        view_615: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_416, [1, 1, 4096]);  convert_element_type_416 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_71: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_70, view_615);  add_70 = view_615 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_616: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_71, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_73: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_36 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 232, constant_args_idx = 231, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_72, 'X_ptr': view_616, 'W_ptr': arg204_1, 'RSTD_ptr': empty_73}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_72 = view_616 = arg204_1 = empty_73 = None
        getitem_144: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_36['Y_ptr'];  triton_kernel_wrapper_functional_proxy_36 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_417: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_144, torch.float32)
        unsqueeze_366: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_417, 2);  convert_element_type_417 = None
        permute_199: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg205_1, [1, 0]);  arg205_1 = None
        convert_element_type_418: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_199, torch.float32);  permute_199 = None
        unsqueeze_367: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_418, 0);  convert_element_type_418 = None
        mul_237: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_366, unsqueeze_367);  unsqueeze_366 = unsqueeze_367 = None
        sum_128: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_237, [1]);  mul_237 = None
        convert_element_type_419: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_128, torch.float16);  sum_128 = None
        view_621: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_419, [1, 1, 4096]);  convert_element_type_419 = None
        view_622: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_621, [1, 1, -1, 128]);  view_621 = None
        permute_200: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_622, [0, 2, 1, 3]);  view_622 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_372: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_240: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_200, unsqueeze_372)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_419: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_200, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_36: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_419);  slice_419 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_418: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_200, 3, 0, 64);  permute_200 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_36: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_36, slice_418], -1);  neg_36 = slice_418 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_373: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_241: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_36, unsqueeze_373);  cat_36 = None
        add_72: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_240, mul_241);  mul_240 = mul_241 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_420: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_144, torch.float32)
        unsqueeze_368: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_420, 2);  convert_element_type_420 = None
        permute_201: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg206_1, [1, 0]);  arg206_1 = None
        convert_element_type_421: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_201, torch.float32);  permute_201 = None
        unsqueeze_369: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_421, 0);  convert_element_type_421 = None
        mul_238: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_368, unsqueeze_369);  unsqueeze_368 = unsqueeze_369 = None
        sum_129: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_238, [1]);  mul_238 = None
        convert_element_type_422: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_129, torch.float16);  sum_129 = None
        view_626: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_422, [1, 1, 1024]);  convert_element_type_422 = None
        view_627: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_626, [1, 1, -1, 128]);  view_626 = None
        permute_202: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_627, [0, 2, 1, 3]);  view_627 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_242: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_202, unsqueeze_372);  unsqueeze_372 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_421: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_202, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_37: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_421);  slice_421 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_420: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_202, 3, 0, 64);  permute_202 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_37: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_37, slice_420], -1);  neg_37 = slice_420 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_243: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_37, unsqueeze_373);  cat_37 = unsqueeze_373 = None
        add_73: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_242, mul_243);  mul_242 = mul_243 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_36: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg208_1, [None, None, arg2_1], add_73);  add_73 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_375: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_36, 2)
        expand_95: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_375, [1, 8, 4, 2048, 128]);  unsqueeze_375 = None
        clone_38: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_95, memory_format = torch.contiguous_format);  expand_95 = None
        view_633: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_38, [1, 32, 2048, 128]);  clone_38 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_423: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_144, torch.float32);  getitem_144 = None
        unsqueeze_370: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_423, 2);  convert_element_type_423 = None
        permute_203: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg207_1, [1, 0]);  arg207_1 = None
        convert_element_type_424: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_203, torch.float32);  permute_203 = None
        unsqueeze_371: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_424, 0);  convert_element_type_424 = None
        mul_239: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_370, unsqueeze_371);  unsqueeze_370 = unsqueeze_371 = None
        sum_130: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_239, [1]);  mul_239 = None
        convert_element_type_425: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_130, torch.float16);  sum_130 = None
        view_631: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_425, [1, 1, 1024]);  convert_element_type_425 = None
        view_632: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_631, [1, 1, -1, 128]);  view_631 = None
        permute_204: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_632, [0, 2, 1, 3]);  view_632 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_37: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg209_1, [None, None, arg2_1], permute_204);  permute_204 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_377: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_37, 2)
        expand_97: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_377, [1, 8, 4, 2048, 128]);  unsqueeze_377 = None
        clone_39: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_97, memory_format = torch.contiguous_format);  expand_97 = None
        view_634: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_39, [1, 32, 2048, 128]);  clone_39 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_37: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_36: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_18: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_37, full_default_36);  full_default_37 = full_default_36 = None
        expand_98: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_18, [1, 32, 1, 2048]);  where_18 = None
        _scaled_dot_product_efficient_attention_18 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_72, view_633, view_634, expand_98, False, scale = 0.08838834764831845);  add_72 = view_633 = view_634 = expand_98 = None
        getitem_146: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_18[0];  _scaled_dot_product_efficient_attention_18 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_74: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_205: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_146, [0, 2, 1, 3]);  getitem_146 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_635: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_205, [1, 1, -1]);  permute_205 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_636: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_635, [1, 4096]);  view_635 = None
        convert_element_type_426: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_636, torch.float32);  view_636 = None
        unsqueeze_378: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_426, 2);  convert_element_type_426 = None
        permute_206: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg210_1, [1, 0]);  arg210_1 = None
        convert_element_type_427: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_206, torch.float32);  permute_206 = None
        unsqueeze_379: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_427, 0);  convert_element_type_427 = None
        mul_244: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_378, unsqueeze_379);  unsqueeze_378 = unsqueeze_379 = None
        sum_131: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_244, [1]);  mul_244 = None
        convert_element_type_428: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_131, torch.float16);  sum_131 = None
        view_637: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_428, [1, 1, 4096]);  convert_element_type_428 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_74: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_71, view_637);  add_71 = view_637 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_638: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_74, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_75: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_37 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 233, constant_args_idx = 232, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_74, 'X_ptr': view_638, 'W_ptr': arg211_1, 'RSTD_ptr': empty_75}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_74 = view_638 = arg211_1 = empty_75 = None
        getitem_150: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_37['Y_ptr'];  triton_kernel_wrapper_functional_proxy_37 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_76: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_429: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_150, torch.float32)
        unsqueeze_380: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_429, 2);  convert_element_type_429 = None
        permute_207: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg212_1, [1, 0]);  arg212_1 = None
        convert_element_type_430: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_207, torch.float32);  permute_207 = None
        unsqueeze_381: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_430, 0);  convert_element_type_430 = None
        mul_245: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_380, unsqueeze_381);  unsqueeze_380 = unsqueeze_381 = None
        sum_132: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_245, [1]);  mul_245 = None
        convert_element_type_431: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_132, torch.float16);  sum_132 = None
        view_643: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_431, [1, 1, 14336]);  convert_element_type_431 = None
        convert_element_type_432: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_643, torch.float32);  view_643 = None
        sigmoid_18: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_432)
        mul_246: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_432, sigmoid_18);  convert_element_type_432 = sigmoid_18 = None
        convert_element_type_433: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_246, torch.float16);  mul_246 = None
        convert_element_type_434: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_150, torch.float32);  getitem_150 = None
        unsqueeze_382: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_434, 2);  convert_element_type_434 = None
        permute_208: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg213_1, [1, 0]);  arg213_1 = None
        convert_element_type_435: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_208, torch.float32);  permute_208 = None
        unsqueeze_383: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_435, 0);  convert_element_type_435 = None
        mul_247: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_382, unsqueeze_383);  unsqueeze_382 = unsqueeze_383 = None
        sum_133: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_247, [1]);  mul_247 = None
        convert_element_type_436: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_133, torch.float16);  sum_133 = None
        view_647: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_436, [1, 1, 14336]);  convert_element_type_436 = None
        mul_248: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_433, view_647);  convert_element_type_433 = view_647 = None
        view_648: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_248, [1, 14336]);  mul_248 = None
        convert_element_type_437: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_648, torch.float32);  view_648 = None
        unsqueeze_384: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_437, 2);  convert_element_type_437 = None
        permute_209: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg214_1, [1, 0]);  arg214_1 = None
        convert_element_type_438: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_209, torch.float32);  permute_209 = None
        unsqueeze_385: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_438, 0);  convert_element_type_438 = None
        mul_249: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_384, unsqueeze_385);  unsqueeze_384 = unsqueeze_385 = None
        sum_134: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_249, [1]);  mul_249 = None
        convert_element_type_439: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_134, torch.float16);  sum_134 = None
        view_649: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_439, [1, 1, 4096]);  convert_element_type_439 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_75: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_74, view_649);  add_74 = view_649 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_650: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_75, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_77: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_38 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 234, constant_args_idx = 233, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_76, 'X_ptr': view_650, 'W_ptr': arg215_1, 'RSTD_ptr': empty_77}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_76 = view_650 = arg215_1 = empty_77 = None
        getitem_152: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_38['Y_ptr'];  triton_kernel_wrapper_functional_proxy_38 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_440: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_152, torch.float32)
        unsqueeze_386: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_440, 2);  convert_element_type_440 = None
        permute_210: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg216_1, [1, 0]);  arg216_1 = None
        convert_element_type_441: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_210, torch.float32);  permute_210 = None
        unsqueeze_387: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_441, 0);  convert_element_type_441 = None
        mul_250: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_386, unsqueeze_387);  unsqueeze_386 = unsqueeze_387 = None
        sum_135: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_250, [1]);  mul_250 = None
        convert_element_type_442: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_135, torch.float16);  sum_135 = None
        view_655: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_442, [1, 1, 4096]);  convert_element_type_442 = None
        view_656: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_655, [1, 1, -1, 128]);  view_655 = None
        permute_211: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_656, [0, 2, 1, 3]);  view_656 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_392: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_253: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_211, unsqueeze_392)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_442: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_211, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_38: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_442);  slice_442 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_441: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_211, 3, 0, 64);  permute_211 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_38: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_38, slice_441], -1);  neg_38 = slice_441 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_393: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_254: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_38, unsqueeze_393);  cat_38 = None
        add_76: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_253, mul_254);  mul_253 = mul_254 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_443: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_152, torch.float32)
        unsqueeze_388: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_443, 2);  convert_element_type_443 = None
        permute_212: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg217_1, [1, 0]);  arg217_1 = None
        convert_element_type_444: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_212, torch.float32);  permute_212 = None
        unsqueeze_389: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_444, 0);  convert_element_type_444 = None
        mul_251: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_388, unsqueeze_389);  unsqueeze_388 = unsqueeze_389 = None
        sum_136: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_251, [1]);  mul_251 = None
        convert_element_type_445: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_136, torch.float16);  sum_136 = None
        view_660: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_445, [1, 1, 1024]);  convert_element_type_445 = None
        view_661: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_660, [1, 1, -1, 128]);  view_660 = None
        permute_213: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_661, [0, 2, 1, 3]);  view_661 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_255: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_213, unsqueeze_392);  unsqueeze_392 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_444: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_213, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_39: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_444);  slice_444 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_443: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_213, 3, 0, 64);  permute_213 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_39: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_39, slice_443], -1);  neg_39 = slice_443 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_256: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_39, unsqueeze_393);  cat_39 = unsqueeze_393 = None
        add_77: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_255, mul_256);  mul_255 = mul_256 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_38: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg219_1, [None, None, arg2_1], add_77);  add_77 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_395: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_38, 2)
        expand_100: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_395, [1, 8, 4, 2048, 128]);  unsqueeze_395 = None
        clone_40: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_100, memory_format = torch.contiguous_format);  expand_100 = None
        view_667: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_40, [1, 32, 2048, 128]);  clone_40 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_446: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_152, torch.float32);  getitem_152 = None
        unsqueeze_390: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_446, 2);  convert_element_type_446 = None
        permute_214: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg218_1, [1, 0]);  arg218_1 = None
        convert_element_type_447: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_214, torch.float32);  permute_214 = None
        unsqueeze_391: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_447, 0);  convert_element_type_447 = None
        mul_252: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_390, unsqueeze_391);  unsqueeze_390 = unsqueeze_391 = None
        sum_137: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_252, [1]);  mul_252 = None
        convert_element_type_448: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_137, torch.float16);  sum_137 = None
        view_665: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_448, [1, 1, 1024]);  convert_element_type_448 = None
        view_666: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_665, [1, 1, -1, 128]);  view_665 = None
        permute_215: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_666, [0, 2, 1, 3]);  view_666 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_39: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg220_1, [None, None, arg2_1], permute_215);  permute_215 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_397: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_39, 2)
        expand_102: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_397, [1, 8, 4, 2048, 128]);  unsqueeze_397 = None
        clone_41: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_102, memory_format = torch.contiguous_format);  expand_102 = None
        view_668: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_41, [1, 32, 2048, 128]);  clone_41 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_39: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_38: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_19: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_39, full_default_38);  full_default_39 = full_default_38 = None
        expand_103: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_19, [1, 32, 1, 2048]);  where_19 = None
        _scaled_dot_product_efficient_attention_19 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_76, view_667, view_668, expand_103, False, scale = 0.08838834764831845);  add_76 = view_667 = view_668 = expand_103 = None
        getitem_154: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_19[0];  _scaled_dot_product_efficient_attention_19 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_78: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_216: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_154, [0, 2, 1, 3]);  getitem_154 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_669: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_216, [1, 1, -1]);  permute_216 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_670: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_669, [1, 4096]);  view_669 = None
        convert_element_type_449: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_670, torch.float32);  view_670 = None
        unsqueeze_398: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_449, 2);  convert_element_type_449 = None
        permute_217: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg221_1, [1, 0]);  arg221_1 = None
        convert_element_type_450: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_217, torch.float32);  permute_217 = None
        unsqueeze_399: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_450, 0);  convert_element_type_450 = None
        mul_257: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_398, unsqueeze_399);  unsqueeze_398 = unsqueeze_399 = None
        sum_138: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_257, [1]);  mul_257 = None
        convert_element_type_451: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_138, torch.float16);  sum_138 = None
        view_671: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_451, [1, 1, 4096]);  convert_element_type_451 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_78: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_75, view_671);  add_75 = view_671 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_672: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_78, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_79: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_39 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 235, constant_args_idx = 234, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_78, 'X_ptr': view_672, 'W_ptr': arg222_1, 'RSTD_ptr': empty_79}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_78 = view_672 = arg222_1 = empty_79 = None
        getitem_158: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_39['Y_ptr'];  triton_kernel_wrapper_functional_proxy_39 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_80: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_452: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_158, torch.float32)
        unsqueeze_400: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_452, 2);  convert_element_type_452 = None
        permute_218: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg223_1, [1, 0]);  arg223_1 = None
        convert_element_type_453: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_218, torch.float32);  permute_218 = None
        unsqueeze_401: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_453, 0);  convert_element_type_453 = None
        mul_258: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_400, unsqueeze_401);  unsqueeze_400 = unsqueeze_401 = None
        sum_139: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_258, [1]);  mul_258 = None
        convert_element_type_454: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_139, torch.float16);  sum_139 = None
        view_677: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_454, [1, 1, 14336]);  convert_element_type_454 = None
        convert_element_type_455: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_677, torch.float32);  view_677 = None
        sigmoid_19: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_455)
        mul_259: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_455, sigmoid_19);  convert_element_type_455 = sigmoid_19 = None
        convert_element_type_456: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_259, torch.float16);  mul_259 = None
        convert_element_type_457: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_158, torch.float32);  getitem_158 = None
        unsqueeze_402: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_457, 2);  convert_element_type_457 = None
        permute_219: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg224_1, [1, 0]);  arg224_1 = None
        convert_element_type_458: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_219, torch.float32);  permute_219 = None
        unsqueeze_403: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_458, 0);  convert_element_type_458 = None
        mul_260: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_402, unsqueeze_403);  unsqueeze_402 = unsqueeze_403 = None
        sum_140: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_260, [1]);  mul_260 = None
        convert_element_type_459: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_140, torch.float16);  sum_140 = None
        view_681: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_459, [1, 1, 14336]);  convert_element_type_459 = None
        mul_261: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_456, view_681);  convert_element_type_456 = view_681 = None
        view_682: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_261, [1, 14336]);  mul_261 = None
        convert_element_type_460: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_682, torch.float32);  view_682 = None
        unsqueeze_404: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_460, 2);  convert_element_type_460 = None
        permute_220: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg225_1, [1, 0]);  arg225_1 = None
        convert_element_type_461: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_220, torch.float32);  permute_220 = None
        unsqueeze_405: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_461, 0);  convert_element_type_461 = None
        mul_262: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_404, unsqueeze_405);  unsqueeze_404 = unsqueeze_405 = None
        sum_141: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_262, [1]);  mul_262 = None
        convert_element_type_462: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_141, torch.float16);  sum_141 = None
        view_683: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_462, [1, 1, 4096]);  convert_element_type_462 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_79: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_78, view_683);  add_78 = view_683 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_684: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_79, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_81: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_40 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 236, constant_args_idx = 235, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_80, 'X_ptr': view_684, 'W_ptr': arg226_1, 'RSTD_ptr': empty_81}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_80 = view_684 = arg226_1 = empty_81 = None
        getitem_160: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_40['Y_ptr'];  triton_kernel_wrapper_functional_proxy_40 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_463: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_160, torch.float32)
        unsqueeze_406: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_463, 2);  convert_element_type_463 = None
        permute_221: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg227_1, [1, 0]);  arg227_1 = None
        convert_element_type_464: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_221, torch.float32);  permute_221 = None
        unsqueeze_407: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_464, 0);  convert_element_type_464 = None
        mul_263: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_406, unsqueeze_407);  unsqueeze_406 = unsqueeze_407 = None
        sum_142: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_263, [1]);  mul_263 = None
        convert_element_type_465: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_142, torch.float16);  sum_142 = None
        view_689: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_465, [1, 1, 4096]);  convert_element_type_465 = None
        view_690: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_689, [1, 1, -1, 128]);  view_689 = None
        permute_222: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_690, [0, 2, 1, 3]);  view_690 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_412: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_266: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_222, unsqueeze_412)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_465: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_222, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_40: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_465);  slice_465 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_464: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_222, 3, 0, 64);  permute_222 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_40: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_40, slice_464], -1);  neg_40 = slice_464 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_413: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_267: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_40, unsqueeze_413);  cat_40 = None
        add_80: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_266, mul_267);  mul_266 = mul_267 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_466: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_160, torch.float32)
        unsqueeze_408: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_466, 2);  convert_element_type_466 = None
        permute_223: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg228_1, [1, 0]);  arg228_1 = None
        convert_element_type_467: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_223, torch.float32);  permute_223 = None
        unsqueeze_409: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_467, 0);  convert_element_type_467 = None
        mul_264: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_408, unsqueeze_409);  unsqueeze_408 = unsqueeze_409 = None
        sum_143: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_264, [1]);  mul_264 = None
        convert_element_type_468: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_143, torch.float16);  sum_143 = None
        view_694: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_468, [1, 1, 1024]);  convert_element_type_468 = None
        view_695: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_694, [1, 1, -1, 128]);  view_694 = None
        permute_224: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_695, [0, 2, 1, 3]);  view_695 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_268: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_224, unsqueeze_412);  unsqueeze_412 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_467: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_224, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_41: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_467);  slice_467 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_466: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_224, 3, 0, 64);  permute_224 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_41: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_41, slice_466], -1);  neg_41 = slice_466 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_269: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_41, unsqueeze_413);  cat_41 = unsqueeze_413 = None
        add_81: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_268, mul_269);  mul_268 = mul_269 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_40: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg230_1, [None, None, arg2_1], add_81);  add_81 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_415: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_40, 2)
        expand_105: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_415, [1, 8, 4, 2048, 128]);  unsqueeze_415 = None
        clone_42: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_105, memory_format = torch.contiguous_format);  expand_105 = None
        view_701: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_42, [1, 32, 2048, 128]);  clone_42 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_469: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_160, torch.float32);  getitem_160 = None
        unsqueeze_410: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_469, 2);  convert_element_type_469 = None
        permute_225: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg229_1, [1, 0]);  arg229_1 = None
        convert_element_type_470: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_225, torch.float32);  permute_225 = None
        unsqueeze_411: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_470, 0);  convert_element_type_470 = None
        mul_265: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_410, unsqueeze_411);  unsqueeze_410 = unsqueeze_411 = None
        sum_144: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_265, [1]);  mul_265 = None
        convert_element_type_471: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_144, torch.float16);  sum_144 = None
        view_699: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_471, [1, 1, 1024]);  convert_element_type_471 = None
        view_700: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_699, [1, 1, -1, 128]);  view_699 = None
        permute_226: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_700, [0, 2, 1, 3]);  view_700 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_41: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg231_1, [None, None, arg2_1], permute_226);  permute_226 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_417: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_41, 2)
        expand_107: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_417, [1, 8, 4, 2048, 128]);  unsqueeze_417 = None
        clone_43: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_107, memory_format = torch.contiguous_format);  expand_107 = None
        view_702: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_43, [1, 32, 2048, 128]);  clone_43 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_41: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_40: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_20: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_41, full_default_40);  full_default_41 = full_default_40 = None
        expand_108: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_20, [1, 32, 1, 2048]);  where_20 = None
        _scaled_dot_product_efficient_attention_20 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_80, view_701, view_702, expand_108, False, scale = 0.08838834764831845);  add_80 = view_701 = view_702 = expand_108 = None
        getitem_162: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_20[0];  _scaled_dot_product_efficient_attention_20 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_82: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_227: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_162, [0, 2, 1, 3]);  getitem_162 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_703: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_227, [1, 1, -1]);  permute_227 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_704: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_703, [1, 4096]);  view_703 = None
        convert_element_type_472: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_704, torch.float32);  view_704 = None
        unsqueeze_418: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_472, 2);  convert_element_type_472 = None
        permute_228: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg232_1, [1, 0]);  arg232_1 = None
        convert_element_type_473: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_228, torch.float32);  permute_228 = None
        unsqueeze_419: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_473, 0);  convert_element_type_473 = None
        mul_270: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_418, unsqueeze_419);  unsqueeze_418 = unsqueeze_419 = None
        sum_145: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_270, [1]);  mul_270 = None
        convert_element_type_474: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_145, torch.float16);  sum_145 = None
        view_705: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_474, [1, 1, 4096]);  convert_element_type_474 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_82: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_79, view_705);  add_79 = view_705 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_706: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_82, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_83: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_41 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 237, constant_args_idx = 236, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_82, 'X_ptr': view_706, 'W_ptr': arg233_1, 'RSTD_ptr': empty_83}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_82 = view_706 = arg233_1 = empty_83 = None
        getitem_166: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_41['Y_ptr'];  triton_kernel_wrapper_functional_proxy_41 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_84: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_475: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_166, torch.float32)
        unsqueeze_420: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_475, 2);  convert_element_type_475 = None
        permute_229: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg234_1, [1, 0]);  arg234_1 = None
        convert_element_type_476: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_229, torch.float32);  permute_229 = None
        unsqueeze_421: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_476, 0);  convert_element_type_476 = None
        mul_271: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_420, unsqueeze_421);  unsqueeze_420 = unsqueeze_421 = None
        sum_146: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_271, [1]);  mul_271 = None
        convert_element_type_477: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_146, torch.float16);  sum_146 = None
        view_711: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_477, [1, 1, 14336]);  convert_element_type_477 = None
        convert_element_type_478: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_711, torch.float32);  view_711 = None
        sigmoid_20: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_478)
        mul_272: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_478, sigmoid_20);  convert_element_type_478 = sigmoid_20 = None
        convert_element_type_479: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_272, torch.float16);  mul_272 = None
        convert_element_type_480: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_166, torch.float32);  getitem_166 = None
        unsqueeze_422: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_480, 2);  convert_element_type_480 = None
        permute_230: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg235_1, [1, 0]);  arg235_1 = None
        convert_element_type_481: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_230, torch.float32);  permute_230 = None
        unsqueeze_423: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_481, 0);  convert_element_type_481 = None
        mul_273: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_422, unsqueeze_423);  unsqueeze_422 = unsqueeze_423 = None
        sum_147: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_273, [1]);  mul_273 = None
        convert_element_type_482: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_147, torch.float16);  sum_147 = None
        view_715: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_482, [1, 1, 14336]);  convert_element_type_482 = None
        mul_274: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_479, view_715);  convert_element_type_479 = view_715 = None
        view_716: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_274, [1, 14336]);  mul_274 = None
        convert_element_type_483: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_716, torch.float32);  view_716 = None
        unsqueeze_424: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_483, 2);  convert_element_type_483 = None
        permute_231: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg236_1, [1, 0]);  arg236_1 = None
        convert_element_type_484: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_231, torch.float32);  permute_231 = None
        unsqueeze_425: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_484, 0);  convert_element_type_484 = None
        mul_275: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_424, unsqueeze_425);  unsqueeze_424 = unsqueeze_425 = None
        sum_148: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_275, [1]);  mul_275 = None
        convert_element_type_485: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_148, torch.float16);  sum_148 = None
        view_717: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_485, [1, 1, 4096]);  convert_element_type_485 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_83: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_82, view_717);  add_82 = view_717 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_718: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_83, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_85: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_42 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 238, constant_args_idx = 237, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_84, 'X_ptr': view_718, 'W_ptr': arg237_1, 'RSTD_ptr': empty_85}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_84 = view_718 = arg237_1 = empty_85 = None
        getitem_168: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_42['Y_ptr'];  triton_kernel_wrapper_functional_proxy_42 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_486: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_168, torch.float32)
        unsqueeze_426: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_486, 2);  convert_element_type_486 = None
        permute_232: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg238_1, [1, 0]);  arg238_1 = None
        convert_element_type_487: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_232, torch.float32);  permute_232 = None
        unsqueeze_427: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_487, 0);  convert_element_type_487 = None
        mul_276: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_426, unsqueeze_427);  unsqueeze_426 = unsqueeze_427 = None
        sum_149: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_276, [1]);  mul_276 = None
        convert_element_type_488: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_149, torch.float16);  sum_149 = None
        view_723: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_488, [1, 1, 4096]);  convert_element_type_488 = None
        view_724: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_723, [1, 1, -1, 128]);  view_723 = None
        permute_233: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_724, [0, 2, 1, 3]);  view_724 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_432: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_279: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_233, unsqueeze_432)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_488: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_233, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_42: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_488);  slice_488 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_487: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_233, 3, 0, 64);  permute_233 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_42: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_42, slice_487], -1);  neg_42 = slice_487 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_433: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_280: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_42, unsqueeze_433);  cat_42 = None
        add_84: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_279, mul_280);  mul_279 = mul_280 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_489: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_168, torch.float32)
        unsqueeze_428: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_489, 2);  convert_element_type_489 = None
        permute_234: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg239_1, [1, 0]);  arg239_1 = None
        convert_element_type_490: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_234, torch.float32);  permute_234 = None
        unsqueeze_429: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_490, 0);  convert_element_type_490 = None
        mul_277: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_428, unsqueeze_429);  unsqueeze_428 = unsqueeze_429 = None
        sum_150: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_277, [1]);  mul_277 = None
        convert_element_type_491: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_150, torch.float16);  sum_150 = None
        view_728: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_491, [1, 1, 1024]);  convert_element_type_491 = None
        view_729: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_728, [1, 1, -1, 128]);  view_728 = None
        permute_235: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_729, [0, 2, 1, 3]);  view_729 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_281: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_235, unsqueeze_432);  unsqueeze_432 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_490: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_235, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_43: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_490);  slice_490 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_489: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_235, 3, 0, 64);  permute_235 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_43: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_43, slice_489], -1);  neg_43 = slice_489 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_282: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_43, unsqueeze_433);  cat_43 = unsqueeze_433 = None
        add_85: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_281, mul_282);  mul_281 = mul_282 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_42: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg241_1, [None, None, arg2_1], add_85);  add_85 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_435: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_42, 2)
        expand_110: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_435, [1, 8, 4, 2048, 128]);  unsqueeze_435 = None
        clone_44: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_110, memory_format = torch.contiguous_format);  expand_110 = None
        view_735: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_44, [1, 32, 2048, 128]);  clone_44 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_492: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_168, torch.float32);  getitem_168 = None
        unsqueeze_430: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_492, 2);  convert_element_type_492 = None
        permute_236: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg240_1, [1, 0]);  arg240_1 = None
        convert_element_type_493: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_236, torch.float32);  permute_236 = None
        unsqueeze_431: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_493, 0);  convert_element_type_493 = None
        mul_278: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_430, unsqueeze_431);  unsqueeze_430 = unsqueeze_431 = None
        sum_151: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_278, [1]);  mul_278 = None
        convert_element_type_494: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_151, torch.float16);  sum_151 = None
        view_733: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_494, [1, 1, 1024]);  convert_element_type_494 = None
        view_734: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_733, [1, 1, -1, 128]);  view_733 = None
        permute_237: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_734, [0, 2, 1, 3]);  view_734 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_43: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg242_1, [None, None, arg2_1], permute_237);  permute_237 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_437: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_43, 2)
        expand_112: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_437, [1, 8, 4, 2048, 128]);  unsqueeze_437 = None
        clone_45: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_112, memory_format = torch.contiguous_format);  expand_112 = None
        view_736: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_45, [1, 32, 2048, 128]);  clone_45 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_43: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_42: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_21: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_43, full_default_42);  full_default_43 = full_default_42 = None
        expand_113: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_21, [1, 32, 1, 2048]);  where_21 = None
        _scaled_dot_product_efficient_attention_21 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_84, view_735, view_736, expand_113, False, scale = 0.08838834764831845);  add_84 = view_735 = view_736 = expand_113 = None
        getitem_170: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_21[0];  _scaled_dot_product_efficient_attention_21 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_86: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_238: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_170, [0, 2, 1, 3]);  getitem_170 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_737: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_238, [1, 1, -1]);  permute_238 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_738: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_737, [1, 4096]);  view_737 = None
        convert_element_type_495: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_738, torch.float32);  view_738 = None
        unsqueeze_438: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_495, 2);  convert_element_type_495 = None
        permute_239: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg243_1, [1, 0]);  arg243_1 = None
        convert_element_type_496: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_239, torch.float32);  permute_239 = None
        unsqueeze_439: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_496, 0);  convert_element_type_496 = None
        mul_283: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_438, unsqueeze_439);  unsqueeze_438 = unsqueeze_439 = None
        sum_152: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_283, [1]);  mul_283 = None
        convert_element_type_497: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_152, torch.float16);  sum_152 = None
        view_739: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_497, [1, 1, 4096]);  convert_element_type_497 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_86: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_83, view_739);  add_83 = view_739 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_740: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_86, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_87: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_43 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 239, constant_args_idx = 238, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_86, 'X_ptr': view_740, 'W_ptr': arg244_1, 'RSTD_ptr': empty_87}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_86 = view_740 = arg244_1 = empty_87 = None
        getitem_174: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_43['Y_ptr'];  triton_kernel_wrapper_functional_proxy_43 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_88: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_498: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_174, torch.float32)
        unsqueeze_440: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_498, 2);  convert_element_type_498 = None
        permute_240: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg245_1, [1, 0]);  arg245_1 = None
        convert_element_type_499: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_240, torch.float32);  permute_240 = None
        unsqueeze_441: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_499, 0);  convert_element_type_499 = None
        mul_284: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_440, unsqueeze_441);  unsqueeze_440 = unsqueeze_441 = None
        sum_153: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_284, [1]);  mul_284 = None
        convert_element_type_500: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_153, torch.float16);  sum_153 = None
        view_745: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_500, [1, 1, 14336]);  convert_element_type_500 = None
        convert_element_type_501: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_745, torch.float32);  view_745 = None
        sigmoid_21: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_501)
        mul_285: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_501, sigmoid_21);  convert_element_type_501 = sigmoid_21 = None
        convert_element_type_502: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_285, torch.float16);  mul_285 = None
        convert_element_type_503: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_174, torch.float32);  getitem_174 = None
        unsqueeze_442: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_503, 2);  convert_element_type_503 = None
        permute_241: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg246_1, [1, 0]);  arg246_1 = None
        convert_element_type_504: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_241, torch.float32);  permute_241 = None
        unsqueeze_443: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_504, 0);  convert_element_type_504 = None
        mul_286: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_442, unsqueeze_443);  unsqueeze_442 = unsqueeze_443 = None
        sum_154: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_286, [1]);  mul_286 = None
        convert_element_type_505: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_154, torch.float16);  sum_154 = None
        view_749: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_505, [1, 1, 14336]);  convert_element_type_505 = None
        mul_287: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_502, view_749);  convert_element_type_502 = view_749 = None
        view_750: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_287, [1, 14336]);  mul_287 = None
        convert_element_type_506: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_750, torch.float32);  view_750 = None
        unsqueeze_444: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_506, 2);  convert_element_type_506 = None
        permute_242: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg247_1, [1, 0]);  arg247_1 = None
        convert_element_type_507: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_242, torch.float32);  permute_242 = None
        unsqueeze_445: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_507, 0);  convert_element_type_507 = None
        mul_288: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_444, unsqueeze_445);  unsqueeze_444 = unsqueeze_445 = None
        sum_155: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_288, [1]);  mul_288 = None
        convert_element_type_508: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_155, torch.float16);  sum_155 = None
        view_751: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_508, [1, 1, 4096]);  convert_element_type_508 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_87: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_86, view_751);  add_86 = view_751 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_752: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_87, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_89: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_44 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 240, constant_args_idx = 239, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_88, 'X_ptr': view_752, 'W_ptr': arg248_1, 'RSTD_ptr': empty_89}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_88 = view_752 = arg248_1 = empty_89 = None
        getitem_176: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_44['Y_ptr'];  triton_kernel_wrapper_functional_proxy_44 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_509: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_176, torch.float32)
        unsqueeze_446: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_509, 2);  convert_element_type_509 = None
        permute_243: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg249_1, [1, 0]);  arg249_1 = None
        convert_element_type_510: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_243, torch.float32);  permute_243 = None
        unsqueeze_447: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_510, 0);  convert_element_type_510 = None
        mul_289: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_446, unsqueeze_447);  unsqueeze_446 = unsqueeze_447 = None
        sum_156: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_289, [1]);  mul_289 = None
        convert_element_type_511: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_156, torch.float16);  sum_156 = None
        view_757: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_511, [1, 1, 4096]);  convert_element_type_511 = None
        view_758: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_757, [1, 1, -1, 128]);  view_757 = None
        permute_244: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_758, [0, 2, 1, 3]);  view_758 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_452: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_292: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_244, unsqueeze_452)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_511: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_244, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_44: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_511);  slice_511 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_510: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_244, 3, 0, 64);  permute_244 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_44: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_44, slice_510], -1);  neg_44 = slice_510 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_453: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_293: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_44, unsqueeze_453);  cat_44 = None
        add_88: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_292, mul_293);  mul_292 = mul_293 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_512: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_176, torch.float32)
        unsqueeze_448: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_512, 2);  convert_element_type_512 = None
        permute_245: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg250_1, [1, 0]);  arg250_1 = None
        convert_element_type_513: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_245, torch.float32);  permute_245 = None
        unsqueeze_449: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_513, 0);  convert_element_type_513 = None
        mul_290: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_448, unsqueeze_449);  unsqueeze_448 = unsqueeze_449 = None
        sum_157: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_290, [1]);  mul_290 = None
        convert_element_type_514: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_157, torch.float16);  sum_157 = None
        view_762: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_514, [1, 1, 1024]);  convert_element_type_514 = None
        view_763: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_762, [1, 1, -1, 128]);  view_762 = None
        permute_246: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_763, [0, 2, 1, 3]);  view_763 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_294: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_246, unsqueeze_452);  unsqueeze_452 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_513: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_246, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_45: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_513);  slice_513 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_512: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_246, 3, 0, 64);  permute_246 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_45: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_45, slice_512], -1);  neg_45 = slice_512 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_295: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_45, unsqueeze_453);  cat_45 = unsqueeze_453 = None
        add_89: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_294, mul_295);  mul_294 = mul_295 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_44: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg252_1, [None, None, arg2_1], add_89);  add_89 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_455: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_44, 2)
        expand_115: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_455, [1, 8, 4, 2048, 128]);  unsqueeze_455 = None
        clone_46: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_115, memory_format = torch.contiguous_format);  expand_115 = None
        view_769: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_46, [1, 32, 2048, 128]);  clone_46 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_515: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_176, torch.float32);  getitem_176 = None
        unsqueeze_450: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_515, 2);  convert_element_type_515 = None
        permute_247: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg251_1, [1, 0]);  arg251_1 = None
        convert_element_type_516: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_247, torch.float32);  permute_247 = None
        unsqueeze_451: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_516, 0);  convert_element_type_516 = None
        mul_291: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_450, unsqueeze_451);  unsqueeze_450 = unsqueeze_451 = None
        sum_158: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_291, [1]);  mul_291 = None
        convert_element_type_517: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_158, torch.float16);  sum_158 = None
        view_767: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_517, [1, 1, 1024]);  convert_element_type_517 = None
        view_768: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_767, [1, 1, -1, 128]);  view_767 = None
        permute_248: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_768, [0, 2, 1, 3]);  view_768 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_45: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg253_1, [None, None, arg2_1], permute_248);  permute_248 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_457: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_45, 2)
        expand_117: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_457, [1, 8, 4, 2048, 128]);  unsqueeze_457 = None
        clone_47: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_117, memory_format = torch.contiguous_format);  expand_117 = None
        view_770: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_47, [1, 32, 2048, 128]);  clone_47 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_45: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_44: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_22: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_45, full_default_44);  full_default_45 = full_default_44 = None
        expand_118: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_22, [1, 32, 1, 2048]);  where_22 = None
        _scaled_dot_product_efficient_attention_22 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_88, view_769, view_770, expand_118, False, scale = 0.08838834764831845);  add_88 = view_769 = view_770 = expand_118 = None
        getitem_178: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_22[0];  _scaled_dot_product_efficient_attention_22 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_90: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_249: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_178, [0, 2, 1, 3]);  getitem_178 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_771: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_249, [1, 1, -1]);  permute_249 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_772: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_771, [1, 4096]);  view_771 = None
        convert_element_type_518: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_772, torch.float32);  view_772 = None
        unsqueeze_458: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_518, 2);  convert_element_type_518 = None
        permute_250: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg254_1, [1, 0]);  arg254_1 = None
        convert_element_type_519: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_250, torch.float32);  permute_250 = None
        unsqueeze_459: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_519, 0);  convert_element_type_519 = None
        mul_296: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_458, unsqueeze_459);  unsqueeze_458 = unsqueeze_459 = None
        sum_159: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_296, [1]);  mul_296 = None
        convert_element_type_520: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_159, torch.float16);  sum_159 = None
        view_773: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_520, [1, 1, 4096]);  convert_element_type_520 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_90: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_87, view_773);  add_87 = view_773 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_774: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_90, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_91: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_45 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 241, constant_args_idx = 240, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_90, 'X_ptr': view_774, 'W_ptr': arg255_1, 'RSTD_ptr': empty_91}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_90 = view_774 = arg255_1 = empty_91 = None
        getitem_182: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_45['Y_ptr'];  triton_kernel_wrapper_functional_proxy_45 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_92: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_521: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_182, torch.float32)
        unsqueeze_460: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_521, 2);  convert_element_type_521 = None
        permute_251: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg256_1, [1, 0]);  arg256_1 = None
        convert_element_type_522: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_251, torch.float32);  permute_251 = None
        unsqueeze_461: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_522, 0);  convert_element_type_522 = None
        mul_297: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_460, unsqueeze_461);  unsqueeze_460 = unsqueeze_461 = None
        sum_160: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_297, [1]);  mul_297 = None
        convert_element_type_523: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_160, torch.float16);  sum_160 = None
        view_779: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_523, [1, 1, 14336]);  convert_element_type_523 = None
        convert_element_type_524: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_779, torch.float32);  view_779 = None
        sigmoid_22: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_524)
        mul_298: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_524, sigmoid_22);  convert_element_type_524 = sigmoid_22 = None
        convert_element_type_525: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_298, torch.float16);  mul_298 = None
        convert_element_type_526: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_182, torch.float32);  getitem_182 = None
        unsqueeze_462: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_526, 2);  convert_element_type_526 = None
        permute_252: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg257_1, [1, 0]);  arg257_1 = None
        convert_element_type_527: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_252, torch.float32);  permute_252 = None
        unsqueeze_463: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_527, 0);  convert_element_type_527 = None
        mul_299: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_462, unsqueeze_463);  unsqueeze_462 = unsqueeze_463 = None
        sum_161: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_299, [1]);  mul_299 = None
        convert_element_type_528: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_161, torch.float16);  sum_161 = None
        view_783: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_528, [1, 1, 14336]);  convert_element_type_528 = None
        mul_300: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_525, view_783);  convert_element_type_525 = view_783 = None
        view_784: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_300, [1, 14336]);  mul_300 = None
        convert_element_type_529: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_784, torch.float32);  view_784 = None
        unsqueeze_464: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_529, 2);  convert_element_type_529 = None
        permute_253: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg258_1, [1, 0]);  arg258_1 = None
        convert_element_type_530: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_253, torch.float32);  permute_253 = None
        unsqueeze_465: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_530, 0);  convert_element_type_530 = None
        mul_301: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_464, unsqueeze_465);  unsqueeze_464 = unsqueeze_465 = None
        sum_162: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_301, [1]);  mul_301 = None
        convert_element_type_531: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_162, torch.float16);  sum_162 = None
        view_785: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_531, [1, 1, 4096]);  convert_element_type_531 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_91: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_90, view_785);  add_90 = view_785 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_786: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_91, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_93: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_46 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 242, constant_args_idx = 241, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_92, 'X_ptr': view_786, 'W_ptr': arg259_1, 'RSTD_ptr': empty_93}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_92 = view_786 = arg259_1 = empty_93 = None
        getitem_184: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_46['Y_ptr'];  triton_kernel_wrapper_functional_proxy_46 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_532: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_184, torch.float32)
        unsqueeze_466: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_532, 2);  convert_element_type_532 = None
        permute_254: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg260_1, [1, 0]);  arg260_1 = None
        convert_element_type_533: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_254, torch.float32);  permute_254 = None
        unsqueeze_467: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_533, 0);  convert_element_type_533 = None
        mul_302: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_466, unsqueeze_467);  unsqueeze_466 = unsqueeze_467 = None
        sum_163: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_302, [1]);  mul_302 = None
        convert_element_type_534: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_163, torch.float16);  sum_163 = None
        view_791: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_534, [1, 1, 4096]);  convert_element_type_534 = None
        view_792: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_791, [1, 1, -1, 128]);  view_791 = None
        permute_255: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_792, [0, 2, 1, 3]);  view_792 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_472: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_305: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_255, unsqueeze_472)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_534: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_255, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_46: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_534);  slice_534 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_533: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_255, 3, 0, 64);  permute_255 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_46: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_46, slice_533], -1);  neg_46 = slice_533 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_473: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_306: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_46, unsqueeze_473);  cat_46 = None
        add_92: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_305, mul_306);  mul_305 = mul_306 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_535: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_184, torch.float32)
        unsqueeze_468: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_535, 2);  convert_element_type_535 = None
        permute_256: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg261_1, [1, 0]);  arg261_1 = None
        convert_element_type_536: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_256, torch.float32);  permute_256 = None
        unsqueeze_469: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_536, 0);  convert_element_type_536 = None
        mul_303: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_468, unsqueeze_469);  unsqueeze_468 = unsqueeze_469 = None
        sum_164: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_303, [1]);  mul_303 = None
        convert_element_type_537: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_164, torch.float16);  sum_164 = None
        view_796: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_537, [1, 1, 1024]);  convert_element_type_537 = None
        view_797: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_796, [1, 1, -1, 128]);  view_796 = None
        permute_257: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_797, [0, 2, 1, 3]);  view_797 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_307: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_257, unsqueeze_472);  unsqueeze_472 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_536: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_257, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_47: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_536);  slice_536 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_535: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_257, 3, 0, 64);  permute_257 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_47: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_47, slice_535], -1);  neg_47 = slice_535 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_308: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_47, unsqueeze_473);  cat_47 = unsqueeze_473 = None
        add_93: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_307, mul_308);  mul_307 = mul_308 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_46: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg263_1, [None, None, arg2_1], add_93);  add_93 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_475: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_46, 2)
        expand_120: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_475, [1, 8, 4, 2048, 128]);  unsqueeze_475 = None
        clone_48: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_120, memory_format = torch.contiguous_format);  expand_120 = None
        view_803: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_48, [1, 32, 2048, 128]);  clone_48 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_538: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_184, torch.float32);  getitem_184 = None
        unsqueeze_470: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_538, 2);  convert_element_type_538 = None
        permute_258: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg262_1, [1, 0]);  arg262_1 = None
        convert_element_type_539: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_258, torch.float32);  permute_258 = None
        unsqueeze_471: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_539, 0);  convert_element_type_539 = None
        mul_304: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_470, unsqueeze_471);  unsqueeze_470 = unsqueeze_471 = None
        sum_165: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_304, [1]);  mul_304 = None
        convert_element_type_540: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_165, torch.float16);  sum_165 = None
        view_801: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_540, [1, 1, 1024]);  convert_element_type_540 = None
        view_802: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_801, [1, 1, -1, 128]);  view_801 = None
        permute_259: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_802, [0, 2, 1, 3]);  view_802 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_47: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg264_1, [None, None, arg2_1], permute_259);  permute_259 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_477: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_47, 2)
        expand_122: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_477, [1, 8, 4, 2048, 128]);  unsqueeze_477 = None
        clone_49: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_122, memory_format = torch.contiguous_format);  expand_122 = None
        view_804: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_49, [1, 32, 2048, 128]);  clone_49 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_47: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_46: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_23: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_47, full_default_46);  full_default_47 = full_default_46 = None
        expand_123: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_23, [1, 32, 1, 2048]);  where_23 = None
        _scaled_dot_product_efficient_attention_23 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_92, view_803, view_804, expand_123, False, scale = 0.08838834764831845);  add_92 = view_803 = view_804 = expand_123 = None
        getitem_186: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_23[0];  _scaled_dot_product_efficient_attention_23 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_94: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_260: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_186, [0, 2, 1, 3]);  getitem_186 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_805: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_260, [1, 1, -1]);  permute_260 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_806: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_805, [1, 4096]);  view_805 = None
        convert_element_type_541: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_806, torch.float32);  view_806 = None
        unsqueeze_478: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_541, 2);  convert_element_type_541 = None
        permute_261: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg265_1, [1, 0]);  arg265_1 = None
        convert_element_type_542: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_261, torch.float32);  permute_261 = None
        unsqueeze_479: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_542, 0);  convert_element_type_542 = None
        mul_309: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_478, unsqueeze_479);  unsqueeze_478 = unsqueeze_479 = None
        sum_166: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_309, [1]);  mul_309 = None
        convert_element_type_543: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_166, torch.float16);  sum_166 = None
        view_807: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_543, [1, 1, 4096]);  convert_element_type_543 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_94: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_91, view_807);  add_91 = view_807 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_808: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_94, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_95: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_47 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 243, constant_args_idx = 242, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_94, 'X_ptr': view_808, 'W_ptr': arg266_1, 'RSTD_ptr': empty_95}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_94 = view_808 = arg266_1 = empty_95 = None
        getitem_190: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_47['Y_ptr'];  triton_kernel_wrapper_functional_proxy_47 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_96: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_544: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_190, torch.float32)
        unsqueeze_480: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_544, 2);  convert_element_type_544 = None
        permute_262: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg267_1, [1, 0]);  arg267_1 = None
        convert_element_type_545: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_262, torch.float32);  permute_262 = None
        unsqueeze_481: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_545, 0);  convert_element_type_545 = None
        mul_310: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_480, unsqueeze_481);  unsqueeze_480 = unsqueeze_481 = None
        sum_167: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_310, [1]);  mul_310 = None
        convert_element_type_546: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_167, torch.float16);  sum_167 = None
        view_813: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_546, [1, 1, 14336]);  convert_element_type_546 = None
        convert_element_type_547: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_813, torch.float32);  view_813 = None
        sigmoid_23: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_547)
        mul_311: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_547, sigmoid_23);  convert_element_type_547 = sigmoid_23 = None
        convert_element_type_548: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_311, torch.float16);  mul_311 = None
        convert_element_type_549: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_190, torch.float32);  getitem_190 = None
        unsqueeze_482: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_549, 2);  convert_element_type_549 = None
        permute_263: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg268_1, [1, 0]);  arg268_1 = None
        convert_element_type_550: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_263, torch.float32);  permute_263 = None
        unsqueeze_483: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_550, 0);  convert_element_type_550 = None
        mul_312: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_482, unsqueeze_483);  unsqueeze_482 = unsqueeze_483 = None
        sum_168: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_312, [1]);  mul_312 = None
        convert_element_type_551: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_168, torch.float16);  sum_168 = None
        view_817: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_551, [1, 1, 14336]);  convert_element_type_551 = None
        mul_313: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_548, view_817);  convert_element_type_548 = view_817 = None
        view_818: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_313, [1, 14336]);  mul_313 = None
        convert_element_type_552: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_818, torch.float32);  view_818 = None
        unsqueeze_484: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_552, 2);  convert_element_type_552 = None
        permute_264: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg269_1, [1, 0]);  arg269_1 = None
        convert_element_type_553: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_264, torch.float32);  permute_264 = None
        unsqueeze_485: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_553, 0);  convert_element_type_553 = None
        mul_314: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_484, unsqueeze_485);  unsqueeze_484 = unsqueeze_485 = None
        sum_169: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_314, [1]);  mul_314 = None
        convert_element_type_554: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_169, torch.float16);  sum_169 = None
        view_819: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_554, [1, 1, 4096]);  convert_element_type_554 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_95: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_94, view_819);  add_94 = view_819 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_820: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_95, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_97: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_48 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 244, constant_args_idx = 243, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_96, 'X_ptr': view_820, 'W_ptr': arg270_1, 'RSTD_ptr': empty_97}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_96 = view_820 = arg270_1 = empty_97 = None
        getitem_192: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_48['Y_ptr'];  triton_kernel_wrapper_functional_proxy_48 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_555: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_192, torch.float32)
        unsqueeze_486: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_555, 2);  convert_element_type_555 = None
        permute_265: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg271_1, [1, 0]);  arg271_1 = None
        convert_element_type_556: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_265, torch.float32);  permute_265 = None
        unsqueeze_487: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_556, 0);  convert_element_type_556 = None
        mul_315: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_486, unsqueeze_487);  unsqueeze_486 = unsqueeze_487 = None
        sum_170: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_315, [1]);  mul_315 = None
        convert_element_type_557: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_170, torch.float16);  sum_170 = None
        view_825: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_557, [1, 1, 4096]);  convert_element_type_557 = None
        view_826: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_825, [1, 1, -1, 128]);  view_825 = None
        permute_266: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_826, [0, 2, 1, 3]);  view_826 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_492: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_318: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_266, unsqueeze_492)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_557: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_266, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_48: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_557);  slice_557 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_556: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_266, 3, 0, 64);  permute_266 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_48: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_48, slice_556], -1);  neg_48 = slice_556 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_493: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_319: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_48, unsqueeze_493);  cat_48 = None
        add_96: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_318, mul_319);  mul_318 = mul_319 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_558: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_192, torch.float32)
        unsqueeze_488: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_558, 2);  convert_element_type_558 = None
        permute_267: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg272_1, [1, 0]);  arg272_1 = None
        convert_element_type_559: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_267, torch.float32);  permute_267 = None
        unsqueeze_489: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_559, 0);  convert_element_type_559 = None
        mul_316: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_488, unsqueeze_489);  unsqueeze_488 = unsqueeze_489 = None
        sum_171: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_316, [1]);  mul_316 = None
        convert_element_type_560: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_171, torch.float16);  sum_171 = None
        view_830: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_560, [1, 1, 1024]);  convert_element_type_560 = None
        view_831: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_830, [1, 1, -1, 128]);  view_830 = None
        permute_268: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_831, [0, 2, 1, 3]);  view_831 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_320: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_268, unsqueeze_492);  unsqueeze_492 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_559: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_268, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_49: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_559);  slice_559 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_558: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_268, 3, 0, 64);  permute_268 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_49: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_49, slice_558], -1);  neg_49 = slice_558 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_321: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_49, unsqueeze_493);  cat_49 = unsqueeze_493 = None
        add_97: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_320, mul_321);  mul_320 = mul_321 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_48: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg274_1, [None, None, arg2_1], add_97);  add_97 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_495: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_48, 2)
        expand_125: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_495, [1, 8, 4, 2048, 128]);  unsqueeze_495 = None
        clone_50: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_125, memory_format = torch.contiguous_format);  expand_125 = None
        view_837: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_50, [1, 32, 2048, 128]);  clone_50 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_561: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_192, torch.float32);  getitem_192 = None
        unsqueeze_490: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_561, 2);  convert_element_type_561 = None
        permute_269: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg273_1, [1, 0]);  arg273_1 = None
        convert_element_type_562: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_269, torch.float32);  permute_269 = None
        unsqueeze_491: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_562, 0);  convert_element_type_562 = None
        mul_317: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_490, unsqueeze_491);  unsqueeze_490 = unsqueeze_491 = None
        sum_172: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_317, [1]);  mul_317 = None
        convert_element_type_563: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_172, torch.float16);  sum_172 = None
        view_835: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_563, [1, 1, 1024]);  convert_element_type_563 = None
        view_836: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_835, [1, 1, -1, 128]);  view_835 = None
        permute_270: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_836, [0, 2, 1, 3]);  view_836 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_49: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg275_1, [None, None, arg2_1], permute_270);  permute_270 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_497: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_49, 2)
        expand_127: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_497, [1, 8, 4, 2048, 128]);  unsqueeze_497 = None
        clone_51: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_127, memory_format = torch.contiguous_format);  expand_127 = None
        view_838: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_51, [1, 32, 2048, 128]);  clone_51 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_49: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_48: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_24: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_49, full_default_48);  full_default_49 = full_default_48 = None
        expand_128: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_24, [1, 32, 1, 2048]);  where_24 = None
        _scaled_dot_product_efficient_attention_24 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_96, view_837, view_838, expand_128, False, scale = 0.08838834764831845);  add_96 = view_837 = view_838 = expand_128 = None
        getitem_194: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_24[0];  _scaled_dot_product_efficient_attention_24 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_98: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_271: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_194, [0, 2, 1, 3]);  getitem_194 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_839: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_271, [1, 1, -1]);  permute_271 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_840: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_839, [1, 4096]);  view_839 = None
        convert_element_type_564: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_840, torch.float32);  view_840 = None
        unsqueeze_498: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_564, 2);  convert_element_type_564 = None
        permute_272: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg276_1, [1, 0]);  arg276_1 = None
        convert_element_type_565: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_272, torch.float32);  permute_272 = None
        unsqueeze_499: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_565, 0);  convert_element_type_565 = None
        mul_322: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_498, unsqueeze_499);  unsqueeze_498 = unsqueeze_499 = None
        sum_173: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_322, [1]);  mul_322 = None
        convert_element_type_566: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_173, torch.float16);  sum_173 = None
        view_841: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_566, [1, 1, 4096]);  convert_element_type_566 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_98: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_95, view_841);  add_95 = view_841 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_842: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_98, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_99: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_49 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 245, constant_args_idx = 244, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_98, 'X_ptr': view_842, 'W_ptr': arg277_1, 'RSTD_ptr': empty_99}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_98 = view_842 = arg277_1 = empty_99 = None
        getitem_198: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_49['Y_ptr'];  triton_kernel_wrapper_functional_proxy_49 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_100: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_567: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_198, torch.float32)
        unsqueeze_500: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_567, 2);  convert_element_type_567 = None
        permute_273: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg278_1, [1, 0]);  arg278_1 = None
        convert_element_type_568: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_273, torch.float32);  permute_273 = None
        unsqueeze_501: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_568, 0);  convert_element_type_568 = None
        mul_323: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_500, unsqueeze_501);  unsqueeze_500 = unsqueeze_501 = None
        sum_174: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_323, [1]);  mul_323 = None
        convert_element_type_569: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_174, torch.float16);  sum_174 = None
        view_847: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_569, [1, 1, 14336]);  convert_element_type_569 = None
        convert_element_type_570: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_847, torch.float32);  view_847 = None
        sigmoid_24: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_570)
        mul_324: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_570, sigmoid_24);  convert_element_type_570 = sigmoid_24 = None
        convert_element_type_571: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_324, torch.float16);  mul_324 = None
        convert_element_type_572: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_198, torch.float32);  getitem_198 = None
        unsqueeze_502: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_572, 2);  convert_element_type_572 = None
        permute_274: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg279_1, [1, 0]);  arg279_1 = None
        convert_element_type_573: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_274, torch.float32);  permute_274 = None
        unsqueeze_503: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_573, 0);  convert_element_type_573 = None
        mul_325: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_502, unsqueeze_503);  unsqueeze_502 = unsqueeze_503 = None
        sum_175: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_325, [1]);  mul_325 = None
        convert_element_type_574: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_175, torch.float16);  sum_175 = None
        view_851: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_574, [1, 1, 14336]);  convert_element_type_574 = None
        mul_326: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_571, view_851);  convert_element_type_571 = view_851 = None
        view_852: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_326, [1, 14336]);  mul_326 = None
        convert_element_type_575: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_852, torch.float32);  view_852 = None
        unsqueeze_504: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_575, 2);  convert_element_type_575 = None
        permute_275: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg280_1, [1, 0]);  arg280_1 = None
        convert_element_type_576: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_275, torch.float32);  permute_275 = None
        unsqueeze_505: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_576, 0);  convert_element_type_576 = None
        mul_327: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_504, unsqueeze_505);  unsqueeze_504 = unsqueeze_505 = None
        sum_176: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_327, [1]);  mul_327 = None
        convert_element_type_577: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_176, torch.float16);  sum_176 = None
        view_853: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_577, [1, 1, 4096]);  convert_element_type_577 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_99: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_98, view_853);  add_98 = view_853 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_854: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_99, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_101: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_50 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 246, constant_args_idx = 245, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_100, 'X_ptr': view_854, 'W_ptr': arg281_1, 'RSTD_ptr': empty_101}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_100 = view_854 = arg281_1 = empty_101 = None
        getitem_200: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_50['Y_ptr'];  triton_kernel_wrapper_functional_proxy_50 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_578: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_200, torch.float32)
        unsqueeze_506: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_578, 2);  convert_element_type_578 = None
        permute_276: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg282_1, [1, 0]);  arg282_1 = None
        convert_element_type_579: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_276, torch.float32);  permute_276 = None
        unsqueeze_507: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_579, 0);  convert_element_type_579 = None
        mul_328: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_506, unsqueeze_507);  unsqueeze_506 = unsqueeze_507 = None
        sum_177: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_328, [1]);  mul_328 = None
        convert_element_type_580: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_177, torch.float16);  sum_177 = None
        view_859: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_580, [1, 1, 4096]);  convert_element_type_580 = None
        view_860: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_859, [1, 1, -1, 128]);  view_859 = None
        permute_277: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_860, [0, 2, 1, 3]);  view_860 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_512: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_331: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_277, unsqueeze_512)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_580: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_277, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_50: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_580);  slice_580 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_579: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_277, 3, 0, 64);  permute_277 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_50: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_50, slice_579], -1);  neg_50 = slice_579 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_513: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_332: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_50, unsqueeze_513);  cat_50 = None
        add_100: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_331, mul_332);  mul_331 = mul_332 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_581: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_200, torch.float32)
        unsqueeze_508: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_581, 2);  convert_element_type_581 = None
        permute_278: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg283_1, [1, 0]);  arg283_1 = None
        convert_element_type_582: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_278, torch.float32);  permute_278 = None
        unsqueeze_509: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_582, 0);  convert_element_type_582 = None
        mul_329: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_508, unsqueeze_509);  unsqueeze_508 = unsqueeze_509 = None
        sum_178: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_329, [1]);  mul_329 = None
        convert_element_type_583: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_178, torch.float16);  sum_178 = None
        view_864: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_583, [1, 1, 1024]);  convert_element_type_583 = None
        view_865: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_864, [1, 1, -1, 128]);  view_864 = None
        permute_279: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_865, [0, 2, 1, 3]);  view_865 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_333: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_279, unsqueeze_512);  unsqueeze_512 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_582: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_279, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_51: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_582);  slice_582 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_581: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_279, 3, 0, 64);  permute_279 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_51: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_51, slice_581], -1);  neg_51 = slice_581 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_334: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_51, unsqueeze_513);  cat_51 = unsqueeze_513 = None
        add_101: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_333, mul_334);  mul_333 = mul_334 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_50: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg285_1, [None, None, arg2_1], add_101);  add_101 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_515: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_50, 2)
        expand_130: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_515, [1, 8, 4, 2048, 128]);  unsqueeze_515 = None
        clone_52: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_130, memory_format = torch.contiguous_format);  expand_130 = None
        view_871: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_52, [1, 32, 2048, 128]);  clone_52 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_584: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_200, torch.float32);  getitem_200 = None
        unsqueeze_510: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_584, 2);  convert_element_type_584 = None
        permute_280: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg284_1, [1, 0]);  arg284_1 = None
        convert_element_type_585: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_280, torch.float32);  permute_280 = None
        unsqueeze_511: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_585, 0);  convert_element_type_585 = None
        mul_330: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_510, unsqueeze_511);  unsqueeze_510 = unsqueeze_511 = None
        sum_179: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_330, [1]);  mul_330 = None
        convert_element_type_586: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_179, torch.float16);  sum_179 = None
        view_869: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_586, [1, 1, 1024]);  convert_element_type_586 = None
        view_870: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_869, [1, 1, -1, 128]);  view_869 = None
        permute_281: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_870, [0, 2, 1, 3]);  view_870 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_51: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg286_1, [None, None, arg2_1], permute_281);  permute_281 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_517: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_51, 2)
        expand_132: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_517, [1, 8, 4, 2048, 128]);  unsqueeze_517 = None
        clone_53: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_132, memory_format = torch.contiguous_format);  expand_132 = None
        view_872: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_53, [1, 32, 2048, 128]);  clone_53 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_51: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_50: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_25: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_51, full_default_50);  full_default_51 = full_default_50 = None
        expand_133: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_25, [1, 32, 1, 2048]);  where_25 = None
        _scaled_dot_product_efficient_attention_25 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_100, view_871, view_872, expand_133, False, scale = 0.08838834764831845);  add_100 = view_871 = view_872 = expand_133 = None
        getitem_202: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_25[0];  _scaled_dot_product_efficient_attention_25 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_102: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_282: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_202, [0, 2, 1, 3]);  getitem_202 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_873: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_282, [1, 1, -1]);  permute_282 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_874: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_873, [1, 4096]);  view_873 = None
        convert_element_type_587: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_874, torch.float32);  view_874 = None
        unsqueeze_518: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_587, 2);  convert_element_type_587 = None
        permute_283: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg287_1, [1, 0]);  arg287_1 = None
        convert_element_type_588: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_283, torch.float32);  permute_283 = None
        unsqueeze_519: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_588, 0);  convert_element_type_588 = None
        mul_335: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_518, unsqueeze_519);  unsqueeze_518 = unsqueeze_519 = None
        sum_180: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_335, [1]);  mul_335 = None
        convert_element_type_589: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_180, torch.float16);  sum_180 = None
        view_875: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_589, [1, 1, 4096]);  convert_element_type_589 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_102: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_99, view_875);  add_99 = view_875 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_876: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_102, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_103: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_51 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 247, constant_args_idx = 246, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_102, 'X_ptr': view_876, 'W_ptr': arg288_1, 'RSTD_ptr': empty_103}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_102 = view_876 = arg288_1 = empty_103 = None
        getitem_206: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_51['Y_ptr'];  triton_kernel_wrapper_functional_proxy_51 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_104: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_590: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_206, torch.float32)
        unsqueeze_520: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_590, 2);  convert_element_type_590 = None
        permute_284: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg289_1, [1, 0]);  arg289_1 = None
        convert_element_type_591: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_284, torch.float32);  permute_284 = None
        unsqueeze_521: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_591, 0);  convert_element_type_591 = None
        mul_336: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_520, unsqueeze_521);  unsqueeze_520 = unsqueeze_521 = None
        sum_181: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_336, [1]);  mul_336 = None
        convert_element_type_592: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_181, torch.float16);  sum_181 = None
        view_881: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_592, [1, 1, 14336]);  convert_element_type_592 = None
        convert_element_type_593: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_881, torch.float32);  view_881 = None
        sigmoid_25: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_593)
        mul_337: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_593, sigmoid_25);  convert_element_type_593 = sigmoid_25 = None
        convert_element_type_594: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_337, torch.float16);  mul_337 = None
        convert_element_type_595: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_206, torch.float32);  getitem_206 = None
        unsqueeze_522: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_595, 2);  convert_element_type_595 = None
        permute_285: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg290_1, [1, 0]);  arg290_1 = None
        convert_element_type_596: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_285, torch.float32);  permute_285 = None
        unsqueeze_523: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_596, 0);  convert_element_type_596 = None
        mul_338: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_522, unsqueeze_523);  unsqueeze_522 = unsqueeze_523 = None
        sum_182: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_338, [1]);  mul_338 = None
        convert_element_type_597: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_182, torch.float16);  sum_182 = None
        view_885: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_597, [1, 1, 14336]);  convert_element_type_597 = None
        mul_339: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_594, view_885);  convert_element_type_594 = view_885 = None
        view_886: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_339, [1, 14336]);  mul_339 = None
        convert_element_type_598: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_886, torch.float32);  view_886 = None
        unsqueeze_524: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_598, 2);  convert_element_type_598 = None
        permute_286: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg291_1, [1, 0]);  arg291_1 = None
        convert_element_type_599: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_286, torch.float32);  permute_286 = None
        unsqueeze_525: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_599, 0);  convert_element_type_599 = None
        mul_340: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_524, unsqueeze_525);  unsqueeze_524 = unsqueeze_525 = None
        sum_183: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_340, [1]);  mul_340 = None
        convert_element_type_600: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_183, torch.float16);  sum_183 = None
        view_887: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_600, [1, 1, 4096]);  convert_element_type_600 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_103: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_102, view_887);  add_102 = view_887 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_888: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_103, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_105: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_52 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 248, constant_args_idx = 247, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_104, 'X_ptr': view_888, 'W_ptr': arg292_1, 'RSTD_ptr': empty_105}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_104 = view_888 = arg292_1 = empty_105 = None
        getitem_208: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_52['Y_ptr'];  triton_kernel_wrapper_functional_proxy_52 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_601: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_208, torch.float32)
        unsqueeze_526: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_601, 2);  convert_element_type_601 = None
        permute_287: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg293_1, [1, 0]);  arg293_1 = None
        convert_element_type_602: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_287, torch.float32);  permute_287 = None
        unsqueeze_527: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_602, 0);  convert_element_type_602 = None
        mul_341: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_526, unsqueeze_527);  unsqueeze_526 = unsqueeze_527 = None
        sum_184: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_341, [1]);  mul_341 = None
        convert_element_type_603: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_184, torch.float16);  sum_184 = None
        view_893: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_603, [1, 1, 4096]);  convert_element_type_603 = None
        view_894: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_893, [1, 1, -1, 128]);  view_893 = None
        permute_288: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_894, [0, 2, 1, 3]);  view_894 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_532: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_344: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_288, unsqueeze_532)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_603: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_288, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_52: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_603);  slice_603 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_602: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_288, 3, 0, 64);  permute_288 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_52: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_52, slice_602], -1);  neg_52 = slice_602 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_533: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_345: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_52, unsqueeze_533);  cat_52 = None
        add_104: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_344, mul_345);  mul_344 = mul_345 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_604: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_208, torch.float32)
        unsqueeze_528: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_604, 2);  convert_element_type_604 = None
        permute_289: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg294_1, [1, 0]);  arg294_1 = None
        convert_element_type_605: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_289, torch.float32);  permute_289 = None
        unsqueeze_529: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_605, 0);  convert_element_type_605 = None
        mul_342: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_528, unsqueeze_529);  unsqueeze_528 = unsqueeze_529 = None
        sum_185: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_342, [1]);  mul_342 = None
        convert_element_type_606: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_185, torch.float16);  sum_185 = None
        view_898: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_606, [1, 1, 1024]);  convert_element_type_606 = None
        view_899: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_898, [1, 1, -1, 128]);  view_898 = None
        permute_290: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_899, [0, 2, 1, 3]);  view_899 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_346: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_290, unsqueeze_532);  unsqueeze_532 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_605: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_290, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_53: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_605);  slice_605 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_604: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_290, 3, 0, 64);  permute_290 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_53: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_53, slice_604], -1);  neg_53 = slice_604 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_347: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_53, unsqueeze_533);  cat_53 = unsqueeze_533 = None
        add_105: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_346, mul_347);  mul_346 = mul_347 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_52: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg296_1, [None, None, arg2_1], add_105);  add_105 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_535: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_52, 2)
        expand_135: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_535, [1, 8, 4, 2048, 128]);  unsqueeze_535 = None
        clone_54: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_135, memory_format = torch.contiguous_format);  expand_135 = None
        view_905: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_54, [1, 32, 2048, 128]);  clone_54 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_607: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_208, torch.float32);  getitem_208 = None
        unsqueeze_530: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_607, 2);  convert_element_type_607 = None
        permute_291: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg295_1, [1, 0]);  arg295_1 = None
        convert_element_type_608: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_291, torch.float32);  permute_291 = None
        unsqueeze_531: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_608, 0);  convert_element_type_608 = None
        mul_343: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_530, unsqueeze_531);  unsqueeze_530 = unsqueeze_531 = None
        sum_186: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_343, [1]);  mul_343 = None
        convert_element_type_609: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_186, torch.float16);  sum_186 = None
        view_903: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_609, [1, 1, 1024]);  convert_element_type_609 = None
        view_904: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_903, [1, 1, -1, 128]);  view_903 = None
        permute_292: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_904, [0, 2, 1, 3]);  view_904 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_53: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg297_1, [None, None, arg2_1], permute_292);  permute_292 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_537: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_53, 2)
        expand_137: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_537, [1, 8, 4, 2048, 128]);  unsqueeze_537 = None
        clone_55: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_137, memory_format = torch.contiguous_format);  expand_137 = None
        view_906: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_55, [1, 32, 2048, 128]);  clone_55 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_53: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_52: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_26: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_53, full_default_52);  full_default_53 = full_default_52 = None
        expand_138: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_26, [1, 32, 1, 2048]);  where_26 = None
        _scaled_dot_product_efficient_attention_26 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_104, view_905, view_906, expand_138, False, scale = 0.08838834764831845);  add_104 = view_905 = view_906 = expand_138 = None
        getitem_210: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_26[0];  _scaled_dot_product_efficient_attention_26 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_106: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_293: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_210, [0, 2, 1, 3]);  getitem_210 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_907: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_293, [1, 1, -1]);  permute_293 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_908: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_907, [1, 4096]);  view_907 = None
        convert_element_type_610: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_908, torch.float32);  view_908 = None
        unsqueeze_538: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_610, 2);  convert_element_type_610 = None
        permute_294: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg298_1, [1, 0]);  arg298_1 = None
        convert_element_type_611: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_294, torch.float32);  permute_294 = None
        unsqueeze_539: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_611, 0);  convert_element_type_611 = None
        mul_348: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_538, unsqueeze_539);  unsqueeze_538 = unsqueeze_539 = None
        sum_187: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_348, [1]);  mul_348 = None
        convert_element_type_612: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_187, torch.float16);  sum_187 = None
        view_909: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_612, [1, 1, 4096]);  convert_element_type_612 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_106: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_103, view_909);  add_103 = view_909 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_910: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_106, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_107: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_53 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 249, constant_args_idx = 248, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_106, 'X_ptr': view_910, 'W_ptr': arg299_1, 'RSTD_ptr': empty_107}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_106 = view_910 = arg299_1 = empty_107 = None
        getitem_214: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_53['Y_ptr'];  triton_kernel_wrapper_functional_proxy_53 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_108: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_613: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_214, torch.float32)
        unsqueeze_540: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_613, 2);  convert_element_type_613 = None
        permute_295: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg300_1, [1, 0]);  arg300_1 = None
        convert_element_type_614: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_295, torch.float32);  permute_295 = None
        unsqueeze_541: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_614, 0);  convert_element_type_614 = None
        mul_349: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_540, unsqueeze_541);  unsqueeze_540 = unsqueeze_541 = None
        sum_188: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_349, [1]);  mul_349 = None
        convert_element_type_615: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_188, torch.float16);  sum_188 = None
        view_915: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_615, [1, 1, 14336]);  convert_element_type_615 = None
        convert_element_type_616: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_915, torch.float32);  view_915 = None
        sigmoid_26: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_616)
        mul_350: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_616, sigmoid_26);  convert_element_type_616 = sigmoid_26 = None
        convert_element_type_617: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_350, torch.float16);  mul_350 = None
        convert_element_type_618: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_214, torch.float32);  getitem_214 = None
        unsqueeze_542: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_618, 2);  convert_element_type_618 = None
        permute_296: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg301_1, [1, 0]);  arg301_1 = None
        convert_element_type_619: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_296, torch.float32);  permute_296 = None
        unsqueeze_543: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_619, 0);  convert_element_type_619 = None
        mul_351: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_542, unsqueeze_543);  unsqueeze_542 = unsqueeze_543 = None
        sum_189: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_351, [1]);  mul_351 = None
        convert_element_type_620: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_189, torch.float16);  sum_189 = None
        view_919: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_620, [1, 1, 14336]);  convert_element_type_620 = None
        mul_352: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_617, view_919);  convert_element_type_617 = view_919 = None
        view_920: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_352, [1, 14336]);  mul_352 = None
        convert_element_type_621: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_920, torch.float32);  view_920 = None
        unsqueeze_544: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_621, 2);  convert_element_type_621 = None
        permute_297: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg302_1, [1, 0]);  arg302_1 = None
        convert_element_type_622: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_297, torch.float32);  permute_297 = None
        unsqueeze_545: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_622, 0);  convert_element_type_622 = None
        mul_353: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_544, unsqueeze_545);  unsqueeze_544 = unsqueeze_545 = None
        sum_190: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_353, [1]);  mul_353 = None
        convert_element_type_623: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_190, torch.float16);  sum_190 = None
        view_921: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_623, [1, 1, 4096]);  convert_element_type_623 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_107: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_106, view_921);  add_106 = view_921 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_922: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_107, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_109: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_54 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 250, constant_args_idx = 249, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_108, 'X_ptr': view_922, 'W_ptr': arg303_1, 'RSTD_ptr': empty_109}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_108 = view_922 = arg303_1 = empty_109 = None
        getitem_216: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_54['Y_ptr'];  triton_kernel_wrapper_functional_proxy_54 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_624: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_216, torch.float32)
        unsqueeze_546: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_624, 2);  convert_element_type_624 = None
        permute_298: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg304_1, [1, 0]);  arg304_1 = None
        convert_element_type_625: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_298, torch.float32);  permute_298 = None
        unsqueeze_547: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_625, 0);  convert_element_type_625 = None
        mul_354: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_546, unsqueeze_547);  unsqueeze_546 = unsqueeze_547 = None
        sum_191: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_354, [1]);  mul_354 = None
        convert_element_type_626: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_191, torch.float16);  sum_191 = None
        view_927: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_626, [1, 1, 4096]);  convert_element_type_626 = None
        view_928: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_927, [1, 1, -1, 128]);  view_927 = None
        permute_299: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_928, [0, 2, 1, 3]);  view_928 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_552: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_357: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_299, unsqueeze_552)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_626: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_299, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_54: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_626);  slice_626 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_625: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_299, 3, 0, 64);  permute_299 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_54: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_54, slice_625], -1);  neg_54 = slice_625 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_553: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_358: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_54, unsqueeze_553);  cat_54 = None
        add_108: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_357, mul_358);  mul_357 = mul_358 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_627: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_216, torch.float32)
        unsqueeze_548: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_627, 2);  convert_element_type_627 = None
        permute_300: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg305_1, [1, 0]);  arg305_1 = None
        convert_element_type_628: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_300, torch.float32);  permute_300 = None
        unsqueeze_549: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_628, 0);  convert_element_type_628 = None
        mul_355: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_548, unsqueeze_549);  unsqueeze_548 = unsqueeze_549 = None
        sum_192: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_355, [1]);  mul_355 = None
        convert_element_type_629: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_192, torch.float16);  sum_192 = None
        view_932: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_629, [1, 1, 1024]);  convert_element_type_629 = None
        view_933: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_932, [1, 1, -1, 128]);  view_932 = None
        permute_301: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_933, [0, 2, 1, 3]);  view_933 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_359: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_301, unsqueeze_552);  unsqueeze_552 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_628: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_301, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_55: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_628);  slice_628 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_627: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_301, 3, 0, 64);  permute_301 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_55: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_55, slice_627], -1);  neg_55 = slice_627 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_360: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_55, unsqueeze_553);  cat_55 = unsqueeze_553 = None
        add_109: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_359, mul_360);  mul_359 = mul_360 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_54: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg307_1, [None, None, arg2_1], add_109);  add_109 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_555: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_54, 2)
        expand_140: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_555, [1, 8, 4, 2048, 128]);  unsqueeze_555 = None
        clone_56: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_140, memory_format = torch.contiguous_format);  expand_140 = None
        view_939: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_56, [1, 32, 2048, 128]);  clone_56 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_630: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_216, torch.float32);  getitem_216 = None
        unsqueeze_550: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_630, 2);  convert_element_type_630 = None
        permute_302: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg306_1, [1, 0]);  arg306_1 = None
        convert_element_type_631: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_302, torch.float32);  permute_302 = None
        unsqueeze_551: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_631, 0);  convert_element_type_631 = None
        mul_356: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_550, unsqueeze_551);  unsqueeze_550 = unsqueeze_551 = None
        sum_193: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_356, [1]);  mul_356 = None
        convert_element_type_632: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_193, torch.float16);  sum_193 = None
        view_937: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_632, [1, 1, 1024]);  convert_element_type_632 = None
        view_938: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_937, [1, 1, -1, 128]);  view_937 = None
        permute_303: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_938, [0, 2, 1, 3]);  view_938 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_55: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg308_1, [None, None, arg2_1], permute_303);  permute_303 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_557: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_55, 2)
        expand_142: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_557, [1, 8, 4, 2048, 128]);  unsqueeze_557 = None
        clone_57: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_142, memory_format = torch.contiguous_format);  expand_142 = None
        view_940: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_57, [1, 32, 2048, 128]);  clone_57 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_55: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_54: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_27: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_55, full_default_54);  full_default_55 = full_default_54 = None
        expand_143: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_27, [1, 32, 1, 2048]);  where_27 = None
        _scaled_dot_product_efficient_attention_27 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_108, view_939, view_940, expand_143, False, scale = 0.08838834764831845);  add_108 = view_939 = view_940 = expand_143 = None
        getitem_218: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_27[0];  _scaled_dot_product_efficient_attention_27 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_110: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_304: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_218, [0, 2, 1, 3]);  getitem_218 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_941: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_304, [1, 1, -1]);  permute_304 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_942: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_941, [1, 4096]);  view_941 = None
        convert_element_type_633: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_942, torch.float32);  view_942 = None
        unsqueeze_558: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_633, 2);  convert_element_type_633 = None
        permute_305: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg309_1, [1, 0]);  arg309_1 = None
        convert_element_type_634: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_305, torch.float32);  permute_305 = None
        unsqueeze_559: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_634, 0);  convert_element_type_634 = None
        mul_361: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_558, unsqueeze_559);  unsqueeze_558 = unsqueeze_559 = None
        sum_194: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_361, [1]);  mul_361 = None
        convert_element_type_635: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_194, torch.float16);  sum_194 = None
        view_943: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_635, [1, 1, 4096]);  convert_element_type_635 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_110: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_107, view_943);  add_107 = view_943 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_944: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_110, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_111: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_55 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 251, constant_args_idx = 250, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_110, 'X_ptr': view_944, 'W_ptr': arg310_1, 'RSTD_ptr': empty_111}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_110 = view_944 = arg310_1 = empty_111 = None
        getitem_222: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_55['Y_ptr'];  triton_kernel_wrapper_functional_proxy_55 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_112: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_636: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_222, torch.float32)
        unsqueeze_560: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_636, 2);  convert_element_type_636 = None
        permute_306: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg311_1, [1, 0]);  arg311_1 = None
        convert_element_type_637: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_306, torch.float32);  permute_306 = None
        unsqueeze_561: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_637, 0);  convert_element_type_637 = None
        mul_362: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_560, unsqueeze_561);  unsqueeze_560 = unsqueeze_561 = None
        sum_195: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_362, [1]);  mul_362 = None
        convert_element_type_638: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_195, torch.float16);  sum_195 = None
        view_949: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_638, [1, 1, 14336]);  convert_element_type_638 = None
        convert_element_type_639: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_949, torch.float32);  view_949 = None
        sigmoid_27: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_639)
        mul_363: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_639, sigmoid_27);  convert_element_type_639 = sigmoid_27 = None
        convert_element_type_640: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_363, torch.float16);  mul_363 = None
        convert_element_type_641: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_222, torch.float32);  getitem_222 = None
        unsqueeze_562: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_641, 2);  convert_element_type_641 = None
        permute_307: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg312_1, [1, 0]);  arg312_1 = None
        convert_element_type_642: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_307, torch.float32);  permute_307 = None
        unsqueeze_563: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_642, 0);  convert_element_type_642 = None
        mul_364: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_562, unsqueeze_563);  unsqueeze_562 = unsqueeze_563 = None
        sum_196: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_364, [1]);  mul_364 = None
        convert_element_type_643: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_196, torch.float16);  sum_196 = None
        view_953: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_643, [1, 1, 14336]);  convert_element_type_643 = None
        mul_365: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_640, view_953);  convert_element_type_640 = view_953 = None
        view_954: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_365, [1, 14336]);  mul_365 = None
        convert_element_type_644: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_954, torch.float32);  view_954 = None
        unsqueeze_564: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_644, 2);  convert_element_type_644 = None
        permute_308: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg313_1, [1, 0]);  arg313_1 = None
        convert_element_type_645: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_308, torch.float32);  permute_308 = None
        unsqueeze_565: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_645, 0);  convert_element_type_645 = None
        mul_366: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_564, unsqueeze_565);  unsqueeze_564 = unsqueeze_565 = None
        sum_197: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_366, [1]);  mul_366 = None
        convert_element_type_646: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_197, torch.float16);  sum_197 = None
        view_955: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_646, [1, 1, 4096]);  convert_element_type_646 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_111: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_110, view_955);  add_110 = view_955 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_956: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_111, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_113: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_56 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 252, constant_args_idx = 251, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_112, 'X_ptr': view_956, 'W_ptr': arg314_1, 'RSTD_ptr': empty_113}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_112 = view_956 = arg314_1 = empty_113 = None
        getitem_224: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_56['Y_ptr'];  triton_kernel_wrapper_functional_proxy_56 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_647: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_224, torch.float32)
        unsqueeze_566: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_647, 2);  convert_element_type_647 = None
        permute_309: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg315_1, [1, 0]);  arg315_1 = None
        convert_element_type_648: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_309, torch.float32);  permute_309 = None
        unsqueeze_567: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_648, 0);  convert_element_type_648 = None
        mul_367: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_566, unsqueeze_567);  unsqueeze_566 = unsqueeze_567 = None
        sum_198: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_367, [1]);  mul_367 = None
        convert_element_type_649: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_198, torch.float16);  sum_198 = None
        view_961: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_649, [1, 1, 4096]);  convert_element_type_649 = None
        view_962: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_961, [1, 1, -1, 128]);  view_961 = None
        permute_310: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_962, [0, 2, 1, 3]);  view_962 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_572: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_370: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_310, unsqueeze_572)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_649: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_310, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_56: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_649);  slice_649 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_648: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_310, 3, 0, 64);  permute_310 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_56: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_56, slice_648], -1);  neg_56 = slice_648 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_573: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_371: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_56, unsqueeze_573);  cat_56 = None
        add_112: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_370, mul_371);  mul_370 = mul_371 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_650: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_224, torch.float32)
        unsqueeze_568: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_650, 2);  convert_element_type_650 = None
        permute_311: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg316_1, [1, 0]);  arg316_1 = None
        convert_element_type_651: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_311, torch.float32);  permute_311 = None
        unsqueeze_569: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_651, 0);  convert_element_type_651 = None
        mul_368: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_568, unsqueeze_569);  unsqueeze_568 = unsqueeze_569 = None
        sum_199: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_368, [1]);  mul_368 = None
        convert_element_type_652: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_199, torch.float16);  sum_199 = None
        view_966: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_652, [1, 1, 1024]);  convert_element_type_652 = None
        view_967: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_966, [1, 1, -1, 128]);  view_966 = None
        permute_312: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_967, [0, 2, 1, 3]);  view_967 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_372: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_312, unsqueeze_572);  unsqueeze_572 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_651: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_312, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_57: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_651);  slice_651 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_650: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_312, 3, 0, 64);  permute_312 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_57: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_57, slice_650], -1);  neg_57 = slice_650 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_373: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_57, unsqueeze_573);  cat_57 = unsqueeze_573 = None
        add_113: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_372, mul_373);  mul_372 = mul_373 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_56: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg318_1, [None, None, arg2_1], add_113);  add_113 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_575: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_56, 2)
        expand_145: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_575, [1, 8, 4, 2048, 128]);  unsqueeze_575 = None
        clone_58: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_145, memory_format = torch.contiguous_format);  expand_145 = None
        view_973: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_58, [1, 32, 2048, 128]);  clone_58 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_653: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_224, torch.float32);  getitem_224 = None
        unsqueeze_570: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_653, 2);  convert_element_type_653 = None
        permute_313: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg317_1, [1, 0]);  arg317_1 = None
        convert_element_type_654: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_313, torch.float32);  permute_313 = None
        unsqueeze_571: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_654, 0);  convert_element_type_654 = None
        mul_369: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_570, unsqueeze_571);  unsqueeze_570 = unsqueeze_571 = None
        sum_200: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_369, [1]);  mul_369 = None
        convert_element_type_655: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_200, torch.float16);  sum_200 = None
        view_971: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_655, [1, 1, 1024]);  convert_element_type_655 = None
        view_972: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_971, [1, 1, -1, 128]);  view_971 = None
        permute_314: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_972, [0, 2, 1, 3]);  view_972 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_57: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg319_1, [None, None, arg2_1], permute_314);  permute_314 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_577: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_57, 2)
        expand_147: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_577, [1, 8, 4, 2048, 128]);  unsqueeze_577 = None
        clone_59: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_147, memory_format = torch.contiguous_format);  expand_147 = None
        view_974: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_59, [1, 32, 2048, 128]);  clone_59 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_57: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_56: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_28: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_57, full_default_56);  full_default_57 = full_default_56 = None
        expand_148: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_28, [1, 32, 1, 2048]);  where_28 = None
        _scaled_dot_product_efficient_attention_28 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_112, view_973, view_974, expand_148, False, scale = 0.08838834764831845);  add_112 = view_973 = view_974 = expand_148 = None
        getitem_226: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_28[0];  _scaled_dot_product_efficient_attention_28 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_114: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_315: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_226, [0, 2, 1, 3]);  getitem_226 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_975: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_315, [1, 1, -1]);  permute_315 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_976: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_975, [1, 4096]);  view_975 = None
        convert_element_type_656: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_976, torch.float32);  view_976 = None
        unsqueeze_578: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_656, 2);  convert_element_type_656 = None
        permute_316: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg320_1, [1, 0]);  arg320_1 = None
        convert_element_type_657: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_316, torch.float32);  permute_316 = None
        unsqueeze_579: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_657, 0);  convert_element_type_657 = None
        mul_374: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_578, unsqueeze_579);  unsqueeze_578 = unsqueeze_579 = None
        sum_201: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_374, [1]);  mul_374 = None
        convert_element_type_658: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_201, torch.float16);  sum_201 = None
        view_977: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_658, [1, 1, 4096]);  convert_element_type_658 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_114: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_111, view_977);  add_111 = view_977 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_978: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_114, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_115: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_57 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 253, constant_args_idx = 252, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_114, 'X_ptr': view_978, 'W_ptr': arg321_1, 'RSTD_ptr': empty_115}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_114 = view_978 = arg321_1 = empty_115 = None
        getitem_230: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_57['Y_ptr'];  triton_kernel_wrapper_functional_proxy_57 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_116: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_659: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_230, torch.float32)
        unsqueeze_580: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_659, 2);  convert_element_type_659 = None
        permute_317: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg322_1, [1, 0]);  arg322_1 = None
        convert_element_type_660: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_317, torch.float32);  permute_317 = None
        unsqueeze_581: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_660, 0);  convert_element_type_660 = None
        mul_375: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_580, unsqueeze_581);  unsqueeze_580 = unsqueeze_581 = None
        sum_202: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_375, [1]);  mul_375 = None
        convert_element_type_661: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_202, torch.float16);  sum_202 = None
        view_983: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_661, [1, 1, 14336]);  convert_element_type_661 = None
        convert_element_type_662: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_983, torch.float32);  view_983 = None
        sigmoid_28: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_662)
        mul_376: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_662, sigmoid_28);  convert_element_type_662 = sigmoid_28 = None
        convert_element_type_663: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_376, torch.float16);  mul_376 = None
        convert_element_type_664: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_230, torch.float32);  getitem_230 = None
        unsqueeze_582: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_664, 2);  convert_element_type_664 = None
        permute_318: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg323_1, [1, 0]);  arg323_1 = None
        convert_element_type_665: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_318, torch.float32);  permute_318 = None
        unsqueeze_583: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_665, 0);  convert_element_type_665 = None
        mul_377: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_582, unsqueeze_583);  unsqueeze_582 = unsqueeze_583 = None
        sum_203: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_377, [1]);  mul_377 = None
        convert_element_type_666: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_203, torch.float16);  sum_203 = None
        view_987: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_666, [1, 1, 14336]);  convert_element_type_666 = None
        mul_378: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_663, view_987);  convert_element_type_663 = view_987 = None
        view_988: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_378, [1, 14336]);  mul_378 = None
        convert_element_type_667: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_988, torch.float32);  view_988 = None
        unsqueeze_584: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_667, 2);  convert_element_type_667 = None
        permute_319: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg324_1, [1, 0]);  arg324_1 = None
        convert_element_type_668: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_319, torch.float32);  permute_319 = None
        unsqueeze_585: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_668, 0);  convert_element_type_668 = None
        mul_379: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_584, unsqueeze_585);  unsqueeze_584 = unsqueeze_585 = None
        sum_204: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_379, [1]);  mul_379 = None
        convert_element_type_669: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_204, torch.float16);  sum_204 = None
        view_989: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_669, [1, 1, 4096]);  convert_element_type_669 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_115: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_114, view_989);  add_114 = view_989 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_990: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_115, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_117: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_58 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 254, constant_args_idx = 253, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_116, 'X_ptr': view_990, 'W_ptr': arg325_1, 'RSTD_ptr': empty_117}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_116 = view_990 = arg325_1 = empty_117 = None
        getitem_232: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_58['Y_ptr'];  triton_kernel_wrapper_functional_proxy_58 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_670: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_232, torch.float32)
        unsqueeze_586: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_670, 2);  convert_element_type_670 = None
        permute_320: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg326_1, [1, 0]);  arg326_1 = None
        convert_element_type_671: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_320, torch.float32);  permute_320 = None
        unsqueeze_587: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_671, 0);  convert_element_type_671 = None
        mul_380: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_586, unsqueeze_587);  unsqueeze_586 = unsqueeze_587 = None
        sum_205: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_380, [1]);  mul_380 = None
        convert_element_type_672: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_205, torch.float16);  sum_205 = None
        view_995: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_672, [1, 1, 4096]);  convert_element_type_672 = None
        view_996: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_995, [1, 1, -1, 128]);  view_995 = None
        permute_321: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_996, [0, 2, 1, 3]);  view_996 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_592: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_383: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_321, unsqueeze_592)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_672: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_321, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_58: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_672);  slice_672 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_671: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_321, 3, 0, 64);  permute_321 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_58: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_58, slice_671], -1);  neg_58 = slice_671 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_593: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_384: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_58, unsqueeze_593);  cat_58 = None
        add_116: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_383, mul_384);  mul_383 = mul_384 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_673: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_232, torch.float32)
        unsqueeze_588: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_673, 2);  convert_element_type_673 = None
        permute_322: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg327_1, [1, 0]);  arg327_1 = None
        convert_element_type_674: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_322, torch.float32);  permute_322 = None
        unsqueeze_589: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_674, 0);  convert_element_type_674 = None
        mul_381: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_588, unsqueeze_589);  unsqueeze_588 = unsqueeze_589 = None
        sum_206: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_381, [1]);  mul_381 = None
        convert_element_type_675: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_206, torch.float16);  sum_206 = None
        view_1000: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_675, [1, 1, 1024]);  convert_element_type_675 = None
        view_1001: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_1000, [1, 1, -1, 128]);  view_1000 = None
        permute_323: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_1001, [0, 2, 1, 3]);  view_1001 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_385: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_323, unsqueeze_592);  unsqueeze_592 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_674: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_323, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_59: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_674);  slice_674 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_673: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_323, 3, 0, 64);  permute_323 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_59: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_59, slice_673], -1);  neg_59 = slice_673 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_386: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_59, unsqueeze_593);  cat_59 = unsqueeze_593 = None
        add_117: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_385, mul_386);  mul_385 = mul_386 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_58: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg329_1, [None, None, arg2_1], add_117);  add_117 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_595: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_58, 2)
        expand_150: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_595, [1, 8, 4, 2048, 128]);  unsqueeze_595 = None
        clone_60: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_150, memory_format = torch.contiguous_format);  expand_150 = None
        view_1007: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_60, [1, 32, 2048, 128]);  clone_60 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_676: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_232, torch.float32);  getitem_232 = None
        unsqueeze_590: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_676, 2);  convert_element_type_676 = None
        permute_324: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg328_1, [1, 0]);  arg328_1 = None
        convert_element_type_677: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_324, torch.float32);  permute_324 = None
        unsqueeze_591: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_677, 0);  convert_element_type_677 = None
        mul_382: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_590, unsqueeze_591);  unsqueeze_590 = unsqueeze_591 = None
        sum_207: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_382, [1]);  mul_382 = None
        convert_element_type_678: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_207, torch.float16);  sum_207 = None
        view_1005: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_678, [1, 1, 1024]);  convert_element_type_678 = None
        view_1006: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_1005, [1, 1, -1, 128]);  view_1005 = None
        permute_325: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_1006, [0, 2, 1, 3]);  view_1006 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_59: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg330_1, [None, None, arg2_1], permute_325);  permute_325 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_597: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_59, 2)
        expand_152: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_597, [1, 8, 4, 2048, 128]);  unsqueeze_597 = None
        clone_61: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_152, memory_format = torch.contiguous_format);  expand_152 = None
        view_1008: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_61, [1, 32, 2048, 128]);  clone_61 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_59: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_58: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_29: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_59, full_default_58);  full_default_59 = full_default_58 = None
        expand_153: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_29, [1, 32, 1, 2048]);  where_29 = None
        _scaled_dot_product_efficient_attention_29 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_116, view_1007, view_1008, expand_153, False, scale = 0.08838834764831845);  add_116 = view_1007 = view_1008 = expand_153 = None
        getitem_234: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_29[0];  _scaled_dot_product_efficient_attention_29 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_118: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_326: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_234, [0, 2, 1, 3]);  getitem_234 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_1009: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_326, [1, 1, -1]);  permute_326 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_1010: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_1009, [1, 4096]);  view_1009 = None
        convert_element_type_679: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1010, torch.float32);  view_1010 = None
        unsqueeze_598: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_679, 2);  convert_element_type_679 = None
        permute_327: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg331_1, [1, 0]);  arg331_1 = None
        convert_element_type_680: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_327, torch.float32);  permute_327 = None
        unsqueeze_599: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_680, 0);  convert_element_type_680 = None
        mul_387: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_598, unsqueeze_599);  unsqueeze_598 = unsqueeze_599 = None
        sum_208: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_387, [1]);  mul_387 = None
        convert_element_type_681: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_208, torch.float16);  sum_208 = None
        view_1011: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_681, [1, 1, 4096]);  convert_element_type_681 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_118: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_115, view_1011);  add_115 = view_1011 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_1012: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_118, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_119: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_59 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 255, constant_args_idx = 254, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_118, 'X_ptr': view_1012, 'W_ptr': arg332_1, 'RSTD_ptr': empty_119}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_118 = view_1012 = arg332_1 = empty_119 = None
        getitem_238: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_59['Y_ptr'];  triton_kernel_wrapper_functional_proxy_59 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_120: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_682: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_238, torch.float32)
        unsqueeze_600: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_682, 2);  convert_element_type_682 = None
        permute_328: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg333_1, [1, 0]);  arg333_1 = None
        convert_element_type_683: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_328, torch.float32);  permute_328 = None
        unsqueeze_601: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_683, 0);  convert_element_type_683 = None
        mul_388: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_600, unsqueeze_601);  unsqueeze_600 = unsqueeze_601 = None
        sum_209: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_388, [1]);  mul_388 = None
        convert_element_type_684: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_209, torch.float16);  sum_209 = None
        view_1017: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_684, [1, 1, 14336]);  convert_element_type_684 = None
        convert_element_type_685: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1017, torch.float32);  view_1017 = None
        sigmoid_29: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_685)
        mul_389: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_685, sigmoid_29);  convert_element_type_685 = sigmoid_29 = None
        convert_element_type_686: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_389, torch.float16);  mul_389 = None
        convert_element_type_687: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_238, torch.float32);  getitem_238 = None
        unsqueeze_602: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_687, 2);  convert_element_type_687 = None
        permute_329: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg334_1, [1, 0]);  arg334_1 = None
        convert_element_type_688: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_329, torch.float32);  permute_329 = None
        unsqueeze_603: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_688, 0);  convert_element_type_688 = None
        mul_390: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_602, unsqueeze_603);  unsqueeze_602 = unsqueeze_603 = None
        sum_210: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_390, [1]);  mul_390 = None
        convert_element_type_689: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_210, torch.float16);  sum_210 = None
        view_1021: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_689, [1, 1, 14336]);  convert_element_type_689 = None
        mul_391: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_686, view_1021);  convert_element_type_686 = view_1021 = None
        view_1022: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_391, [1, 14336]);  mul_391 = None
        convert_element_type_690: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1022, torch.float32);  view_1022 = None
        unsqueeze_604: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_690, 2);  convert_element_type_690 = None
        permute_330: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg335_1, [1, 0]);  arg335_1 = None
        convert_element_type_691: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_330, torch.float32);  permute_330 = None
        unsqueeze_605: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_691, 0);  convert_element_type_691 = None
        mul_392: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_604, unsqueeze_605);  unsqueeze_604 = unsqueeze_605 = None
        sum_211: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_392, [1]);  mul_392 = None
        convert_element_type_692: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_211, torch.float16);  sum_211 = None
        view_1023: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_692, [1, 1, 4096]);  convert_element_type_692 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_119: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_118, view_1023);  add_118 = view_1023 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_1024: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_119, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_121: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_60 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 256, constant_args_idx = 255, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_120, 'X_ptr': view_1024, 'W_ptr': arg336_1, 'RSTD_ptr': empty_121}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_120 = view_1024 = arg336_1 = empty_121 = None
        getitem_240: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_60['Y_ptr'];  triton_kernel_wrapper_functional_proxy_60 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_693: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_240, torch.float32)
        unsqueeze_606: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_693, 2);  convert_element_type_693 = None
        permute_331: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg337_1, [1, 0]);  arg337_1 = None
        convert_element_type_694: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_331, torch.float32);  permute_331 = None
        unsqueeze_607: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_694, 0);  convert_element_type_694 = None
        mul_393: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_606, unsqueeze_607);  unsqueeze_606 = unsqueeze_607 = None
        sum_212: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_393, [1]);  mul_393 = None
        convert_element_type_695: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_212, torch.float16);  sum_212 = None
        view_1029: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_695, [1, 1, 4096]);  convert_element_type_695 = None
        view_1030: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_1029, [1, 1, -1, 128]);  view_1029 = None
        permute_332: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_1030, [0, 2, 1, 3]);  view_1030 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_612: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_396: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_332, unsqueeze_612)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_695: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_332, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_60: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_695);  slice_695 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_694: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_332, 3, 0, 64);  permute_332 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_60: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_60, slice_694], -1);  neg_60 = slice_694 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_613: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1)
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_397: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_60, unsqueeze_613);  cat_60 = None
        add_120: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_396, mul_397);  mul_396 = mul_397 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_696: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_240, torch.float32)
        unsqueeze_608: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_696, 2);  convert_element_type_696 = None
        permute_333: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg338_1, [1, 0]);  arg338_1 = None
        convert_element_type_697: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_333, torch.float32);  permute_333 = None
        unsqueeze_609: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_697, 0);  convert_element_type_697 = None
        mul_394: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_608, unsqueeze_609);  unsqueeze_608 = unsqueeze_609 = None
        sum_213: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_394, [1]);  mul_394 = None
        convert_element_type_698: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_213, torch.float16);  sum_213 = None
        view_1034: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_698, [1, 1, 1024]);  convert_element_type_698 = None
        view_1035: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_1034, [1, 1, -1, 128]);  view_1034 = None
        permute_334: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_1035, [0, 2, 1, 3]);  view_1035 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_398: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_334, unsqueeze_612);  unsqueeze_612 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_697: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_334, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_61: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_697);  slice_697 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_696: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_334, 3, 0, 64);  permute_334 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_61: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_61, slice_696], -1);  neg_61 = slice_696 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_399: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_61, unsqueeze_613);  cat_61 = unsqueeze_613 = None
        add_121: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_398, mul_399);  mul_398 = mul_399 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_60: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg340_1, [None, None, arg2_1], add_121);  add_121 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_615: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_60, 2)
        expand_155: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_615, [1, 8, 4, 2048, 128]);  unsqueeze_615 = None
        clone_62: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_155, memory_format = torch.contiguous_format);  expand_155 = None
        view_1041: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_62, [1, 32, 2048, 128]);  clone_62 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_699: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_240, torch.float32);  getitem_240 = None
        unsqueeze_610: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_699, 2);  convert_element_type_699 = None
        permute_335: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg339_1, [1, 0]);  arg339_1 = None
        convert_element_type_700: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_335, torch.float32);  permute_335 = None
        unsqueeze_611: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_700, 0);  convert_element_type_700 = None
        mul_395: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_610, unsqueeze_611);  unsqueeze_610 = unsqueeze_611 = None
        sum_214: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_395, [1]);  mul_395 = None
        convert_element_type_701: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_214, torch.float16);  sum_214 = None
        view_1039: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_701, [1, 1, 1024]);  convert_element_type_701 = None
        view_1040: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_1039, [1, 1, -1, 128]);  view_1039 = None
        permute_336: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_1040, [0, 2, 1, 3]);  view_1040 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_61: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg341_1, [None, None, arg2_1], permute_336);  permute_336 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_617: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_61, 2)
        expand_157: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_617, [1, 8, 4, 2048, 128]);  unsqueeze_617 = None
        clone_63: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_157, memory_format = torch.contiguous_format);  expand_157 = None
        view_1042: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_63, [1, 32, 2048, 128]);  clone_63 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_61: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_60: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_30: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_61, full_default_60);  full_default_61 = full_default_60 = None
        expand_158: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_30, [1, 32, 1, 2048]);  where_30 = None
        _scaled_dot_product_efficient_attention_30 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_120, view_1041, view_1042, expand_158, False, scale = 0.08838834764831845);  add_120 = view_1041 = view_1042 = expand_158 = None
        getitem_242: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_30[0];  _scaled_dot_product_efficient_attention_30 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_122: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_337: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_242, [0, 2, 1, 3]);  getitem_242 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_1043: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_337, [1, 1, -1]);  permute_337 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_1044: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_1043, [1, 4096]);  view_1043 = None
        convert_element_type_702: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1044, torch.float32);  view_1044 = None
        unsqueeze_618: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_702, 2);  convert_element_type_702 = None
        permute_338: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg342_1, [1, 0]);  arg342_1 = None
        convert_element_type_703: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_338, torch.float32);  permute_338 = None
        unsqueeze_619: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_703, 0);  convert_element_type_703 = None
        mul_400: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_618, unsqueeze_619);  unsqueeze_618 = unsqueeze_619 = None
        sum_215: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_400, [1]);  mul_400 = None
        convert_element_type_704: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_215, torch.float16);  sum_215 = None
        view_1045: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_704, [1, 1, 4096]);  convert_element_type_704 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_122: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_119, view_1045);  add_119 = view_1045 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_1046: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_122, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_123: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_61 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 257, constant_args_idx = 256, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_122, 'X_ptr': view_1046, 'W_ptr': arg343_1, 'RSTD_ptr': empty_123}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_122 = view_1046 = arg343_1 = empty_123 = None
        getitem_246: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_61['Y_ptr'];  triton_kernel_wrapper_functional_proxy_61 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_124: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_705: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_246, torch.float32)
        unsqueeze_620: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_705, 2);  convert_element_type_705 = None
        permute_339: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg344_1, [1, 0]);  arg344_1 = None
        convert_element_type_706: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_339, torch.float32);  permute_339 = None
        unsqueeze_621: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_706, 0);  convert_element_type_706 = None
        mul_401: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_620, unsqueeze_621);  unsqueeze_620 = unsqueeze_621 = None
        sum_216: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_401, [1]);  mul_401 = None
        convert_element_type_707: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_216, torch.float16);  sum_216 = None
        view_1051: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_707, [1, 1, 14336]);  convert_element_type_707 = None
        convert_element_type_708: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1051, torch.float32);  view_1051 = None
        sigmoid_30: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_708)
        mul_402: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_708, sigmoid_30);  convert_element_type_708 = sigmoid_30 = None
        convert_element_type_709: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_402, torch.float16);  mul_402 = None
        convert_element_type_710: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_246, torch.float32);  getitem_246 = None
        unsqueeze_622: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_710, 2);  convert_element_type_710 = None
        permute_340: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg345_1, [1, 0]);  arg345_1 = None
        convert_element_type_711: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_340, torch.float32);  permute_340 = None
        unsqueeze_623: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_711, 0);  convert_element_type_711 = None
        mul_403: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_622, unsqueeze_623);  unsqueeze_622 = unsqueeze_623 = None
        sum_217: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_403, [1]);  mul_403 = None
        convert_element_type_712: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_217, torch.float16);  sum_217 = None
        view_1055: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_712, [1, 1, 14336]);  convert_element_type_712 = None
        mul_404: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_709, view_1055);  convert_element_type_709 = view_1055 = None
        view_1056: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_404, [1, 14336]);  mul_404 = None
        convert_element_type_713: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1056, torch.float32);  view_1056 = None
        unsqueeze_624: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_713, 2);  convert_element_type_713 = None
        permute_341: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg346_1, [1, 0]);  arg346_1 = None
        convert_element_type_714: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_341, torch.float32);  permute_341 = None
        unsqueeze_625: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_714, 0);  convert_element_type_714 = None
        mul_405: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_624, unsqueeze_625);  unsqueeze_624 = unsqueeze_625 = None
        sum_218: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_405, [1]);  mul_405 = None
        convert_element_type_715: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_218, torch.float16);  sum_218 = None
        view_1057: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_715, [1, 1, 4096]);  convert_element_type_715 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_123: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_122, view_1057);  add_122 = view_1057 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_1058: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_123, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_125: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_62 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 258, constant_args_idx = 257, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_124, 'X_ptr': view_1058, 'W_ptr': arg347_1, 'RSTD_ptr': empty_125}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_124 = view_1058 = arg347_1 = empty_125 = None
        getitem_248: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_62['Y_ptr'];  triton_kernel_wrapper_functional_proxy_62 = None
        
         # File: /app/low_bit_inference/model.py:206 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_716: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_248, torch.float32)
        unsqueeze_626: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_716, 2);  convert_element_type_716 = None
        permute_342: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg348_1, [1, 0]);  arg348_1 = None
        convert_element_type_717: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_342, torch.float32);  permute_342 = None
        unsqueeze_627: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_717, 0);  convert_element_type_717 = None
        mul_406: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_626, unsqueeze_627);  unsqueeze_626 = unsqueeze_627 = None
        sum_219: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_406, [1]);  mul_406 = None
        convert_element_type_718: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_219, torch.float16);  sum_219 = None
        view_1063: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_718, [1, 1, 4096]);  convert_element_type_718 = None
        view_1064: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_1063, [1, 1, -1, 128]);  view_1063 = None
        permute_343: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.permute.default(view_1064, [0, 2, 1, 3]);  view_1064 = None
        
         # File: /app/low_bit_inference/model.py:105 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze_632: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_1, 1);  convert_element_type_1 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_409: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_343, unsqueeze_632)
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_718: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_343, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_62: "f16[1, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_718);  slice_718 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_717: "f16[1, 32, 1, 64][4096, 128, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_343, 3, 0, 64);  permute_343 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_62: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_62, slice_717], -1);  neg_62 = slice_717 = None
        
         # File: /app/low_bit_inference/model.py:106 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_633: "f16[1, 1, 1, 128][128, 128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_2, 1);  convert_element_type_2 = None
        
         # File: /app/low_bit_inference/model.py:107 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_410: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_62, unsqueeze_633);  cat_62 = None
        add_124: "f16[1, 32, 1, 128][4096, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_409, mul_410);  mul_409 = mul_410 = None
        
         # File: /app/low_bit_inference/model.py:207 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_719: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_248, torch.float32)
        unsqueeze_628: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_719, 2);  convert_element_type_719 = None
        permute_344: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg349_1, [1, 0]);  arg349_1 = None
        convert_element_type_720: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_344, torch.float32);  permute_344 = None
        unsqueeze_629: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_720, 0);  convert_element_type_720 = None
        mul_407: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_628, unsqueeze_629);  unsqueeze_628 = unsqueeze_629 = None
        sum_220: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_407, [1]);  mul_407 = None
        convert_element_type_721: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_220, torch.float16);  sum_220 = None
        view_1068: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_721, [1, 1, 1024]);  convert_element_type_721 = None
        view_1069: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_1068, [1, 1, -1, 128]);  view_1068 = None
        permute_345: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_1069, [0, 2, 1, 3]);  view_1069 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_411: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(permute_345, unsqueeze_632);  unsqueeze_632 = None
        
         # File: /app/low_bit_inference/model.py:81 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_720: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_345, 3, 64, 9223372036854775807)
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_63: "f16[1, 8, 1, 64][512, 64, 64, 1]cuda:0" = torch.ops.aten.neg.default(slice_720);  slice_720 = None
        
         # File: /app/low_bit_inference/model.py:80 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_719: "f16[1, 8, 1, 64][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.slice.Tensor(permute_345, 3, 0, 64);  permute_345 = None
        
         # File: /app/low_bit_inference/model.py:82 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        cat_63: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.cat.default([neg_63, slice_719], -1);  neg_63 = slice_719 = None
        
         # File: /app/low_bit_inference/model.py:108 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_412: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_63, unsqueeze_633);  cat_63 = unsqueeze_633 = None
        add_125: "f16[1, 8, 1, 128][1024, 128, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_411, mul_412);  mul_411 = mul_412 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        index_put_62: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg351_1, [None, None, arg2_1], add_125);  add_125 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_635: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_62, 2)
        expand_160: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_635, [1, 8, 4, 2048, 128]);  unsqueeze_635 = None
        clone_64: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_160, memory_format = torch.contiguous_format);  expand_160 = None
        view_1075: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_64, [1, 32, 2048, 128]);  clone_64 = None
        
         # File: /app/low_bit_inference/model.py:208 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        convert_element_type_722: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_248, torch.float32);  getitem_248 = None
        unsqueeze_630: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_722, 2);  convert_element_type_722 = None
        permute_346: "f16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg350_1, [1, 0]);  arg350_1 = None
        convert_element_type_723: "f32[4096, 1024][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_346, torch.float32);  permute_346 = None
        unsqueeze_631: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_723, 0);  convert_element_type_723 = None
        mul_408: "f32[1, 4096, 1024][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_630, unsqueeze_631);  unsqueeze_630 = unsqueeze_631 = None
        sum_221: "f32[1, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_408, [1]);  mul_408 = None
        convert_element_type_724: "f16[1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_221, torch.float16);  sum_221 = None
        view_1073: "f16[1, 1, 1024][1024, 1024, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_724, [1, 1, 1024]);  convert_element_type_724 = None
        view_1074: "f16[1, 1, 8, 128][1024, 1024, 128, 1]cuda:0" = torch.ops.aten.reshape.default(view_1073, [1, 1, -1, 128]);  view_1073 = None
        permute_347: "f16[1, 8, 1, 128][1024, 128, 1024, 1]cuda:0" = torch.ops.aten.permute.default(view_1074, [0, 2, 1, 3]);  view_1074 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        index_put_63: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.index_put.default(arg352_1, [None, None, arg2_1], permute_347);  arg2_1 = permute_347 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:15 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        unsqueeze_637: "f16[1, 8, 1, 2048, 128][2097152, 262144, 262144, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(index_put_63, 2)
        expand_162: "f16[1, 8, 4, 2048, 128][2097152, 262144, 0, 128, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_637, [1, 8, 4, 2048, 128]);  unsqueeze_637 = None
        clone_65: "f16[1, 8, 4, 2048, 128][8388608, 1048576, 262144, 128, 1]cuda:0" = torch.ops.aten.clone.default(expand_162, memory_format = torch.contiguous_format);  expand_162 = None
        view_1076: "f16[1, 32, 2048, 128][8388608, 262144, 128, 1]cuda:0" = torch.ops.aten.reshape.default(clone_65, [1, 32, 2048, 128]);  clone_65 = None
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        full_default_63: "f16[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_62: "f16[][]cuda:0" = torch.ops.aten.full.default([], -inf, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_31: "f16[1, 1, 1, 2048][2048, 2048, 2048, 1]cuda:0" = torch.ops.aten.where.self(arg4_1, full_default_63, full_default_62);  arg4_1 = full_default_63 = full_default_62 = None
        expand_163: "f16[1, 32, 1, 2048][2048, 0, 2048, 1]cuda:0" = torch.ops.aten.expand.default(where_31, [1, 32, 1, 2048]);  where_31 = None
        _scaled_dot_product_efficient_attention_31 = torch.ops.aten._scaled_dot_product_efficient_attention.default(add_124, view_1075, view_1076, expand_163, False, scale = 0.08838834764831845);  add_124 = view_1075 = view_1076 = expand_163 = None
        getitem_250: "f16[1, 32, 1, 128][4096, 128, 4096, 1]cuda:0" = _scaled_dot_product_efficient_attention_31[0];  _scaled_dot_product_efficient_attention_31 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_126: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_348: "f16[1, 1, 32, 128][4096, 4096, 128, 1]cuda:0" = torch.ops.aten.permute.default(getitem_250, [0, 2, 1, 3]);  getitem_250 = None
        
         # File: /app/low_bit_inference/model.py:233 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_1077: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(permute_348, [1, 1, -1]);  permute_348 = None
        
         # File: /app/low_bit_inference/model.py:234 in forward, code: attn_output = self.o_proj(attn_output)
        view_1078: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(view_1077, [1, 4096]);  view_1077 = None
        convert_element_type_725: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1078, torch.float32);  view_1078 = None
        unsqueeze_638: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_725, 2);  convert_element_type_725 = None
        permute_349: "f16[4096, 4096][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg353_1, [1, 0]);  arg353_1 = None
        convert_element_type_726: "f32[4096, 4096][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_349, torch.float32);  permute_349 = None
        unsqueeze_639: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_726, 0);  convert_element_type_726 = None
        mul_413: "f32[1, 4096, 4096][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_638, unsqueeze_639);  unsqueeze_638 = unsqueeze_639 = None
        sum_222: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_413, [1]);  mul_413 = None
        convert_element_type_727: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_222, torch.float16);  sum_222 = None
        view_1079: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_727, [1, 1, 4096]);  convert_element_type_727 = None
        
         # File: /app/low_bit_inference/model.py:273 in forward, code: hidden_states = residual + hidden_states
        add_126: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_123, view_1079);  add_123 = view_1079 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_1080: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_126, [-1, 4096])
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_127: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_63 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 259, constant_args_idx = 258, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_126, 'X_ptr': view_1080, 'W_ptr': arg354_1, 'RSTD_ptr': empty_127}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_126 = view_1080 = arg354_1 = empty_127 = None
        getitem_254: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_63['Y_ptr'];  triton_kernel_wrapper_functional_proxy_63 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:447 in rms_norm_forward, code: Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)
        empty_128: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.empty.memory_format([1, 4096], dtype = torch.float16, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/model.py:124 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        convert_element_type_728: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_254, torch.float32)
        unsqueeze_640: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_728, 2);  convert_element_type_728 = None
        permute_350: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg355_1, [1, 0]);  arg355_1 = None
        convert_element_type_729: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_350, torch.float32);  permute_350 = None
        unsqueeze_641: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_729, 0);  convert_element_type_729 = None
        mul_414: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_640, unsqueeze_641);  unsqueeze_640 = unsqueeze_641 = None
        sum_223: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_414, [1]);  mul_414 = None
        convert_element_type_730: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_223, torch.float16);  sum_223 = None
        view_1085: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_730, [1, 1, 14336]);  convert_element_type_730 = None
        convert_element_type_731: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1085, torch.float32);  view_1085 = None
        sigmoid_31: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.sigmoid.default(convert_element_type_731)
        mul_415: "f32[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_731, sigmoid_31);  convert_element_type_731 = sigmoid_31 = None
        convert_element_type_732: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_415, torch.float16);  mul_415 = None
        convert_element_type_733: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_254, torch.float32);  getitem_254 = None
        unsqueeze_642: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_733, 2);  convert_element_type_733 = None
        permute_351: "f16[4096, 14336][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg356_1, [1, 0]);  arg356_1 = None
        convert_element_type_734: "f32[4096, 14336][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_351, torch.float32);  permute_351 = None
        unsqueeze_643: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_734, 0);  convert_element_type_734 = None
        mul_416: "f32[1, 4096, 14336][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_642, unsqueeze_643);  unsqueeze_642 = unsqueeze_643 = None
        sum_224: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_416, [1]);  mul_416 = None
        convert_element_type_735: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_224, torch.float16);  sum_224 = None
        view_1089: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_735, [1, 1, 14336]);  convert_element_type_735 = None
        mul_417: "f16[1, 1, 14336][14336, 14336, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_732, view_1089);  convert_element_type_732 = view_1089 = None
        view_1090: "f16[1, 14336][14336, 1]cuda:0" = torch.ops.aten.reshape.default(mul_417, [1, 14336]);  mul_417 = None
        convert_element_type_736: "f32[1, 14336][14336, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1090, torch.float32);  view_1090 = None
        unsqueeze_644: "f32[1, 14336, 1][14336, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_736, 2);  convert_element_type_736 = None
        permute_352: "f16[14336, 4096][1, 14336]cuda:0" = torch.ops.aten.permute.default(arg357_1, [1, 0]);  arg357_1 = None
        convert_element_type_737: "f32[14336, 4096][1, 14336]cuda:0" = torch.ops.prims.convert_element_type.default(permute_352, torch.float32);  permute_352 = None
        unsqueeze_645: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_737, 0);  convert_element_type_737 = None
        mul_418: "f32[1, 14336, 4096][14336, 1, 14336]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_644, unsqueeze_645);  unsqueeze_644 = unsqueeze_645 = None
        sum_225: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_418, [1]);  mul_418 = None
        convert_element_type_738: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_225, torch.float16);  sum_225 = None
        view_1091: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_738, [1, 1, 4096]);  convert_element_type_738 = None
        
         # File: /app/low_bit_inference/model.py:279 in forward, code: hidden_states = residual + hidden_states
        add_127: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.add.Tensor(add_126, view_1091);  add_126 = view_1091 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:443 in rms_norm_forward, code: X = X.view(-1, dim)
        view_1092: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(add_127, [-1, 4096]);  add_127 = None
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:451 in rms_norm_forward, code: RSTD = torch.empty(n_rows, dtype=rstd_dtype, device=X.device)
        empty_129: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
         # File: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:461 in rms_norm_forward, code: _rms_norm_forward_kernel[(n_rows,)](
        triton_kernel_wrapper_functional_proxy_64 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 260, constant_args_idx = 259, grid = [(1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'Y_ptr': empty_128, 'X_ptr': view_1092, 'W_ptr': arg358_1, 'RSTD_ptr': empty_129}, tensors_to_clone = ['Y_ptr', 'RSTD_ptr']);  empty_128 = view_1092 = arg358_1 = empty_129 = None
        getitem_256: "f16[1, 4096][4096, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_64['Y_ptr'];  triton_kernel_wrapper_functional_proxy_64 = None
        
         # File: /app/low_bit_inference/model.py:451 in forward, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
        view_1095: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_256, [1, 1, 4096]);  getitem_256 = None
        slice_744: "f16[1, 1, 4096][4096, 4096, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1095, 1, -1, 9223372036854775807);  view_1095 = None
        view_1096: "f16[1, 4096][4096, 1]cuda:0" = torch.ops.aten.reshape.default(slice_744, [1, 4096]);  slice_744 = None
        convert_element_type_739: "f32[1, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1096, torch.float32);  view_1096 = None
        unsqueeze_646: "f32[1, 4096, 1][4096, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_739, 2);  convert_element_type_739 = None
        permute_353: "f16[4096, 128256][1, 4096]cuda:0" = torch.ops.aten.permute.default(arg359_1, [1, 0]);  arg359_1 = None
        convert_element_type_740: "f32[4096, 128256][1, 4096]cuda:0" = torch.ops.prims.convert_element_type.default(permute_353, torch.float32);  permute_353 = None
        unsqueeze_647: "f32[1, 4096, 128256][4096, 1, 4096]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_740, 0);  convert_element_type_740 = None
        mul_419: "f32[1, 4096, 128256][4096, 1, 4096]cuda:0" = torch.ops.aten.mul.Tensor(unsqueeze_646, unsqueeze_647);  unsqueeze_646 = unsqueeze_647 = None
        sum_226: "f32[1, 128256][128256, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_419, [1]);  mul_419 = None
        convert_element_type_741: "f16[1, 128256][128256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_226, torch.float16);  sum_226 = None
        view_1097: "f16[1, 1, 128256][128256, 128256, 1]cuda:0" = torch.ops.aten.reshape.default(convert_element_type_741, [1, 1, 128256]);  convert_element_type_741 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy_: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg10_1, index_put);  arg10_1 = index_put = copy_ = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__1: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg11_1, index_put_1);  arg11_1 = index_put_1 = copy__1 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__2: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg21_1, index_put_2);  arg21_1 = index_put_2 = copy__2 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__3: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg22_1, index_put_3);  arg22_1 = index_put_3 = copy__3 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__4: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg32_1, index_put_4);  arg32_1 = index_put_4 = copy__4 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__5: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg33_1, index_put_5);  arg33_1 = index_put_5 = copy__5 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__6: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg43_1, index_put_6);  arg43_1 = index_put_6 = copy__6 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__7: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg44_1, index_put_7);  arg44_1 = index_put_7 = copy__7 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__8: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg54_1, index_put_8);  arg54_1 = index_put_8 = copy__8 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__9: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg55_1, index_put_9);  arg55_1 = index_put_9 = copy__9 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__10: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg65_1, index_put_10);  arg65_1 = index_put_10 = copy__10 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__11: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg66_1, index_put_11);  arg66_1 = index_put_11 = copy__11 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__12: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg76_1, index_put_12);  arg76_1 = index_put_12 = copy__12 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__13: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg77_1, index_put_13);  arg77_1 = index_put_13 = copy__13 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__14: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg87_1, index_put_14);  arg87_1 = index_put_14 = copy__14 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__15: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg88_1, index_put_15);  arg88_1 = index_put_15 = copy__15 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__16: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg98_1, index_put_16);  arg98_1 = index_put_16 = copy__16 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__17: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg99_1, index_put_17);  arg99_1 = index_put_17 = copy__17 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__18: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg109_1, index_put_18);  arg109_1 = index_put_18 = copy__18 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__19: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg110_1, index_put_19);  arg110_1 = index_put_19 = copy__19 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__20: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg120_1, index_put_20);  arg120_1 = index_put_20 = copy__20 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__21: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg121_1, index_put_21);  arg121_1 = index_put_21 = copy__21 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__22: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg131_1, index_put_22);  arg131_1 = index_put_22 = copy__22 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__23: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg132_1, index_put_23);  arg132_1 = index_put_23 = copy__23 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__24: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg142_1, index_put_24);  arg142_1 = index_put_24 = copy__24 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__25: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg143_1, index_put_25);  arg143_1 = index_put_25 = copy__25 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__26: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg153_1, index_put_26);  arg153_1 = index_put_26 = copy__26 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__27: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg154_1, index_put_27);  arg154_1 = index_put_27 = copy__27 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__28: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg164_1, index_put_28);  arg164_1 = index_put_28 = copy__28 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__29: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg165_1, index_put_29);  arg165_1 = index_put_29 = copy__29 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__30: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg175_1, index_put_30);  arg175_1 = index_put_30 = copy__30 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__31: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg176_1, index_put_31);  arg176_1 = index_put_31 = copy__31 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__32: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg186_1, index_put_32);  arg186_1 = index_put_32 = copy__32 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__33: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg187_1, index_put_33);  arg187_1 = index_put_33 = copy__33 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__34: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg197_1, index_put_34);  arg197_1 = index_put_34 = copy__34 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__35: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg198_1, index_put_35);  arg198_1 = index_put_35 = copy__35 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__36: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg208_1, index_put_36);  arg208_1 = index_put_36 = copy__36 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__37: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg209_1, index_put_37);  arg209_1 = index_put_37 = copy__37 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__38: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg219_1, index_put_38);  arg219_1 = index_put_38 = copy__38 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__39: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg220_1, index_put_39);  arg220_1 = index_put_39 = copy__39 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__40: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg230_1, index_put_40);  arg230_1 = index_put_40 = copy__40 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__41: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg231_1, index_put_41);  arg231_1 = index_put_41 = copy__41 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__42: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg241_1, index_put_42);  arg241_1 = index_put_42 = copy__42 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__43: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg242_1, index_put_43);  arg242_1 = index_put_43 = copy__43 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__44: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg252_1, index_put_44);  arg252_1 = index_put_44 = copy__44 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__45: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg253_1, index_put_45);  arg253_1 = index_put_45 = copy__45 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__46: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg263_1, index_put_46);  arg263_1 = index_put_46 = copy__46 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__47: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg264_1, index_put_47);  arg264_1 = index_put_47 = copy__47 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__48: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg274_1, index_put_48);  arg274_1 = index_put_48 = copy__48 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__49: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg275_1, index_put_49);  arg275_1 = index_put_49 = copy__49 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__50: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg285_1, index_put_50);  arg285_1 = index_put_50 = copy__50 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__51: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg286_1, index_put_51);  arg286_1 = index_put_51 = copy__51 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__52: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg296_1, index_put_52);  arg296_1 = index_put_52 = copy__52 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__53: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg297_1, index_put_53);  arg297_1 = index_put_53 = copy__53 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__54: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg307_1, index_put_54);  arg307_1 = index_put_54 = copy__54 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__55: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg308_1, index_put_55);  arg308_1 = index_put_55 = copy__55 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__56: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg318_1, index_put_56);  arg318_1 = index_put_56 = copy__56 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__57: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg319_1, index_put_57);  arg319_1 = index_put_57 = copy__57 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__58: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg329_1, index_put_58);  arg329_1 = index_put_58 = copy__58 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__59: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg330_1, index_put_59);  arg330_1 = index_put_59 = copy__59 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__60: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg340_1, index_put_60);  arg340_1 = index_put_60 = copy__60 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__61: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg341_1, index_put_61);  arg341_1 = index_put_61 = copy__61 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:156 in update, code: self.keys.index_copy_(2, cache_position, key_states)
        copy__62: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg351_1, index_put_62);  arg351_1 = index_put_62 = copy__62 = None
        
         # File: /app/low_bit_inference/optims/cache_optim.py:157 in update, code: self.values.index_copy_(2, cache_position, value_states)
        copy__63: "f16[1, 8, 2048, 128][2097152, 262144, 128, 1]cuda:0" = torch.ops.aten.copy_.default(arg352_1, index_put_63);  arg352_1 = index_put_63 = copy__63 = None
        return (view_1097,)
        