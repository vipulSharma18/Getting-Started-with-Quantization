# AOT ID: ['0_inference']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from cmath import nanj
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()
empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p


# kernel path: /tmp/torchinductor_root/dq/cdqk3nibulbp6wlofbpnfly3ltdohevtedcgpxhr2krsjnwnwbks.py
# Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation], Original ATen: []
# Source node to ATen node mapping:
#   triton_kernel_wrapper_mutation => triton_kernel_wrapper_mutation_64
# Graph fragment:
#   %triton_kernel_wrapper_mutation_64 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 1, constant_args_idx: 0, grid: [(%arg0_1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty, X_ptr: %view_4, W_ptr: %arg8_1, RSTD_ptr: %empty_1}})
triton_poi_fused_0 = async_compile.triton('triton_poi_fused_0', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 65536}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*i64', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.000147528},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x1 = xindex // 4096
    x0 = (xindex % 4096)
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x1), None, eviction_policy='evict_last')
    tmp1 = tl.full([XBLOCK], 128256, tl.int32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp0 < 0
    tmp4 = tl.where(tmp3, tmp2, tmp0)
    tl.device_assert((0 <= tmp4) & (tmp4 < 128256), "index out of bounds: 0 <= tmp4 < 128256")
    tmp6 = tl.load(in_ptr1 + (x0 + 4096*tmp4), None).to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp6, None)


def get_args():
    arg_0 = rand_strided((1, 9), (9, 1), device='cuda:0', dtype=torch.int64)
    arg_1 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 36864,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_0.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_0.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000147528
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# Original path: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:102
_rms_norm_forward_kernel_0 = async_compile.triton('_rms_norm_forward_kernel', '''

import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.user_autotune(
    configs=[{'num_warps': 8, 'num_stages': 3}],
    inductor_meta={'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_name': '_rms_norm_forward_kernel_0', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False},
    triton_meta={'signature': {'Y_ptr': '*fp16', 'Y_row_stride': 'i32', 'X_ptr': '*fp16', 'X_row_stride': 'i32', 'W_ptr': '*fp16', 'W_row_stride': 'constexpr', 'RSTD_ptr': '*fp32', 'RSTD_row_stride': 'constexpr', 'n_cols': 'i32', 'eps': 'fp32', 'offset': 'fp32', 'casting_mode': 'constexpr', 'BLOCK_SIZE': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {'W_row_stride': 1, 'RSTD_row_stride': 1, 'casting_mode': 0, 'BLOCK_SIZE': 4096}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    filename=__file__,
    custom_kernel=True,
)
@triton.jit
def _rms_norm_forward_kernel(
    Y_ptr,
    Y_row_stride,
    X_ptr,
    X_row_stride,
    W_ptr,
    W_row_stride,
    RSTD_ptr,
    RSTD_row_stride,
    n_cols,
    eps,
    offset,
    casting_mode: tl.constexpr,  # constexpr so the `if` blocks can be optimized out
    BLOCK_SIZE: tl.constexpr,
):
    """
    y_i = (x_i / (RMS)) * (offset + wi), RMS = sqrt(sum(x_i^2) / N)

    Reference:
    1. https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html
    2. https://github.com/unslothai/unsloth/blob/fd753fed99ed5f10ef8a9b7139588d9de9ddecfb/unsloth/kernels/rms_layernorm.py#L22
    3. https://arxiv.org/pdf/1910.07467
    """

    row_idx = tl.program_id(0).to(tl.int64)
    col_offsets = tl.arange(0, BLOCK_SIZE)
    mask = col_offsets < n_cols

    Y_ptr += row_idx * Y_row_stride
    X_ptr += row_idx * X_row_stride
    RSTD_ptr += row_idx * RSTD_row_stride

    X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0)
    X_row_dtype = X_row.dtype
    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0)

    # On Llama, only rstd is computed on fp32
    if casting_mode == _CASTING_MODE_LLAMA:
        X_row = X_row.to(tl.float32)

    # Gemma computes everything on fp32, and then casts back the output to the original dtype
    if casting_mode == _CASTING_MODE_GEMMA:
        W_row = W_row.to(tl.float32)
        X_row = X_row.to(tl.float32)

    if casting_mode == _CASTING_MODE_NONE:
        eps = eps.to(X_row_dtype)
        offset = offset.to(X_row_dtype)

    mean_square = tl.sum(X_row * X_row, axis=0) / n_cols
    rstd = rsqrt(mean_square + eps)

    # We can save time by caching rms with minimal memory overhead
    # because rms is much smaller compared to X_row, as rms is for each row.
    # However, on the computation side, it can save 4 operations (*, sum, /, sqrt).
    tl.store(RSTD_ptr, rstd)

    X_row = X_row * rstd

    # On Llama, the multiplication with the weight is done on the original dtype
    if casting_mode == _CASTING_MODE_LLAMA:
        X_row = X_row.to(X_row_dtype)

    Y_row = X_row * (offset + W_row)

    if casting_mode == _CASTING_MODE_GEMMA:
        Y_row = Y_row.to(X_row_dtype)

    tl.store(Y_ptr + col_offsets, Y_row, mask=mask)

_CASTING_MODE_LLAMA: triton.language.core.constexpr = tl.constexpr(0)

_CASTING_MODE_GEMMA: triton.language.core.constexpr = tl.constexpr(1)

_CASTING_MODE_NONE: triton.language.core.constexpr = tl.constexpr(-1)
from triton.language.extra.libdevice import rsqrt as rsqrt
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/qe/cqengdpefzbggwwhisb2owoaen35w6xb6p6ftck2hwkyst4u2hy7.py
# Topologically Sorted Source Nodes: [linear], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   linear => mm
# Graph fragment:
#   %mm : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_8, %permute_1), kwargs = {})
triton_tem_fused_mm_1 = async_compile.triton('triton_tem_fused_mm_1', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=2,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_1', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.033701888},
)
@triton.jit
def triton_tem_fused_mm_1(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 4096
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 4096
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4096

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 4096*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 4096*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)


def get_args():
    arg_0 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = 9
    return arg_0, arg_1, arg_2, arg_3, 128, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_1.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.033701888
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/ud/cudf7phdt3ujo3e7rjtnj3flfq27w2iyn7q6deif3ij3oped2szx.py
# Topologically Sorted Source Nodes: [position_ids_expanded], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   position_ids_expanded => convert_element_type
# Graph fragment:
#   %convert_element_type : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%unsqueeze_2, torch.float32), kwargs = {})
triton_poi_fused__to_copy_2 = async_compile.triton('triton_poi_fused__to_copy_2', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*i64', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_2', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 1.08e-07},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_2(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((1, 9), (9, 1), device='cuda:0', dtype=torch.int64)
    arg_1 = rand_strided((1, 1, 9), (9, 9, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 9,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_2.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_2.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.08e-07
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/rs/crs45qjgwbnivuv54fcuchfghstoxz5vedrxp6g6les5vq6joupx.py
# Topologically Sorted Source Nodes: [position_ids_expanded, matmul], Original ATen: [aten._to_copy, aten.view, aten.bmm]
# Source node to ATen node mapping:
#   matmul => bmm, view_1
#   position_ids_expanded => convert_element_type
# Graph fragment:
#   %convert_element_type : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%unsqueeze_2, torch.float32), kwargs = {})
#   %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%expand_2, [1, 1, %arg0_1]), kwargs = {})
#   %bmm : [num_users=1] = call_function[target=torch.ops.aten.bmm.default](args = (%expand_1, %view_1), kwargs = {})
triton_tem_fused__to_copy_bmm_view_3 = async_compile.triton('triton_tem_fused__to_copy_bmm_view_3', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp32', 'arg_B': '*fp32', 'out_ptr0': '*fp32', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused__to_copy_bmm_view_3', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 2.596e-06},
)
@triton.jit
def triton_tem_fused__to_copy_bmm_view_3(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 16
    BLOCK_K : tl.constexpr = 16
    A = arg_A
    B = arg_B

    M = 64
    N = ks0
    K = 1

    stride_aq = 0
    stride_am = 1
    stride_ak = 0

    stride_bq = ks0
    stride_bk = ks0
    stride_bn = 1

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N

    rk = tl.arange(0, BLOCK_K)

    idx_q = tl.program_id(1)  # batch dimension for BMM
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak + idx_q*stride_aq)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn + idx_q*stride_bq)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_q = tl.program_id(1)  # batch dimension for BMM
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + idx_m*ks0 + 64*idx_q*ks0
    tl.store(out_ptr0 + (tl.broadcast_to(idx_n + idx_m*ks0, acc.shape)), acc, mask)


def get_args():
    arg_0 = rand_strided((64,), (1,), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 1, 9), (9, 9, 1), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 64, 9), (576, 9, 1), device='cuda:0', dtype=torch.float32)
    arg_3 = 9
    return arg_0, arg_1, arg_2, arg_3, 1, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused__to_copy_bmm_view_3.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused__to_copy_bmm_view_3.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.596e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/my/cmy3i5sbxrojdcasdt5iqbn5v3xkylpywyutsqdn2o65rr6xnped.py
# Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   linear_1 => mm_1
# Graph fragment:
#   %mm_1 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_13, %permute_3), kwargs = {})
triton_tem_fused_mm_4 = async_compile.triton('triton_tem_fused_mm_4', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=5,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_4', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.008480768},
)
@triton.jit
def triton_tem_fused_mm_4(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 1024
    K = 4096
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 4096
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4096

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 4096*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 1024*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 4096*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 1024*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)


def get_args():
    arg_0 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((9, 1024), (1024, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = 9
    return arg_0, arg_1, arg_2, arg_3, 16, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_4.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.008480768
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/z4/cz4evsl64os5pvhgxct4zpfbwauc2ildfqli4e3q23mn2innx24v.py
# Topologically Sorted Source Nodes: [keys], Original ATen: [aten.zeros]
# Source node to ATen node mapping:
#   keys => full_default
# Graph fragment:
#   %full_default : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([1, 8, %arg322_1, 128], 0), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
triton_poi_fused_zeros_5 = async_compile.triton('triton_poi_fused_zeros_5', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_zeros_5', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 0, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.004194304},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_zeros_5(out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = 0.0
    tl.store(out_ptr0 + (x0), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, 2097152,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_zeros_5.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.004194304
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/kh/ckh3awb2usi42s7ivqodzibcdjslbl3rf345frdgz2mbzlg5abjy.py
# Topologically Sorted Source Nodes: [keys, mul_4, cat_2, mul_5, k_embed, index_copy_], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
# Source node to ATen node mapping:
#   cat_2 => cat_1
#   index_copy_ => index_put
#   k_embed => add_159
#   keys => full_default
#   mul_4 => mul_161
#   mul_5 => mul_178
# Graph fragment:
#   %full_default : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([1, 8, %arg322_1, 128], 0), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %mul_161 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_4, %unsqueeze_4), kwargs = {})
#   %cat_1 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg_1, %slice_6], -1), kwargs = {})
#   %mul_178 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_1, %unsqueeze_5), kwargs = {})
#   %add_159 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_161, %mul_178), kwargs = {})
#   %index_put : [num_users=2] = call_function[target=torch.ops.aten.index_put_.default](args = (%full_default, [None, None, %arg3_1], %add_159), kwargs = {})
triton_poi_fused_add_cat_index_copy_mul_zeros_6 = async_compile.triton('triton_poi_fused_add_cat_index_copy_mul_zeros_6', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 128, 'x': 128}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*i64', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'ks0': 'i32', 'ks1': 'i32', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_cat_index_copy_mul_zeros_6', 'mutated_arg_names': ['out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 3.924e-05},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_cat_index_copy_mul_zeros_6(in_ptr0, in_ptr1, in_ptr2, out_ptr0, ks0, ks1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 128
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex // 8
    x3 = xindex
    y0 = yindex
    x1 = (xindex % 8)
    tmp0 = tl.load(in_ptr0 + (x2), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr1 + (y0 + 128*x3), ymask & xmask, eviction_policy='evict_last').to(tl.float32)
    tmp7 = tl.load(in_ptr2 + (x2 + ks1*((y0 % 64))), xmask & ymask, eviction_policy='evict_last')
    tmp1 = ks0
    tmp2 = tmp0 + tmp1
    tmp3 = tmp0 < 0
    tmp4 = tl.where(tmp3, tmp2, tmp0)
    tl.device_assert(((0 <= tmp4) & (tmp4 < ks0)) | ~(xmask), "index out of bounds: 0 <= tmp4 < ks0")
    tmp8 = tl_math.cos(tmp7)
    tmp9 = 1.0
    tmp10 = tmp8 * tmp9
    tmp11 = tmp10.to(tl.float32)
    tmp12 = tmp6 * tmp11
    tmp13 = y0
    tmp14 = tl.full([1, 1], 0, tl.int64)
    tmp15 = tmp13 >= tmp14
    tmp16 = tl.full([1, 1], 64, tl.int64)
    tmp17 = tmp13 < tmp16
    tmp18 = tl.load(in_ptr1 + (64 + 128*x3 + (y0)), ymask & xmask & tmp17, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp19 = -tmp18
    tmp20 = tl.full(tmp19.shape, 0.0, tmp19.dtype)
    tmp21 = tl.where(tmp17, tmp19, tmp20)
    tmp22 = tmp13 >= tmp16
    tmp23 = tl.full([1, 1], 128, tl.int64)
    tmp24 = tmp13 < tmp23
    tmp25 = tl.load(in_ptr1 + (128*x3 + ((-64) + y0)), ymask & xmask & tmp22, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp26 = tl.where(tmp17, tmp21, tmp25)
    tmp27 = tl_math.sin(tmp7)
    tmp28 = tmp27 * tmp9
    tmp29 = tmp28.to(tl.float32)
    tmp30 = tmp26 * tmp29
    tmp31 = tmp12 + tmp30
    tl.store(out_ptr0 + (y0 + 128*tmp4 + 128*ks0*x1), tmp31, xmask & ymask)


def get_args():
    arg_0 = rand_strided((9,), (1,), device='cuda:0', dtype=torch.int64)
    arg_1 = rand_strided((9, 1024), (1024, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 64, 9), (576, 9, 1), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = 2048
    arg_5 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 128, 72,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_cat_index_copy_mul_zeros_6.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.924e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/fr/cfrasxyopfpsjaxpw67iz5rp6upua7wso7cjubi6xqhfgmqu6jlz.py
# Topologically Sorted Source Nodes: [values, index_copy__1], Original ATen: [aten.zeros, aten.index_copy]
# Source node to ATen node mapping:
#   index_copy__1 => index_put_1
#   values => full_default_1
# Graph fragment:
#   %full_default_1 : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([1, 8, %arg322_1, 128], 0), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %index_put_1 : [num_users=2] = call_function[target=torch.ops.aten.index_put_.default](args = (%full_default_1, [None, None, %arg3_1], %permute_6), kwargs = {})
triton_poi_fused_index_copy_zeros_7 = async_compile.triton('triton_poi_fused_index_copy_zeros_7', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*i64', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_index_copy_zeros_7', 'mutated_arg_names': ['out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 3.6936e-05},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_index_copy_zeros_7(in_ptr0, in_ptr1, out_ptr0, ks0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex // 1024
    x3 = xindex
    x0 = (xindex % 128)
    x1 = ((xindex // 128) % 8)
    tmp0 = tl.load(in_ptr0 + (x2), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr1 + (x3), xmask).to(tl.float32)
    tmp1 = ks0
    tmp2 = tmp0 + tmp1
    tmp3 = tmp0 < 0
    tmp4 = tl.where(tmp3, tmp2, tmp0)
    tl.device_assert(((0 <= tmp4) & (tmp4 < ks0)) | ~(xmask), "index out of bounds: 0 <= tmp4 < ks0")
    tl.store(out_ptr0 + (x0 + 128*tmp4 + 128*ks0*x1), tmp6, xmask)


def get_args():
    arg_0 = rand_strided((9,), (1,), device='cuda:0', dtype=torch.int64)
    arg_1 = rand_strided((9, 1024), (1024, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = 2048
    return arg_0, arg_1, arg_2, arg_3, 9216,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_index_copy_zeros_7.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.6936e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/wt/cwt5k4s56gxnhaev4mhd5yakbk5os3gojaik36ske2fzb2rw4ktu.py
# Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed, query, attn_output], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
# Source node to ATen node mapping:
#   attn_output => _scaled_dot_product_efficient_attention
#   cat_1 => cat
#   mul_2 => mul_136
#   mul_3 => mul_153
#   q_embed => add_135
#   query => clone_4
# Graph fragment:
#   %mul_136 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_2, %unsqueeze_4), kwargs = {})
#   %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg, %slice_4], -1), kwargs = {})
#   %mul_153 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat, %unsqueeze_5), kwargs = {})
#   %add_135 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_136, %mul_153), kwargs = {})
#   %clone_4 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%add_135,), kwargs = {memory_format: torch.contiguous_format})
#   %_scaled_dot_product_efficient_attention : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%clone_4, %view_21, %view_22, %expand_8, False), kwargs = {scale: 0.08838834764831845})
triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8 = async_compile.triton('triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 65536}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'out_ptr0': '*fp16', 'ks0': 'i32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.00014976},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8(in_ptr0, in_ptr1, out_ptr0, ks0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x3 = xindex
    x0 = (xindex % 128)
    x2 = xindex // 4096
    x4 = xindex // 128
    x1 = ((xindex // 128) % 32)
    tmp0 = tl.load(in_ptr0 + (x3), None).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (x2 + ks0*((x0 % 64))), None, eviction_policy='evict_last')
    tmp2 = tl_math.cos(tmp1)
    tmp3 = 1.0
    tmp4 = tmp2 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tmp6 = tmp0 * tmp5
    tmp7 = x0
    tmp8 = tl.full([1], 0, tl.int64)
    tmp9 = tmp7 >= tmp8
    tmp10 = tl.full([1], 64, tl.int64)
    tmp11 = tmp7 < tmp10
    tmp12 = tl.load(in_ptr0 + (64 + 128*x4 + (x0)), tmp11, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp13 = -tmp12
    tmp14 = tl.full(tmp13.shape, 0.0, tmp13.dtype)
    tmp15 = tl.where(tmp11, tmp13, tmp14)
    tmp16 = tmp7 >= tmp10
    tmp17 = tl.full([1], 128, tl.int64)
    tmp18 = tmp7 < tmp17
    tmp19 = tl.load(in_ptr0 + (128*x4 + ((-64) + x0)), tmp16, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp20 = tl.where(tmp11, tmp15, tmp19)
    tmp21 = tl_math.sin(tmp1)
    tmp22 = tmp21 * tmp3
    tmp23 = tmp22.to(tl.float32)
    tmp24 = tmp20 * tmp23
    tmp25 = tmp6 + tmp24
    tl.store(out_ptr0 + (x0 + 128*x2 + 128*ks0*x1), tmp25, None)


def get_args():
    arg_0 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 64, 9), (576, 9, 1), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 32, 9, 128), (36864, 1152, 128, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = 9
    return arg_0, arg_1, arg_2, arg_3, 36864,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00014976
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/6c/c6cxxictw3snrck6byxvcehztraeglotku34um4l2ziqmofmhifk.py
# Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed, query, attn_output], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
# Source node to ATen node mapping:
#   attn_output => _scaled_dot_product_efficient_attention
#   cat_1 => cat
#   mul_2 => mul_136
#   mul_3 => mul_153
#   q_embed => add_135
#   query => clone_4
# Graph fragment:
#   %mul_136 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_2, %unsqueeze_4), kwargs = {})
#   %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg, %slice_4], -1), kwargs = {})
#   %mul_153 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat, %unsqueeze_5), kwargs = {})
#   %add_135 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_136, %mul_153), kwargs = {})
#   %clone_4 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%add_135,), kwargs = {memory_format: torch.contiguous_format})
#   %_scaled_dot_product_efficient_attention : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%clone_4, %view_21, %view_22, %expand_8, False), kwargs = {scale: 0.08838834764831845})
triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9 = async_compile.triton('triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32', 'ks1': 'i32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.016777216},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9(in_ptr0, out_ptr0, ks0, ks1, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % ks0)
    x1 = xindex // ks0
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 128*ks1*(x1 // 4)), None, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp0, None)


def get_args():
    arg_0 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 32, 2048, 128), (8388608, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = 262144
    arg_3 = 2048
    return arg_0, arg_1, arg_2, arg_3, 8388608,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.016777216
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/h6/ch6teqgmuubuhhm2bphmgfkbvqya7cgwncdw7hfmu6os2xp6g2qe.py
# Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed, query, attn_output, mul_7, cat_3, mul_8, q_embed_1, query_1, attn_output_4, mul_12, cat_5, mul_13, q_embed_2, query_2, attn_output_8], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
# Source node to ATen node mapping:
#   attn_output => _scaled_dot_product_efficient_attention
#   attn_output_4 => _scaled_dot_product_efficient_attention_1
#   attn_output_8 => _scaled_dot_product_efficient_attention_2
#   cat_1 => cat
#   cat_3 => cat_2
#   cat_5 => cat_4
#   mul_12 => mul_954
#   mul_13 => mul_971
#   mul_2 => mul_136
#   mul_3 => mul_153
#   mul_7 => mul_545
#   mul_8 => mul_562
#   q_embed => add_135
#   q_embed_1 => add_464
#   q_embed_2 => add_793
#   query => clone_4
#   query_1 => clone_7
#   query_2 => clone_10
# Graph fragment:
#   %mul_136 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_2, %unsqueeze_4), kwargs = {})
#   %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg, %slice_4], -1), kwargs = {})
#   %mul_153 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat, %unsqueeze_5), kwargs = {})
#   %add_135 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_136, %mul_153), kwargs = {})
#   %clone_4 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%add_135,), kwargs = {memory_format: torch.contiguous_format})
#   %_scaled_dot_product_efficient_attention : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%clone_4, %view_21, %view_22, %expand_8, False), kwargs = {scale: 0.08838834764831845})
#   %mul_545 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_13, %unsqueeze_10), kwargs = {})
#   %cat_2 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg_2, %slice_27], -1), kwargs = {})
#   %mul_562 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_2, %unsqueeze_11), kwargs = {})
#   %add_464 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_545, %mul_562), kwargs = {})
#   %clone_7 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%add_464,), kwargs = {memory_format: torch.contiguous_format})
#   %_scaled_dot_product_efficient_attention_1 : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%clone_7, %view_55, %view_56, %expand_13, False), kwargs = {scale: 0.08838834764831845})
#   %mul_954 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_24, %unsqueeze_16), kwargs = {})
#   %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg_4, %slice_50], -1), kwargs = {})
#   %mul_971 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_4, %unsqueeze_17), kwargs = {})
#   %add_793 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_954, %mul_971), kwargs = {})
#   %clone_10 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%add_793,), kwargs = {memory_format: torch.contiguous_format})
#   %_scaled_dot_product_efficient_attention_2 : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%clone_10, %view_89, %view_90, %expand_18, False), kwargs = {scale: 0.08838834764831845})
triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10 = async_compile.triton('triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*i1', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'out_ptr2': '*fp16', 'ks0': 'i32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.000129024},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10(in_ptr0, out_ptr0, out_ptr1, out_ptr2, ks0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % ks0)
    x1 = xindex // ks0
    tmp0 = tl.load(in_ptr0 + (x2), xmask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = 0.0
    tmp2 = float("-inf")
    tmp3 = tl.where(tmp0, tmp1, tmp2)
    tl.store(out_ptr0 + (x0 + 8*x1*((7 + ks0) // 8)), tmp3, xmask)
    tl.store(out_ptr1 + (x0 + 8*x1*((7 + ks0) // 8)), tmp3, xmask)
    tl.store(out_ptr2 + (x0 + 8*x1*((7 + ks0) // 8)), tmp3, xmask)


def get_args():
    arg_0 = rand_strided((1, 1, 9, 2048), (18432, 18432, 2048, 1), device='cuda:0', dtype=torch.bool)
    arg_1 = rand_strided((1, 1, 9, 2048), (18432, 0, 2048, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 1, 9, 2048), (18432, 0, 2048, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 1, 9, 2048), (18432, 0, 2048, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = 2048
    return arg_0, arg_1, arg_2, arg_3, arg_4, 18432,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000129024
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/qe/cqec5nj7rch7nvr7siogsarzilrr6jh2dx342ncdxsivwa5vfhnx.py
# Topologically Sorted Source Nodes: [attn_output_3, triton_kernel_wrapper_mutation_1], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   attn_output_3 => mm_3
#   triton_kernel_wrapper_mutation_1 => triton_kernel_wrapper_mutation_63
# Graph fragment:
#   %mm_3 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_24, %permute_8), kwargs = {})
#   %triton_kernel_wrapper_mutation_63 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 2, constant_args_idx: 1, grid: [(%arg0_1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_2, X_ptr: %view_26, W_ptr: %arg14_1, RSTD_ptr: %empty_3}})
triton_tem_fused_mm_11 = async_compile.triton('triton_tem_fused_mm_11', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=2,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'in_ptr2': '*i64', 'in_ptr3': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_11', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.033701888},
)
@triton.jit
def triton_tem_fused_mm_11(arg_A, arg_B, in_ptr2, in_ptr3, out_ptr0, out_ptr1, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 4096
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 4096
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4096

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 4096*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 4096*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
    tmp0 = tl.load(in_ptr2 + (tl.broadcast_to(idx_m, acc.shape)), mask, eviction_policy='evict_last')
    tmp1 = tl.full(acc.shape, 128256, tl.int32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp0 < 0
    tmp4 = tl.where(tmp3, tmp2, tmp0)
    tl.device_assert(((0 <= tl.broadcast_to(tmp4, acc.shape)) & (tl.broadcast_to(tmp4, acc.shape) < 128256)) | ~(mask), "index out of bounds: 0 <= tl.broadcast_to(tmp4, acc.shape) < 128256")
    tmp6 = tl.load(in_ptr3 + (tl.broadcast_to(idx_n + 4096*tmp4, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp7 = tmp6 + acc
    tl.store(out_ptr1 + (tl.broadcast_to(idx_n + 4096*idx_m, acc.shape)), tmp7, mask)


def get_args():
    arg_0 = rand_strided((1, 32, 9, 128), (36864, 128, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 9), (9, 1), device='cuda:0', dtype=torch.int64)
    arg_3 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_6 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 128, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_11.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.033701888
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/sx/csxlqz3vnda3n7ellahbggtlhvzvr56rjxmcc426pz35fxd4s423.py
# Topologically Sorted Source Nodes: [linear_4], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   linear_4 => mm_4
# Graph fragment:
#   %mm_4 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_30, %permute_9), kwargs = {})
triton_tem_fused_mm_12 = async_compile.triton('triton_tem_fused_mm_12', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=2,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_12', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.117772288},
)
@triton.jit
def triton_tem_fused_mm_12(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 14336
    K = 4096
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 4096
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4096

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 4096*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 14336*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 4096*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 14336*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)


def get_args():
    arg_0 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((9, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = 9
    return arg_0, arg_1, arg_2, arg_3, 448, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_12.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.117772288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/ef/cefyqvx5dygn3o2e3vv5sbisk44vpcspypyugy7ipctyvqjuaejw.py
# Topologically Sorted Source Nodes: [silu, linear_5, mul_6], Original ATen: [aten.silu, aten.mm, aten.mul]
# Source node to ATen node mapping:
#   linear_5 => mm_5
#   mul_6 => mul_443
#   silu => convert_element_type_13, convert_element_type_14, mul_426, sigmoid
# Graph fragment:
#   %convert_element_type_13 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%view_31, torch.float32), kwargs = {})
#   %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_13,), kwargs = {})
#   %mul_426 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_13, %sigmoid), kwargs = {})
#   %convert_element_type_14 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_426, torch.float16), kwargs = {})
#   %mm_5 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_34, %permute_10), kwargs = {})
#   %mul_443 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_14, %view_35), kwargs = {})
triton_tem_fused_mm_mul_silu_13 = async_compile.triton('triton_tem_fused_mm_mul_silu_13', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=2,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'in_ptr2': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_mul_silu_13', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.117772288},
)
@triton.jit
def triton_tem_fused_mm_mul_silu_13(arg_A, arg_B, in_ptr2, out_ptr1, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 14336
    K = 4096
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 4096
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4096

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 4096*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 14336*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 4096*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 14336*idx_m
    tmp0 = tl.load(in_ptr2 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tmp2 = tl.sigmoid(tmp1)
    tmp3 = tmp1 * tmp2
    tmp4 = tmp3.to(tl.float32)
    tmp5 = tmp4 * acc
    tl.store(out_ptr1 + (tl.broadcast_to(xindex, acc.shape)), tmp5, mask)


def get_args():
    arg_0 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((9, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 9, 14336), (129024, 14336, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, 448, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_mul_silu_13.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.117772288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/2x/c2xigef7cy5xtnh3bzvnukz2d66x2nrryl2okg7fct7nxoeaew5u.py
# Topologically Sorted Source Nodes: [down_proj, triton_kernel_wrapper_mutation_2], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   down_proj => mm_6
#   triton_kernel_wrapper_mutation_2 => triton_kernel_wrapper_mutation_62
# Graph fragment:
#   %mm_6 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_36, %permute_11), kwargs = {})
#   %triton_kernel_wrapper_mutation_62 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 3, constant_args_idx: 2, grid: [(%arg0_1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_4, X_ptr: %view_38, W_ptr: %arg18_1, RSTD_ptr: %empty_5}})
triton_tem_fused_mm_14 = async_compile.triton('triton_tem_fused_mm_14', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'in_ptr2': '*i64', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_14', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.117772288},
)
@triton.jit
def triton_tem_fused_mm_14(arg_A, arg_B, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 14336
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 14336
    stride_ak = 1
    stride_bk = 1
    stride_bn = 14336

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 14336*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 14336*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
    tmp0 = tl.load(in_ptr2 + (tl.broadcast_to(idx_m, acc.shape)), mask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (tl.broadcast_to(idx_n + 4096*idx_m, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tl.full(acc.shape, 128256, tl.int32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp0 < 0
    tmp4 = tl.where(tmp3, tmp2, tmp0)
    tl.device_assert(((0 <= tl.broadcast_to(tmp4, acc.shape)) & (tl.broadcast_to(tmp4, acc.shape) < 128256)) | ~(mask), "index out of bounds: 0 <= tl.broadcast_to(tmp4, acc.shape) < 128256")
    tmp6 = tl.load(in_ptr3 + (tl.broadcast_to(idx_n + 4096*tmp4, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp8 = tmp6 + tmp7
    tmp9 = tmp8 + acc
    tl.store(out_ptr1 + (tl.broadcast_to(idx_n + 4096*idx_m, acc.shape)), tmp9, mask)


def get_args():
    arg_0 = rand_strided((1, 9, 14336), (129024, 14336, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 9), (9, 1), device='cuda:0', dtype=torch.int64)
    arg_3 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_7 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 64, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_14.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_14.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.117772288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/xo/cxoaxen3k6qd3x5ah7rfozx5ud26hahqoy6vi3j72p5dga2wbckf.py
# Topologically Sorted Source Nodes: [attn_output_7], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   attn_output_7 => mm_10
# Graph fragment:
#   %mm_10 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_58, %permute_19), kwargs = {})
triton_tem_fused_mm_15 = async_compile.triton('triton_tem_fused_mm_15', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=2,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_15', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.033701888},
)
@triton.jit
def triton_tem_fused_mm_15(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 4096
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 4096
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4096

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 4096*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 4096*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)


def get_args():
    arg_0 = rand_strided((1, 32, 9, 128), (36864, 128, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = 9
    return arg_0, arg_1, arg_2, arg_3, 128, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_15.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_15.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.033701888
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/nn/cnn5ogtrrp4niyas2bn6q7tjkhngr7ndcp7p6cn2jtufdpoepikz.py
# Topologically Sorted Source Nodes: [inputs_embeds, hidden_states_2, hidden_states_3, hidden_states_6], Original ATen: [aten.embedding, aten.add]
# Source node to ATen node mapping:
#   hidden_states_2 => add_317
#   hidden_states_3 => add_370
#   hidden_states_6 => add_646
#   inputs_embeds => embedding
# Graph fragment:
#   %embedding : [num_users=2] = call_function[target=torch.ops.aten.embedding.default](args = (%arg2_1, %arg1_1, 128004), kwargs = {})
#   %add_317 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%embedding, %view_25), kwargs = {})
#   %add_370 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_317, %view_37), kwargs = {})
#   %add_646 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_370, %view_59), kwargs = {})
triton_poi_fused_add_embedding_16 = async_compile.triton('triton_poi_fused_add_embedding_16', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 65536}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*i64', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_embedding_16', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.000368712},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_embedding_16(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x1 = xindex // 4096
    x0 = (xindex % 4096)
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x1), None, eviction_policy='evict_last')
    tmp7 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp9 = tl.load(in_ptr2 + (x2), None).to(tl.float32)
    tmp11 = tl.load(in_ptr3 + (x2), None).to(tl.float32)
    tmp1 = tl.full([XBLOCK], 128256, tl.int32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp0 < 0
    tmp4 = tl.where(tmp3, tmp2, tmp0)
    tl.device_assert((0 <= tmp4) & (tmp4 < 128256), "index out of bounds: 0 <= tmp4 < 128256")
    tmp6 = tl.load(in_ptr1 + (x0 + 4096*tmp4), None).to(tl.float32)
    tmp8 = tmp6 + tmp7
    tmp10 = tmp8 + tmp9
    tmp12 = tmp10 + tmp11
    tl.store(in_out_ptr0 + (x2), tmp12, None)


def get_args():
    arg_0 = rand_strided((1, 9, 4096), (36864, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 9), (9, 1), device='cuda:0', dtype=torch.int64)
    arg_2 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 36864,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_embedding_16.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_embedding_16.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000368712
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/22/c22e4ux2spldkpuqa7lbi3a7acjxpnlefc2btpig3t2wrwcrsnhc.py
# Topologically Sorted Source Nodes: [down_proj_1, triton_kernel_wrapper_mutation_4], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   down_proj_1 => mm_13
#   triton_kernel_wrapper_mutation_4 => triton_kernel_wrapper_mutation_60
# Graph fragment:
#   %mm_13 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_70, %permute_22), kwargs = {})
#   %triton_kernel_wrapper_mutation_60 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 5, constant_args_idx: 4, grid: [(%arg0_1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_8, X_ptr: %view_72, W_ptr: %arg28_1, RSTD_ptr: %empty_9}})
triton_tem_fused_mm_17 = async_compile.triton('triton_tem_fused_mm_17', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'in_ptr2': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_17', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.117772288},
)
@triton.jit
def triton_tem_fused_mm_17(arg_A, arg_B, in_ptr2, out_ptr0, out_ptr1, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 14336
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 14336
    stride_ak = 1
    stride_bk = 1
    stride_bn = 14336

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 14336*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 14336*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
    tmp0 = tl.load(in_ptr2 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tmp0 + acc
    tl.store(out_ptr1 + (tl.broadcast_to(xindex, acc.shape)), tmp1, mask)


def get_args():
    arg_0 = rand_strided((1, 9, 14336), (129024, 14336, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 9, 4096), (36864, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_5 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 64, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_17.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.117772288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/oo/coonpjp5zdndhlbywtb6e3ivorg4m35n6u76362qaecht5f7obxu.py
# Topologically Sorted Source Nodes: [attn_output_11, triton_kernel_wrapper_mutation_5], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   attn_output_11 => mm_17
#   triton_kernel_wrapper_mutation_5 => triton_kernel_wrapper_mutation_59
# Graph fragment:
#   %mm_17 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_92, %permute_30), kwargs = {})
#   %triton_kernel_wrapper_mutation_59 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 6, constant_args_idx: 5, grid: [(%arg0_1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_10, X_ptr: %view_94, W_ptr: %arg34_1, RSTD_ptr: %empty_11}})
triton_tem_fused_mm_18 = async_compile.triton('triton_tem_fused_mm_18', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=2,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_18', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.033701888},
)
@triton.jit
def triton_tem_fused_mm_18(arg_A, arg_B, in_ptr2, in_ptr3, out_ptr0, out_ptr1, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 4096
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 4096
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4096

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 4096*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 4096*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
    tmp0 = tl.load(in_ptr2 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tl.load(in_ptr3 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2 + acc
    tl.store(out_ptr1 + (tl.broadcast_to(xindex, acc.shape)), tmp3, mask)


def get_args():
    arg_0 = rand_strided((1, 32, 9, 128), (36864, 128, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 9, 4096), (36864, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_6 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 128, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_18.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.033701888
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/m6/cm6vc3sj77lug3t535pmyybhxdskppvmtubgkabkjcfd6bkapxwj.py
# Topologically Sorted Source Nodes: [down_proj_2, triton_kernel_wrapper_mutation_6], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   down_proj_2 => mm_20
#   triton_kernel_wrapper_mutation_6 => triton_kernel_wrapper_mutation_58
# Graph fragment:
#   %mm_20 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_104, %permute_33), kwargs = {})
#   %triton_kernel_wrapper_mutation_58 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 7, constant_args_idx: 6, grid: [(%arg0_1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_12, X_ptr: %view_106, W_ptr: %arg38_1, RSTD_ptr: %empty_13}})
triton_tem_fused_mm_19 = async_compile.triton('triton_tem_fused_mm_19', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_19', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.117772288},
)
@triton.jit
def triton_tem_fused_mm_19(arg_A, arg_B, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 14336
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 14336
    stride_ak = 1
    stride_bk = 1
    stride_bn = 14336

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 14336*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 14336*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
    tmp0 = tl.load(in_ptr2 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tl.load(in_ptr3 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp3 = tl.load(in_ptr4 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp4 = tmp2 + tmp3
    tmp5 = tmp4 + acc
    tl.store(out_ptr1 + (tl.broadcast_to(xindex, acc.shape)), tmp5, mask)


def get_args():
    arg_0 = rand_strided((1, 9, 14336), (129024, 14336, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 9, 4096), (36864, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_7 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 64, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_19.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_19.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.117772288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/qb/cqbxs3dnzfzwc7gzmlmool3sy7zwtnbuihuggp3od6pf35fr35x3.py
# Topologically Sorted Source Nodes: [hidden_states_7, hidden_states_10, hidden_states_11, attn_output_15, hidden_states_14], Original ATen: [aten.add, aten.mm]
# Source node to ATen node mapping:
#   attn_output_15 => mm_24
#   hidden_states_10 => add_975
#   hidden_states_11 => add_1028
#   hidden_states_14 => add_1304
#   hidden_states_7 => add_699
# Graph fragment:
#   %add_699 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_646, %view_71), kwargs = {})
#   %add_975 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_699, %view_93), kwargs = {})
#   %add_1028 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_975, %view_105), kwargs = {})
#   %mm_24 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_126, %permute_41), kwargs = {})
#   %add_1304 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_1028, %view_127), kwargs = {})
triton_tem_fused_add_mm_20 = async_compile.triton('triton_tem_fused_add_mm_20', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=2,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_add_mm_20', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.033701888},
)
@triton.jit
def triton_tem_fused_add_mm_20(arg_A, arg_B, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr1, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 4096
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 4096
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4096

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 4096*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 4096*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tmp0 = tl.load(in_ptr2 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tl.load(in_ptr3 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp3 = tl.load(in_ptr4 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp5 = tl.load(in_ptr5 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp4 = tmp2 + tmp3
    tmp6 = tmp4 + tmp5
    tmp7 = tmp6 + acc
    tl.store(out_ptr1 + (tl.broadcast_to(xindex, acc.shape)), tmp7, mask)


def get_args():
    arg_0 = rand_strided((1, 32, 9, 128), (36864, 128, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 9, 4096), (36864, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((1, 9, 4096), (36864, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_7 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 128, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_20.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_add_mm_20.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.033701888
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/ds/cds4d7hbobjfhx33u4oa2cwjgwqsomiopuk2iditdecjx2slw7rl.py
# Topologically Sorted Source Nodes: [mul_152, cat_61, mul_153, q_embed_30, query_30, attn_output_120, mul_157, cat_63, mul_158, q_embed_31, query_31, attn_output_124], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
# Source node to ATen node mapping:
#   attn_output_120 => _scaled_dot_product_efficient_attention_30
#   attn_output_124 => _scaled_dot_product_efficient_attention_31
#   cat_61 => cat_60
#   cat_63 => cat_62
#   mul_152 => mul_12406
#   mul_153 => mul_12423
#   mul_157 => mul_12815
#   mul_158 => mul_12832
#   q_embed_30 => add_10005
#   q_embed_31 => add_10334
#   query_30 => clone_94
#   query_31 => clone_97
# Graph fragment:
#   %mul_12406 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_332, %unsqueeze_184), kwargs = {})
#   %cat_60 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg_60, %slice_694], -1), kwargs = {})
#   %mul_12423 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_60, %unsqueeze_185), kwargs = {})
#   %add_10005 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_12406, %mul_12423), kwargs = {})
#   %clone_94 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%add_10005,), kwargs = {memory_format: torch.contiguous_format})
#   %_scaled_dot_product_efficient_attention_30 : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%clone_94, %view_1041, %view_1042, %expand_158, False), kwargs = {scale: 0.08838834764831845})
#   %mul_12815 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_343, %unsqueeze_190), kwargs = {})
#   %cat_62 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg_62, %slice_717], -1), kwargs = {})
#   %mul_12832 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_62, %unsqueeze_191), kwargs = {})
#   %add_10334 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_12815, %mul_12832), kwargs = {})
#   %clone_97 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%add_10334,), kwargs = {memory_format: torch.contiguous_format})
#   %_scaled_dot_product_efficient_attention_31 : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%clone_97, %view_1075, %view_1076, %expand_163, False), kwargs = {scale: 0.08838834764831845})
triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_21 = async_compile.triton('triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_21', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*i1', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_21', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 9.216e-05},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_21(in_ptr0, out_ptr0, out_ptr1, ks0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % ks0)
    x1 = xindex // ks0
    tmp0 = tl.load(in_ptr0 + (x2), xmask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = 0.0
    tmp2 = float("-inf")
    tmp3 = tl.where(tmp0, tmp1, tmp2)
    tl.store(out_ptr0 + (x0 + 8*x1*((7 + ks0) // 8)), tmp3, xmask)
    tl.store(out_ptr1 + (x0 + 8*x1*((7 + ks0) // 8)), tmp3, xmask)


def get_args():
    arg_0 = rand_strided((1, 1, 9, 2048), (18432, 18432, 2048, 1), device='cuda:0', dtype=torch.bool)
    arg_1 = rand_strided((1, 1, 9, 2048), (18432, 0, 2048, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 1, 9, 2048), (18432, 0, 2048, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = 2048
    return arg_0, arg_1, arg_2, arg_3, 18432,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_21.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_21.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9.216e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/hd/chdaxgmog2ujclwf3xk6xhbdxc2qzywhx6gk3iqceftzofosok7k.py
# Topologically Sorted Source Nodes: [down_proj_31, triton_kernel_wrapper_mutation_64], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   down_proj_31 => mm_223
#   triton_kernel_wrapper_mutation_64 => triton_kernel_wrapper_mutation
# Graph fragment:
#   %mm_223 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_1090, %permute_352), kwargs = {})
#   %triton_kernel_wrapper_mutation : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 65, constant_args_idx: 64, grid: [(%arg0_1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_128, X_ptr: %view_1092, W_ptr: %arg328_1, RSTD_ptr: %empty_129}})
triton_tem_fused_mm_22 = async_compile.triton('triton_tem_fused_mm_22', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'in_ptr2': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_22', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.117772288},
)
@triton.jit
def triton_tem_fused_mm_22(arg_A, arg_B, in_ptr2, out_ptr1, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 14336
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 14336
    stride_ak = 1
    stride_bk = 1
    stride_bn = 14336

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 14336*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 14336*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tmp0 = tl.load(in_ptr2 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tmp0 + acc
    tl.store(out_ptr1 + (tl.broadcast_to(xindex, acc.shape)), tmp1, mask)


def get_args():
    arg_0 = rand_strided((1, 9, 14336), (129024, 14336, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 9, 4096), (36864, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, 64, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_22.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_22.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.117772288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/b6/cb6psr6groypxq6jynp5dejfqnr5zwmaq3unnfttfohxxmqrrdiq.py
# Topologically Sorted Source Nodes: [logits], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   logits => convert_element_type_517, mul_13166, sum_1
# Graph fragment:
#   %mul_13166 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%unsqueeze_196, %unsqueeze_197), kwargs = {})
#   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_13166, [1]), kwargs = {})
#   %convert_element_type_517 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_1, torch.float16), kwargs = {})
triton_red_fused_mm_23 = async_compile.triton('triton_red_fused_mm_23', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 4096},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mm_23', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 1.050929664}
)
@triton.jit
def triton_red_fused_mm_23(in_ptr0, in_ptr1, out_ptr1, ks0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 128256
    r0_numel = 4096
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp6 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + ((-4096) + r0_1 + 4096*ks0), r0_mask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tl.load(in_ptr1 + (r0_1 + 4096*x0), xmask & r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tmp1 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, R0_BLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(r0_mask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tmp8 = tmp6.to(tl.float32)
    tl.store(out_ptr1 + (x0), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 128256), (128256, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = 9
    return arg_0, arg_1, arg_2, arg_3, 128256, 4096,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_23.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mm_23.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.050929664
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1, arg27_1, arg28_1, arg29_1, arg30_1, arg31_1, arg32_1, arg33_1, arg34_1, arg35_1, arg36_1, arg37_1, arg38_1, arg39_1, arg40_1, arg41_1, arg42_1, arg43_1, arg44_1, arg45_1, arg46_1, arg47_1, arg48_1, arg49_1, arg50_1, arg51_1, arg52_1, arg53_1, arg54_1, arg55_1, arg56_1, arg57_1, arg58_1, arg59_1, arg60_1, arg61_1, arg62_1, arg63_1, arg64_1, arg65_1, arg66_1, arg67_1, arg68_1, arg69_1, arg70_1, arg71_1, arg72_1, arg73_1, arg74_1, arg75_1, arg76_1, arg77_1, arg78_1, arg79_1, arg80_1, arg81_1, arg82_1, arg83_1, arg84_1, arg85_1, arg86_1, arg87_1, arg88_1, arg89_1, arg90_1, arg91_1, arg92_1, arg93_1, arg94_1, arg95_1, arg96_1, arg97_1, arg98_1, arg99_1, arg100_1, arg101_1, arg102_1, arg103_1, arg104_1, arg105_1, arg106_1, arg107_1, arg108_1, arg109_1, arg110_1, arg111_1, arg112_1, arg113_1, arg114_1, arg115_1, arg116_1, arg117_1, arg118_1, arg119_1, arg120_1, arg121_1, arg122_1, arg123_1, arg124_1, arg125_1, arg126_1, arg127_1, arg128_1, arg129_1, arg130_1, arg131_1, arg132_1, arg133_1, arg134_1, arg135_1, arg136_1, arg137_1, arg138_1, arg139_1, arg140_1, arg141_1, arg142_1, arg143_1, arg144_1, arg145_1, arg146_1, arg147_1, arg148_1, arg149_1, arg150_1, arg151_1, arg152_1, arg153_1, arg154_1, arg155_1, arg156_1, arg157_1, arg158_1, arg159_1, arg160_1, arg161_1, arg162_1, arg163_1, arg164_1, arg165_1, arg166_1, arg167_1, arg168_1, arg169_1, arg170_1, arg171_1, arg172_1, arg173_1, arg174_1, arg175_1, arg176_1, arg177_1, arg178_1, arg179_1, arg180_1, arg181_1, arg182_1, arg183_1, arg184_1, arg185_1, arg186_1, arg187_1, arg188_1, arg189_1, arg190_1, arg191_1, arg192_1, arg193_1, arg194_1, arg195_1, arg196_1, arg197_1, arg198_1, arg199_1, arg200_1, arg201_1, arg202_1, arg203_1, arg204_1, arg205_1, arg206_1, arg207_1, arg208_1, arg209_1, arg210_1, arg211_1, arg212_1, arg213_1, arg214_1, arg215_1, arg216_1, arg217_1, arg218_1, arg219_1, arg220_1, arg221_1, arg222_1, arg223_1, arg224_1, arg225_1, arg226_1, arg227_1, arg228_1, arg229_1, arg230_1, arg231_1, arg232_1, arg233_1, arg234_1, arg235_1, arg236_1, arg237_1, arg238_1, arg239_1, arg240_1, arg241_1, arg242_1, arg243_1, arg244_1, arg245_1, arg246_1, arg247_1, arg248_1, arg249_1, arg250_1, arg251_1, arg252_1, arg253_1, arg254_1, arg255_1, arg256_1, arg257_1, arg258_1, arg259_1, arg260_1, arg261_1, arg262_1, arg263_1, arg264_1, arg265_1, arg266_1, arg267_1, arg268_1, arg269_1, arg270_1, arg271_1, arg272_1, arg273_1, arg274_1, arg275_1, arg276_1, arg277_1, arg278_1, arg279_1, arg280_1, arg281_1, arg282_1, arg283_1, arg284_1, arg285_1, arg286_1, arg287_1, arg288_1, arg289_1, arg290_1, arg291_1, arg292_1, arg293_1, arg294_1, arg295_1, arg296_1, arg297_1, arg298_1, arg299_1, arg300_1, arg301_1, arg302_1, arg303_1, arg304_1, arg305_1, arg306_1, arg307_1, arg308_1, arg309_1, arg310_1, arg311_1, arg312_1, arg313_1, arg314_1, arg315_1, arg316_1, arg317_1, arg318_1, arg319_1, arg320_1, arg321_1, arg322_1, arg323_1, arg324_1, arg325_1, arg326_1, arg327_1, arg328_1, arg329_1, arg330_1 = args
    args.clear()
    s0 = arg0_1
    s1 = arg5_1
    assert_size_stride(arg1_1, (1, s0), (s0, 1))
    assert_size_stride(arg2_1, (128256, 4096), (4096, 1))
    assert_size_stride(arg3_1, (s0, ), (1, ))
    assert_size_stride(arg4_1, (1, s0), (s0, 1))
    assert_size_stride(arg6_1, (1, 1, s0, s1), (s0*s1, s0*s1, s1, 1))
    assert_size_stride(arg7_1, (64, ), (1, ))
    assert_size_stride(arg8_1, (4096, ), (1, ))
    assert_size_stride(arg9_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg10_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg11_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg13_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg14_1, (4096, ), (1, ))
    assert_size_stride(arg15_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg16_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg17_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg18_1, (4096, ), (1, ))
    assert_size_stride(arg19_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg20_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg21_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg23_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg24_1, (4096, ), (1, ))
    assert_size_stride(arg25_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg26_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg27_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg28_1, (4096, ), (1, ))
    assert_size_stride(arg29_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg30_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg31_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg33_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg34_1, (4096, ), (1, ))
    assert_size_stride(arg35_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg36_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg37_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg38_1, (4096, ), (1, ))
    assert_size_stride(arg39_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg40_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg41_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg43_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg44_1, (4096, ), (1, ))
    assert_size_stride(arg45_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg46_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg47_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg48_1, (4096, ), (1, ))
    assert_size_stride(arg49_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg50_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg51_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg53_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg54_1, (4096, ), (1, ))
    assert_size_stride(arg55_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg56_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg57_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg58_1, (4096, ), (1, ))
    assert_size_stride(arg59_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg60_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg61_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg63_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg64_1, (4096, ), (1, ))
    assert_size_stride(arg65_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg66_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg67_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg68_1, (4096, ), (1, ))
    assert_size_stride(arg69_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg70_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg71_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg73_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg74_1, (4096, ), (1, ))
    assert_size_stride(arg75_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg76_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg77_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg78_1, (4096, ), (1, ))
    assert_size_stride(arg79_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg80_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg81_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg83_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg84_1, (4096, ), (1, ))
    assert_size_stride(arg85_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg86_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg87_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg88_1, (4096, ), (1, ))
    assert_size_stride(arg89_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg90_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg91_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg93_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg94_1, (4096, ), (1, ))
    assert_size_stride(arg95_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg96_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg97_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg98_1, (4096, ), (1, ))
    assert_size_stride(arg99_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg100_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg101_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg103_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg104_1, (4096, ), (1, ))
    assert_size_stride(arg105_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg106_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg107_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg108_1, (4096, ), (1, ))
    assert_size_stride(arg109_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg110_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg111_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg113_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg114_1, (4096, ), (1, ))
    assert_size_stride(arg115_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg116_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg117_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg118_1, (4096, ), (1, ))
    assert_size_stride(arg119_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg120_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg121_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg123_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg124_1, (4096, ), (1, ))
    assert_size_stride(arg125_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg126_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg127_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg128_1, (4096, ), (1, ))
    assert_size_stride(arg129_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg130_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg131_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg133_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg134_1, (4096, ), (1, ))
    assert_size_stride(arg135_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg136_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg137_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg138_1, (4096, ), (1, ))
    assert_size_stride(arg139_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg140_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg141_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg143_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg144_1, (4096, ), (1, ))
    assert_size_stride(arg145_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg146_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg147_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg148_1, (4096, ), (1, ))
    assert_size_stride(arg149_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg150_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg151_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg153_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg154_1, (4096, ), (1, ))
    assert_size_stride(arg155_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg156_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg157_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg158_1, (4096, ), (1, ))
    assert_size_stride(arg159_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg160_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg161_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg163_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg164_1, (4096, ), (1, ))
    assert_size_stride(arg165_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg166_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg167_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg168_1, (4096, ), (1, ))
    assert_size_stride(arg169_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg170_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg171_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg173_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg174_1, (4096, ), (1, ))
    assert_size_stride(arg175_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg176_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg177_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg178_1, (4096, ), (1, ))
    assert_size_stride(arg179_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg180_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg181_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg183_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg184_1, (4096, ), (1, ))
    assert_size_stride(arg185_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg186_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg187_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg188_1, (4096, ), (1, ))
    assert_size_stride(arg189_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg190_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg191_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg193_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg194_1, (4096, ), (1, ))
    assert_size_stride(arg195_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg196_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg197_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg198_1, (4096, ), (1, ))
    assert_size_stride(arg199_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg200_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg201_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg203_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg204_1, (4096, ), (1, ))
    assert_size_stride(arg205_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg206_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg207_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg208_1, (4096, ), (1, ))
    assert_size_stride(arg209_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg210_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg211_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg213_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg214_1, (4096, ), (1, ))
    assert_size_stride(arg215_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg216_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg217_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg218_1, (4096, ), (1, ))
    assert_size_stride(arg219_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg220_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg221_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg223_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg224_1, (4096, ), (1, ))
    assert_size_stride(arg225_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg226_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg227_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg228_1, (4096, ), (1, ))
    assert_size_stride(arg229_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg230_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg231_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg233_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg234_1, (4096, ), (1, ))
    assert_size_stride(arg235_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg236_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg237_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg238_1, (4096, ), (1, ))
    assert_size_stride(arg239_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg240_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg241_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg243_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg244_1, (4096, ), (1, ))
    assert_size_stride(arg245_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg246_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg247_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg248_1, (4096, ), (1, ))
    assert_size_stride(arg249_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg250_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg251_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg253_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg254_1, (4096, ), (1, ))
    assert_size_stride(arg255_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg256_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg257_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg258_1, (4096, ), (1, ))
    assert_size_stride(arg259_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg260_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg261_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg263_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg264_1, (4096, ), (1, ))
    assert_size_stride(arg265_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg266_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg267_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg268_1, (4096, ), (1, ))
    assert_size_stride(arg269_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg270_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg271_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg273_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg274_1, (4096, ), (1, ))
    assert_size_stride(arg275_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg276_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg277_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg278_1, (4096, ), (1, ))
    assert_size_stride(arg279_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg280_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg281_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg283_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg284_1, (4096, ), (1, ))
    assert_size_stride(arg285_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg286_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg287_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg288_1, (4096, ), (1, ))
    assert_size_stride(arg289_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg290_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg291_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg293_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg294_1, (4096, ), (1, ))
    assert_size_stride(arg295_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg296_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg297_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg298_1, (4096, ), (1, ))
    assert_size_stride(arg299_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg300_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg301_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg303_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg304_1, (4096, ), (1, ))
    assert_size_stride(arg305_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg306_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg307_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg308_1, (4096, ), (1, ))
    assert_size_stride(arg309_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg310_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg311_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg313_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg314_1, (4096, ), (1, ))
    assert_size_stride(arg315_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg316_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg317_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg318_1, (4096, ), (1, ))
    assert_size_stride(arg319_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg320_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg321_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg323_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg324_1, (4096, ), (1, ))
    assert_size_stride(arg325_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg326_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg327_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg328_1, (4096, ), (1, ))
    assert_size_stride(arg330_1, (128256, 4096), (4096, 1))
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = s0*s1
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((s0, 4096), (4096, 1), torch.float16)
        buf1 = empty_strided_cuda((s0, ), (1, ), torch.float32)
        _xnumel = 4096*s0
        buf2 = empty_strided_cuda((s0, 4096), (4096, 1), torch.float16)
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation], Original ATen: []
        triton_poi_fused_0_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_0.run(arg1_1, arg2_1, buf2, triton_poi_fused_0_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf0, 4096, buf2, 4096, arg8_1, 1, buf1, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg8_1
        buf5 = buf2; del buf2  # reuse
        # Topologically Sorted Source Nodes: [linear], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf0, arg9_1, buf5, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg9_1
        buf6 = reinterpret_tensor(buf1, (1, 1, s0), (s0, s0, 1), 0); del buf1  # reuse
        # Topologically Sorted Source Nodes: [position_ids_expanded], Original ATen: [aten._to_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_2.run(arg4_1, buf6, s0, stream=stream0)
        del arg4_1
        buf7 = empty_strided_cuda((1, 64, s0), (64*s0, s0, 1), torch.float32)
        # Topologically Sorted Source Nodes: [position_ids_expanded, matmul], Original ATen: [aten._to_copy, aten.view, aten.bmm]
        stream0 = get_raw_stream(0)
        triton_tem_fused__to_copy_bmm_view_3.run(arg7_1, buf6, buf7, s0, (15 + s0) // 16, 1, 1, stream=stream0)
        del arg7_1
        buf8 = empty_strided_cuda((s0, 1024), (1024, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf0, arg10_1, buf8, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg10_1
        _xnumel = 1024*s1
        buf9 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf9, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys, mul_4, cat_2, mul_5, k_embed, index_copy_], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf8, buf7, buf9, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf11 = buf8; del buf8  # reuse
        # Topologically Sorted Source Nodes: [linear_2], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf0, arg11_1, buf11, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg11_1
        _xnumel = 1024*s1
        buf12 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf12, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values, index_copy__1], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf11, buf12, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf14 = reinterpret_tensor(buf0, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf0  # reuse
        # Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed, query, attn_output], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf5, buf7, buf14, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        ps0 = 128*s1
        _xnumel = 4096*s1
        buf15 = empty_strided_cuda((1, 32, s1, 128), (4096*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed, query, attn_output], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf9, buf15, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf16 = empty_strided_cuda((1, 32, s1, 128), (4096*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed, query, attn_output], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf12, buf16, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = s0*s1
        buf17 = empty_strided_cuda((1, 1, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), torch.float16)
        buf48 = empty_strided_cuda((1, 1, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), torch.float16)
        buf79 = empty_strided_cuda((1, 1, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), torch.float16)
        # Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed, query, attn_output, mul_7, cat_3, mul_8, q_embed_1, query_1, attn_output_4, mul_12, cat_5, mul_13, q_embed_2, query_2, attn_output_8], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel = s0*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10.run(arg6_1, buf17, buf48, buf79, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed, query, attn_output], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf18 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf14, buf15, buf16, reinterpret_tensor(buf17, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf19 = buf18[0]
        assert_size_stride(buf19, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf18
        buf23 = reinterpret_tensor(buf14, (s0, 4096), (4096, 1), 0); del buf14  # reuse
        buf24 = buf5; del buf5  # reuse
        buf26 = empty_strided_cuda((s0, 4096), (4096, 1), torch.float16)
        # Topologically Sorted Source Nodes: [attn_output_3, triton_kernel_wrapper_mutation_1], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf19, arg13_1, arg1_1, arg2_1, buf24, buf26, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg13_1
        buf25 = reinterpret_tensor(buf6, (s0, ), (1, ), 0); del buf6  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_1], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf23, 4096, buf26, 4096, arg14_1, 1, buf25, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg14_1
        buf29 = buf26; del buf26  # reuse
        buf30 = empty_strided_cuda((s0, 14336), (14336, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_4], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf23, arg15_1, buf30, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg15_1
        buf32 = empty_strided_cuda((1, s0, 14336), (14336*s0, 14336, 1), torch.float16)
        # Topologically Sorted Source Nodes: [silu, linear_5, mul_6], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf23, arg16_1, buf30, buf32, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg16_1
        buf33 = buf23; del buf23  # reuse
        buf35 = reinterpret_tensor(buf19, (s0, 4096), (4096, 1), 0); del buf19  # reuse
        # Topologically Sorted Source Nodes: [down_proj, triton_kernel_wrapper_mutation_2], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_14.run(buf32, arg17_1, arg1_1, arg2_1, buf24, buf33, buf35, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg17_1
        buf34 = buf25; del buf25  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_2], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf29, 4096, buf35, 4096, arg18_1, 1, buf34, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg18_1
        buf38 = buf35; del buf35  # reuse
        # Topologically Sorted Source Nodes: [linear_7], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf29, arg19_1, buf38, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg19_1
        buf39 = buf11; del buf11  # reuse
        # Topologically Sorted Source Nodes: [linear_8], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf29, arg20_1, buf39, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg20_1
        _xnumel = 1024*s1
        buf40 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_1], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf40, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_1, mul_9, cat_4, mul_10, k_embed_1, index_copy__2], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf39, buf7, buf40, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf42 = buf39; del buf39  # reuse
        # Topologically Sorted Source Nodes: [linear_9], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf29, arg21_1, buf42, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg21_1
        _xnumel = 1024*s1
        buf43 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_1], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf43, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_1, index_copy__3], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf42, buf43, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf45 = reinterpret_tensor(buf29, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf29  # reuse
        # Topologically Sorted Source Nodes: [mul_7, cat_3, mul_8, q_embed_1, query_1, attn_output_4], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf38, buf7, buf45, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf46 = buf16; del buf16  # reuse
        # Topologically Sorted Source Nodes: [mul_7, cat_3, mul_8, q_embed_1, query_1, attn_output_4], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf40, buf46, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf47 = buf15; del buf15  # reuse
        # Topologically Sorted Source Nodes: [mul_7, cat_3, mul_8, q_embed_1, query_1, attn_output_4], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf43, buf47, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_7, cat_3, mul_8, q_embed_1, query_1, attn_output_4], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf49 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf45, buf46, buf47, reinterpret_tensor(buf48, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf50 = buf49[0]
        assert_size_stride(buf50, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf49
        buf54 = reinterpret_tensor(buf45, (s0, 4096), (4096, 1), 0); del buf45  # reuse
        buf55 = buf38; del buf38  # reuse
        # Topologically Sorted Source Nodes: [attn_output_7], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_15.run(buf50, arg23_1, buf55, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg23_1
        _xnumel = 4096*s0
        buf56 = reinterpret_tensor(buf24, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf24  # reuse
        # Topologically Sorted Source Nodes: [inputs_embeds, hidden_states_2, hidden_states_3, hidden_states_6], Original ATen: [aten.embedding, aten.add]
        triton_poi_fused_add_embedding_16_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_embedding_16.run(buf56, arg1_1, arg2_1, buf33, buf55, triton_poi_fused_add_embedding_16_xnumel, stream=stream0)
        del arg1_1
        del arg2_1
        buf57 = buf34; del buf34  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_3], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf54, 4096, reinterpret_tensor(buf56, (s0, 4096), (4096, 1), 0), 4096, arg24_1, 1, buf57, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg24_1
        buf60 = buf55; del buf55  # reuse
        buf61 = reinterpret_tensor(buf32, (s0, 14336), (14336, 1), 0); del buf32  # reuse
        # Topologically Sorted Source Nodes: [linear_11], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf54, arg25_1, buf61, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg25_1
        buf63 = reinterpret_tensor(buf30, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf30  # reuse
        # Topologically Sorted Source Nodes: [silu_1, linear_12, mul_11], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf54, arg26_1, buf61, buf63, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg26_1
        buf64 = buf54; del buf54  # reuse
        buf66 = buf33; del buf33  # reuse
        # Topologically Sorted Source Nodes: [down_proj_1, triton_kernel_wrapper_mutation_4], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf63, arg27_1, buf56, buf64, buf66, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg27_1
        buf65 = buf57; del buf57  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_4], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf60, 4096, buf66, 4096, arg28_1, 1, buf65, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg28_1
        buf69 = buf66; del buf66  # reuse
        # Topologically Sorted Source Nodes: [linear_14], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf60, arg29_1, buf69, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg29_1
        buf70 = buf42; del buf42  # reuse
        # Topologically Sorted Source Nodes: [linear_15], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf60, arg30_1, buf70, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg30_1
        _xnumel = 1024*s1
        buf71 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_2], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf71, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_2, mul_14, cat_6, mul_15, k_embed_2, index_copy__4], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf70, buf7, buf71, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf73 = buf70; del buf70  # reuse
        # Topologically Sorted Source Nodes: [linear_16], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf60, arg31_1, buf73, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg31_1
        _xnumel = 1024*s1
        buf74 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_2], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf74, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_2, index_copy__5], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf73, buf74, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf76 = reinterpret_tensor(buf60, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf60  # reuse
        # Topologically Sorted Source Nodes: [mul_12, cat_5, mul_13, q_embed_2, query_2, attn_output_8], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf69, buf7, buf76, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf77 = buf47; del buf47  # reuse
        # Topologically Sorted Source Nodes: [mul_12, cat_5, mul_13, q_embed_2, query_2, attn_output_8], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf71, buf77, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf78 = buf46; del buf46  # reuse
        # Topologically Sorted Source Nodes: [mul_12, cat_5, mul_13, q_embed_2, query_2, attn_output_8], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf74, buf78, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_12, cat_5, mul_13, q_embed_2, query_2, attn_output_8], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf80 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf76, buf77, buf78, reinterpret_tensor(buf79, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf81 = buf80[0]
        assert_size_stride(buf81, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf80
        buf85 = reinterpret_tensor(buf76, (s0, 4096), (4096, 1), 0); del buf76  # reuse
        buf86 = buf69; del buf69  # reuse
        buf88 = reinterpret_tensor(buf50, (s0, 4096), (4096, 1), 0); del buf50  # reuse
        # Topologically Sorted Source Nodes: [attn_output_11, triton_kernel_wrapper_mutation_5], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf81, arg33_1, buf56, buf64, buf86, buf88, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg33_1
        buf87 = buf65; del buf65  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_5], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf85, 4096, buf88, 4096, arg34_1, 1, buf87, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg34_1
        buf91 = buf88; del buf88  # reuse
        buf92 = reinterpret_tensor(buf63, (s0, 14336), (14336, 1), 0); del buf63  # reuse
        # Topologically Sorted Source Nodes: [linear_18], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf85, arg35_1, buf92, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg35_1
        buf94 = reinterpret_tensor(buf61, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf61  # reuse
        # Topologically Sorted Source Nodes: [silu_2, linear_19, mul_16], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf85, arg36_1, buf92, buf94, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg36_1
        buf95 = buf85; del buf85  # reuse
        buf97 = reinterpret_tensor(buf81, (s0, 4096), (4096, 1), 0); del buf81  # reuse
        # Topologically Sorted Source Nodes: [down_proj_2, triton_kernel_wrapper_mutation_6], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_19.run(buf94, arg37_1, buf56, buf64, buf86, buf95, buf97, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg37_1
        buf96 = buf87; del buf87  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_6], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf91, 4096, buf97, 4096, arg38_1, 1, buf96, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg38_1
        buf100 = buf97; del buf97  # reuse
        # Topologically Sorted Source Nodes: [linear_21], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf91, arg39_1, buf100, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg39_1
        buf101 = buf73; del buf73  # reuse
        # Topologically Sorted Source Nodes: [linear_22], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf91, arg40_1, buf101, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg40_1
        _xnumel = 1024*s1
        buf102 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_3], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf102, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_3, mul_19, cat_8, mul_20, k_embed_3, index_copy__6], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf101, buf7, buf102, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf104 = buf101; del buf101  # reuse
        # Topologically Sorted Source Nodes: [linear_23], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf91, arg41_1, buf104, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg41_1
        _xnumel = 1024*s1
        buf105 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_3], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf105, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_3, index_copy__7], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf104, buf105, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf107 = reinterpret_tensor(buf91, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf91  # reuse
        # Topologically Sorted Source Nodes: [mul_17, cat_7, mul_18, q_embed_3, query_3, attn_output_12], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf100, buf7, buf107, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf108 = buf78; del buf78  # reuse
        # Topologically Sorted Source Nodes: [mul_17, cat_7, mul_18, q_embed_3, query_3, attn_output_12], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf102, buf108, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf109 = buf77; del buf77  # reuse
        # Topologically Sorted Source Nodes: [mul_17, cat_7, mul_18, q_embed_3, query_3, attn_output_12], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf105, buf109, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = s0*s1
        buf110 = buf79; del buf79  # reuse
        buf141 = buf48; del buf48  # reuse
        buf172 = buf17; del buf17  # reuse
        # Topologically Sorted Source Nodes: [mul_17, cat_7, mul_18, q_embed_3, query_3, attn_output_12, mul_22, cat_9, mul_23, q_embed_4, query_4, attn_output_16, mul_27, cat_11, mul_28, q_embed_5, query_5, attn_output_20], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel = s0*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10.run(arg6_1, buf110, buf141, buf172, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_17, cat_7, mul_18, q_embed_3, query_3, attn_output_12], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf111 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf107, buf108, buf109, reinterpret_tensor(buf110, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf112 = buf111[0]
        assert_size_stride(buf112, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf111
        buf116 = reinterpret_tensor(buf107, (s0, 4096), (4096, 1), 0); del buf107  # reuse
        buf118 = reinterpret_tensor(buf100, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf100  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_7, hidden_states_10, hidden_states_11, attn_output_15, hidden_states_14], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_20.run(buf112, arg43_1, buf56, buf64, buf86, buf95, buf118, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg43_1
        del buf112
        del buf56
        buf119 = buf96; del buf96  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_7], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf116, 4096, reinterpret_tensor(buf118, (s0, 4096), (4096, 1), 0), 4096, arg44_1, 1, buf119, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg44_1
        buf122 = buf95; del buf95  # reuse
        buf123 = reinterpret_tensor(buf94, (s0, 14336), (14336, 1), 0); del buf94  # reuse
        # Topologically Sorted Source Nodes: [linear_25], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf116, arg45_1, buf123, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg45_1
        buf125 = reinterpret_tensor(buf92, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf92  # reuse
        # Topologically Sorted Source Nodes: [silu_3, linear_26, mul_21], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf116, arg46_1, buf123, buf125, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg46_1
        buf126 = buf116; del buf116  # reuse
        buf128 = buf86; del buf86  # reuse
        # Topologically Sorted Source Nodes: [down_proj_3, triton_kernel_wrapper_mutation_8], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf125, arg47_1, buf118, buf126, buf128, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg47_1
        buf127 = buf119; del buf119  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_8], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf122, 4096, buf128, 4096, arg48_1, 1, buf127, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg48_1
        buf131 = buf128; del buf128  # reuse
        # Topologically Sorted Source Nodes: [linear_28], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf122, arg49_1, buf131, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg49_1
        buf132 = buf104; del buf104  # reuse
        # Topologically Sorted Source Nodes: [linear_29], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf122, arg50_1, buf132, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg50_1
        _xnumel = 1024*s1
        buf133 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_4], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf133, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_4, mul_24, cat_10, mul_25, k_embed_4, index_copy__8], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf132, buf7, buf133, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf135 = buf132; del buf132  # reuse
        # Topologically Sorted Source Nodes: [linear_30], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf122, arg51_1, buf135, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg51_1
        _xnumel = 1024*s1
        buf136 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_4], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf136, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_4, index_copy__9], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf135, buf136, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf138 = reinterpret_tensor(buf122, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf122  # reuse
        # Topologically Sorted Source Nodes: [mul_22, cat_9, mul_23, q_embed_4, query_4, attn_output_16], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf131, buf7, buf138, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf139 = buf109; del buf109  # reuse
        # Topologically Sorted Source Nodes: [mul_22, cat_9, mul_23, q_embed_4, query_4, attn_output_16], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf133, buf139, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf140 = buf108; del buf108  # reuse
        # Topologically Sorted Source Nodes: [mul_22, cat_9, mul_23, q_embed_4, query_4, attn_output_16], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf136, buf140, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_22, cat_9, mul_23, q_embed_4, query_4, attn_output_16], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf142 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf138, buf139, buf140, reinterpret_tensor(buf141, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf143 = buf142[0]
        assert_size_stride(buf143, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf142
        buf147 = reinterpret_tensor(buf138, (s0, 4096), (4096, 1), 0); del buf138  # reuse
        buf148 = buf131; del buf131  # reuse
        buf150 = buf64; del buf64  # reuse
        # Topologically Sorted Source Nodes: [attn_output_19, triton_kernel_wrapper_mutation_9], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf143, arg53_1, buf118, buf126, buf148, buf150, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg53_1
        buf149 = buf127; del buf127  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_9], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf147, 4096, buf150, 4096, arg54_1, 1, buf149, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg54_1
        buf153 = buf150; del buf150  # reuse
        buf154 = reinterpret_tensor(buf125, (s0, 14336), (14336, 1), 0); del buf125  # reuse
        # Topologically Sorted Source Nodes: [linear_32], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf147, arg55_1, buf154, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg55_1
        buf156 = reinterpret_tensor(buf123, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf123  # reuse
        # Topologically Sorted Source Nodes: [silu_4, linear_33, mul_26], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf147, arg56_1, buf154, buf156, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg56_1
        buf157 = buf147; del buf147  # reuse
        buf159 = reinterpret_tensor(buf143, (s0, 4096), (4096, 1), 0); del buf143  # reuse
        # Topologically Sorted Source Nodes: [down_proj_4, triton_kernel_wrapper_mutation_10], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_19.run(buf156, arg57_1, buf118, buf126, buf148, buf157, buf159, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg57_1
        buf158 = buf149; del buf149  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_10], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf153, 4096, buf159, 4096, arg58_1, 1, buf158, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg58_1
        buf162 = buf159; del buf159  # reuse
        # Topologically Sorted Source Nodes: [linear_35], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf153, arg59_1, buf162, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg59_1
        buf163 = buf135; del buf135  # reuse
        # Topologically Sorted Source Nodes: [linear_36], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf153, arg60_1, buf163, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg60_1
        _xnumel = 1024*s1
        buf164 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_5], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf164, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_5, mul_29, cat_12, mul_30, k_embed_5, index_copy__10], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf163, buf7, buf164, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf166 = buf163; del buf163  # reuse
        # Topologically Sorted Source Nodes: [linear_37], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf153, arg61_1, buf166, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg61_1
        _xnumel = 1024*s1
        buf167 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_5], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf167, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_5, index_copy__11], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf166, buf167, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf169 = reinterpret_tensor(buf153, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf153  # reuse
        # Topologically Sorted Source Nodes: [mul_27, cat_11, mul_28, q_embed_5, query_5, attn_output_20], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf162, buf7, buf169, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf170 = buf140; del buf140  # reuse
        # Topologically Sorted Source Nodes: [mul_27, cat_11, mul_28, q_embed_5, query_5, attn_output_20], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf164, buf170, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf171 = buf139; del buf139  # reuse
        # Topologically Sorted Source Nodes: [mul_27, cat_11, mul_28, q_embed_5, query_5, attn_output_20], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf167, buf171, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_27, cat_11, mul_28, q_embed_5, query_5, attn_output_20], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf173 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf169, buf170, buf171, reinterpret_tensor(buf172, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf174 = buf173[0]
        assert_size_stride(buf174, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf173
        buf178 = reinterpret_tensor(buf169, (s0, 4096), (4096, 1), 0); del buf169  # reuse
        buf180 = reinterpret_tensor(buf162, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf162  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_15, hidden_states_18, hidden_states_19, attn_output_23, hidden_states_22], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_20.run(buf174, arg63_1, buf118, buf126, buf148, buf157, buf180, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg63_1
        del buf118
        del buf126
        buf181 = buf158; del buf158  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_11], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf178, 4096, reinterpret_tensor(buf180, (s0, 4096), (4096, 1), 0), 4096, arg64_1, 1, buf181, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg64_1
        buf184 = reinterpret_tensor(buf174, (s0, 4096), (4096, 1), 0); del buf174  # reuse
        buf185 = reinterpret_tensor(buf156, (s0, 14336), (14336, 1), 0); del buf156  # reuse
        # Topologically Sorted Source Nodes: [linear_39], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf178, arg65_1, buf185, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg65_1
        buf187 = reinterpret_tensor(buf154, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf154  # reuse
        # Topologically Sorted Source Nodes: [silu_5, linear_40, mul_31], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf178, arg66_1, buf185, buf187, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg66_1
        buf188 = buf178; del buf178  # reuse
        buf190 = buf157; del buf157  # reuse
        # Topologically Sorted Source Nodes: [down_proj_5, triton_kernel_wrapper_mutation_12], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf187, arg67_1, buf180, buf188, buf190, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg67_1
        buf189 = buf181; del buf181  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_12], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf184, 4096, buf190, 4096, arg68_1, 1, buf189, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg68_1
        buf193 = buf190; del buf190  # reuse
        # Topologically Sorted Source Nodes: [linear_42], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf184, arg69_1, buf193, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg69_1
        buf194 = buf166; del buf166  # reuse
        # Topologically Sorted Source Nodes: [linear_43], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf184, arg70_1, buf194, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg70_1
        _xnumel = 1024*s1
        buf195 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_6], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf195, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_6, mul_34, cat_14, mul_35, k_embed_6, index_copy__12], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf194, buf7, buf195, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf197 = buf194; del buf194  # reuse
        # Topologically Sorted Source Nodes: [linear_44], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf184, arg71_1, buf197, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg71_1
        _xnumel = 1024*s1
        buf198 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_6], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf198, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_6, index_copy__13], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf197, buf198, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf200 = reinterpret_tensor(buf184, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf184  # reuse
        # Topologically Sorted Source Nodes: [mul_32, cat_13, mul_33, q_embed_6, query_6, attn_output_24], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf193, buf7, buf200, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf201 = buf171; del buf171  # reuse
        # Topologically Sorted Source Nodes: [mul_32, cat_13, mul_33, q_embed_6, query_6, attn_output_24], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf195, buf201, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf202 = buf170; del buf170  # reuse
        # Topologically Sorted Source Nodes: [mul_32, cat_13, mul_33, q_embed_6, query_6, attn_output_24], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf198, buf202, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = s0*s1
        buf203 = buf172; del buf172  # reuse
        buf234 = buf141; del buf141  # reuse
        buf265 = buf110; del buf110  # reuse
        # Topologically Sorted Source Nodes: [mul_32, cat_13, mul_33, q_embed_6, query_6, attn_output_24, mul_37, cat_15, mul_38, q_embed_7, query_7, attn_output_28, mul_42, cat_17, mul_43, q_embed_8, query_8, attn_output_32], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel = s0*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10.run(arg6_1, buf203, buf234, buf265, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_32, cat_13, mul_33, q_embed_6, query_6, attn_output_24], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf204 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf200, buf201, buf202, reinterpret_tensor(buf203, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf205 = buf204[0]
        assert_size_stride(buf205, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf204
        buf209 = reinterpret_tensor(buf200, (s0, 4096), (4096, 1), 0); del buf200  # reuse
        buf210 = buf193; del buf193  # reuse
        buf212 = buf148; del buf148  # reuse
        # Topologically Sorted Source Nodes: [attn_output_27, triton_kernel_wrapper_mutation_13], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf205, arg73_1, buf180, buf188, buf210, buf212, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg73_1
        buf211 = buf189; del buf189  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_13], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf209, 4096, buf212, 4096, arg74_1, 1, buf211, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg74_1
        buf215 = buf212; del buf212  # reuse
        buf216 = reinterpret_tensor(buf187, (s0, 14336), (14336, 1), 0); del buf187  # reuse
        # Topologically Sorted Source Nodes: [linear_46], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf209, arg75_1, buf216, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg75_1
        buf218 = reinterpret_tensor(buf185, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf185  # reuse
        # Topologically Sorted Source Nodes: [silu_6, linear_47, mul_36], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf209, arg76_1, buf216, buf218, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg76_1
        buf219 = buf209; del buf209  # reuse
        buf221 = reinterpret_tensor(buf205, (s0, 4096), (4096, 1), 0); del buf205  # reuse
        # Topologically Sorted Source Nodes: [down_proj_6, triton_kernel_wrapper_mutation_14], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_19.run(buf218, arg77_1, buf180, buf188, buf210, buf219, buf221, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg77_1
        buf220 = buf211; del buf211  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_14], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf215, 4096, buf221, 4096, arg78_1, 1, buf220, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg78_1
        buf224 = buf221; del buf221  # reuse
        # Topologically Sorted Source Nodes: [linear_49], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf215, arg79_1, buf224, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg79_1
        buf225 = buf197; del buf197  # reuse
        # Topologically Sorted Source Nodes: [linear_50], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf215, arg80_1, buf225, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg80_1
        _xnumel = 1024*s1
        buf226 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_7], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf226, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_7, mul_39, cat_16, mul_40, k_embed_7, index_copy__14], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf225, buf7, buf226, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf228 = buf225; del buf225  # reuse
        # Topologically Sorted Source Nodes: [linear_51], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf215, arg81_1, buf228, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg81_1
        _xnumel = 1024*s1
        buf229 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_7], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf229, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_7, index_copy__15], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf228, buf229, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf231 = reinterpret_tensor(buf215, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf215  # reuse
        # Topologically Sorted Source Nodes: [mul_37, cat_15, mul_38, q_embed_7, query_7, attn_output_28], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf224, buf7, buf231, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf232 = buf202; del buf202  # reuse
        # Topologically Sorted Source Nodes: [mul_37, cat_15, mul_38, q_embed_7, query_7, attn_output_28], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf226, buf232, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf233 = buf201; del buf201  # reuse
        # Topologically Sorted Source Nodes: [mul_37, cat_15, mul_38, q_embed_7, query_7, attn_output_28], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf229, buf233, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_37, cat_15, mul_38, q_embed_7, query_7, attn_output_28], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf235 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf231, buf232, buf233, reinterpret_tensor(buf234, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf236 = buf235[0]
        assert_size_stride(buf236, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf235
        buf240 = reinterpret_tensor(buf231, (s0, 4096), (4096, 1), 0); del buf231  # reuse
        buf242 = reinterpret_tensor(buf224, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf224  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_23, hidden_states_26, hidden_states_27, attn_output_31, hidden_states_30], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_20.run(buf236, arg83_1, buf180, buf188, buf210, buf219, buf242, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg83_1
        del buf180
        del buf188
        buf243 = buf220; del buf220  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_15], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf240, 4096, reinterpret_tensor(buf242, (s0, 4096), (4096, 1), 0), 4096, arg84_1, 1, buf243, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg84_1
        buf246 = reinterpret_tensor(buf236, (s0, 4096), (4096, 1), 0); del buf236  # reuse
        buf247 = reinterpret_tensor(buf218, (s0, 14336), (14336, 1), 0); del buf218  # reuse
        # Topologically Sorted Source Nodes: [linear_53], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf240, arg85_1, buf247, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg85_1
        buf249 = reinterpret_tensor(buf216, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf216  # reuse
        # Topologically Sorted Source Nodes: [silu_7, linear_54, mul_41], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf240, arg86_1, buf247, buf249, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg86_1
        buf250 = buf240; del buf240  # reuse
        buf252 = buf219; del buf219  # reuse
        # Topologically Sorted Source Nodes: [down_proj_7, triton_kernel_wrapper_mutation_16], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf249, arg87_1, buf242, buf250, buf252, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg87_1
        buf251 = buf243; del buf243  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_16], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf246, 4096, buf252, 4096, arg88_1, 1, buf251, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg88_1
        buf255 = buf252; del buf252  # reuse
        # Topologically Sorted Source Nodes: [linear_56], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf246, arg89_1, buf255, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg89_1
        buf256 = buf228; del buf228  # reuse
        # Topologically Sorted Source Nodes: [linear_57], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf246, arg90_1, buf256, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg90_1
        _xnumel = 1024*s1
        buf257 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_8], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf257, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_8, mul_44, cat_18, mul_45, k_embed_8, index_copy__16], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf256, buf7, buf257, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf259 = buf256; del buf256  # reuse
        # Topologically Sorted Source Nodes: [linear_58], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf246, arg91_1, buf259, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg91_1
        _xnumel = 1024*s1
        buf260 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_8], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf260, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_8, index_copy__17], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf259, buf260, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf262 = reinterpret_tensor(buf246, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf246  # reuse
        # Topologically Sorted Source Nodes: [mul_42, cat_17, mul_43, q_embed_8, query_8, attn_output_32], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf255, buf7, buf262, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf263 = buf233; del buf233  # reuse
        # Topologically Sorted Source Nodes: [mul_42, cat_17, mul_43, q_embed_8, query_8, attn_output_32], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf257, buf263, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf264 = buf232; del buf232  # reuse
        # Topologically Sorted Source Nodes: [mul_42, cat_17, mul_43, q_embed_8, query_8, attn_output_32], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf260, buf264, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_42, cat_17, mul_43, q_embed_8, query_8, attn_output_32], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf266 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf262, buf263, buf264, reinterpret_tensor(buf265, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf267 = buf266[0]
        assert_size_stride(buf267, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf266
        buf271 = reinterpret_tensor(buf262, (s0, 4096), (4096, 1), 0); del buf262  # reuse
        buf272 = buf255; del buf255  # reuse
        buf274 = buf210; del buf210  # reuse
        # Topologically Sorted Source Nodes: [attn_output_35, triton_kernel_wrapper_mutation_17], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf267, arg93_1, buf242, buf250, buf272, buf274, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg93_1
        buf273 = buf251; del buf251  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_17], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf271, 4096, buf274, 4096, arg94_1, 1, buf273, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg94_1
        buf277 = buf274; del buf274  # reuse
        buf278 = reinterpret_tensor(buf249, (s0, 14336), (14336, 1), 0); del buf249  # reuse
        # Topologically Sorted Source Nodes: [linear_60], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf271, arg95_1, buf278, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg95_1
        buf280 = reinterpret_tensor(buf247, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf247  # reuse
        # Topologically Sorted Source Nodes: [silu_8, linear_61, mul_46], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf271, arg96_1, buf278, buf280, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg96_1
        buf281 = buf271; del buf271  # reuse
        buf283 = reinterpret_tensor(buf267, (s0, 4096), (4096, 1), 0); del buf267  # reuse
        # Topologically Sorted Source Nodes: [down_proj_8, triton_kernel_wrapper_mutation_18], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_19.run(buf280, arg97_1, buf242, buf250, buf272, buf281, buf283, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg97_1
        buf282 = buf273; del buf273  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_18], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf277, 4096, buf283, 4096, arg98_1, 1, buf282, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg98_1
        buf286 = buf283; del buf283  # reuse
        # Topologically Sorted Source Nodes: [linear_63], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf277, arg99_1, buf286, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg99_1
        buf287 = buf259; del buf259  # reuse
        # Topologically Sorted Source Nodes: [linear_64], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf277, arg100_1, buf287, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg100_1
        _xnumel = 1024*s1
        buf288 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_9], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf288, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_9, mul_49, cat_20, mul_50, k_embed_9, index_copy__18], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf287, buf7, buf288, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf290 = buf287; del buf287  # reuse
        # Topologically Sorted Source Nodes: [linear_65], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf277, arg101_1, buf290, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg101_1
        _xnumel = 1024*s1
        buf291 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_9], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf291, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_9, index_copy__19], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf290, buf291, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf293 = reinterpret_tensor(buf277, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf277  # reuse
        # Topologically Sorted Source Nodes: [mul_47, cat_19, mul_48, q_embed_9, query_9, attn_output_36], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf286, buf7, buf293, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf294 = buf264; del buf264  # reuse
        # Topologically Sorted Source Nodes: [mul_47, cat_19, mul_48, q_embed_9, query_9, attn_output_36], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf288, buf294, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf295 = buf263; del buf263  # reuse
        # Topologically Sorted Source Nodes: [mul_47, cat_19, mul_48, q_embed_9, query_9, attn_output_36], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf291, buf295, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = s0*s1
        buf296 = buf265; del buf265  # reuse
        buf327 = buf234; del buf234  # reuse
        buf358 = buf203; del buf203  # reuse
        # Topologically Sorted Source Nodes: [mul_47, cat_19, mul_48, q_embed_9, query_9, attn_output_36, mul_52, cat_21, mul_53, q_embed_10, query_10, attn_output_40, mul_57, cat_23, mul_58, q_embed_11, query_11, attn_output_44], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel = s0*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10.run(arg6_1, buf296, buf327, buf358, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_47, cat_19, mul_48, q_embed_9, query_9, attn_output_36], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf297 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf293, buf294, buf295, reinterpret_tensor(buf296, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf298 = buf297[0]
        assert_size_stride(buf298, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf297
        buf302 = reinterpret_tensor(buf293, (s0, 4096), (4096, 1), 0); del buf293  # reuse
        buf304 = reinterpret_tensor(buf286, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf286  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_31, hidden_states_34, hidden_states_35, attn_output_39, hidden_states_38], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_20.run(buf298, arg103_1, buf242, buf250, buf272, buf281, buf304, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg103_1
        del buf242
        del buf250
        buf305 = buf282; del buf282  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_19], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf302, 4096, reinterpret_tensor(buf304, (s0, 4096), (4096, 1), 0), 4096, arg104_1, 1, buf305, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg104_1
        buf308 = reinterpret_tensor(buf298, (s0, 4096), (4096, 1), 0); del buf298  # reuse
        buf309 = reinterpret_tensor(buf280, (s0, 14336), (14336, 1), 0); del buf280  # reuse
        # Topologically Sorted Source Nodes: [linear_67], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf302, arg105_1, buf309, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg105_1
        buf311 = reinterpret_tensor(buf278, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf278  # reuse
        # Topologically Sorted Source Nodes: [silu_9, linear_68, mul_51], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf302, arg106_1, buf309, buf311, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg106_1
        buf312 = buf302; del buf302  # reuse
        buf314 = buf281; del buf281  # reuse
        # Topologically Sorted Source Nodes: [down_proj_9, triton_kernel_wrapper_mutation_20], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf311, arg107_1, buf304, buf312, buf314, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg107_1
        buf313 = buf305; del buf305  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_20], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf308, 4096, buf314, 4096, arg108_1, 1, buf313, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg108_1
        buf317 = buf314; del buf314  # reuse
        # Topologically Sorted Source Nodes: [linear_70], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf308, arg109_1, buf317, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg109_1
        buf318 = buf290; del buf290  # reuse
        # Topologically Sorted Source Nodes: [linear_71], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf308, arg110_1, buf318, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg110_1
        _xnumel = 1024*s1
        buf319 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_10], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf319, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_10, mul_54, cat_22, mul_55, k_embed_10, index_copy__20], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf318, buf7, buf319, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf321 = buf318; del buf318  # reuse
        # Topologically Sorted Source Nodes: [linear_72], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf308, arg111_1, buf321, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg111_1
        _xnumel = 1024*s1
        buf322 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_10], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf322, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_10, index_copy__21], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf321, buf322, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf324 = reinterpret_tensor(buf308, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf308  # reuse
        # Topologically Sorted Source Nodes: [mul_52, cat_21, mul_53, q_embed_10, query_10, attn_output_40], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf317, buf7, buf324, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf325 = buf295; del buf295  # reuse
        # Topologically Sorted Source Nodes: [mul_52, cat_21, mul_53, q_embed_10, query_10, attn_output_40], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf319, buf325, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf326 = buf294; del buf294  # reuse
        # Topologically Sorted Source Nodes: [mul_52, cat_21, mul_53, q_embed_10, query_10, attn_output_40], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf322, buf326, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_52, cat_21, mul_53, q_embed_10, query_10, attn_output_40], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf328 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf324, buf325, buf326, reinterpret_tensor(buf327, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf329 = buf328[0]
        assert_size_stride(buf329, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf328
        buf333 = reinterpret_tensor(buf324, (s0, 4096), (4096, 1), 0); del buf324  # reuse
        buf334 = buf317; del buf317  # reuse
        buf336 = buf272; del buf272  # reuse
        # Topologically Sorted Source Nodes: [attn_output_43, triton_kernel_wrapper_mutation_21], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf329, arg113_1, buf304, buf312, buf334, buf336, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg113_1
        buf335 = buf313; del buf313  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_21], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf333, 4096, buf336, 4096, arg114_1, 1, buf335, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg114_1
        buf339 = buf336; del buf336  # reuse
        buf340 = reinterpret_tensor(buf311, (s0, 14336), (14336, 1), 0); del buf311  # reuse
        # Topologically Sorted Source Nodes: [linear_74], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf333, arg115_1, buf340, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg115_1
        buf342 = reinterpret_tensor(buf309, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf309  # reuse
        # Topologically Sorted Source Nodes: [silu_10, linear_75, mul_56], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf333, arg116_1, buf340, buf342, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg116_1
        buf343 = buf333; del buf333  # reuse
        buf345 = reinterpret_tensor(buf329, (s0, 4096), (4096, 1), 0); del buf329  # reuse
        # Topologically Sorted Source Nodes: [down_proj_10, triton_kernel_wrapper_mutation_22], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_19.run(buf342, arg117_1, buf304, buf312, buf334, buf343, buf345, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg117_1
        buf344 = buf335; del buf335  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_22], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf339, 4096, buf345, 4096, arg118_1, 1, buf344, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg118_1
        buf348 = buf345; del buf345  # reuse
        # Topologically Sorted Source Nodes: [linear_77], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf339, arg119_1, buf348, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg119_1
        buf349 = buf321; del buf321  # reuse
        # Topologically Sorted Source Nodes: [linear_78], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf339, arg120_1, buf349, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg120_1
        _xnumel = 1024*s1
        buf350 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_11], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf350, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_11, mul_59, cat_24, mul_60, k_embed_11, index_copy__22], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf349, buf7, buf350, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf352 = buf349; del buf349  # reuse
        # Topologically Sorted Source Nodes: [linear_79], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf339, arg121_1, buf352, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg121_1
        _xnumel = 1024*s1
        buf353 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_11], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf353, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_11, index_copy__23], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf352, buf353, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf355 = reinterpret_tensor(buf339, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf339  # reuse
        # Topologically Sorted Source Nodes: [mul_57, cat_23, mul_58, q_embed_11, query_11, attn_output_44], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf348, buf7, buf355, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf356 = buf326; del buf326  # reuse
        # Topologically Sorted Source Nodes: [mul_57, cat_23, mul_58, q_embed_11, query_11, attn_output_44], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf350, buf356, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf357 = buf325; del buf325  # reuse
        # Topologically Sorted Source Nodes: [mul_57, cat_23, mul_58, q_embed_11, query_11, attn_output_44], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf353, buf357, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_57, cat_23, mul_58, q_embed_11, query_11, attn_output_44], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf359 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf355, buf356, buf357, reinterpret_tensor(buf358, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf360 = buf359[0]
        assert_size_stride(buf360, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf359
        buf364 = reinterpret_tensor(buf355, (s0, 4096), (4096, 1), 0); del buf355  # reuse
        buf366 = reinterpret_tensor(buf348, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf348  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_39, hidden_states_42, hidden_states_43, attn_output_47, hidden_states_46], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_20.run(buf360, arg123_1, buf304, buf312, buf334, buf343, buf366, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg123_1
        del buf304
        del buf312
        buf367 = buf344; del buf344  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_23], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf364, 4096, reinterpret_tensor(buf366, (s0, 4096), (4096, 1), 0), 4096, arg124_1, 1, buf367, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg124_1
        buf370 = reinterpret_tensor(buf360, (s0, 4096), (4096, 1), 0); del buf360  # reuse
        buf371 = reinterpret_tensor(buf342, (s0, 14336), (14336, 1), 0); del buf342  # reuse
        # Topologically Sorted Source Nodes: [linear_81], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf364, arg125_1, buf371, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg125_1
        buf373 = reinterpret_tensor(buf340, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf340  # reuse
        # Topologically Sorted Source Nodes: [silu_11, linear_82, mul_61], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf364, arg126_1, buf371, buf373, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg126_1
        buf374 = buf364; del buf364  # reuse
        buf376 = buf343; del buf343  # reuse
        # Topologically Sorted Source Nodes: [down_proj_11, triton_kernel_wrapper_mutation_24], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf373, arg127_1, buf366, buf374, buf376, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg127_1
        buf375 = buf367; del buf367  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_24], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf370, 4096, buf376, 4096, arg128_1, 1, buf375, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg128_1
        buf379 = buf376; del buf376  # reuse
        # Topologically Sorted Source Nodes: [linear_84], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf370, arg129_1, buf379, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg129_1
        buf380 = buf352; del buf352  # reuse
        # Topologically Sorted Source Nodes: [linear_85], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf370, arg130_1, buf380, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg130_1
        _xnumel = 1024*s1
        buf381 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_12], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf381, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_12, mul_64, cat_26, mul_65, k_embed_12, index_copy__24], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf380, buf7, buf381, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf383 = buf380; del buf380  # reuse
        # Topologically Sorted Source Nodes: [linear_86], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf370, arg131_1, buf383, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg131_1
        _xnumel = 1024*s1
        buf384 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_12], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf384, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_12, index_copy__25], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf383, buf384, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf386 = reinterpret_tensor(buf370, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf370  # reuse
        # Topologically Sorted Source Nodes: [mul_62, cat_25, mul_63, q_embed_12, query_12, attn_output_48], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf379, buf7, buf386, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf387 = buf357; del buf357  # reuse
        # Topologically Sorted Source Nodes: [mul_62, cat_25, mul_63, q_embed_12, query_12, attn_output_48], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf381, buf387, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf388 = buf356; del buf356  # reuse
        # Topologically Sorted Source Nodes: [mul_62, cat_25, mul_63, q_embed_12, query_12, attn_output_48], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf384, buf388, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = s0*s1
        buf389 = buf358; del buf358  # reuse
        buf420 = buf327; del buf327  # reuse
        buf451 = buf296; del buf296  # reuse
        # Topologically Sorted Source Nodes: [mul_62, cat_25, mul_63, q_embed_12, query_12, attn_output_48, mul_67, cat_27, mul_68, q_embed_13, query_13, attn_output_52, mul_72, cat_29, mul_73, q_embed_14, query_14, attn_output_56], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel = s0*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10.run(arg6_1, buf389, buf420, buf451, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_62, cat_25, mul_63, q_embed_12, query_12, attn_output_48], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf390 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf386, buf387, buf388, reinterpret_tensor(buf389, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf391 = buf390[0]
        assert_size_stride(buf391, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf390
        buf395 = reinterpret_tensor(buf386, (s0, 4096), (4096, 1), 0); del buf386  # reuse
        buf396 = buf379; del buf379  # reuse
        buf398 = buf334; del buf334  # reuse
        # Topologically Sorted Source Nodes: [attn_output_51, triton_kernel_wrapper_mutation_25], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf391, arg133_1, buf366, buf374, buf396, buf398, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg133_1
        buf397 = buf375; del buf375  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_25], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf395, 4096, buf398, 4096, arg134_1, 1, buf397, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg134_1
        buf401 = buf398; del buf398  # reuse
        buf402 = reinterpret_tensor(buf373, (s0, 14336), (14336, 1), 0); del buf373  # reuse
        # Topologically Sorted Source Nodes: [linear_88], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf395, arg135_1, buf402, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg135_1
        buf404 = reinterpret_tensor(buf371, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf371  # reuse
        # Topologically Sorted Source Nodes: [silu_12, linear_89, mul_66], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf395, arg136_1, buf402, buf404, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg136_1
        buf405 = buf395; del buf395  # reuse
        buf407 = reinterpret_tensor(buf391, (s0, 4096), (4096, 1), 0); del buf391  # reuse
        # Topologically Sorted Source Nodes: [down_proj_12, triton_kernel_wrapper_mutation_26], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_19.run(buf404, arg137_1, buf366, buf374, buf396, buf405, buf407, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg137_1
        buf406 = buf397; del buf397  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_26], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf401, 4096, buf407, 4096, arg138_1, 1, buf406, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg138_1
        buf410 = buf407; del buf407  # reuse
        # Topologically Sorted Source Nodes: [linear_91], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf401, arg139_1, buf410, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg139_1
        buf411 = buf383; del buf383  # reuse
        # Topologically Sorted Source Nodes: [linear_92], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf401, arg140_1, buf411, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg140_1
        _xnumel = 1024*s1
        buf412 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_13], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf412, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_13, mul_69, cat_28, mul_70, k_embed_13, index_copy__26], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf411, buf7, buf412, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf414 = buf411; del buf411  # reuse
        # Topologically Sorted Source Nodes: [linear_93], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf401, arg141_1, buf414, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg141_1
        _xnumel = 1024*s1
        buf415 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_13], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf415, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_13, index_copy__27], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf414, buf415, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf417 = reinterpret_tensor(buf401, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf401  # reuse
        # Topologically Sorted Source Nodes: [mul_67, cat_27, mul_68, q_embed_13, query_13, attn_output_52], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf410, buf7, buf417, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf418 = buf388; del buf388  # reuse
        # Topologically Sorted Source Nodes: [mul_67, cat_27, mul_68, q_embed_13, query_13, attn_output_52], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf412, buf418, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf419 = buf387; del buf387  # reuse
        # Topologically Sorted Source Nodes: [mul_67, cat_27, mul_68, q_embed_13, query_13, attn_output_52], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf415, buf419, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_67, cat_27, mul_68, q_embed_13, query_13, attn_output_52], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf421 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf417, buf418, buf419, reinterpret_tensor(buf420, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf422 = buf421[0]
        assert_size_stride(buf422, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf421
        buf426 = reinterpret_tensor(buf417, (s0, 4096), (4096, 1), 0); del buf417  # reuse
        buf428 = reinterpret_tensor(buf410, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf410  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_47, hidden_states_50, hidden_states_51, attn_output_55, hidden_states_54], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_20.run(buf422, arg143_1, buf366, buf374, buf396, buf405, buf428, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg143_1
        del buf366
        del buf374
        buf429 = buf406; del buf406  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_27], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf426, 4096, reinterpret_tensor(buf428, (s0, 4096), (4096, 1), 0), 4096, arg144_1, 1, buf429, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg144_1
        buf432 = reinterpret_tensor(buf422, (s0, 4096), (4096, 1), 0); del buf422  # reuse
        buf433 = reinterpret_tensor(buf404, (s0, 14336), (14336, 1), 0); del buf404  # reuse
        # Topologically Sorted Source Nodes: [linear_95], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf426, arg145_1, buf433, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg145_1
        buf435 = reinterpret_tensor(buf402, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf402  # reuse
        # Topologically Sorted Source Nodes: [silu_13, linear_96, mul_71], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf426, arg146_1, buf433, buf435, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg146_1
        buf436 = buf426; del buf426  # reuse
        buf438 = buf405; del buf405  # reuse
        # Topologically Sorted Source Nodes: [down_proj_13, triton_kernel_wrapper_mutation_28], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf435, arg147_1, buf428, buf436, buf438, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg147_1
        buf437 = buf429; del buf429  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_28], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf432, 4096, buf438, 4096, arg148_1, 1, buf437, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg148_1
        buf441 = buf438; del buf438  # reuse
        # Topologically Sorted Source Nodes: [linear_98], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf432, arg149_1, buf441, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg149_1
        buf442 = buf414; del buf414  # reuse
        # Topologically Sorted Source Nodes: [linear_99], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf432, arg150_1, buf442, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg150_1
        _xnumel = 1024*s1
        buf443 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_14], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf443, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_14, mul_74, cat_30, mul_75, k_embed_14, index_copy__28], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf442, buf7, buf443, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf445 = buf442; del buf442  # reuse
        # Topologically Sorted Source Nodes: [linear_100], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf432, arg151_1, buf445, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg151_1
        _xnumel = 1024*s1
        buf446 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_14], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf446, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_14, index_copy__29], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf445, buf446, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf448 = reinterpret_tensor(buf432, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf432  # reuse
        # Topologically Sorted Source Nodes: [mul_72, cat_29, mul_73, q_embed_14, query_14, attn_output_56], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf441, buf7, buf448, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf449 = buf419; del buf419  # reuse
        # Topologically Sorted Source Nodes: [mul_72, cat_29, mul_73, q_embed_14, query_14, attn_output_56], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf443, buf449, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf450 = buf418; del buf418  # reuse
        # Topologically Sorted Source Nodes: [mul_72, cat_29, mul_73, q_embed_14, query_14, attn_output_56], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf446, buf450, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_72, cat_29, mul_73, q_embed_14, query_14, attn_output_56], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf452 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf448, buf449, buf450, reinterpret_tensor(buf451, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf453 = buf452[0]
        assert_size_stride(buf453, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf452
        buf457 = reinterpret_tensor(buf448, (s0, 4096), (4096, 1), 0); del buf448  # reuse
        buf458 = buf441; del buf441  # reuse
        buf460 = buf396; del buf396  # reuse
        # Topologically Sorted Source Nodes: [attn_output_59, triton_kernel_wrapper_mutation_29], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf453, arg153_1, buf428, buf436, buf458, buf460, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg153_1
        buf459 = buf437; del buf437  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_29], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf457, 4096, buf460, 4096, arg154_1, 1, buf459, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg154_1
        buf463 = buf460; del buf460  # reuse
        buf464 = reinterpret_tensor(buf435, (s0, 14336), (14336, 1), 0); del buf435  # reuse
        # Topologically Sorted Source Nodes: [linear_102], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf457, arg155_1, buf464, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg155_1
        buf466 = reinterpret_tensor(buf433, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf433  # reuse
        # Topologically Sorted Source Nodes: [silu_14, linear_103, mul_76], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf457, arg156_1, buf464, buf466, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg156_1
        buf467 = buf457; del buf457  # reuse
        buf469 = reinterpret_tensor(buf453, (s0, 4096), (4096, 1), 0); del buf453  # reuse
        # Topologically Sorted Source Nodes: [down_proj_14, triton_kernel_wrapper_mutation_30], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_19.run(buf466, arg157_1, buf428, buf436, buf458, buf467, buf469, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg157_1
        buf468 = buf459; del buf459  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_30], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf463, 4096, buf469, 4096, arg158_1, 1, buf468, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg158_1
        buf472 = buf469; del buf469  # reuse
        # Topologically Sorted Source Nodes: [linear_105], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf463, arg159_1, buf472, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg159_1
        buf473 = buf445; del buf445  # reuse
        # Topologically Sorted Source Nodes: [linear_106], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf463, arg160_1, buf473, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg160_1
        _xnumel = 1024*s1
        buf474 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_15], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf474, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_15, mul_79, cat_32, mul_80, k_embed_15, index_copy__30], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf473, buf7, buf474, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf476 = buf473; del buf473  # reuse
        # Topologically Sorted Source Nodes: [linear_107], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf463, arg161_1, buf476, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg161_1
        _xnumel = 1024*s1
        buf477 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_15], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf477, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_15, index_copy__31], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf476, buf477, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf479 = reinterpret_tensor(buf463, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf463  # reuse
        # Topologically Sorted Source Nodes: [mul_77, cat_31, mul_78, q_embed_15, query_15, attn_output_60], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf472, buf7, buf479, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf480 = buf450; del buf450  # reuse
        # Topologically Sorted Source Nodes: [mul_77, cat_31, mul_78, q_embed_15, query_15, attn_output_60], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf474, buf480, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf481 = buf449; del buf449  # reuse
        # Topologically Sorted Source Nodes: [mul_77, cat_31, mul_78, q_embed_15, query_15, attn_output_60], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf477, buf481, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = s0*s1
        buf482 = buf451; del buf451  # reuse
        buf513 = buf420; del buf420  # reuse
        buf544 = buf389; del buf389  # reuse
        # Topologically Sorted Source Nodes: [mul_77, cat_31, mul_78, q_embed_15, query_15, attn_output_60, mul_82, cat_33, mul_83, q_embed_16, query_16, attn_output_64, mul_87, cat_35, mul_88, q_embed_17, query_17, attn_output_68], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel = s0*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10.run(arg6_1, buf482, buf513, buf544, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_77, cat_31, mul_78, q_embed_15, query_15, attn_output_60], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf483 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf479, buf480, buf481, reinterpret_tensor(buf482, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf484 = buf483[0]
        assert_size_stride(buf484, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf483
        buf488 = reinterpret_tensor(buf479, (s0, 4096), (4096, 1), 0); del buf479  # reuse
        buf490 = reinterpret_tensor(buf472, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf472  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_55, hidden_states_58, hidden_states_59, attn_output_63, hidden_states_62], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_20.run(buf484, arg163_1, buf428, buf436, buf458, buf467, buf490, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg163_1
        del buf428
        del buf436
        buf491 = buf468; del buf468  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_31], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf488, 4096, reinterpret_tensor(buf490, (s0, 4096), (4096, 1), 0), 4096, arg164_1, 1, buf491, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg164_1
        buf494 = reinterpret_tensor(buf484, (s0, 4096), (4096, 1), 0); del buf484  # reuse
        buf495 = reinterpret_tensor(buf466, (s0, 14336), (14336, 1), 0); del buf466  # reuse
        # Topologically Sorted Source Nodes: [linear_109], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf488, arg165_1, buf495, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg165_1
        buf497 = reinterpret_tensor(buf464, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf464  # reuse
        # Topologically Sorted Source Nodes: [silu_15, linear_110, mul_81], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf488, arg166_1, buf495, buf497, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg166_1
        buf498 = buf488; del buf488  # reuse
        buf500 = buf467; del buf467  # reuse
        # Topologically Sorted Source Nodes: [down_proj_15, triton_kernel_wrapper_mutation_32], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf497, arg167_1, buf490, buf498, buf500, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg167_1
        buf499 = buf491; del buf491  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_32], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf494, 4096, buf500, 4096, arg168_1, 1, buf499, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg168_1
        buf503 = buf500; del buf500  # reuse
        # Topologically Sorted Source Nodes: [linear_112], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf494, arg169_1, buf503, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg169_1
        buf504 = buf476; del buf476  # reuse
        # Topologically Sorted Source Nodes: [linear_113], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf494, arg170_1, buf504, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg170_1
        _xnumel = 1024*s1
        buf505 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_16], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf505, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_16, mul_84, cat_34, mul_85, k_embed_16, index_copy__32], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf504, buf7, buf505, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf507 = buf504; del buf504  # reuse
        # Topologically Sorted Source Nodes: [linear_114], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf494, arg171_1, buf507, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg171_1
        _xnumel = 1024*s1
        buf508 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_16], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf508, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_16, index_copy__33], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf507, buf508, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf510 = reinterpret_tensor(buf494, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf494  # reuse
        # Topologically Sorted Source Nodes: [mul_82, cat_33, mul_83, q_embed_16, query_16, attn_output_64], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf503, buf7, buf510, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf511 = buf481; del buf481  # reuse
        # Topologically Sorted Source Nodes: [mul_82, cat_33, mul_83, q_embed_16, query_16, attn_output_64], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf505, buf511, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf512 = buf480; del buf480  # reuse
        # Topologically Sorted Source Nodes: [mul_82, cat_33, mul_83, q_embed_16, query_16, attn_output_64], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf508, buf512, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_82, cat_33, mul_83, q_embed_16, query_16, attn_output_64], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf514 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf510, buf511, buf512, reinterpret_tensor(buf513, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf515 = buf514[0]
        assert_size_stride(buf515, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf514
        buf519 = reinterpret_tensor(buf510, (s0, 4096), (4096, 1), 0); del buf510  # reuse
        buf520 = buf503; del buf503  # reuse
        buf522 = buf458; del buf458  # reuse
        # Topologically Sorted Source Nodes: [attn_output_67, triton_kernel_wrapper_mutation_33], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf515, arg173_1, buf490, buf498, buf520, buf522, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg173_1
        buf521 = buf499; del buf499  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_33], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf519, 4096, buf522, 4096, arg174_1, 1, buf521, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg174_1
        buf525 = buf522; del buf522  # reuse
        buf526 = reinterpret_tensor(buf497, (s0, 14336), (14336, 1), 0); del buf497  # reuse
        # Topologically Sorted Source Nodes: [linear_116], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf519, arg175_1, buf526, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg175_1
        buf528 = reinterpret_tensor(buf495, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf495  # reuse
        # Topologically Sorted Source Nodes: [silu_16, linear_117, mul_86], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf519, arg176_1, buf526, buf528, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg176_1
        buf529 = buf519; del buf519  # reuse
        buf531 = reinterpret_tensor(buf515, (s0, 4096), (4096, 1), 0); del buf515  # reuse
        # Topologically Sorted Source Nodes: [down_proj_16, triton_kernel_wrapper_mutation_34], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_19.run(buf528, arg177_1, buf490, buf498, buf520, buf529, buf531, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg177_1
        buf530 = buf521; del buf521  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_34], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf525, 4096, buf531, 4096, arg178_1, 1, buf530, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg178_1
        buf534 = buf531; del buf531  # reuse
        # Topologically Sorted Source Nodes: [linear_119], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf525, arg179_1, buf534, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg179_1
        buf535 = buf507; del buf507  # reuse
        # Topologically Sorted Source Nodes: [linear_120], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf525, arg180_1, buf535, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg180_1
        _xnumel = 1024*s1
        buf536 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_17], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf536, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_17, mul_89, cat_36, mul_90, k_embed_17, index_copy__34], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf535, buf7, buf536, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf538 = buf535; del buf535  # reuse
        # Topologically Sorted Source Nodes: [linear_121], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf525, arg181_1, buf538, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg181_1
        _xnumel = 1024*s1
        buf539 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_17], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf539, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_17, index_copy__35], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf538, buf539, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf541 = reinterpret_tensor(buf525, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf525  # reuse
        # Topologically Sorted Source Nodes: [mul_87, cat_35, mul_88, q_embed_17, query_17, attn_output_68], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf534, buf7, buf541, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf542 = buf512; del buf512  # reuse
        # Topologically Sorted Source Nodes: [mul_87, cat_35, mul_88, q_embed_17, query_17, attn_output_68], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf536, buf542, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf543 = buf511; del buf511  # reuse
        # Topologically Sorted Source Nodes: [mul_87, cat_35, mul_88, q_embed_17, query_17, attn_output_68], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf539, buf543, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_87, cat_35, mul_88, q_embed_17, query_17, attn_output_68], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf545 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf541, buf542, buf543, reinterpret_tensor(buf544, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf546 = buf545[0]
        assert_size_stride(buf546, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf545
        buf550 = reinterpret_tensor(buf541, (s0, 4096), (4096, 1), 0); del buf541  # reuse
        buf552 = reinterpret_tensor(buf534, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf534  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_63, hidden_states_66, hidden_states_67, attn_output_71, hidden_states_70], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_20.run(buf546, arg183_1, buf490, buf498, buf520, buf529, buf552, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg183_1
        del buf490
        del buf498
        buf553 = buf530; del buf530  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_35], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf550, 4096, reinterpret_tensor(buf552, (s0, 4096), (4096, 1), 0), 4096, arg184_1, 1, buf553, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg184_1
        buf556 = reinterpret_tensor(buf546, (s0, 4096), (4096, 1), 0); del buf546  # reuse
        buf557 = reinterpret_tensor(buf528, (s0, 14336), (14336, 1), 0); del buf528  # reuse
        # Topologically Sorted Source Nodes: [linear_123], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf550, arg185_1, buf557, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg185_1
        buf559 = reinterpret_tensor(buf526, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf526  # reuse
        # Topologically Sorted Source Nodes: [silu_17, linear_124, mul_91], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf550, arg186_1, buf557, buf559, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg186_1
        buf560 = buf550; del buf550  # reuse
        buf562 = buf529; del buf529  # reuse
        # Topologically Sorted Source Nodes: [down_proj_17, triton_kernel_wrapper_mutation_36], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf559, arg187_1, buf552, buf560, buf562, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg187_1
        buf561 = buf553; del buf553  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_36], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf556, 4096, buf562, 4096, arg188_1, 1, buf561, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg188_1
        buf565 = buf562; del buf562  # reuse
        # Topologically Sorted Source Nodes: [linear_126], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf556, arg189_1, buf565, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg189_1
        buf566 = buf538; del buf538  # reuse
        # Topologically Sorted Source Nodes: [linear_127], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf556, arg190_1, buf566, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg190_1
        _xnumel = 1024*s1
        buf567 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_18], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf567, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_18, mul_94, cat_38, mul_95, k_embed_18, index_copy__36], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf566, buf7, buf567, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf569 = buf566; del buf566  # reuse
        # Topologically Sorted Source Nodes: [linear_128], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf556, arg191_1, buf569, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg191_1
        _xnumel = 1024*s1
        buf570 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_18], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf570, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_18, index_copy__37], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf569, buf570, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf572 = reinterpret_tensor(buf556, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf556  # reuse
        # Topologically Sorted Source Nodes: [mul_92, cat_37, mul_93, q_embed_18, query_18, attn_output_72], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf565, buf7, buf572, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf573 = buf543; del buf543  # reuse
        # Topologically Sorted Source Nodes: [mul_92, cat_37, mul_93, q_embed_18, query_18, attn_output_72], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf567, buf573, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf574 = buf542; del buf542  # reuse
        # Topologically Sorted Source Nodes: [mul_92, cat_37, mul_93, q_embed_18, query_18, attn_output_72], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf570, buf574, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = s0*s1
        buf575 = buf544; del buf544  # reuse
        buf606 = buf513; del buf513  # reuse
        buf637 = buf482; del buf482  # reuse
        # Topologically Sorted Source Nodes: [mul_92, cat_37, mul_93, q_embed_18, query_18, attn_output_72, mul_97, cat_39, mul_98, q_embed_19, query_19, attn_output_76, mul_102, cat_41, mul_103, q_embed_20, query_20, attn_output_80], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel = s0*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10.run(arg6_1, buf575, buf606, buf637, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_92, cat_37, mul_93, q_embed_18, query_18, attn_output_72], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf576 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf572, buf573, buf574, reinterpret_tensor(buf575, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf577 = buf576[0]
        assert_size_stride(buf577, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf576
        buf581 = reinterpret_tensor(buf572, (s0, 4096), (4096, 1), 0); del buf572  # reuse
        buf582 = buf565; del buf565  # reuse
        buf584 = buf520; del buf520  # reuse
        # Topologically Sorted Source Nodes: [attn_output_75, triton_kernel_wrapper_mutation_37], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf577, arg193_1, buf552, buf560, buf582, buf584, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg193_1
        buf583 = buf561; del buf561  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_37], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf581, 4096, buf584, 4096, arg194_1, 1, buf583, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg194_1
        buf587 = buf584; del buf584  # reuse
        buf588 = reinterpret_tensor(buf559, (s0, 14336), (14336, 1), 0); del buf559  # reuse
        # Topologically Sorted Source Nodes: [linear_130], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf581, arg195_1, buf588, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg195_1
        buf590 = reinterpret_tensor(buf557, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf557  # reuse
        # Topologically Sorted Source Nodes: [silu_18, linear_131, mul_96], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf581, arg196_1, buf588, buf590, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg196_1
        buf591 = buf581; del buf581  # reuse
        buf593 = reinterpret_tensor(buf577, (s0, 4096), (4096, 1), 0); del buf577  # reuse
        # Topologically Sorted Source Nodes: [down_proj_18, triton_kernel_wrapper_mutation_38], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_19.run(buf590, arg197_1, buf552, buf560, buf582, buf591, buf593, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg197_1
        buf592 = buf583; del buf583  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_38], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf587, 4096, buf593, 4096, arg198_1, 1, buf592, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg198_1
        buf596 = buf593; del buf593  # reuse
        # Topologically Sorted Source Nodes: [linear_133], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf587, arg199_1, buf596, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg199_1
        buf597 = buf569; del buf569  # reuse
        # Topologically Sorted Source Nodes: [linear_134], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf587, arg200_1, buf597, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg200_1
        _xnumel = 1024*s1
        buf598 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_19], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf598, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_19, mul_99, cat_40, mul_100, k_embed_19, index_copy__38], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf597, buf7, buf598, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf600 = buf597; del buf597  # reuse
        # Topologically Sorted Source Nodes: [linear_135], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf587, arg201_1, buf600, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg201_1
        _xnumel = 1024*s1
        buf601 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_19], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf601, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_19, index_copy__39], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf600, buf601, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf603 = reinterpret_tensor(buf587, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf587  # reuse
        # Topologically Sorted Source Nodes: [mul_97, cat_39, mul_98, q_embed_19, query_19, attn_output_76], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf596, buf7, buf603, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf604 = buf574; del buf574  # reuse
        # Topologically Sorted Source Nodes: [mul_97, cat_39, mul_98, q_embed_19, query_19, attn_output_76], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf598, buf604, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf605 = buf573; del buf573  # reuse
        # Topologically Sorted Source Nodes: [mul_97, cat_39, mul_98, q_embed_19, query_19, attn_output_76], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf601, buf605, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_97, cat_39, mul_98, q_embed_19, query_19, attn_output_76], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf607 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf603, buf604, buf605, reinterpret_tensor(buf606, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf608 = buf607[0]
        assert_size_stride(buf608, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf607
        buf612 = reinterpret_tensor(buf603, (s0, 4096), (4096, 1), 0); del buf603  # reuse
        buf614 = reinterpret_tensor(buf596, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf596  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_71, hidden_states_74, hidden_states_75, attn_output_79, hidden_states_78], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_20.run(buf608, arg203_1, buf552, buf560, buf582, buf591, buf614, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg203_1
        del buf552
        del buf560
        buf615 = buf592; del buf592  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_39], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf612, 4096, reinterpret_tensor(buf614, (s0, 4096), (4096, 1), 0), 4096, arg204_1, 1, buf615, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg204_1
        buf618 = reinterpret_tensor(buf608, (s0, 4096), (4096, 1), 0); del buf608  # reuse
        buf619 = reinterpret_tensor(buf590, (s0, 14336), (14336, 1), 0); del buf590  # reuse
        # Topologically Sorted Source Nodes: [linear_137], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf612, arg205_1, buf619, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg205_1
        buf621 = reinterpret_tensor(buf588, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf588  # reuse
        # Topologically Sorted Source Nodes: [silu_19, linear_138, mul_101], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf612, arg206_1, buf619, buf621, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg206_1
        buf622 = buf612; del buf612  # reuse
        buf624 = buf591; del buf591  # reuse
        # Topologically Sorted Source Nodes: [down_proj_19, triton_kernel_wrapper_mutation_40], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf621, arg207_1, buf614, buf622, buf624, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg207_1
        buf623 = buf615; del buf615  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_40], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf618, 4096, buf624, 4096, arg208_1, 1, buf623, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg208_1
        buf627 = buf624; del buf624  # reuse
        # Topologically Sorted Source Nodes: [linear_140], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf618, arg209_1, buf627, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg209_1
        buf628 = buf600; del buf600  # reuse
        # Topologically Sorted Source Nodes: [linear_141], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf618, arg210_1, buf628, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg210_1
        _xnumel = 1024*s1
        buf629 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_20], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf629, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_20, mul_104, cat_42, mul_105, k_embed_20, index_copy__40], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf628, buf7, buf629, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf631 = buf628; del buf628  # reuse
        # Topologically Sorted Source Nodes: [linear_142], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf618, arg211_1, buf631, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg211_1
        _xnumel = 1024*s1
        buf632 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_20], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf632, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_20, index_copy__41], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf631, buf632, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf634 = reinterpret_tensor(buf618, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf618  # reuse
        # Topologically Sorted Source Nodes: [mul_102, cat_41, mul_103, q_embed_20, query_20, attn_output_80], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf627, buf7, buf634, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf635 = buf605; del buf605  # reuse
        # Topologically Sorted Source Nodes: [mul_102, cat_41, mul_103, q_embed_20, query_20, attn_output_80], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf629, buf635, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf636 = buf604; del buf604  # reuse
        # Topologically Sorted Source Nodes: [mul_102, cat_41, mul_103, q_embed_20, query_20, attn_output_80], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf632, buf636, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_102, cat_41, mul_103, q_embed_20, query_20, attn_output_80], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf638 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf634, buf635, buf636, reinterpret_tensor(buf637, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf639 = buf638[0]
        assert_size_stride(buf639, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf638
        buf643 = reinterpret_tensor(buf634, (s0, 4096), (4096, 1), 0); del buf634  # reuse
        buf644 = buf627; del buf627  # reuse
        buf646 = buf582; del buf582  # reuse
        # Topologically Sorted Source Nodes: [attn_output_83, triton_kernel_wrapper_mutation_41], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf639, arg213_1, buf614, buf622, buf644, buf646, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg213_1
        buf645 = buf623; del buf623  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_41], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf643, 4096, buf646, 4096, arg214_1, 1, buf645, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg214_1
        buf649 = buf646; del buf646  # reuse
        buf650 = reinterpret_tensor(buf621, (s0, 14336), (14336, 1), 0); del buf621  # reuse
        # Topologically Sorted Source Nodes: [linear_144], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf643, arg215_1, buf650, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg215_1
        buf652 = reinterpret_tensor(buf619, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf619  # reuse
        # Topologically Sorted Source Nodes: [silu_20, linear_145, mul_106], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf643, arg216_1, buf650, buf652, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg216_1
        buf653 = buf643; del buf643  # reuse
        buf655 = reinterpret_tensor(buf639, (s0, 4096), (4096, 1), 0); del buf639  # reuse
        # Topologically Sorted Source Nodes: [down_proj_20, triton_kernel_wrapper_mutation_42], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_19.run(buf652, arg217_1, buf614, buf622, buf644, buf653, buf655, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg217_1
        buf654 = buf645; del buf645  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_42], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf649, 4096, buf655, 4096, arg218_1, 1, buf654, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg218_1
        buf658 = buf655; del buf655  # reuse
        # Topologically Sorted Source Nodes: [linear_147], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf649, arg219_1, buf658, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg219_1
        buf659 = buf631; del buf631  # reuse
        # Topologically Sorted Source Nodes: [linear_148], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf649, arg220_1, buf659, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg220_1
        _xnumel = 1024*s1
        buf660 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_21], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf660, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_21, mul_109, cat_44, mul_110, k_embed_21, index_copy__42], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf659, buf7, buf660, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf662 = buf659; del buf659  # reuse
        # Topologically Sorted Source Nodes: [linear_149], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf649, arg221_1, buf662, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg221_1
        _xnumel = 1024*s1
        buf663 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_21], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf663, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_21, index_copy__43], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf662, buf663, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf665 = reinterpret_tensor(buf649, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf649  # reuse
        # Topologically Sorted Source Nodes: [mul_107, cat_43, mul_108, q_embed_21, query_21, attn_output_84], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf658, buf7, buf665, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf666 = buf636; del buf636  # reuse
        # Topologically Sorted Source Nodes: [mul_107, cat_43, mul_108, q_embed_21, query_21, attn_output_84], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf660, buf666, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf667 = buf635; del buf635  # reuse
        # Topologically Sorted Source Nodes: [mul_107, cat_43, mul_108, q_embed_21, query_21, attn_output_84], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf663, buf667, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = s0*s1
        buf668 = buf637; del buf637  # reuse
        buf699 = buf606; del buf606  # reuse
        buf730 = buf575; del buf575  # reuse
        # Topologically Sorted Source Nodes: [mul_107, cat_43, mul_108, q_embed_21, query_21, attn_output_84, mul_112, cat_45, mul_113, q_embed_22, query_22, attn_output_88, mul_117, cat_47, mul_118, q_embed_23, query_23, attn_output_92], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel = s0*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10.run(arg6_1, buf668, buf699, buf730, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_107, cat_43, mul_108, q_embed_21, query_21, attn_output_84], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf669 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf665, buf666, buf667, reinterpret_tensor(buf668, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf670 = buf669[0]
        assert_size_stride(buf670, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf669
        buf674 = reinterpret_tensor(buf665, (s0, 4096), (4096, 1), 0); del buf665  # reuse
        buf676 = reinterpret_tensor(buf658, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf658  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_79, hidden_states_82, hidden_states_83, attn_output_87, hidden_states_86], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_20.run(buf670, arg223_1, buf614, buf622, buf644, buf653, buf676, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg223_1
        del buf614
        del buf622
        buf677 = buf654; del buf654  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_43], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf674, 4096, reinterpret_tensor(buf676, (s0, 4096), (4096, 1), 0), 4096, arg224_1, 1, buf677, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg224_1
        buf680 = reinterpret_tensor(buf670, (s0, 4096), (4096, 1), 0); del buf670  # reuse
        buf681 = reinterpret_tensor(buf652, (s0, 14336), (14336, 1), 0); del buf652  # reuse
        # Topologically Sorted Source Nodes: [linear_151], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf674, arg225_1, buf681, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg225_1
        buf683 = reinterpret_tensor(buf650, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf650  # reuse
        # Topologically Sorted Source Nodes: [silu_21, linear_152, mul_111], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf674, arg226_1, buf681, buf683, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg226_1
        buf684 = buf674; del buf674  # reuse
        buf686 = buf653; del buf653  # reuse
        # Topologically Sorted Source Nodes: [down_proj_21, triton_kernel_wrapper_mutation_44], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf683, arg227_1, buf676, buf684, buf686, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg227_1
        buf685 = buf677; del buf677  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_44], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf680, 4096, buf686, 4096, arg228_1, 1, buf685, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg228_1
        buf689 = buf686; del buf686  # reuse
        # Topologically Sorted Source Nodes: [linear_154], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf680, arg229_1, buf689, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg229_1
        buf690 = buf662; del buf662  # reuse
        # Topologically Sorted Source Nodes: [linear_155], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf680, arg230_1, buf690, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg230_1
        _xnumel = 1024*s1
        buf691 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_22], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf691, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_22, mul_114, cat_46, mul_115, k_embed_22, index_copy__44], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf690, buf7, buf691, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf693 = buf690; del buf690  # reuse
        # Topologically Sorted Source Nodes: [linear_156], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf680, arg231_1, buf693, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg231_1
        _xnumel = 1024*s1
        buf694 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_22], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf694, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_22, index_copy__45], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf693, buf694, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf696 = reinterpret_tensor(buf680, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf680  # reuse
        # Topologically Sorted Source Nodes: [mul_112, cat_45, mul_113, q_embed_22, query_22, attn_output_88], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf689, buf7, buf696, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf697 = buf667; del buf667  # reuse
        # Topologically Sorted Source Nodes: [mul_112, cat_45, mul_113, q_embed_22, query_22, attn_output_88], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf691, buf697, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf698 = buf666; del buf666  # reuse
        # Topologically Sorted Source Nodes: [mul_112, cat_45, mul_113, q_embed_22, query_22, attn_output_88], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf694, buf698, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_112, cat_45, mul_113, q_embed_22, query_22, attn_output_88], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf700 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf696, buf697, buf698, reinterpret_tensor(buf699, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf701 = buf700[0]
        assert_size_stride(buf701, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf700
        buf705 = reinterpret_tensor(buf696, (s0, 4096), (4096, 1), 0); del buf696  # reuse
        buf706 = buf689; del buf689  # reuse
        buf708 = buf644; del buf644  # reuse
        # Topologically Sorted Source Nodes: [attn_output_91, triton_kernel_wrapper_mutation_45], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf701, arg233_1, buf676, buf684, buf706, buf708, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg233_1
        buf707 = buf685; del buf685  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_45], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf705, 4096, buf708, 4096, arg234_1, 1, buf707, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg234_1
        buf711 = buf708; del buf708  # reuse
        buf712 = reinterpret_tensor(buf683, (s0, 14336), (14336, 1), 0); del buf683  # reuse
        # Topologically Sorted Source Nodes: [linear_158], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf705, arg235_1, buf712, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg235_1
        buf714 = reinterpret_tensor(buf681, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf681  # reuse
        # Topologically Sorted Source Nodes: [silu_22, linear_159, mul_116], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf705, arg236_1, buf712, buf714, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg236_1
        buf715 = buf705; del buf705  # reuse
        buf717 = reinterpret_tensor(buf701, (s0, 4096), (4096, 1), 0); del buf701  # reuse
        # Topologically Sorted Source Nodes: [down_proj_22, triton_kernel_wrapper_mutation_46], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_19.run(buf714, arg237_1, buf676, buf684, buf706, buf715, buf717, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg237_1
        buf716 = buf707; del buf707  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_46], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf711, 4096, buf717, 4096, arg238_1, 1, buf716, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg238_1
        buf720 = buf717; del buf717  # reuse
        # Topologically Sorted Source Nodes: [linear_161], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf711, arg239_1, buf720, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg239_1
        buf721 = buf693; del buf693  # reuse
        # Topologically Sorted Source Nodes: [linear_162], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf711, arg240_1, buf721, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg240_1
        _xnumel = 1024*s1
        buf722 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_23], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf722, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_23, mul_119, cat_48, mul_120, k_embed_23, index_copy__46], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf721, buf7, buf722, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf724 = buf721; del buf721  # reuse
        # Topologically Sorted Source Nodes: [linear_163], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf711, arg241_1, buf724, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg241_1
        _xnumel = 1024*s1
        buf725 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_23], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf725, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_23, index_copy__47], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf724, buf725, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf727 = reinterpret_tensor(buf711, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf711  # reuse
        # Topologically Sorted Source Nodes: [mul_117, cat_47, mul_118, q_embed_23, query_23, attn_output_92], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf720, buf7, buf727, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf728 = buf698; del buf698  # reuse
        # Topologically Sorted Source Nodes: [mul_117, cat_47, mul_118, q_embed_23, query_23, attn_output_92], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf722, buf728, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf729 = buf697; del buf697  # reuse
        # Topologically Sorted Source Nodes: [mul_117, cat_47, mul_118, q_embed_23, query_23, attn_output_92], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf725, buf729, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_117, cat_47, mul_118, q_embed_23, query_23, attn_output_92], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf731 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf727, buf728, buf729, reinterpret_tensor(buf730, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf732 = buf731[0]
        assert_size_stride(buf732, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf731
        buf736 = reinterpret_tensor(buf727, (s0, 4096), (4096, 1), 0); del buf727  # reuse
        buf738 = reinterpret_tensor(buf720, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf720  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_87, hidden_states_90, hidden_states_91, attn_output_95, hidden_states_94], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_20.run(buf732, arg243_1, buf676, buf684, buf706, buf715, buf738, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg243_1
        del buf676
        del buf684
        buf739 = buf716; del buf716  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_47], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf736, 4096, reinterpret_tensor(buf738, (s0, 4096), (4096, 1), 0), 4096, arg244_1, 1, buf739, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg244_1
        buf742 = reinterpret_tensor(buf732, (s0, 4096), (4096, 1), 0); del buf732  # reuse
        buf743 = reinterpret_tensor(buf714, (s0, 14336), (14336, 1), 0); del buf714  # reuse
        # Topologically Sorted Source Nodes: [linear_165], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf736, arg245_1, buf743, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg245_1
        buf745 = reinterpret_tensor(buf712, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf712  # reuse
        # Topologically Sorted Source Nodes: [silu_23, linear_166, mul_121], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf736, arg246_1, buf743, buf745, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg246_1
        buf746 = buf736; del buf736  # reuse
        buf748 = buf715; del buf715  # reuse
        # Topologically Sorted Source Nodes: [down_proj_23, triton_kernel_wrapper_mutation_48], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf745, arg247_1, buf738, buf746, buf748, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg247_1
        buf747 = buf739; del buf739  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_48], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf742, 4096, buf748, 4096, arg248_1, 1, buf747, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg248_1
        buf751 = buf748; del buf748  # reuse
        # Topologically Sorted Source Nodes: [linear_168], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf742, arg249_1, buf751, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg249_1
        buf752 = buf724; del buf724  # reuse
        # Topologically Sorted Source Nodes: [linear_169], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf742, arg250_1, buf752, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg250_1
        _xnumel = 1024*s1
        buf753 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_24], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf753, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_24, mul_124, cat_50, mul_125, k_embed_24, index_copy__48], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf752, buf7, buf753, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf755 = buf752; del buf752  # reuse
        # Topologically Sorted Source Nodes: [linear_170], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf742, arg251_1, buf755, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg251_1
        _xnumel = 1024*s1
        buf756 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_24], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf756, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_24, index_copy__49], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf755, buf756, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf758 = reinterpret_tensor(buf742, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf742  # reuse
        # Topologically Sorted Source Nodes: [mul_122, cat_49, mul_123, q_embed_24, query_24, attn_output_96], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf751, buf7, buf758, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf759 = buf729; del buf729  # reuse
        # Topologically Sorted Source Nodes: [mul_122, cat_49, mul_123, q_embed_24, query_24, attn_output_96], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf753, buf759, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf760 = buf728; del buf728  # reuse
        # Topologically Sorted Source Nodes: [mul_122, cat_49, mul_123, q_embed_24, query_24, attn_output_96], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf756, buf760, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = s0*s1
        buf761 = buf730; del buf730  # reuse
        buf792 = buf699; del buf699  # reuse
        buf823 = buf668; del buf668  # reuse
        # Topologically Sorted Source Nodes: [mul_122, cat_49, mul_123, q_embed_24, query_24, attn_output_96, mul_127, cat_51, mul_128, q_embed_25, query_25, attn_output_100, mul_132, cat_53, mul_133, q_embed_26, query_26, attn_output_104], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel = s0*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10.run(arg6_1, buf761, buf792, buf823, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_122, cat_49, mul_123, q_embed_24, query_24, attn_output_96], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf762 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf758, buf759, buf760, reinterpret_tensor(buf761, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf763 = buf762[0]
        assert_size_stride(buf763, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf762
        buf767 = reinterpret_tensor(buf758, (s0, 4096), (4096, 1), 0); del buf758  # reuse
        buf768 = buf751; del buf751  # reuse
        buf770 = buf706; del buf706  # reuse
        # Topologically Sorted Source Nodes: [attn_output_99, triton_kernel_wrapper_mutation_49], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf763, arg253_1, buf738, buf746, buf768, buf770, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg253_1
        buf769 = buf747; del buf747  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_49], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf767, 4096, buf770, 4096, arg254_1, 1, buf769, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg254_1
        buf773 = buf770; del buf770  # reuse
        buf774 = reinterpret_tensor(buf745, (s0, 14336), (14336, 1), 0); del buf745  # reuse
        # Topologically Sorted Source Nodes: [linear_172], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf767, arg255_1, buf774, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg255_1
        buf776 = reinterpret_tensor(buf743, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf743  # reuse
        # Topologically Sorted Source Nodes: [silu_24, linear_173, mul_126], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf767, arg256_1, buf774, buf776, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg256_1
        buf777 = buf767; del buf767  # reuse
        buf779 = reinterpret_tensor(buf763, (s0, 4096), (4096, 1), 0); del buf763  # reuse
        # Topologically Sorted Source Nodes: [down_proj_24, triton_kernel_wrapper_mutation_50], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_19.run(buf776, arg257_1, buf738, buf746, buf768, buf777, buf779, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg257_1
        buf778 = buf769; del buf769  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_50], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf773, 4096, buf779, 4096, arg258_1, 1, buf778, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg258_1
        buf782 = buf779; del buf779  # reuse
        # Topologically Sorted Source Nodes: [linear_175], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf773, arg259_1, buf782, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg259_1
        buf783 = buf755; del buf755  # reuse
        # Topologically Sorted Source Nodes: [linear_176], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf773, arg260_1, buf783, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg260_1
        _xnumel = 1024*s1
        buf784 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_25], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf784, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_25, mul_129, cat_52, mul_130, k_embed_25, index_copy__50], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf783, buf7, buf784, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf786 = buf783; del buf783  # reuse
        # Topologically Sorted Source Nodes: [linear_177], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf773, arg261_1, buf786, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg261_1
        _xnumel = 1024*s1
        buf787 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_25], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf787, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_25, index_copy__51], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf786, buf787, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf789 = reinterpret_tensor(buf773, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf773  # reuse
        # Topologically Sorted Source Nodes: [mul_127, cat_51, mul_128, q_embed_25, query_25, attn_output_100], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf782, buf7, buf789, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf790 = buf760; del buf760  # reuse
        # Topologically Sorted Source Nodes: [mul_127, cat_51, mul_128, q_embed_25, query_25, attn_output_100], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf784, buf790, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf791 = buf759; del buf759  # reuse
        # Topologically Sorted Source Nodes: [mul_127, cat_51, mul_128, q_embed_25, query_25, attn_output_100], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf787, buf791, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_127, cat_51, mul_128, q_embed_25, query_25, attn_output_100], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf793 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf789, buf790, buf791, reinterpret_tensor(buf792, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf794 = buf793[0]
        assert_size_stride(buf794, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf793
        buf798 = reinterpret_tensor(buf789, (s0, 4096), (4096, 1), 0); del buf789  # reuse
        buf800 = reinterpret_tensor(buf782, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf782  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_95, hidden_states_98, hidden_states_99, attn_output_103, hidden_states_102], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_20.run(buf794, arg263_1, buf738, buf746, buf768, buf777, buf800, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg263_1
        del buf738
        del buf746
        buf801 = buf778; del buf778  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_51], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf798, 4096, reinterpret_tensor(buf800, (s0, 4096), (4096, 1), 0), 4096, arg264_1, 1, buf801, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg264_1
        buf804 = reinterpret_tensor(buf794, (s0, 4096), (4096, 1), 0); del buf794  # reuse
        buf805 = reinterpret_tensor(buf776, (s0, 14336), (14336, 1), 0); del buf776  # reuse
        # Topologically Sorted Source Nodes: [linear_179], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf798, arg265_1, buf805, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg265_1
        buf807 = reinterpret_tensor(buf774, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf774  # reuse
        # Topologically Sorted Source Nodes: [silu_25, linear_180, mul_131], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf798, arg266_1, buf805, buf807, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg266_1
        buf808 = buf798; del buf798  # reuse
        buf810 = buf777; del buf777  # reuse
        # Topologically Sorted Source Nodes: [down_proj_25, triton_kernel_wrapper_mutation_52], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf807, arg267_1, buf800, buf808, buf810, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg267_1
        buf809 = buf801; del buf801  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_52], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf804, 4096, buf810, 4096, arg268_1, 1, buf809, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg268_1
        buf813 = buf810; del buf810  # reuse
        # Topologically Sorted Source Nodes: [linear_182], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf804, arg269_1, buf813, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg269_1
        buf814 = buf786; del buf786  # reuse
        # Topologically Sorted Source Nodes: [linear_183], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf804, arg270_1, buf814, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg270_1
        _xnumel = 1024*s1
        buf815 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_26], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf815, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_26, mul_134, cat_54, mul_135, k_embed_26, index_copy__52], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf814, buf7, buf815, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf817 = buf814; del buf814  # reuse
        # Topologically Sorted Source Nodes: [linear_184], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf804, arg271_1, buf817, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg271_1
        _xnumel = 1024*s1
        buf818 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_26], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf818, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_26, index_copy__53], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf817, buf818, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf820 = reinterpret_tensor(buf804, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf804  # reuse
        # Topologically Sorted Source Nodes: [mul_132, cat_53, mul_133, q_embed_26, query_26, attn_output_104], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf813, buf7, buf820, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf821 = buf791; del buf791  # reuse
        # Topologically Sorted Source Nodes: [mul_132, cat_53, mul_133, q_embed_26, query_26, attn_output_104], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf815, buf821, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf822 = buf790; del buf790  # reuse
        # Topologically Sorted Source Nodes: [mul_132, cat_53, mul_133, q_embed_26, query_26, attn_output_104], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf818, buf822, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_132, cat_53, mul_133, q_embed_26, query_26, attn_output_104], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf824 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf820, buf821, buf822, reinterpret_tensor(buf823, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf825 = buf824[0]
        assert_size_stride(buf825, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf824
        buf829 = reinterpret_tensor(buf820, (s0, 4096), (4096, 1), 0); del buf820  # reuse
        buf830 = buf813; del buf813  # reuse
        buf832 = buf768; del buf768  # reuse
        # Topologically Sorted Source Nodes: [attn_output_107, triton_kernel_wrapper_mutation_53], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf825, arg273_1, buf800, buf808, buf830, buf832, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg273_1
        buf831 = buf809; del buf809  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_53], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf829, 4096, buf832, 4096, arg274_1, 1, buf831, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg274_1
        buf835 = buf832; del buf832  # reuse
        buf836 = reinterpret_tensor(buf807, (s0, 14336), (14336, 1), 0); del buf807  # reuse
        # Topologically Sorted Source Nodes: [linear_186], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf829, arg275_1, buf836, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg275_1
        buf838 = reinterpret_tensor(buf805, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf805  # reuse
        # Topologically Sorted Source Nodes: [silu_26, linear_187, mul_136], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf829, arg276_1, buf836, buf838, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg276_1
        buf839 = buf829; del buf829  # reuse
        buf841 = reinterpret_tensor(buf825, (s0, 4096), (4096, 1), 0); del buf825  # reuse
        # Topologically Sorted Source Nodes: [down_proj_26, triton_kernel_wrapper_mutation_54], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_19.run(buf838, arg277_1, buf800, buf808, buf830, buf839, buf841, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg277_1
        buf840 = buf831; del buf831  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_54], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf835, 4096, buf841, 4096, arg278_1, 1, buf840, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg278_1
        buf844 = buf841; del buf841  # reuse
        # Topologically Sorted Source Nodes: [linear_189], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf835, arg279_1, buf844, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg279_1
        buf845 = buf817; del buf817  # reuse
        # Topologically Sorted Source Nodes: [linear_190], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf835, arg280_1, buf845, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg280_1
        _xnumel = 1024*s1
        buf846 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_27], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf846, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_27, mul_139, cat_56, mul_140, k_embed_27, index_copy__54], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf845, buf7, buf846, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf848 = buf845; del buf845  # reuse
        # Topologically Sorted Source Nodes: [linear_191], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf835, arg281_1, buf848, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg281_1
        _xnumel = 1024*s1
        buf849 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_27], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf849, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_27, index_copy__55], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf848, buf849, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf851 = reinterpret_tensor(buf835, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf835  # reuse
        # Topologically Sorted Source Nodes: [mul_137, cat_55, mul_138, q_embed_27, query_27, attn_output_108], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf844, buf7, buf851, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf852 = buf822; del buf822  # reuse
        # Topologically Sorted Source Nodes: [mul_137, cat_55, mul_138, q_embed_27, query_27, attn_output_108], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf846, buf852, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf853 = buf821; del buf821  # reuse
        # Topologically Sorted Source Nodes: [mul_137, cat_55, mul_138, q_embed_27, query_27, attn_output_108], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf849, buf853, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = s0*s1
        buf854 = buf823; del buf823  # reuse
        buf885 = buf792; del buf792  # reuse
        buf916 = buf761; del buf761  # reuse
        # Topologically Sorted Source Nodes: [mul_137, cat_55, mul_138, q_embed_27, query_27, attn_output_108, mul_142, cat_57, mul_143, q_embed_28, query_28, attn_output_112, mul_147, cat_59, mul_148, q_embed_29, query_29, attn_output_116], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel = s0*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10.run(arg6_1, buf854, buf885, buf916, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_10_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_137, cat_55, mul_138, q_embed_27, query_27, attn_output_108], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf855 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf851, buf852, buf853, reinterpret_tensor(buf854, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        del buf854
        buf856 = buf855[0]
        assert_size_stride(buf856, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf855
        buf860 = reinterpret_tensor(buf851, (s0, 4096), (4096, 1), 0); del buf851  # reuse
        buf862 = reinterpret_tensor(buf844, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf844  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_103, hidden_states_106, hidden_states_107, attn_output_111, hidden_states_110], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_20.run(buf856, arg283_1, buf800, buf808, buf830, buf839, buf862, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg283_1
        del buf800
        del buf808
        buf863 = buf840; del buf840  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_55], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf860, 4096, reinterpret_tensor(buf862, (s0, 4096), (4096, 1), 0), 4096, arg284_1, 1, buf863, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg284_1
        buf866 = reinterpret_tensor(buf856, (s0, 4096), (4096, 1), 0); del buf856  # reuse
        buf867 = reinterpret_tensor(buf838, (s0, 14336), (14336, 1), 0); del buf838  # reuse
        # Topologically Sorted Source Nodes: [linear_193], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf860, arg285_1, buf867, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg285_1
        buf869 = reinterpret_tensor(buf836, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf836  # reuse
        # Topologically Sorted Source Nodes: [silu_27, linear_194, mul_141], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf860, arg286_1, buf867, buf869, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg286_1
        buf870 = buf860; del buf860  # reuse
        buf872 = buf839; del buf839  # reuse
        # Topologically Sorted Source Nodes: [down_proj_27, triton_kernel_wrapper_mutation_56], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf869, arg287_1, buf862, buf870, buf872, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg287_1
        buf871 = buf863; del buf863  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_56], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf866, 4096, buf872, 4096, arg288_1, 1, buf871, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg288_1
        buf875 = buf872; del buf872  # reuse
        # Topologically Sorted Source Nodes: [linear_196], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf866, arg289_1, buf875, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg289_1
        buf876 = buf848; del buf848  # reuse
        # Topologically Sorted Source Nodes: [linear_197], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf866, arg290_1, buf876, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg290_1
        _xnumel = 1024*s1
        buf877 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_28], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf877, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_28, mul_144, cat_58, mul_145, k_embed_28, index_copy__56], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf876, buf7, buf877, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf879 = buf876; del buf876  # reuse
        # Topologically Sorted Source Nodes: [linear_198], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf866, arg291_1, buf879, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg291_1
        _xnumel = 1024*s1
        buf880 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_28], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf880, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_28, index_copy__57], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf879, buf880, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf882 = reinterpret_tensor(buf866, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf866  # reuse
        # Topologically Sorted Source Nodes: [mul_142, cat_57, mul_143, q_embed_28, query_28, attn_output_112], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf875, buf7, buf882, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf883 = buf853; del buf853  # reuse
        # Topologically Sorted Source Nodes: [mul_142, cat_57, mul_143, q_embed_28, query_28, attn_output_112], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf877, buf883, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf884 = buf852; del buf852  # reuse
        # Topologically Sorted Source Nodes: [mul_142, cat_57, mul_143, q_embed_28, query_28, attn_output_112], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf880, buf884, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_142, cat_57, mul_143, q_embed_28, query_28, attn_output_112], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf886 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf882, buf883, buf884, reinterpret_tensor(buf885, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf887 = buf886[0]
        assert_size_stride(buf887, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf886
        buf891 = reinterpret_tensor(buf882, (s0, 4096), (4096, 1), 0); del buf882  # reuse
        buf892 = buf875; del buf875  # reuse
        buf894 = buf830; del buf830  # reuse
        # Topologically Sorted Source Nodes: [attn_output_115, triton_kernel_wrapper_mutation_57], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf887, arg293_1, buf862, buf870, buf892, buf894, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg293_1
        buf893 = buf871; del buf871  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_57], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf891, 4096, buf894, 4096, arg294_1, 1, buf893, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg294_1
        buf897 = buf894; del buf894  # reuse
        buf898 = reinterpret_tensor(buf869, (s0, 14336), (14336, 1), 0); del buf869  # reuse
        # Topologically Sorted Source Nodes: [linear_200], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf891, arg295_1, buf898, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg295_1
        buf900 = reinterpret_tensor(buf867, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf867  # reuse
        # Topologically Sorted Source Nodes: [silu_28, linear_201, mul_146], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf891, arg296_1, buf898, buf900, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg296_1
        buf901 = buf891; del buf891  # reuse
        buf903 = reinterpret_tensor(buf887, (s0, 4096), (4096, 1), 0); del buf887  # reuse
        # Topologically Sorted Source Nodes: [down_proj_28, triton_kernel_wrapper_mutation_58], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_19.run(buf900, arg297_1, buf862, buf870, buf892, buf901, buf903, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg297_1
        buf902 = buf893; del buf893  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_58], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf897, 4096, buf903, 4096, arg298_1, 1, buf902, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg298_1
        buf906 = buf903; del buf903  # reuse
        # Topologically Sorted Source Nodes: [linear_203], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf897, arg299_1, buf906, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg299_1
        buf907 = buf879; del buf879  # reuse
        # Topologically Sorted Source Nodes: [linear_204], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf897, arg300_1, buf907, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg300_1
        _xnumel = 1024*s1
        buf908 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_29], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf908, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_29, mul_149, cat_60, mul_150, k_embed_29, index_copy__58], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf907, buf7, buf908, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf910 = buf907; del buf907  # reuse
        # Topologically Sorted Source Nodes: [linear_205], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf897, arg301_1, buf910, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg301_1
        _xnumel = 1024*s1
        buf911 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_29], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf911, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_29, index_copy__59], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf910, buf911, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf913 = reinterpret_tensor(buf897, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf897  # reuse
        # Topologically Sorted Source Nodes: [mul_147, cat_59, mul_148, q_embed_29, query_29, attn_output_116], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf906, buf7, buf913, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf914 = buf884; del buf884  # reuse
        # Topologically Sorted Source Nodes: [mul_147, cat_59, mul_148, q_embed_29, query_29, attn_output_116], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf908, buf914, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf915 = buf883; del buf883  # reuse
        # Topologically Sorted Source Nodes: [mul_147, cat_59, mul_148, q_embed_29, query_29, attn_output_116], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf911, buf915, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_147, cat_59, mul_148, q_embed_29, query_29, attn_output_116], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf917 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf913, buf914, buf915, reinterpret_tensor(buf916, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        buf918 = buf917[0]
        assert_size_stride(buf918, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf917
        buf922 = reinterpret_tensor(buf913, (s0, 4096), (4096, 1), 0); del buf913  # reuse
        buf924 = reinterpret_tensor(buf906, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf906  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_111, hidden_states_114, hidden_states_115, attn_output_119, hidden_states_118], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_20.run(buf918, arg303_1, buf862, buf870, buf892, buf901, buf924, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg303_1
        del buf862
        del buf870
        buf925 = buf902; del buf902  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_59], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf922, 4096, reinterpret_tensor(buf924, (s0, 4096), (4096, 1), 0), 4096, arg304_1, 1, buf925, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg304_1
        buf928 = reinterpret_tensor(buf918, (s0, 4096), (4096, 1), 0); del buf918  # reuse
        buf929 = reinterpret_tensor(buf900, (s0, 14336), (14336, 1), 0); del buf900  # reuse
        # Topologically Sorted Source Nodes: [linear_207], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf922, arg305_1, buf929, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg305_1
        buf931 = reinterpret_tensor(buf898, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf898  # reuse
        # Topologically Sorted Source Nodes: [silu_29, linear_208, mul_151], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf922, arg306_1, buf929, buf931, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg306_1
        buf932 = buf922; del buf922  # reuse
        buf934 = buf901; del buf901  # reuse
        # Topologically Sorted Source Nodes: [down_proj_29, triton_kernel_wrapper_mutation_60], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf931, arg307_1, buf924, buf932, buf934, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg307_1
        buf933 = buf925; del buf925  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_60], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf928, 4096, buf934, 4096, arg308_1, 1, buf933, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg308_1
        buf937 = buf934; del buf934  # reuse
        # Topologically Sorted Source Nodes: [linear_210], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf928, arg309_1, buf937, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg309_1
        buf938 = buf910; del buf910  # reuse
        # Topologically Sorted Source Nodes: [linear_211], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf928, arg310_1, buf938, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg310_1
        _xnumel = 1024*s1
        buf939 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_30], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf939, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_30, mul_154, cat_62, mul_155, k_embed_30, index_copy__60], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf938, buf7, buf939, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf941 = buf938; del buf938  # reuse
        # Topologically Sorted Source Nodes: [linear_212], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf928, arg311_1, buf941, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg311_1
        _xnumel = 1024*s1
        buf942 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_30], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf942, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_30, index_copy__61], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf941, buf942, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf944 = reinterpret_tensor(buf928, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf928  # reuse
        # Topologically Sorted Source Nodes: [mul_152, cat_61, mul_153, q_embed_30, query_30, attn_output_120], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf937, buf7, buf944, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf945 = buf915; del buf915  # reuse
        # Topologically Sorted Source Nodes: [mul_152, cat_61, mul_153, q_embed_30, query_30, attn_output_120], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf939, buf945, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf946 = buf914; del buf914  # reuse
        # Topologically Sorted Source Nodes: [mul_152, cat_61, mul_153, q_embed_30, query_30, attn_output_120], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf942, buf946, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = s0*s1
        buf947 = buf916; del buf916  # reuse
        buf978 = buf885; del buf885  # reuse
        # Topologically Sorted Source Nodes: [mul_152, cat_61, mul_153, q_embed_30, query_30, attn_output_120, mul_157, cat_63, mul_158, q_embed_31, query_31, attn_output_124], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_21_xnumel = s0*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_21.run(arg6_1, buf947, buf978, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_21_xnumel, stream=stream0)
        del arg6_1
        # Topologically Sorted Source Nodes: [mul_152, cat_61, mul_153, q_embed_30, query_30, attn_output_120], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf948 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf944, buf945, buf946, reinterpret_tensor(buf947, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        del buf947
        buf949 = buf948[0]
        assert_size_stride(buf949, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf948
        buf953 = reinterpret_tensor(buf944, (s0, 4096), (4096, 1), 0); del buf944  # reuse
        buf954 = buf937; del buf937  # reuse
        buf956 = buf892; del buf892  # reuse
        # Topologically Sorted Source Nodes: [attn_output_123, triton_kernel_wrapper_mutation_61], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf949, arg313_1, buf924, buf932, buf954, buf956, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg313_1
        buf955 = buf933; del buf933  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_61], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf953, 4096, buf956, 4096, arg314_1, 1, buf955, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg314_1
        buf959 = buf956; del buf956  # reuse
        buf960 = reinterpret_tensor(buf931, (s0, 14336), (14336, 1), 0); del buf931  # reuse
        # Topologically Sorted Source Nodes: [linear_214], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf953, arg315_1, buf960, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg315_1
        buf962 = reinterpret_tensor(buf929, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf929  # reuse
        # Topologically Sorted Source Nodes: [silu_30, linear_215, mul_156], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf953, arg316_1, buf960, buf962, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg316_1
        buf963 = buf953; del buf953  # reuse
        buf965 = reinterpret_tensor(buf949, (s0, 4096), (4096, 1), 0); del buf949  # reuse
        # Topologically Sorted Source Nodes: [down_proj_30, triton_kernel_wrapper_mutation_62], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_19.run(buf962, arg317_1, buf924, buf932, buf954, buf963, buf965, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg317_1
        buf964 = buf955; del buf955  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_62], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf959, 4096, buf965, 4096, arg318_1, 1, buf964, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg318_1
        buf968 = buf965; del buf965  # reuse
        # Topologically Sorted Source Nodes: [linear_217], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf959, arg319_1, buf968, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg319_1
        buf969 = buf941; del buf941  # reuse
        # Topologically Sorted Source Nodes: [linear_218], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf959, arg320_1, buf969, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg320_1
        _xnumel = 1024*s1
        buf970 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [keys_31], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf970, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 8*s0
        # Topologically Sorted Source Nodes: [keys_31, mul_159, cat_64, mul_160, k_embed_31, index_copy__62], Original ATen: [aten.zeros, aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel = 8*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_zeros_6.run(arg3_1, buf969, buf7, buf970, s1, s0, 128, triton_poi_fused_add_cat_index_copy_mul_zeros_6_xnumel, stream=stream0)
        buf972 = buf969; del buf969  # reuse
        # Topologically Sorted Source Nodes: [linear_219], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf959, arg321_1, buf972, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg321_1
        _xnumel = 1024*s1
        buf973 = empty_strided_cuda((1, 8, s1, 128), (1024*s1, 128*s1, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [values_31], Original ATen: [aten.zeros]
        triton_poi_fused_zeros_5_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_zeros_5.run(buf973, triton_poi_fused_zeros_5_xnumel, stream=stream0)
        _xnumel = 1024*s0
        # Topologically Sorted Source Nodes: [values_31, index_copy__63], Original ATen: [aten.zeros, aten.index_copy]
        triton_poi_fused_index_copy_zeros_7_xnumel = 1024*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_zeros_7.run(arg3_1, buf972, buf973, s1, triton_poi_fused_index_copy_zeros_7_xnumel, stream=stream0)
        del arg3_1
        del buf972
        _xnumel = 4096*s0
        buf975 = reinterpret_tensor(buf959, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf959  # reuse
        # Topologically Sorted Source Nodes: [mul_157, cat_63, mul_158, q_embed_31, query_31, attn_output_124], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(buf968, buf7, buf975, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del buf7
        _xnumel = 4096*s1
        buf976 = buf946; del buf946  # reuse
        # Topologically Sorted Source Nodes: [mul_157, cat_63, mul_158, q_embed_31, query_31, attn_output_124], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf970, buf976, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        _xnumel = 4096*s1
        buf977 = buf945; del buf945  # reuse
        # Topologically Sorted Source Nodes: [mul_157, cat_63, mul_158, q_embed_31, query_31, attn_output_124], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = 4096*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(buf973, buf977, ps0, s1, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_157, cat_63, mul_158, q_embed_31, query_31, attn_output_124], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf979 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf975, buf976, buf977, reinterpret_tensor(buf978, (1, 32, s0, s1), (8*s0*((7 + s1) // 8), 0, 8*((7 + s1) // 8), 1), 0), False, scale=0.08838834764831845)
        del buf976
        del buf977
        del buf978
        buf980 = buf979[0]
        assert_size_stride(buf980, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf979
        buf984 = reinterpret_tensor(buf975, (s0, 4096), (4096, 1), 0); del buf975  # reuse
        buf986 = reinterpret_tensor(buf968, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf968  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_119, hidden_states_122, hidden_states_123, attn_output_127, hidden_states_126], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_20.run(buf980, arg323_1, buf924, buf932, buf954, buf963, buf986, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg323_1
        del buf924
        del buf932
        del buf954
        del buf963
        buf987 = buf964; del buf964  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_63], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf984, 4096, reinterpret_tensor(buf986, (s0, 4096), (4096, 1), 0), 4096, arg324_1, 1, buf987, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg324_1
        buf990 = reinterpret_tensor(buf980, (s0, 4096), (4096, 1), 0); del buf980  # reuse
        buf991 = reinterpret_tensor(buf962, (s0, 14336), (14336, 1), 0); del buf962  # reuse
        # Topologically Sorted Source Nodes: [linear_221], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_12.run(buf984, arg325_1, buf991, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg325_1
        buf993 = reinterpret_tensor(buf960, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf960  # reuse
        # Topologically Sorted Source Nodes: [silu_31, linear_222, mul_161], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_13.run(buf984, arg326_1, buf991, buf993, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg326_1
        del buf991
        buf996 = buf984; del buf984  # reuse
        # Topologically Sorted Source Nodes: [down_proj_31, triton_kernel_wrapper_mutation_64], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_22.run(buf993, arg327_1, buf986, buf996, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg327_1
        del buf986
        del buf993
        buf995 = buf987; del buf987  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_64], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf990, 4096, buf996, 4096, arg328_1, 1, buf995, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg328_1
        del buf995
        del buf996
        buf1000 = empty_strided_cuda((1, 128256), (128256, 1), torch.float16)
        # Topologically Sorted Source Nodes: [logits], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_23.run(buf990, arg330_1, buf1000, s0, 128256, 4096, stream=stream0)
        del arg330_1
        del buf990
    return (buf12, buf9, buf43, buf40, buf74, buf71, buf105, buf102, buf136, buf133, buf167, buf164, buf198, buf195, buf229, buf226, buf260, buf257, buf291, buf288, buf322, buf319, buf353, buf350, buf384, buf381, buf415, buf412, buf446, buf443, buf477, buf474, buf508, buf505, buf539, buf536, buf570, buf567, buf601, buf598, buf632, buf629, buf663, buf660, buf694, buf691, buf725, buf722, buf756, buf753, buf787, buf784, buf818, buf815, buf849, buf846, buf880, buf877, buf911, buf908, buf942, buf939, buf973, buf970, reinterpret_tensor(buf1000, (1, 1, 128256), (128256, 128256, 1), 0), )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = 9
    arg1_1 = rand_strided((1, 9), (9, 1), device='cuda:0', dtype=torch.int64)
    arg2_1 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg3_1 = rand_strided((9, ), (1, ), device='cuda:0', dtype=torch.int64)
    arg4_1 = rand_strided((1, 9), (9, 1), device='cuda:0', dtype=torch.int64)
    arg5_1 = 2048
    arg6_1 = rand_strided((1, 1, 9, 2048), (18432, 18432, 2048, 1), device='cuda:0', dtype=torch.bool)
    arg7_1 = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float32)
    arg8_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg9_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg10_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg11_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg12_1 = 2048
    arg13_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg14_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg15_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg16_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg17_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg18_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg19_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg20_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg21_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg22_1 = 2048
    arg23_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg24_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg25_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg26_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg27_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg28_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg29_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg30_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg31_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg32_1 = 2048
    arg33_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg34_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg35_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg36_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg37_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg38_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg39_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg40_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg41_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg42_1 = 2048
    arg43_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg44_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg45_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg46_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg47_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg48_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg49_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg50_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg51_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg52_1 = 2048
    arg53_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg54_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg55_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg56_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg57_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg58_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg59_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg60_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg61_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg62_1 = 2048
    arg63_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg64_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg65_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg66_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg67_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg68_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg69_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg70_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg71_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg72_1 = 2048
    arg73_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg74_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg75_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg76_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg77_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg78_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg79_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg80_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg81_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg82_1 = 2048
    arg83_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg84_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg85_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg86_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg87_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg88_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg89_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg90_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg91_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg92_1 = 2048
    arg93_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg94_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg95_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg96_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg97_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg98_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg99_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg100_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg101_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg102_1 = 2048
    arg103_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg104_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg105_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg106_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg107_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg108_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg109_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg110_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg111_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg112_1 = 2048
    arg113_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg114_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg115_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg116_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg117_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg118_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg119_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg120_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg121_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg122_1 = 2048
    arg123_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg124_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg125_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg126_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg127_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg128_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg129_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg130_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg131_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg132_1 = 2048
    arg133_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg134_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg135_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg136_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg137_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg138_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg139_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg140_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg141_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg142_1 = 2048
    arg143_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg144_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg145_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg146_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg147_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg148_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg149_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg150_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg151_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg152_1 = 2048
    arg153_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg154_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg155_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg156_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg157_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg158_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg159_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg160_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg161_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg162_1 = 2048
    arg163_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg164_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg165_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg166_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg167_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg168_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg169_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg170_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg171_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg172_1 = 2048
    arg173_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg174_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg175_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg176_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg177_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg178_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg179_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg180_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg181_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg182_1 = 2048
    arg183_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg184_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg185_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg186_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg187_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg188_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg189_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg190_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg191_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg192_1 = 2048
    arg193_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg194_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg195_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg196_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg197_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg198_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg199_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg200_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg201_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg202_1 = 2048
    arg203_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg204_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg205_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg206_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg207_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg208_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg209_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg210_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg211_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg212_1 = 2048
    arg213_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg214_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg215_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg216_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg217_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg218_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg219_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg220_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg221_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg222_1 = 2048
    arg223_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg224_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg225_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg226_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg227_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg228_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg229_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg230_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg231_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg232_1 = 2048
    arg233_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg234_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg235_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg236_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg237_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg238_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg239_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg240_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg241_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg242_1 = 2048
    arg243_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg244_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg245_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg246_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg247_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg248_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg249_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg250_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg251_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg252_1 = 2048
    arg253_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg254_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg255_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg256_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg257_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg258_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg259_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg260_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg261_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg262_1 = 2048
    arg263_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg264_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg265_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg266_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg267_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg268_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg269_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg270_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg271_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg272_1 = 2048
    arg273_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg274_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg275_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg276_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg277_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg278_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg279_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg280_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg281_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg282_1 = 2048
    arg283_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg284_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg285_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg286_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg287_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg288_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg289_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg290_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg291_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg292_1 = 2048
    arg293_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg294_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg295_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg296_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg297_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg298_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg299_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg300_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg301_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg302_1 = 2048
    arg303_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg304_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg305_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg306_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg307_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg308_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg309_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg310_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg311_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg312_1 = 2048
    arg313_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg314_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg315_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg316_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg317_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg318_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg319_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg320_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg321_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg322_1 = 2048
    arg323_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg324_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg325_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg326_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg327_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg328_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg329_1 = 1
    arg330_1 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    fn = lambda: call([arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1, arg27_1, arg28_1, arg29_1, arg30_1, arg31_1, arg32_1, arg33_1, arg34_1, arg35_1, arg36_1, arg37_1, arg38_1, arg39_1, arg40_1, arg41_1, arg42_1, arg43_1, arg44_1, arg45_1, arg46_1, arg47_1, arg48_1, arg49_1, arg50_1, arg51_1, arg52_1, arg53_1, arg54_1, arg55_1, arg56_1, arg57_1, arg58_1, arg59_1, arg60_1, arg61_1, arg62_1, arg63_1, arg64_1, arg65_1, arg66_1, arg67_1, arg68_1, arg69_1, arg70_1, arg71_1, arg72_1, arg73_1, arg74_1, arg75_1, arg76_1, arg77_1, arg78_1, arg79_1, arg80_1, arg81_1, arg82_1, arg83_1, arg84_1, arg85_1, arg86_1, arg87_1, arg88_1, arg89_1, arg90_1, arg91_1, arg92_1, arg93_1, arg94_1, arg95_1, arg96_1, arg97_1, arg98_1, arg99_1, arg100_1, arg101_1, arg102_1, arg103_1, arg104_1, arg105_1, arg106_1, arg107_1, arg108_1, arg109_1, arg110_1, arg111_1, arg112_1, arg113_1, arg114_1, arg115_1, arg116_1, arg117_1, arg118_1, arg119_1, arg120_1, arg121_1, arg122_1, arg123_1, arg124_1, arg125_1, arg126_1, arg127_1, arg128_1, arg129_1, arg130_1, arg131_1, arg132_1, arg133_1, arg134_1, arg135_1, arg136_1, arg137_1, arg138_1, arg139_1, arg140_1, arg141_1, arg142_1, arg143_1, arg144_1, arg145_1, arg146_1, arg147_1, arg148_1, arg149_1, arg150_1, arg151_1, arg152_1, arg153_1, arg154_1, arg155_1, arg156_1, arg157_1, arg158_1, arg159_1, arg160_1, arg161_1, arg162_1, arg163_1, arg164_1, arg165_1, arg166_1, arg167_1, arg168_1, arg169_1, arg170_1, arg171_1, arg172_1, arg173_1, arg174_1, arg175_1, arg176_1, arg177_1, arg178_1, arg179_1, arg180_1, arg181_1, arg182_1, arg183_1, arg184_1, arg185_1, arg186_1, arg187_1, arg188_1, arg189_1, arg190_1, arg191_1, arg192_1, arg193_1, arg194_1, arg195_1, arg196_1, arg197_1, arg198_1, arg199_1, arg200_1, arg201_1, arg202_1, arg203_1, arg204_1, arg205_1, arg206_1, arg207_1, arg208_1, arg209_1, arg210_1, arg211_1, arg212_1, arg213_1, arg214_1, arg215_1, arg216_1, arg217_1, arg218_1, arg219_1, arg220_1, arg221_1, arg222_1, arg223_1, arg224_1, arg225_1, arg226_1, arg227_1, arg228_1, arg229_1, arg230_1, arg231_1, arg232_1, arg233_1, arg234_1, arg235_1, arg236_1, arg237_1, arg238_1, arg239_1, arg240_1, arg241_1, arg242_1, arg243_1, arg244_1, arg245_1, arg246_1, arg247_1, arg248_1, arg249_1, arg250_1, arg251_1, arg252_1, arg253_1, arg254_1, arg255_1, arg256_1, arg257_1, arg258_1, arg259_1, arg260_1, arg261_1, arg262_1, arg263_1, arg264_1, arg265_1, arg266_1, arg267_1, arg268_1, arg269_1, arg270_1, arg271_1, arg272_1, arg273_1, arg274_1, arg275_1, arg276_1, arg277_1, arg278_1, arg279_1, arg280_1, arg281_1, arg282_1, arg283_1, arg284_1, arg285_1, arg286_1, arg287_1, arg288_1, arg289_1, arg290_1, arg291_1, arg292_1, arg293_1, arg294_1, arg295_1, arg296_1, arg297_1, arg298_1, arg299_1, arg300_1, arg301_1, arg302_1, arg303_1, arg304_1, arg305_1, arg306_1, arg307_1, arg308_1, arg309_1, arg310_1, arg311_1, arg312_1, arg313_1, arg314_1, arg315_1, arg316_1, arg317_1, arg318_1, arg319_1, arg320_1, arg321_1, arg322_1, arg323_1, arg324_1, arg325_1, arg326_1, arg327_1, arg328_1, arg329_1, arg330_1])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
