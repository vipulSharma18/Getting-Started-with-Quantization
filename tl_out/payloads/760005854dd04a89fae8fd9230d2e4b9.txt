# AOT ID: ['2_inference']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from cmath import nanj
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()
empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p


# kernel path: /tmp/torchinductor_root/dq/cdqk3nibulbp6wlofbpnfly3ltdohevtedcgpxhr2krsjnwnwbks.py
# Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation], Original ATen: []
# Source node to ATen node mapping:
#   triton_kernel_wrapper_mutation => triton_kernel_wrapper_mutation_64
# Graph fragment:
#   %triton_kernel_wrapper_mutation_64 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 131, constant_args_idx: 130, grid: [(%arg0_1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty, X_ptr: %view_4, W_ptr: %arg10_1, RSTD_ptr: %empty_1}})
triton_poi_fused_0 = async_compile.triton('triton_poi_fused_0', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 65536}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*i64', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.000147528},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x1 = xindex // 4096
    x0 = (xindex % 4096)
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x1), None, eviction_policy='evict_last')
    tmp1 = tl.full([XBLOCK], 128256, tl.int32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp0 < 0
    tmp4 = tl.where(tmp3, tmp2, tmp0)
    tl.device_assert((0 <= tmp4) & (tmp4 < 128256), "index out of bounds: 0 <= tmp4 < 128256")
    tmp6 = tl.load(in_ptr1 + (x0 + 4096*tmp4), None).to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp6, None)


def get_args():
    arg_0 = rand_strided((1, 9), (9, 1), device='cuda:0', dtype=torch.int64)
    arg_1 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 36864,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_0.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_0.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000147528
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# Original path: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:102
_rms_norm_forward_kernel_0 = async_compile.triton('_rms_norm_forward_kernel', '''

import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.user_autotune(
    configs=[{'num_warps': 8, 'num_stages': 3}],
    inductor_meta={'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_name': '_rms_norm_forward_kernel_0', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False},
    triton_meta={'signature': {'Y_ptr': '*fp16', 'Y_row_stride': 'i32', 'X_ptr': '*fp16', 'X_row_stride': 'i32', 'W_ptr': '*fp16', 'W_row_stride': 'constexpr', 'RSTD_ptr': '*fp32', 'RSTD_row_stride': 'constexpr', 'n_cols': 'i32', 'eps': 'fp32', 'offset': 'fp32', 'casting_mode': 'constexpr', 'BLOCK_SIZE': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {'W_row_stride': 1, 'RSTD_row_stride': 1, 'casting_mode': 0, 'BLOCK_SIZE': 4096}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    filename=__file__,
    custom_kernel=True,
)
@triton.jit
def _rms_norm_forward_kernel(
    Y_ptr,
    Y_row_stride,
    X_ptr,
    X_row_stride,
    W_ptr,
    W_row_stride,
    RSTD_ptr,
    RSTD_row_stride,
    n_cols,
    eps,
    offset,
    casting_mode: tl.constexpr,  # constexpr so the `if` blocks can be optimized out
    BLOCK_SIZE: tl.constexpr,
):
    """
    y_i = (x_i / (RMS)) * (offset + wi), RMS = sqrt(sum(x_i^2) / N)

    Reference:
    1. https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html
    2. https://github.com/unslothai/unsloth/blob/fd753fed99ed5f10ef8a9b7139588d9de9ddecfb/unsloth/kernels/rms_layernorm.py#L22
    3. https://arxiv.org/pdf/1910.07467
    """

    row_idx = tl.program_id(0).to(tl.int64)
    col_offsets = tl.arange(0, BLOCK_SIZE)
    mask = col_offsets < n_cols

    Y_ptr += row_idx * Y_row_stride
    X_ptr += row_idx * X_row_stride
    RSTD_ptr += row_idx * RSTD_row_stride

    X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0)
    X_row_dtype = X_row.dtype
    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0)

    # On Llama, only rstd is computed on fp32
    if casting_mode == _CASTING_MODE_LLAMA:
        X_row = X_row.to(tl.float32)

    # Gemma computes everything on fp32, and then casts back the output to the original dtype
    if casting_mode == _CASTING_MODE_GEMMA:
        W_row = W_row.to(tl.float32)
        X_row = X_row.to(tl.float32)

    if casting_mode == _CASTING_MODE_NONE:
        eps = eps.to(X_row_dtype)
        offset = offset.to(X_row_dtype)

    mean_square = tl.sum(X_row * X_row, axis=0) / n_cols
    rstd = rsqrt(mean_square + eps)

    # We can save time by caching rms with minimal memory overhead
    # because rms is much smaller compared to X_row, as rms is for each row.
    # However, on the computation side, it can save 4 operations (*, sum, /, sqrt).
    tl.store(RSTD_ptr, rstd)

    X_row = X_row * rstd

    # On Llama, the multiplication with the weight is done on the original dtype
    if casting_mode == _CASTING_MODE_LLAMA:
        X_row = X_row.to(X_row_dtype)

    Y_row = X_row * (offset + W_row)

    if casting_mode == _CASTING_MODE_GEMMA:
        Y_row = Y_row.to(X_row_dtype)

    tl.store(Y_ptr + col_offsets, Y_row, mask=mask)

_CASTING_MODE_LLAMA: triton.language.core.constexpr = tl.constexpr(0)

_CASTING_MODE_GEMMA: triton.language.core.constexpr = tl.constexpr(1)

_CASTING_MODE_NONE: triton.language.core.constexpr = tl.constexpr(-1)
from triton.language.extra.libdevice import rsqrt as rsqrt
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/qe/cqengdpefzbggwwhisb2owoaen35w6xb6p6ftck2hwkyst4u2hy7.py
# Topologically Sorted Source Nodes: [linear], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   linear => mm
# Graph fragment:
#   %mm : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_8, %permute_1), kwargs = {})
triton_tem_fused_mm_1 = async_compile.triton('triton_tem_fused_mm_1', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=2,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_1', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.033701888},
)
@triton.jit
def triton_tem_fused_mm_1(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 4096
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 4096
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4096

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 4096*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 4096*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)


def get_args():
    arg_0 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = 9
    return arg_0, arg_1, arg_2, arg_3, 128, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_1.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.033701888
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/ud/cudf7phdt3ujo3e7rjtnj3flfq27w2iyn7q6deif3ij3oped2szx.py
# Topologically Sorted Source Nodes: [position_ids_expanded], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   position_ids_expanded => convert_element_type
# Graph fragment:
#   %convert_element_type : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%unsqueeze_2, torch.float32), kwargs = {})
triton_poi_fused__to_copy_2 = async_compile.triton('triton_poi_fused__to_copy_2', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*i64', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_2', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 1.08e-07},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_2(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((1, 9), (9, 1), device='cuda:0', dtype=torch.int64)
    arg_1 = rand_strided((1, 1, 9), (9, 9, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 9,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_2.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_2.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.08e-07
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/rs/crs45qjgwbnivuv54fcuchfghstoxz5vedrxp6g6les5vq6joupx.py
# Topologically Sorted Source Nodes: [position_ids_expanded, matmul], Original ATen: [aten._to_copy, aten.view, aten.bmm]
# Source node to ATen node mapping:
#   matmul => bmm, view_1
#   position_ids_expanded => convert_element_type
# Graph fragment:
#   %convert_element_type : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%unsqueeze_2, torch.float32), kwargs = {})
#   %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%expand_2, [1, 1, %arg5_1]), kwargs = {})
#   %bmm : [num_users=1] = call_function[target=torch.ops.aten.bmm.default](args = (%expand_1, %view_1), kwargs = {})
triton_tem_fused__to_copy_bmm_view_3 = async_compile.triton('triton_tem_fused__to_copy_bmm_view_3', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp32', 'arg_B': '*fp32', 'out_ptr0': '*fp32', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused__to_copy_bmm_view_3', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 2.596e-06},
)
@triton.jit
def triton_tem_fused__to_copy_bmm_view_3(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 16
    BLOCK_K : tl.constexpr = 16
    A = arg_A
    B = arg_B

    M = 64
    N = ks0
    K = 1

    stride_aq = 0
    stride_am = 1
    stride_ak = 0

    stride_bq = ks0
    stride_bk = ks0
    stride_bn = 1

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N

    rk = tl.arange(0, BLOCK_K)

    idx_q = tl.program_id(1)  # batch dimension for BMM
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak + idx_q*stride_aq)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn + idx_q*stride_bq)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_q = tl.program_id(1)  # batch dimension for BMM
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + idx_m*ks0 + 64*idx_q*ks0
    tl.store(out_ptr0 + (tl.broadcast_to(idx_n + idx_m*ks0, acc.shape)), acc, mask)


def get_args():
    arg_0 = rand_strided((64,), (1,), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 1, 9), (9, 9, 1), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 64, 9), (576, 9, 1), device='cuda:0', dtype=torch.float32)
    arg_3 = 9
    return arg_0, arg_1, arg_2, arg_3, 1, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused__to_copy_bmm_view_3.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused__to_copy_bmm_view_3.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.596e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/my/cmy3i5sbxrojdcasdt5iqbn5v3xkylpywyutsqdn2o65rr6xnped.py
# Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   linear_1 => mm_1
# Graph fragment:
#   %mm_1 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_13, %permute_3), kwargs = {})
triton_tem_fused_mm_4 = async_compile.triton('triton_tem_fused_mm_4', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=5,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_4', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.008480768},
)
@triton.jit
def triton_tem_fused_mm_4(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 1024
    K = 4096
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 4096
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4096

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 4096*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 1024*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 4096*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 1024*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)


def get_args():
    arg_0 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((9, 1024), (1024, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = 9
    return arg_0, arg_1, arg_2, arg_3, 16, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_4.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.008480768
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/vw/cvwk3rnxcuwdkcjf5kwnclwjys3ivclobw2qbmmgnicxnsg4xtce.py
# Topologically Sorted Source Nodes: [mul_4, cat_2, mul_5, k_embed, index_copy_], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
# Source node to ATen node mapping:
#   cat_2 => cat_1
#   index_copy_ => index_put
#   k_embed => add_159
#   mul_4 => mul_161
#   mul_5 => mul_178
# Graph fragment:
#   %mul_161 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_4, %unsqueeze_4), kwargs = {})
#   %cat_1 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg_1, %slice_6], -1), kwargs = {})
#   %mul_178 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_1, %unsqueeze_5), kwargs = {})
#   %add_159 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_161, %mul_178), kwargs = {})
#   %index_put : [num_users=1] = call_function[target=torch.ops.aten.index_put_.default](args = (%arg14_1, [None, None, %arg4_1], %add_159), kwargs = {})
triton_poi_fused_add_cat_index_copy_mul_5 = async_compile.triton('triton_poi_fused_add_cat_index_copy_mul_5', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 128, 'x': 128}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*i64', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'ks0': 'i32', 'ks1': 'i32', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_cat_index_copy_mul_5', 'mutated_arg_names': ['out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 3.924e-05},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_cat_index_copy_mul_5(in_ptr0, in_ptr1, in_ptr2, out_ptr0, ks0, ks1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 128
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex // 8
    x3 = xindex
    y0 = yindex
    x1 = (xindex % 8)
    tmp0 = tl.load(in_ptr0 + (x2), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr1 + (y0 + 128*x3), ymask & xmask, eviction_policy='evict_last').to(tl.float32)
    tmp7 = tl.load(in_ptr2 + (x2 + ks1*((y0 % 64))), xmask & ymask, eviction_policy='evict_last')
    tmp1 = ks0
    tmp2 = tmp0 + tmp1
    tmp3 = tmp0 < 0
    tmp4 = tl.where(tmp3, tmp2, tmp0)
    tl.device_assert(((0 <= tmp4) & (tmp4 < ks0)) | ~(xmask), "index out of bounds: 0 <= tmp4 < ks0")
    tmp8 = tl_math.cos(tmp7)
    tmp9 = 1.0
    tmp10 = tmp8 * tmp9
    tmp11 = tmp10.to(tl.float32)
    tmp12 = tmp6 * tmp11
    tmp13 = y0
    tmp14 = tl.full([1, 1], 0, tl.int64)
    tmp15 = tmp13 >= tmp14
    tmp16 = tl.full([1, 1], 64, tl.int64)
    tmp17 = tmp13 < tmp16
    tmp18 = tl.load(in_ptr1 + (64 + 128*x3 + (y0)), ymask & xmask & tmp17, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp19 = -tmp18
    tmp20 = tl.full(tmp19.shape, 0.0, tmp19.dtype)
    tmp21 = tl.where(tmp17, tmp19, tmp20)
    tmp22 = tmp13 >= tmp16
    tmp23 = tl.full([1, 1], 128, tl.int64)
    tmp24 = tmp13 < tmp23
    tmp25 = tl.load(in_ptr1 + (128*x3 + ((-64) + y0)), ymask & xmask & tmp22, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp26 = tl.where(tmp17, tmp21, tmp25)
    tmp27 = tl_math.sin(tmp7)
    tmp28 = tmp27 * tmp9
    tmp29 = tmp28.to(tl.float32)
    tmp30 = tmp26 * tmp29
    tmp31 = tmp12 + tmp30
    tl.store(out_ptr0 + (y0 + 128*tmp4 + 128*ks0*x1), tmp31, xmask & ymask)


def get_args():
    arg_0 = rand_strided((9,), (1,), device='cuda:0', dtype=torch.int64)
    arg_1 = rand_strided((9, 1024), (1024, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 64, 9), (576, 9, 1), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = 2048
    arg_5 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 128, 72,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_cat_index_copy_mul_5.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.924e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/cj/ccjvxck7gbbap6lsy2jz3xz6ilsdeifkhcghvakvcoqmz6ygx5nk.py
# Topologically Sorted Source Nodes: [index_copy__1], Original ATen: [aten.index_copy]
# Source node to ATen node mapping:
#   index_copy__1 => index_put_1
# Graph fragment:
#   %index_put_1 : [num_users=1] = call_function[target=torch.ops.aten.index_put_.default](args = (%arg15_1, [None, None, %arg4_1], %permute_6), kwargs = {})
triton_poi_fused_index_copy_6 = async_compile.triton('triton_poi_fused_index_copy_6', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*i64', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_index_copy_6', 'mutated_arg_names': ['out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 3.6936e-05},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_index_copy_6(in_ptr0, in_ptr1, out_ptr0, ks0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex // 1024
    x3 = xindex
    x0 = (xindex % 128)
    x1 = ((xindex // 128) % 8)
    tmp0 = tl.load(in_ptr0 + (x2), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr1 + (x3), xmask).to(tl.float32)
    tmp1 = ks0
    tmp2 = tmp0 + tmp1
    tmp3 = tmp0 < 0
    tmp4 = tl.where(tmp3, tmp2, tmp0)
    tl.device_assert(((0 <= tmp4) & (tmp4 < ks0)) | ~(xmask), "index out of bounds: 0 <= tmp4 < ks0")
    tl.store(out_ptr0 + (x0 + 128*tmp4 + 128*ks0*x1), tmp6, xmask)


def get_args():
    arg_0 = rand_strided((9,), (1,), device='cuda:0', dtype=torch.int64)
    arg_1 = rand_strided((9, 1024), (1024, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = 2048
    return arg_0, arg_1, arg_2, arg_3, 9216,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_index_copy_6.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.6936e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/4a/c4awa7jqyu6prtxgmnzwljfnbfi3pa6m2ufnuuq3uwyzq5ct5bo4.py
# Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed, query, attn_output], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
# Source node to ATen node mapping:
#   attn_output => _scaled_dot_product_efficient_attention
#   cat_1 => cat
#   mul_2 => mul_136
#   mul_3 => mul_153
#   q_embed => add_135
#   query => clone_4
# Graph fragment:
#   %mul_136 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_2, %unsqueeze_4), kwargs = {})
#   %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg, %slice_4], -1), kwargs = {})
#   %mul_153 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat, %unsqueeze_5), kwargs = {})
#   %add_135 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_136, %mul_153), kwargs = {})
#   %clone_4 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%add_135,), kwargs = {memory_format: torch.contiguous_format})
#   %_scaled_dot_product_efficient_attention : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%clone_4, %view_21, %view_22, %expand_8, False), kwargs = {scale: 0.08838834764831845})
triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7 = async_compile.triton('triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 65536}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'out_ptr0': '*fp16', 'ks0': 'i32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.00014976},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7(in_ptr0, in_ptr1, out_ptr0, ks0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x3 = xindex
    x0 = (xindex % 128)
    x2 = xindex // 4096
    x4 = xindex // 128
    x1 = ((xindex // 128) % 32)
    tmp0 = tl.load(in_ptr0 + (x3), None).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (x2 + ks0*((x0 % 64))), None, eviction_policy='evict_last')
    tmp2 = tl_math.cos(tmp1)
    tmp3 = 1.0
    tmp4 = tmp2 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tmp6 = tmp0 * tmp5
    tmp7 = x0
    tmp8 = tl.full([1], 0, tl.int64)
    tmp9 = tmp7 >= tmp8
    tmp10 = tl.full([1], 64, tl.int64)
    tmp11 = tmp7 < tmp10
    tmp12 = tl.load(in_ptr0 + (64 + 128*x4 + (x0)), tmp11, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp13 = -tmp12
    tmp14 = tl.full(tmp13.shape, 0.0, tmp13.dtype)
    tmp15 = tl.where(tmp11, tmp13, tmp14)
    tmp16 = tmp7 >= tmp10
    tmp17 = tl.full([1], 128, tl.int64)
    tmp18 = tmp7 < tmp17
    tmp19 = tl.load(in_ptr0 + (128*x4 + ((-64) + x0)), tmp16, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp20 = tl.where(tmp11, tmp15, tmp19)
    tmp21 = tl_math.sin(tmp1)
    tmp22 = tmp21 * tmp3
    tmp23 = tmp22.to(tl.float32)
    tmp24 = tmp20 * tmp23
    tmp25 = tmp6 + tmp24
    tl.store(out_ptr0 + (x0 + 128*x2 + 128*ks0*x1), tmp25, None)


def get_args():
    arg_0 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 64, 9), (576, 9, 1), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 32, 9, 128), (36864, 1152, 128, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = 9
    return arg_0, arg_1, arg_2, arg_3, 36864,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00014976
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/io/cio4axdsox6ri7i332k2lgezt4a4l2iee3bb6kggutb5ouzdga5o.py
# Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed, query, attn_output], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
# Source node to ATen node mapping:
#   attn_output => _scaled_dot_product_efficient_attention
#   cat_1 => cat
#   mul_2 => mul_136
#   mul_3 => mul_153
#   q_embed => add_135
#   query => clone_4
# Graph fragment:
#   %mul_136 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_2, %unsqueeze_4), kwargs = {})
#   %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg, %slice_4], -1), kwargs = {})
#   %mul_153 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat, %unsqueeze_5), kwargs = {})
#   %add_135 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_136, %mul_153), kwargs = {})
#   %clone_4 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%add_135,), kwargs = {memory_format: torch.contiguous_format})
#   %_scaled_dot_product_efficient_attention : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%clone_4, %view_21, %view_22, %expand_8, False), kwargs = {scale: 0.08838834764831845})
triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8 = async_compile.triton('triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32', 'ks1': 'i32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.016777216},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8(in_ptr0, out_ptr0, ks0, ks1, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % ks0)
    x1 = xindex // ks0
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 128*ks1*(x1 // 4)), None, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp0, None)


def get_args():
    arg_0 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 32, 2048, 128), (8388608, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = 262144
    arg_3 = 2048
    return arg_0, arg_1, arg_2, arg_3, 8388608,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.016777216
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/gl/cgl7uaeyljipoasa3iwgw5whgfzgqx5ppq2xu7j6mw5vpzozf2sr.py
# Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed, query, attn_output, mul_9, cat_3, mul_10, q_embed_1, query_1, attn_output_4, mul_14, cat_5, mul_15, q_embed_2, query_2, attn_output_8], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
# Source node to ATen node mapping:
#   attn_output => _scaled_dot_product_efficient_attention
#   attn_output_4 => _scaled_dot_product_efficient_attention_1
#   attn_output_8 => _scaled_dot_product_efficient_attention_2
#   cat_1 => cat
#   cat_3 => cat_2
#   cat_5 => cat_4
#   mul_10 => mul_546
#   mul_14 => mul_922
#   mul_15 => mul_939
#   mul_2 => mul_136
#   mul_3 => mul_153
#   mul_9 => mul_529
#   q_embed => add_135
#   q_embed_1 => add_448
#   q_embed_2 => add_761
#   query => clone_4
#   query_1 => clone_7
#   query_2 => clone_10
# Graph fragment:
#   %mul_136 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_2, %unsqueeze_4), kwargs = {})
#   %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg, %slice_4], -1), kwargs = {})
#   %mul_153 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat, %unsqueeze_5), kwargs = {})
#   %add_135 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_136, %mul_153), kwargs = {})
#   %clone_4 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%add_135,), kwargs = {memory_format: torch.contiguous_format})
#   %_scaled_dot_product_efficient_attention : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%clone_4, %view_21, %view_22, %expand_8, False), kwargs = {scale: 0.08838834764831845})
#   %mul_529 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_13, %unsqueeze_10), kwargs = {})
#   %cat_2 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg_2, %slice_27], -1), kwargs = {})
#   %mul_546 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_2, %unsqueeze_11), kwargs = {})
#   %add_448 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_529, %mul_546), kwargs = {})
#   %clone_7 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%add_448,), kwargs = {memory_format: torch.contiguous_format})
#   %_scaled_dot_product_efficient_attention_1 : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%clone_7, %view_55, %view_56, %expand_13, False), kwargs = {scale: 0.08838834764831845})
#   %mul_922 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_24, %unsqueeze_16), kwargs = {})
#   %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg_4, %slice_50], -1), kwargs = {})
#   %mul_939 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_4, %unsqueeze_17), kwargs = {})
#   %add_761 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_922, %mul_939), kwargs = {})
#   %clone_10 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%add_761,), kwargs = {memory_format: torch.contiguous_format})
#   %_scaled_dot_product_efficient_attention_2 : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%clone_10, %view_89, %view_90, %expand_18, False), kwargs = {scale: 0.08838834764831845})
triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9 = async_compile.triton('triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*i1', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'out_ptr2': '*fp16', 'ks0': 'i32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.000129024},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9(in_ptr0, out_ptr0, out_ptr1, out_ptr2, ks0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % ks0)
    x1 = xindex // ks0
    tmp0 = tl.load(in_ptr0 + (x2), xmask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = 0.0
    tmp2 = float("-inf")
    tmp3 = tl.where(tmp0, tmp1, tmp2)
    tl.store(out_ptr0 + (x0 + 8*x1*((7 + ks0) // 8)), tmp3, xmask)
    tl.store(out_ptr1 + (x0 + 8*x1*((7 + ks0) // 8)), tmp3, xmask)
    tl.store(out_ptr2 + (x0 + 8*x1*((7 + ks0) // 8)), tmp3, xmask)


def get_args():
    arg_0 = rand_strided((1, 1, 9, 2048), (18432, 18432, 2048, 1), device='cuda:0', dtype=torch.bool)
    arg_1 = rand_strided((1, 1, 9, 2048), (18432, 0, 2048, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 1, 9, 2048), (18432, 0, 2048, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 1, 9, 2048), (18432, 0, 2048, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = 2048
    return arg_0, arg_1, arg_2, arg_3, arg_4, 18432,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000129024
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/6n/c6nbzdcavn3nursdbmra76gutfseoor6b6xn3yxagdy54rd7fr4t.py
# Topologically Sorted Source Nodes: [attn_output_3, triton_kernel_wrapper_mutation_1], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   attn_output_3 => mm_3
#   triton_kernel_wrapper_mutation_1 => triton_kernel_wrapper_mutation_63
# Graph fragment:
#   %mm_3 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_24, %permute_8), kwargs = {})
#   %triton_kernel_wrapper_mutation_63 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 132, constant_args_idx: 131, grid: [(%arg0_1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_2, X_ptr: %view_26, W_ptr: %arg17_1, RSTD_ptr: %empty_3}})
triton_tem_fused_mm_10 = async_compile.triton('triton_tem_fused_mm_10', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=2,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'in_ptr2': '*i64', 'in_ptr3': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_10', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.033701888},
)
@triton.jit
def triton_tem_fused_mm_10(arg_A, arg_B, in_ptr2, in_ptr3, out_ptr0, out_ptr1, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 4096
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 4096
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4096

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 4096*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 4096*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
    tmp0 = tl.load(in_ptr2 + (tl.broadcast_to(idx_m, acc.shape)), mask, eviction_policy='evict_last')
    tmp1 = tl.full(acc.shape, 128256, tl.int32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp0 < 0
    tmp4 = tl.where(tmp3, tmp2, tmp0)
    tl.device_assert(((0 <= tl.broadcast_to(tmp4, acc.shape)) & (tl.broadcast_to(tmp4, acc.shape) < 128256)) | ~(mask), "index out of bounds: 0 <= tl.broadcast_to(tmp4, acc.shape) < 128256")
    tmp6 = tl.load(in_ptr3 + (tl.broadcast_to(idx_n + 4096*tmp4, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp7 = tmp6 + acc
    tl.store(out_ptr1 + (tl.broadcast_to(idx_n + 4096*idx_m, acc.shape)), tmp7, mask)


def get_args():
    arg_0 = rand_strided((1, 32, 9, 128), (36864, 128, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 9), (9, 1), device='cuda:0', dtype=torch.int64)
    arg_3 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_6 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 128, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_10.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_10.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.033701888
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/xj/cxjdeujlkmrt6gcxj2stndaatdtwr7o5mglilzqfwb7ogwepnhet.py
# Topologically Sorted Source Nodes: [linear_4], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   linear_4 => mm_4
# Graph fragment:
#   %mm_4 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_30, %permute_9), kwargs = {})
triton_tem_fused_mm_11 = async_compile.triton('triton_tem_fused_mm_11', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=2,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_11', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.117772288},
)
@triton.jit
def triton_tem_fused_mm_11(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 14336
    K = 4096
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 4096
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4096

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 4096*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 14336*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 4096*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 14336*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)


def get_args():
    arg_0 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((9, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = 9
    return arg_0, arg_1, arg_2, arg_3, 448, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_11.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.117772288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/w3/cw3zijxpf3ahrnv6wm7o42l6augaxyu2lpvaplh5tjonor6cibvj.py
# Topologically Sorted Source Nodes: [silu, linear_5, mul_8], Original ATen: [aten.silu, aten.mm, aten.mul]
# Source node to ATen node mapping:
#   linear_5 => mm_5
#   mul_8 => mul_427
#   silu => convert_element_type_13, convert_element_type_14, mul_410, sigmoid
# Graph fragment:
#   %convert_element_type_13 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%view_31, torch.float32), kwargs = {})
#   %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_13,), kwargs = {})
#   %mul_410 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_13, %sigmoid), kwargs = {})
#   %convert_element_type_14 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_410, torch.float16), kwargs = {})
#   %mm_5 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_34, %permute_10), kwargs = {})
#   %mul_427 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_14, %view_35), kwargs = {})
triton_tem_fused_mm_mul_silu_12 = async_compile.triton('triton_tem_fused_mm_mul_silu_12', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=2,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'in_ptr2': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_mul_silu_12', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.117772288},
)
@triton.jit
def triton_tem_fused_mm_mul_silu_12(arg_A, arg_B, in_ptr2, out_ptr1, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 14336
    K = 4096
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 4096
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4096

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 4096*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 14336*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 4096*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 14336*idx_m
    tmp0 = tl.load(in_ptr2 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tmp2 = tl.sigmoid(tmp1)
    tmp3 = tmp1 * tmp2
    tmp4 = tmp3.to(tl.float32)
    tmp5 = tmp4 * acc
    tl.store(out_ptr1 + (tl.broadcast_to(xindex, acc.shape)), tmp5, mask)


def get_args():
    arg_0 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((9, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 9, 14336), (129024, 14336, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, 448, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_mul_silu_12.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.117772288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/tf/ctf3cj2t7awpgyftbnyovjoclwg6ecdrrmwamfeojhobdpvzleu7.py
# Topologically Sorted Source Nodes: [down_proj, triton_kernel_wrapper_mutation_2], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   down_proj => mm_6
#   triton_kernel_wrapper_mutation_2 => triton_kernel_wrapper_mutation_62
# Graph fragment:
#   %mm_6 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_36, %permute_11), kwargs = {})
#   %triton_kernel_wrapper_mutation_62 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 133, constant_args_idx: 132, grid: [(%arg0_1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_4, X_ptr: %view_38, W_ptr: %arg21_1, RSTD_ptr: %empty_5}})
triton_tem_fused_mm_13 = async_compile.triton('triton_tem_fused_mm_13', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'in_ptr2': '*i64', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_13', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.117772288},
)
@triton.jit
def triton_tem_fused_mm_13(arg_A, arg_B, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 14336
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 14336
    stride_ak = 1
    stride_bk = 1
    stride_bn = 14336

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 14336*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 14336*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
    tmp0 = tl.load(in_ptr2 + (tl.broadcast_to(idx_m, acc.shape)), mask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (tl.broadcast_to(idx_n + 4096*idx_m, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tl.full(acc.shape, 128256, tl.int32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp0 < 0
    tmp4 = tl.where(tmp3, tmp2, tmp0)
    tl.device_assert(((0 <= tl.broadcast_to(tmp4, acc.shape)) & (tl.broadcast_to(tmp4, acc.shape) < 128256)) | ~(mask), "index out of bounds: 0 <= tl.broadcast_to(tmp4, acc.shape) < 128256")
    tmp6 = tl.load(in_ptr3 + (tl.broadcast_to(idx_n + 4096*tmp4, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp8 = tmp6 + tmp7
    tmp9 = tmp8 + acc
    tl.store(out_ptr1 + (tl.broadcast_to(idx_n + 4096*idx_m, acc.shape)), tmp9, mask)


def get_args():
    arg_0 = rand_strided((1, 9, 14336), (129024, 14336, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 9), (9, 1), device='cuda:0', dtype=torch.int64)
    arg_3 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_7 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 64, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_13.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_13.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.117772288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/5b/c5bupfbz2y6lqjz4j2iicw7oacshwdapabrdlndofqifwn7hngpn.py
# Topologically Sorted Source Nodes: [attn_output_7], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   attn_output_7 => mm_10
# Graph fragment:
#   %mm_10 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_58, %permute_19), kwargs = {})
triton_tem_fused_mm_14 = async_compile.triton('triton_tem_fused_mm_14', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=2,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_14', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.033701888},
)
@triton.jit
def triton_tem_fused_mm_14(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 4096
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 4096
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4096

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 4096*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 4096*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)


def get_args():
    arg_0 = rand_strided((1, 32, 9, 128), (36864, 128, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = 9
    return arg_0, arg_1, arg_2, arg_3, 128, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_14.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_14.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.033701888
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/qj/cqjs3m4jxszja2xb77x65efgsdqoxnkkd2sehymf4l53vgwesybm.py
# Topologically Sorted Source Nodes: [inputs_embeds, hidden_states_2, hidden_states_3, hidden_states_6], Original ATen: [aten.embedding, aten.add]
# Source node to ATen node mapping:
#   hidden_states_2 => add_301
#   hidden_states_3 => add_354
#   hidden_states_6 => add_614
#   inputs_embeds => embedding
# Graph fragment:
#   %embedding : [num_users=2] = call_function[target=torch.ops.aten.embedding.default](args = (%arg2_1, %arg1_1, 128004), kwargs = {})
#   %add_301 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%embedding, %view_25), kwargs = {})
#   %add_354 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_301, %view_37), kwargs = {})
#   %add_614 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_354, %view_59), kwargs = {})
triton_poi_fused_add_embedding_15 = async_compile.triton('triton_poi_fused_add_embedding_15', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 65536}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*i64', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_embedding_15', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.000368712},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_embedding_15(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x1 = xindex // 4096
    x0 = (xindex % 4096)
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x1), None, eviction_policy='evict_last')
    tmp7 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp9 = tl.load(in_ptr2 + (x2), None).to(tl.float32)
    tmp11 = tl.load(in_ptr3 + (x2), None).to(tl.float32)
    tmp1 = tl.full([XBLOCK], 128256, tl.int32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp0 < 0
    tmp4 = tl.where(tmp3, tmp2, tmp0)
    tl.device_assert((0 <= tmp4) & (tmp4 < 128256), "index out of bounds: 0 <= tmp4 < 128256")
    tmp6 = tl.load(in_ptr1 + (x0 + 4096*tmp4), None).to(tl.float32)
    tmp8 = tmp6 + tmp7
    tmp10 = tmp8 + tmp9
    tmp12 = tmp10 + tmp11
    tl.store(in_out_ptr0 + (x2), tmp12, None)


def get_args():
    arg_0 = rand_strided((1, 9, 4096), (36864, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 9), (9, 1), device='cuda:0', dtype=torch.int64)
    arg_2 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 36864,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_embedding_15.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_embedding_15.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000368712
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/63/c636j3kev4ezjzig5vxd3hlv2a2twxoc63fg2kcodwlr34sm7cai.py
# Topologically Sorted Source Nodes: [down_proj_1, triton_kernel_wrapper_mutation_4], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   down_proj_1 => mm_13
#   triton_kernel_wrapper_mutation_4 => triton_kernel_wrapper_mutation_60
# Graph fragment:
#   %mm_13 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_70, %permute_22), kwargs = {})
#   %triton_kernel_wrapper_mutation_60 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 135, constant_args_idx: 134, grid: [(%arg0_1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_8, X_ptr: %view_72, W_ptr: %arg32_1, RSTD_ptr: %empty_9}})
triton_tem_fused_mm_16 = async_compile.triton('triton_tem_fused_mm_16', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'in_ptr2': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_16', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.117772288},
)
@triton.jit
def triton_tem_fused_mm_16(arg_A, arg_B, in_ptr2, out_ptr0, out_ptr1, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 14336
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 14336
    stride_ak = 1
    stride_bk = 1
    stride_bn = 14336

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 14336*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 14336*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
    tmp0 = tl.load(in_ptr2 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tmp0 + acc
    tl.store(out_ptr1 + (tl.broadcast_to(xindex, acc.shape)), tmp1, mask)


def get_args():
    arg_0 = rand_strided((1, 9, 14336), (129024, 14336, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 9, 4096), (36864, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_5 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 64, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_16.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_16.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.117772288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/a6/ca66f6cweuivuhv26wqhudxffxthpe3bsuilvvxrwzpew6fcg3ic.py
# Topologically Sorted Source Nodes: [attn_output_11, triton_kernel_wrapper_mutation_5], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   attn_output_11 => mm_17
#   triton_kernel_wrapper_mutation_5 => triton_kernel_wrapper_mutation_59
# Graph fragment:
#   %mm_17 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_92, %permute_30), kwargs = {})
#   %triton_kernel_wrapper_mutation_59 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 136, constant_args_idx: 135, grid: [(%arg0_1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_10, X_ptr: %view_94, W_ptr: %arg39_1, RSTD_ptr: %empty_11}})
triton_tem_fused_mm_17 = async_compile.triton('triton_tem_fused_mm_17', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=2,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_17', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.033701888},
)
@triton.jit
def triton_tem_fused_mm_17(arg_A, arg_B, in_ptr2, in_ptr3, out_ptr0, out_ptr1, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 4096
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 4096
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4096

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 4096*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 4096*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
    tmp0 = tl.load(in_ptr2 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tl.load(in_ptr3 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2 + acc
    tl.store(out_ptr1 + (tl.broadcast_to(xindex, acc.shape)), tmp3, mask)


def get_args():
    arg_0 = rand_strided((1, 32, 9, 128), (36864, 128, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 9, 4096), (36864, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_6 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 128, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_17.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.033701888
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/lk/clkgjbcqncitck56vfsvl4r724zordksghudm4lurikvrr7ozh4h.py
# Topologically Sorted Source Nodes: [down_proj_2, triton_kernel_wrapper_mutation_6], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   down_proj_2 => mm_20
#   triton_kernel_wrapper_mutation_6 => triton_kernel_wrapper_mutation_58
# Graph fragment:
#   %mm_20 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_104, %permute_33), kwargs = {})
#   %triton_kernel_wrapper_mutation_58 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 137, constant_args_idx: 136, grid: [(%arg0_1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_12, X_ptr: %view_106, W_ptr: %arg43_1, RSTD_ptr: %empty_13}})
triton_tem_fused_mm_18 = async_compile.triton('triton_tem_fused_mm_18', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_18', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.117772288},
)
@triton.jit
def triton_tem_fused_mm_18(arg_A, arg_B, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 14336
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 14336
    stride_ak = 1
    stride_bk = 1
    stride_bn = 14336

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 14336*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 14336*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
    tmp0 = tl.load(in_ptr2 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tl.load(in_ptr3 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp3 = tl.load(in_ptr4 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp4 = tmp2 + tmp3
    tmp5 = tmp4 + acc
    tl.store(out_ptr1 + (tl.broadcast_to(xindex, acc.shape)), tmp5, mask)


def get_args():
    arg_0 = rand_strided((1, 9, 14336), (129024, 14336, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 9, 4096), (36864, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_7 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 64, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_18.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.117772288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/qf/cqfl3463h5b6ggd6gd5pcn7bqpuycpsc2c2jnmo26d7ugottzqoq.py
# Topologically Sorted Source Nodes: [hidden_states_7, hidden_states_10, hidden_states_11, attn_output_15, hidden_states_14], Original ATen: [aten.add, aten.mm]
# Source node to ATen node mapping:
#   attn_output_15 => mm_24
#   hidden_states_10 => add_927
#   hidden_states_11 => add_980
#   hidden_states_14 => add_1240
#   hidden_states_7 => add_667
# Graph fragment:
#   %add_667 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_614, %view_71), kwargs = {})
#   %add_927 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_667, %view_93), kwargs = {})
#   %add_980 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_927, %view_105), kwargs = {})
#   %mm_24 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_126, %permute_41), kwargs = {})
#   %add_1240 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_980, %view_127), kwargs = {})
triton_tem_fused_add_mm_19 = async_compile.triton('triton_tem_fused_add_mm_19', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=2,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_add_mm_19', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.033701888},
)
@triton.jit
def triton_tem_fused_add_mm_19(arg_A, arg_B, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr1, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 4096
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 4096
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4096

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 4096*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 4096*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tmp0 = tl.load(in_ptr2 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tl.load(in_ptr3 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp3 = tl.load(in_ptr4 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp5 = tl.load(in_ptr5 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp4 = tmp2 + tmp3
    tmp6 = tmp4 + tmp5
    tmp7 = tmp6 + acc
    tl.store(out_ptr1 + (tl.broadcast_to(xindex, acc.shape)), tmp7, mask)


def get_args():
    arg_0 = rand_strided((1, 32, 9, 128), (36864, 128, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 9, 4096), (36864, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((1, 9, 4096), (36864, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_7 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 128, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_19.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_add_mm_19.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.033701888
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/no/cnoak6b6nrnuk7bqtqtihgk6pkdswvfu2mchjvxszxherrdp6vvw.py
# Topologically Sorted Source Nodes: [mul_154, cat_61, mul_155, q_embed_30, query_30, attn_output_120, mul_159, cat_63, mul_160, q_embed_31, query_31, attn_output_124], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
# Source node to ATen node mapping:
#   attn_output_120 => _scaled_dot_product_efficient_attention_30
#   attn_output_124 => _scaled_dot_product_efficient_attention_31
#   cat_61 => cat_60
#   cat_63 => cat_62
#   mul_154 => mul_11926
#   mul_155 => mul_11943
#   mul_159 => mul_12319
#   mul_160 => mul_12336
#   q_embed_30 => add_9525
#   q_embed_31 => add_9838
#   query_30 => clone_94
#   query_31 => clone_97
# Graph fragment:
#   %mul_11926 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_332, %unsqueeze_184), kwargs = {})
#   %cat_60 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg_60, %slice_694], -1), kwargs = {})
#   %mul_11943 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_60, %unsqueeze_185), kwargs = {})
#   %add_9525 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_11926, %mul_11943), kwargs = {})
#   %clone_94 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%add_9525,), kwargs = {memory_format: torch.contiguous_format})
#   %_scaled_dot_product_efficient_attention_30 : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%clone_94, %view_1041, %view_1042, %expand_158, False), kwargs = {scale: 0.08838834764831845})
#   %mul_12319 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_343, %unsqueeze_190), kwargs = {})
#   %cat_62 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg_62, %slice_717], -1), kwargs = {})
#   %mul_12336 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_62, %unsqueeze_191), kwargs = {})
#   %add_9838 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_12319, %mul_12336), kwargs = {})
#   %clone_97 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%add_9838,), kwargs = {memory_format: torch.contiguous_format})
#   %_scaled_dot_product_efficient_attention_31 : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%clone_97, %view_1075, %view_1076, %expand_163, False), kwargs = {scale: 0.08838834764831845})
triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_20 = async_compile.triton('triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_20', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*i1', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_20', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 9.216e-05},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_20(in_ptr0, out_ptr0, out_ptr1, ks0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % ks0)
    x1 = xindex // ks0
    tmp0 = tl.load(in_ptr0 + (x2), xmask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = 0.0
    tmp2 = float("-inf")
    tmp3 = tl.where(tmp0, tmp1, tmp2)
    tl.store(out_ptr0 + (x0 + 8*x1*((7 + ks0) // 8)), tmp3, xmask)
    tl.store(out_ptr1 + (x0 + 8*x1*((7 + ks0) // 8)), tmp3, xmask)


def get_args():
    arg_0 = rand_strided((1, 1, 9, 2048), (18432, 18432, 2048, 1), device='cuda:0', dtype=torch.bool)
    arg_1 = rand_strided((1, 1, 9, 2048), (18432, 0, 2048, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 1, 9, 2048), (18432, 0, 2048, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = 2048
    return arg_0, arg_1, arg_2, arg_3, 18432,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_20.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_20.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9.216e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/bi/cbi6qmrnpblaizrczsg5mnreacds5xanf7gfzqbe7jueunmclq64.py
# Topologically Sorted Source Nodes: [down_proj_31, triton_kernel_wrapper_mutation_64], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   down_proj_31 => mm_223
#   triton_kernel_wrapper_mutation_64 => triton_kernel_wrapper_mutation
# Graph fragment:
#   %mm_223 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_1090, %permute_352), kwargs = {})
#   %triton_kernel_wrapper_mutation : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 195, constant_args_idx: 194, grid: [(%arg0_1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_128, X_ptr: %view_1092, W_ptr: %arg362_1, RSTD_ptr: %empty_129}})
triton_tem_fused_mm_21 = async_compile.triton('triton_tem_fused_mm_21', '''
from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch



import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'in_ptr2': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_21', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_num_gb': 0.117772288},
)
@triton.jit
def triton_tem_fused_mm_21(arg_A, arg_B, in_ptr2, out_ptr1, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = True
    ACC_TYPE : tl.constexpr = tl.float32
    BLOCK_M : tl.constexpr = 16
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = ks0
    N = 4096
    K = 14336
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 14336
    stride_ak = 1
    stride_bk = 1
    stride_bn = 14336

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if ((stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1)) and M >= BLOCK_M:
        offs_a_m = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        offs_a_m = rm % M
    if ((stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1)) and N >= BLOCK_N:
        offs_b_n = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        offs_b_n = rn % N
    offs_k = tl.arange(0, BLOCK_K)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)

    for k_idx in range(0, tl.cdiv(K, BLOCK_K)):

        a_k_idx_vals = offs_k[None, :] + (k_idx * BLOCK_K)
        b_k_idx_vals = offs_k[:, None] + (k_idx * BLOCK_K)

        idx_m = offs_a_m[:, None]
        idx_n = a_k_idx_vals
        xindex = idx_n + 14336*idx_m
        a = tl.load(A + (xindex))

        idx_m = b_k_idx_vals
        idx_n = offs_b_n[None, :]
        xindex = idx_n + 4096*idx_m
        b = tl.load(B + ((tl.broadcast_to(idx_m + 14336*idx_n, xindex.shape)).broadcast_to(xindex.shape)))
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 4096*idx_m
    tmp0 = tl.load(in_ptr2 + (tl.broadcast_to(xindex, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tmp0 + acc
    tl.store(out_ptr1 + (tl.broadcast_to(xindex, acc.shape)), tmp1, mask)


def get_args():
    arg_0 = rand_strided((1, 9, 14336), (129024, 14336, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 9, 4096), (36864, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = 9
    return arg_0, arg_1, arg_2, arg_3, arg_4, 64, 1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_21.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_tem_fused_mm_21.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.117772288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/ih/ciheyjmgvijphpuk2ge4kvnq2bhtfoydluimu3qewvvehqthnpgl.py
# Topologically Sorted Source Nodes: [logits], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   logits => convert_element_type_517, mul_12654, sum_1
# Graph fragment:
#   %mul_12654 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%unsqueeze_196, %unsqueeze_197), kwargs = {})
#   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_12654, [1]), kwargs = {})
#   %convert_element_type_517 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_1, torch.float16), kwargs = {})
triton_red_fused_mm_22 = async_compile.triton('triton_red_fused_mm_22', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 4096},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'out_ptr1': '*fp16', 'ks0': 'i32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mm_22', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 1.050929664}
)
@triton.jit
def triton_red_fused_mm_22(in_ptr0, in_ptr1, out_ptr1, ks0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 128256
    r0_numel = 4096
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp6 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + ((-4096) + r0_1 + 4096*ks0), r0_mask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tl.load(in_ptr1 + (r0_1 + 4096*x0), xmask & r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tmp1 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, R0_BLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(r0_mask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tmp8 = tmp6.to(tl.float32)
    tl.store(out_ptr1 + (x0), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((9, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 128256), (128256, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = 9
    return arg_0, arg_1, arg_2, arg_3, 128256, 4096,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_22.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mm_22.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.050929664
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1, arg27_1, arg28_1, arg29_1, arg30_1, arg31_1, arg32_1, arg33_1, arg34_1, arg35_1, arg36_1, arg37_1, arg38_1, arg39_1, arg40_1, arg41_1, arg42_1, arg43_1, arg44_1, arg45_1, arg46_1, arg47_1, arg48_1, arg49_1, arg50_1, arg51_1, arg52_1, arg53_1, arg54_1, arg55_1, arg56_1, arg57_1, arg58_1, arg59_1, arg60_1, arg61_1, arg62_1, arg63_1, arg64_1, arg65_1, arg66_1, arg67_1, arg68_1, arg69_1, arg70_1, arg71_1, arg72_1, arg73_1, arg74_1, arg75_1, arg76_1, arg77_1, arg78_1, arg79_1, arg80_1, arg81_1, arg82_1, arg83_1, arg84_1, arg85_1, arg86_1, arg87_1, arg88_1, arg89_1, arg90_1, arg91_1, arg92_1, arg93_1, arg94_1, arg95_1, arg96_1, arg97_1, arg98_1, arg99_1, arg100_1, arg101_1, arg102_1, arg103_1, arg104_1, arg105_1, arg106_1, arg107_1, arg108_1, arg109_1, arg110_1, arg111_1, arg112_1, arg113_1, arg114_1, arg115_1, arg116_1, arg117_1, arg118_1, arg119_1, arg120_1, arg121_1, arg122_1, arg123_1, arg124_1, arg125_1, arg126_1, arg127_1, arg128_1, arg129_1, arg130_1, arg131_1, arg132_1, arg133_1, arg134_1, arg135_1, arg136_1, arg137_1, arg138_1, arg139_1, arg140_1, arg141_1, arg142_1, arg143_1, arg144_1, arg145_1, arg146_1, arg147_1, arg148_1, arg149_1, arg150_1, arg151_1, arg152_1, arg153_1, arg154_1, arg155_1, arg156_1, arg157_1, arg158_1, arg159_1, arg160_1, arg161_1, arg162_1, arg163_1, arg164_1, arg165_1, arg166_1, arg167_1, arg168_1, arg169_1, arg170_1, arg171_1, arg172_1, arg173_1, arg174_1, arg175_1, arg176_1, arg177_1, arg178_1, arg179_1, arg180_1, arg181_1, arg182_1, arg183_1, arg184_1, arg185_1, arg186_1, arg187_1, arg188_1, arg189_1, arg190_1, arg191_1, arg192_1, arg193_1, arg194_1, arg195_1, arg196_1, arg197_1, arg198_1, arg199_1, arg200_1, arg201_1, arg202_1, arg203_1, arg204_1, arg205_1, arg206_1, arg207_1, arg208_1, arg209_1, arg210_1, arg211_1, arg212_1, arg213_1, arg214_1, arg215_1, arg216_1, arg217_1, arg218_1, arg219_1, arg220_1, arg221_1, arg222_1, arg223_1, arg224_1, arg225_1, arg226_1, arg227_1, arg228_1, arg229_1, arg230_1, arg231_1, arg232_1, arg233_1, arg234_1, arg235_1, arg236_1, arg237_1, arg238_1, arg239_1, arg240_1, arg241_1, arg242_1, arg243_1, arg244_1, arg245_1, arg246_1, arg247_1, arg248_1, arg249_1, arg250_1, arg251_1, arg252_1, arg253_1, arg254_1, arg255_1, arg256_1, arg257_1, arg258_1, arg259_1, arg260_1, arg261_1, arg262_1, arg263_1, arg264_1, arg265_1, arg266_1, arg267_1, arg268_1, arg269_1, arg270_1, arg271_1, arg272_1, arg273_1, arg274_1, arg275_1, arg276_1, arg277_1, arg278_1, arg279_1, arg280_1, arg281_1, arg282_1, arg283_1, arg284_1, arg285_1, arg286_1, arg287_1, arg288_1, arg289_1, arg290_1, arg291_1, arg292_1, arg293_1, arg294_1, arg295_1, arg296_1, arg297_1, arg298_1, arg299_1, arg300_1, arg301_1, arg302_1, arg303_1, arg304_1, arg305_1, arg306_1, arg307_1, arg308_1, arg309_1, arg310_1, arg311_1, arg312_1, arg313_1, arg314_1, arg315_1, arg316_1, arg317_1, arg318_1, arg319_1, arg320_1, arg321_1, arg322_1, arg323_1, arg324_1, arg325_1, arg326_1, arg327_1, arg328_1, arg329_1, arg330_1, arg331_1, arg332_1, arg333_1, arg334_1, arg335_1, arg336_1, arg337_1, arg338_1, arg339_1, arg340_1, arg341_1, arg342_1, arg343_1, arg344_1, arg345_1, arg346_1, arg347_1, arg348_1, arg349_1, arg350_1, arg351_1, arg352_1, arg353_1, arg354_1, arg355_1, arg356_1, arg357_1, arg358_1, arg359_1, arg360_1, arg361_1, arg362_1, arg363_1, arg364_1 = args
    args.clear()
    s0 = arg0_1
    s1 = arg3_1
    s4 = arg7_1
    assert_size_stride(arg1_1, (1, s0), (s0, 1))
    assert_size_stride(arg2_1, (128256, 4096), (4096, 1))
    assert_size_stride(arg4_1, (s1, ), (1, ))
    assert_size_stride(arg6_1, (1, s0), (s0, 1))
    assert_size_stride(arg8_1, (1, 1, s0, s4), (s0*s4, s0*s4, s4, 1))
    assert_size_stride(arg9_1, (64, ), (1, ))
    assert_size_stride(arg10_1, (4096, ), (1, ))
    assert_size_stride(arg11_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg12_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg13_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg14_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg15_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg16_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg17_1, (4096, ), (1, ))
    assert_size_stride(arg18_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg19_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg20_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg21_1, (4096, ), (1, ))
    assert_size_stride(arg22_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg23_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg24_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg25_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg26_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg27_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg28_1, (4096, ), (1, ))
    assert_size_stride(arg29_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg30_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg31_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg32_1, (4096, ), (1, ))
    assert_size_stride(arg33_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg34_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg35_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg36_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg37_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg38_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg39_1, (4096, ), (1, ))
    assert_size_stride(arg40_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg41_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg42_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg43_1, (4096, ), (1, ))
    assert_size_stride(arg44_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg45_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg46_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg47_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg48_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg49_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg50_1, (4096, ), (1, ))
    assert_size_stride(arg51_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg52_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg53_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg54_1, (4096, ), (1, ))
    assert_size_stride(arg55_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg56_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg57_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg58_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg59_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg60_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg61_1, (4096, ), (1, ))
    assert_size_stride(arg62_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg63_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg64_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg65_1, (4096, ), (1, ))
    assert_size_stride(arg66_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg67_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg68_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg69_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg70_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg71_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg72_1, (4096, ), (1, ))
    assert_size_stride(arg73_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg74_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg75_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg76_1, (4096, ), (1, ))
    assert_size_stride(arg77_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg78_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg79_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg80_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg81_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg82_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg83_1, (4096, ), (1, ))
    assert_size_stride(arg84_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg85_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg86_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg87_1, (4096, ), (1, ))
    assert_size_stride(arg88_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg89_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg90_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg91_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg92_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg93_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg94_1, (4096, ), (1, ))
    assert_size_stride(arg95_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg96_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg97_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg98_1, (4096, ), (1, ))
    assert_size_stride(arg99_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg100_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg101_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg102_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg103_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg104_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg105_1, (4096, ), (1, ))
    assert_size_stride(arg106_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg107_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg108_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg109_1, (4096, ), (1, ))
    assert_size_stride(arg110_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg111_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg112_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg113_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg114_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg115_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg116_1, (4096, ), (1, ))
    assert_size_stride(arg117_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg118_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg119_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg120_1, (4096, ), (1, ))
    assert_size_stride(arg121_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg122_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg123_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg124_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg125_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg126_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg127_1, (4096, ), (1, ))
    assert_size_stride(arg128_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg129_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg130_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg131_1, (4096, ), (1, ))
    assert_size_stride(arg132_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg133_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg134_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg135_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg136_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg137_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg138_1, (4096, ), (1, ))
    assert_size_stride(arg139_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg140_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg141_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg142_1, (4096, ), (1, ))
    assert_size_stride(arg143_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg144_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg145_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg146_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg147_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg148_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg149_1, (4096, ), (1, ))
    assert_size_stride(arg150_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg151_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg152_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg153_1, (4096, ), (1, ))
    assert_size_stride(arg154_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg155_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg156_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg157_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg158_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg159_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg160_1, (4096, ), (1, ))
    assert_size_stride(arg161_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg162_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg163_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg164_1, (4096, ), (1, ))
    assert_size_stride(arg165_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg166_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg167_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg168_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg169_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg170_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg171_1, (4096, ), (1, ))
    assert_size_stride(arg172_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg173_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg174_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg175_1, (4096, ), (1, ))
    assert_size_stride(arg176_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg177_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg178_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg179_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg180_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg181_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg182_1, (4096, ), (1, ))
    assert_size_stride(arg183_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg184_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg185_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg186_1, (4096, ), (1, ))
    assert_size_stride(arg187_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg188_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg189_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg190_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg191_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg192_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg193_1, (4096, ), (1, ))
    assert_size_stride(arg194_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg195_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg196_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg197_1, (4096, ), (1, ))
    assert_size_stride(arg198_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg199_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg200_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg201_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg202_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg203_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg204_1, (4096, ), (1, ))
    assert_size_stride(arg205_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg206_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg207_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg208_1, (4096, ), (1, ))
    assert_size_stride(arg209_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg210_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg211_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg212_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg213_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg214_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg215_1, (4096, ), (1, ))
    assert_size_stride(arg216_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg217_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg218_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg219_1, (4096, ), (1, ))
    assert_size_stride(arg220_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg221_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg222_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg223_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg224_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg225_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg226_1, (4096, ), (1, ))
    assert_size_stride(arg227_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg228_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg229_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg230_1, (4096, ), (1, ))
    assert_size_stride(arg231_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg232_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg233_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg234_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg235_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg236_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg237_1, (4096, ), (1, ))
    assert_size_stride(arg238_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg239_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg240_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg241_1, (4096, ), (1, ))
    assert_size_stride(arg242_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg243_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg244_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg245_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg246_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg247_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg248_1, (4096, ), (1, ))
    assert_size_stride(arg249_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg250_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg251_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg252_1, (4096, ), (1, ))
    assert_size_stride(arg253_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg254_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg255_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg256_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg257_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg258_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg259_1, (4096, ), (1, ))
    assert_size_stride(arg260_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg261_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg262_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg263_1, (4096, ), (1, ))
    assert_size_stride(arg264_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg265_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg266_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg267_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg268_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg269_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg270_1, (4096, ), (1, ))
    assert_size_stride(arg271_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg272_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg273_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg274_1, (4096, ), (1, ))
    assert_size_stride(arg275_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg276_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg277_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg278_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg279_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg280_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg281_1, (4096, ), (1, ))
    assert_size_stride(arg282_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg283_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg284_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg285_1, (4096, ), (1, ))
    assert_size_stride(arg286_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg287_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg288_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg289_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg290_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg291_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg292_1, (4096, ), (1, ))
    assert_size_stride(arg293_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg294_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg295_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg296_1, (4096, ), (1, ))
    assert_size_stride(arg297_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg298_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg299_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg300_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg301_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg302_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg303_1, (4096, ), (1, ))
    assert_size_stride(arg304_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg305_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg306_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg307_1, (4096, ), (1, ))
    assert_size_stride(arg308_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg309_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg310_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg311_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg312_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg313_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg314_1, (4096, ), (1, ))
    assert_size_stride(arg315_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg316_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg317_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg318_1, (4096, ), (1, ))
    assert_size_stride(arg319_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg320_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg321_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg322_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg323_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg324_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg325_1, (4096, ), (1, ))
    assert_size_stride(arg326_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg327_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg328_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg329_1, (4096, ), (1, ))
    assert_size_stride(arg330_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg331_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg332_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg333_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg334_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg335_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg336_1, (4096, ), (1, ))
    assert_size_stride(arg337_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg338_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg339_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg340_1, (4096, ), (1, ))
    assert_size_stride(arg341_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg342_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg343_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg344_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg345_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg346_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg347_1, (4096, ), (1, ))
    assert_size_stride(arg348_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg349_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg350_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg351_1, (4096, ), (1, ))
    assert_size_stride(arg352_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg353_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg354_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg355_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg356_1, (1, 8, s4, 128), (1024*s4, 128*s4, 128, 1))
    assert_size_stride(arg357_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg358_1, (4096, ), (1, ))
    assert_size_stride(arg359_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg360_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg361_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg362_1, (4096, ), (1, ))
    assert_size_stride(arg364_1, (128256, 4096), (4096, 1))
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = s0*s4
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 14336*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    _xnumel = 4096*s0
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((s0, 4096), (4096, 1), torch.float16)
        buf1 = empty_strided_cuda((s0, ), (1, ), torch.float32)
        _xnumel = 4096*s0
        buf2 = empty_strided_cuda((s0, 4096), (4096, 1), torch.float16)
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation], Original ATen: []
        triton_poi_fused_0_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_0.run(arg1_1, arg2_1, buf2, triton_poi_fused_0_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf0, 4096, buf2, 4096, arg10_1, 1, buf1, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg10_1
        buf5 = buf2; del buf2  # reuse
        # Topologically Sorted Source Nodes: [linear], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf0, arg11_1, buf5, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg11_1
        buf6 = reinterpret_tensor(buf1, (1, 1, s0), (s0, s0, 1), 0); del buf1  # reuse
        # Topologically Sorted Source Nodes: [position_ids_expanded], Original ATen: [aten._to_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_2.run(arg6_1, buf6, s0, stream=stream0)
        del arg6_1
        buf7 = empty_strided_cuda((1, 64, s0), (64*s0, s0, 1), torch.float32)
        # Topologically Sorted Source Nodes: [position_ids_expanded, matmul], Original ATen: [aten._to_copy, aten.view, aten.bmm]
        stream0 = get_raw_stream(0)
        triton_tem_fused__to_copy_bmm_view_3.run(arg9_1, buf6, buf7, s0, (15 + s0) // 16, 1, 1, stream=stream0)
        del arg9_1
        buf8 = empty_strided_cuda((s0, 1024), (1024, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf0, arg12_1, buf8, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg12_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_4, cat_2, mul_5, k_embed, index_copy_], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf8, buf7, arg14_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf10 = buf8; del buf8  # reuse
        # Topologically Sorted Source Nodes: [linear_2], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf0, arg13_1, buf10, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg13_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__1], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf10, arg15_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf12 = reinterpret_tensor(buf0, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf0  # reuse
        # Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed, query, attn_output], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf5, buf7, buf12, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        ps0 = 128*s4
        _xnumel = 4096*s4
        buf13 = empty_strided_cuda((1, 32, s4, 128), (4096*s4, 128*s4, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed, query, attn_output], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg14_1, buf13, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg14_1
        _xnumel = 4096*s4
        buf14 = empty_strided_cuda((1, 32, s4, 128), (4096*s4, 128*s4, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed, query, attn_output], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg15_1, buf14, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg15_1
        _xnumel = s0*s4
        buf15 = empty_strided_cuda((1, 1, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), torch.float16)
        buf44 = empty_strided_cuda((1, 1, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), torch.float16)
        buf73 = empty_strided_cuda((1, 1, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), torch.float16)
        # Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed, query, attn_output, mul_9, cat_3, mul_10, q_embed_1, query_1, attn_output_4, mul_14, cat_5, mul_15, q_embed_2, query_2, attn_output_8], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = s0*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(arg8_1, buf15, buf44, buf73, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed, query, attn_output], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf16 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf12, buf13, buf14, reinterpret_tensor(buf15, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf17 = buf16[0]
        assert_size_stride(buf17, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf16
        buf21 = reinterpret_tensor(buf12, (s0, 4096), (4096, 1), 0); del buf12  # reuse
        buf22 = buf5; del buf5  # reuse
        buf24 = empty_strided_cuda((s0, 4096), (4096, 1), torch.float16)
        # Topologically Sorted Source Nodes: [attn_output_3, triton_kernel_wrapper_mutation_1], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_10.run(buf17, arg16_1, arg1_1, arg2_1, buf22, buf24, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg16_1
        buf23 = reinterpret_tensor(buf6, (s0, ), (1, ), 0); del buf6  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_1], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf21, 4096, buf24, 4096, arg17_1, 1, buf23, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg17_1
        buf27 = buf24; del buf24  # reuse
        buf28 = empty_strided_cuda((s0, 14336), (14336, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_4], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf21, arg18_1, buf28, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg18_1
        buf30 = empty_strided_cuda((1, s0, 14336), (14336*s0, 14336, 1), torch.float16)
        # Topologically Sorted Source Nodes: [silu, linear_5, mul_8], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf21, arg19_1, buf28, buf30, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg19_1
        buf31 = buf21; del buf21  # reuse
        buf33 = reinterpret_tensor(buf17, (s0, 4096), (4096, 1), 0); del buf17  # reuse
        # Topologically Sorted Source Nodes: [down_proj, triton_kernel_wrapper_mutation_2], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_13.run(buf30, arg20_1, arg1_1, arg2_1, buf22, buf31, buf33, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg20_1
        buf32 = buf23; del buf23  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_2], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf27, 4096, buf33, 4096, arg21_1, 1, buf32, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg21_1
        buf36 = buf33; del buf33  # reuse
        # Topologically Sorted Source Nodes: [linear_7], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf27, arg22_1, buf36, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg22_1
        buf37 = buf10; del buf10  # reuse
        # Topologically Sorted Source Nodes: [linear_8], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf27, arg23_1, buf37, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg23_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_11, cat_4, mul_12, k_embed_1, index_copy__2], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf37, buf7, arg25_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf39 = buf37; del buf37  # reuse
        # Topologically Sorted Source Nodes: [linear_9], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf27, arg24_1, buf39, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg24_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__3], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf39, arg26_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf41 = reinterpret_tensor(buf27, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf27  # reuse
        # Topologically Sorted Source Nodes: [mul_9, cat_3, mul_10, q_embed_1, query_1, attn_output_4], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf36, buf7, buf41, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf42 = buf14; del buf14  # reuse
        # Topologically Sorted Source Nodes: [mul_9, cat_3, mul_10, q_embed_1, query_1, attn_output_4], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg25_1, buf42, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg25_1
        _xnumel = 4096*s4
        buf43 = buf13; del buf13  # reuse
        # Topologically Sorted Source Nodes: [mul_9, cat_3, mul_10, q_embed_1, query_1, attn_output_4], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg26_1, buf43, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg26_1
        # Topologically Sorted Source Nodes: [mul_9, cat_3, mul_10, q_embed_1, query_1, attn_output_4], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf45 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf41, buf42, buf43, reinterpret_tensor(buf44, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf46 = buf45[0]
        assert_size_stride(buf46, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf45
        buf50 = reinterpret_tensor(buf41, (s0, 4096), (4096, 1), 0); del buf41  # reuse
        buf51 = buf36; del buf36  # reuse
        # Topologically Sorted Source Nodes: [attn_output_7], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_14.run(buf46, arg27_1, buf51, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg27_1
        _xnumel = 4096*s0
        buf52 = reinterpret_tensor(buf22, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf22  # reuse
        # Topologically Sorted Source Nodes: [inputs_embeds, hidden_states_2, hidden_states_3, hidden_states_6], Original ATen: [aten.embedding, aten.add]
        triton_poi_fused_add_embedding_15_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_embedding_15.run(buf52, arg1_1, arg2_1, buf31, buf51, triton_poi_fused_add_embedding_15_xnumel, stream=stream0)
        del arg1_1
        del arg2_1
        buf53 = buf32; del buf32  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_3], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf50, 4096, reinterpret_tensor(buf52, (s0, 4096), (4096, 1), 0), 4096, arg28_1, 1, buf53, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg28_1
        buf56 = buf51; del buf51  # reuse
        buf57 = reinterpret_tensor(buf30, (s0, 14336), (14336, 1), 0); del buf30  # reuse
        # Topologically Sorted Source Nodes: [linear_11], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf50, arg29_1, buf57, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg29_1
        buf59 = reinterpret_tensor(buf28, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf28  # reuse
        # Topologically Sorted Source Nodes: [silu_1, linear_12, mul_13], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf50, arg30_1, buf57, buf59, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg30_1
        buf60 = buf50; del buf50  # reuse
        buf62 = buf31; del buf31  # reuse
        # Topologically Sorted Source Nodes: [down_proj_1, triton_kernel_wrapper_mutation_4], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_16.run(buf59, arg31_1, buf52, buf60, buf62, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg31_1
        buf61 = buf53; del buf53  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_4], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf56, 4096, buf62, 4096, arg32_1, 1, buf61, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg32_1
        buf65 = buf62; del buf62  # reuse
        # Topologically Sorted Source Nodes: [linear_14], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf56, arg33_1, buf65, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg33_1
        buf66 = buf39; del buf39  # reuse
        # Topologically Sorted Source Nodes: [linear_15], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf56, arg34_1, buf66, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg34_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_16, cat_6, mul_17, k_embed_2, index_copy__4], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf66, buf7, arg36_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf68 = buf66; del buf66  # reuse
        # Topologically Sorted Source Nodes: [linear_16], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf56, arg35_1, buf68, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg35_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__5], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf68, arg37_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf70 = reinterpret_tensor(buf56, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf56  # reuse
        # Topologically Sorted Source Nodes: [mul_14, cat_5, mul_15, q_embed_2, query_2, attn_output_8], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf65, buf7, buf70, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf71 = buf43; del buf43  # reuse
        # Topologically Sorted Source Nodes: [mul_14, cat_5, mul_15, q_embed_2, query_2, attn_output_8], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg36_1, buf71, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg36_1
        _xnumel = 4096*s4
        buf72 = buf42; del buf42  # reuse
        # Topologically Sorted Source Nodes: [mul_14, cat_5, mul_15, q_embed_2, query_2, attn_output_8], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg37_1, buf72, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg37_1
        # Topologically Sorted Source Nodes: [mul_14, cat_5, mul_15, q_embed_2, query_2, attn_output_8], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf74 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf70, buf71, buf72, reinterpret_tensor(buf73, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf75 = buf74[0]
        assert_size_stride(buf75, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf74
        buf79 = reinterpret_tensor(buf70, (s0, 4096), (4096, 1), 0); del buf70  # reuse
        buf80 = buf65; del buf65  # reuse
        buf82 = reinterpret_tensor(buf46, (s0, 4096), (4096, 1), 0); del buf46  # reuse
        # Topologically Sorted Source Nodes: [attn_output_11, triton_kernel_wrapper_mutation_5], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf75, arg38_1, buf52, buf60, buf80, buf82, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg38_1
        buf81 = buf61; del buf61  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_5], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf79, 4096, buf82, 4096, arg39_1, 1, buf81, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg39_1
        buf85 = buf82; del buf82  # reuse
        buf86 = reinterpret_tensor(buf59, (s0, 14336), (14336, 1), 0); del buf59  # reuse
        # Topologically Sorted Source Nodes: [linear_18], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf79, arg40_1, buf86, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg40_1
        buf88 = reinterpret_tensor(buf57, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf57  # reuse
        # Topologically Sorted Source Nodes: [silu_2, linear_19, mul_18], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf79, arg41_1, buf86, buf88, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg41_1
        buf89 = buf79; del buf79  # reuse
        buf91 = reinterpret_tensor(buf75, (s0, 4096), (4096, 1), 0); del buf75  # reuse
        # Topologically Sorted Source Nodes: [down_proj_2, triton_kernel_wrapper_mutation_6], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf88, arg42_1, buf52, buf60, buf80, buf89, buf91, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg42_1
        buf90 = buf81; del buf81  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_6], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf85, 4096, buf91, 4096, arg43_1, 1, buf90, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg43_1
        buf94 = buf91; del buf91  # reuse
        # Topologically Sorted Source Nodes: [linear_21], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf85, arg44_1, buf94, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg44_1
        buf95 = buf68; del buf68  # reuse
        # Topologically Sorted Source Nodes: [linear_22], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf85, arg45_1, buf95, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg45_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_21, cat_8, mul_22, k_embed_3, index_copy__6], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf95, buf7, arg47_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf97 = buf95; del buf95  # reuse
        # Topologically Sorted Source Nodes: [linear_23], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf85, arg46_1, buf97, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg46_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__7], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf97, arg48_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf99 = reinterpret_tensor(buf85, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf85  # reuse
        # Topologically Sorted Source Nodes: [mul_19, cat_7, mul_20, q_embed_3, query_3, attn_output_12], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf94, buf7, buf99, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf100 = buf72; del buf72  # reuse
        # Topologically Sorted Source Nodes: [mul_19, cat_7, mul_20, q_embed_3, query_3, attn_output_12], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg47_1, buf100, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg47_1
        _xnumel = 4096*s4
        buf101 = buf71; del buf71  # reuse
        # Topologically Sorted Source Nodes: [mul_19, cat_7, mul_20, q_embed_3, query_3, attn_output_12], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg48_1, buf101, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg48_1
        _xnumel = s0*s4
        buf102 = buf73; del buf73  # reuse
        buf131 = buf44; del buf44  # reuse
        buf160 = buf15; del buf15  # reuse
        # Topologically Sorted Source Nodes: [mul_19, cat_7, mul_20, q_embed_3, query_3, attn_output_12, mul_24, cat_9, mul_25, q_embed_4, query_4, attn_output_16, mul_29, cat_11, mul_30, q_embed_5, query_5, attn_output_20], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = s0*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(arg8_1, buf102, buf131, buf160, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_19, cat_7, mul_20, q_embed_3, query_3, attn_output_12], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf103 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf99, buf100, buf101, reinterpret_tensor(buf102, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf104 = buf103[0]
        assert_size_stride(buf104, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf103
        buf108 = reinterpret_tensor(buf99, (s0, 4096), (4096, 1), 0); del buf99  # reuse
        buf110 = reinterpret_tensor(buf94, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf94  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_7, hidden_states_10, hidden_states_11, attn_output_15, hidden_states_14], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_19.run(buf104, arg49_1, buf52, buf60, buf80, buf89, buf110, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg49_1
        del buf104
        del buf52
        buf111 = buf90; del buf90  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_7], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf108, 4096, reinterpret_tensor(buf110, (s0, 4096), (4096, 1), 0), 4096, arg50_1, 1, buf111, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg50_1
        buf114 = buf89; del buf89  # reuse
        buf115 = reinterpret_tensor(buf88, (s0, 14336), (14336, 1), 0); del buf88  # reuse
        # Topologically Sorted Source Nodes: [linear_25], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf108, arg51_1, buf115, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg51_1
        buf117 = reinterpret_tensor(buf86, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf86  # reuse
        # Topologically Sorted Source Nodes: [silu_3, linear_26, mul_23], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf108, arg52_1, buf115, buf117, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg52_1
        buf118 = buf108; del buf108  # reuse
        buf120 = buf80; del buf80  # reuse
        # Topologically Sorted Source Nodes: [down_proj_3, triton_kernel_wrapper_mutation_8], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_16.run(buf117, arg53_1, buf110, buf118, buf120, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg53_1
        buf119 = buf111; del buf111  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_8], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf114, 4096, buf120, 4096, arg54_1, 1, buf119, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg54_1
        buf123 = buf120; del buf120  # reuse
        # Topologically Sorted Source Nodes: [linear_28], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf114, arg55_1, buf123, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg55_1
        buf124 = buf97; del buf97  # reuse
        # Topologically Sorted Source Nodes: [linear_29], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf114, arg56_1, buf124, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg56_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_26, cat_10, mul_27, k_embed_4, index_copy__8], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf124, buf7, arg58_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf126 = buf124; del buf124  # reuse
        # Topologically Sorted Source Nodes: [linear_30], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf114, arg57_1, buf126, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg57_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__9], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf126, arg59_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf128 = reinterpret_tensor(buf114, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf114  # reuse
        # Topologically Sorted Source Nodes: [mul_24, cat_9, mul_25, q_embed_4, query_4, attn_output_16], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf123, buf7, buf128, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf129 = buf101; del buf101  # reuse
        # Topologically Sorted Source Nodes: [mul_24, cat_9, mul_25, q_embed_4, query_4, attn_output_16], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg58_1, buf129, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg58_1
        _xnumel = 4096*s4
        buf130 = buf100; del buf100  # reuse
        # Topologically Sorted Source Nodes: [mul_24, cat_9, mul_25, q_embed_4, query_4, attn_output_16], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg59_1, buf130, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg59_1
        # Topologically Sorted Source Nodes: [mul_24, cat_9, mul_25, q_embed_4, query_4, attn_output_16], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf132 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf128, buf129, buf130, reinterpret_tensor(buf131, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf133 = buf132[0]
        assert_size_stride(buf133, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf132
        buf137 = reinterpret_tensor(buf128, (s0, 4096), (4096, 1), 0); del buf128  # reuse
        buf138 = buf123; del buf123  # reuse
        buf140 = buf60; del buf60  # reuse
        # Topologically Sorted Source Nodes: [attn_output_19, triton_kernel_wrapper_mutation_9], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf133, arg60_1, buf110, buf118, buf138, buf140, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg60_1
        buf139 = buf119; del buf119  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_9], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf137, 4096, buf140, 4096, arg61_1, 1, buf139, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg61_1
        buf143 = buf140; del buf140  # reuse
        buf144 = reinterpret_tensor(buf117, (s0, 14336), (14336, 1), 0); del buf117  # reuse
        # Topologically Sorted Source Nodes: [linear_32], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf137, arg62_1, buf144, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg62_1
        buf146 = reinterpret_tensor(buf115, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf115  # reuse
        # Topologically Sorted Source Nodes: [silu_4, linear_33, mul_28], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf137, arg63_1, buf144, buf146, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg63_1
        buf147 = buf137; del buf137  # reuse
        buf149 = reinterpret_tensor(buf133, (s0, 4096), (4096, 1), 0); del buf133  # reuse
        # Topologically Sorted Source Nodes: [down_proj_4, triton_kernel_wrapper_mutation_10], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf146, arg64_1, buf110, buf118, buf138, buf147, buf149, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg64_1
        buf148 = buf139; del buf139  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_10], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf143, 4096, buf149, 4096, arg65_1, 1, buf148, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg65_1
        buf152 = buf149; del buf149  # reuse
        # Topologically Sorted Source Nodes: [linear_35], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf143, arg66_1, buf152, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg66_1
        buf153 = buf126; del buf126  # reuse
        # Topologically Sorted Source Nodes: [linear_36], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf143, arg67_1, buf153, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg67_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_31, cat_12, mul_32, k_embed_5, index_copy__10], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf153, buf7, arg69_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf155 = buf153; del buf153  # reuse
        # Topologically Sorted Source Nodes: [linear_37], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf143, arg68_1, buf155, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg68_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__11], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf155, arg70_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf157 = reinterpret_tensor(buf143, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf143  # reuse
        # Topologically Sorted Source Nodes: [mul_29, cat_11, mul_30, q_embed_5, query_5, attn_output_20], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf152, buf7, buf157, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf158 = buf130; del buf130  # reuse
        # Topologically Sorted Source Nodes: [mul_29, cat_11, mul_30, q_embed_5, query_5, attn_output_20], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg69_1, buf158, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg69_1
        _xnumel = 4096*s4
        buf159 = buf129; del buf129  # reuse
        # Topologically Sorted Source Nodes: [mul_29, cat_11, mul_30, q_embed_5, query_5, attn_output_20], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg70_1, buf159, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg70_1
        # Topologically Sorted Source Nodes: [mul_29, cat_11, mul_30, q_embed_5, query_5, attn_output_20], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf161 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf157, buf158, buf159, reinterpret_tensor(buf160, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf162 = buf161[0]
        assert_size_stride(buf162, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf161
        buf166 = reinterpret_tensor(buf157, (s0, 4096), (4096, 1), 0); del buf157  # reuse
        buf168 = reinterpret_tensor(buf152, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf152  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_15, hidden_states_18, hidden_states_19, attn_output_23, hidden_states_22], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_19.run(buf162, arg71_1, buf110, buf118, buf138, buf147, buf168, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg71_1
        del buf110
        del buf118
        buf169 = buf148; del buf148  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_11], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf166, 4096, reinterpret_tensor(buf168, (s0, 4096), (4096, 1), 0), 4096, arg72_1, 1, buf169, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg72_1
        buf172 = reinterpret_tensor(buf162, (s0, 4096), (4096, 1), 0); del buf162  # reuse
        buf173 = reinterpret_tensor(buf146, (s0, 14336), (14336, 1), 0); del buf146  # reuse
        # Topologically Sorted Source Nodes: [linear_39], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf166, arg73_1, buf173, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg73_1
        buf175 = reinterpret_tensor(buf144, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf144  # reuse
        # Topologically Sorted Source Nodes: [silu_5, linear_40, mul_33], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf166, arg74_1, buf173, buf175, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg74_1
        buf176 = buf166; del buf166  # reuse
        buf178 = buf147; del buf147  # reuse
        # Topologically Sorted Source Nodes: [down_proj_5, triton_kernel_wrapper_mutation_12], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_16.run(buf175, arg75_1, buf168, buf176, buf178, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg75_1
        buf177 = buf169; del buf169  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_12], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf172, 4096, buf178, 4096, arg76_1, 1, buf177, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg76_1
        buf181 = buf178; del buf178  # reuse
        # Topologically Sorted Source Nodes: [linear_42], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf172, arg77_1, buf181, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg77_1
        buf182 = buf155; del buf155  # reuse
        # Topologically Sorted Source Nodes: [linear_43], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf172, arg78_1, buf182, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg78_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_36, cat_14, mul_37, k_embed_6, index_copy__12], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf182, buf7, arg80_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf184 = buf182; del buf182  # reuse
        # Topologically Sorted Source Nodes: [linear_44], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf172, arg79_1, buf184, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg79_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__13], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf184, arg81_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf186 = reinterpret_tensor(buf172, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf172  # reuse
        # Topologically Sorted Source Nodes: [mul_34, cat_13, mul_35, q_embed_6, query_6, attn_output_24], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf181, buf7, buf186, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf187 = buf159; del buf159  # reuse
        # Topologically Sorted Source Nodes: [mul_34, cat_13, mul_35, q_embed_6, query_6, attn_output_24], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg80_1, buf187, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg80_1
        _xnumel = 4096*s4
        buf188 = buf158; del buf158  # reuse
        # Topologically Sorted Source Nodes: [mul_34, cat_13, mul_35, q_embed_6, query_6, attn_output_24], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg81_1, buf188, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg81_1
        _xnumel = s0*s4
        buf189 = buf160; del buf160  # reuse
        buf218 = buf131; del buf131  # reuse
        buf247 = buf102; del buf102  # reuse
        # Topologically Sorted Source Nodes: [mul_34, cat_13, mul_35, q_embed_6, query_6, attn_output_24, mul_39, cat_15, mul_40, q_embed_7, query_7, attn_output_28, mul_44, cat_17, mul_45, q_embed_8, query_8, attn_output_32], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = s0*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(arg8_1, buf189, buf218, buf247, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_34, cat_13, mul_35, q_embed_6, query_6, attn_output_24], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf190 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf186, buf187, buf188, reinterpret_tensor(buf189, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf191 = buf190[0]
        assert_size_stride(buf191, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf190
        buf195 = reinterpret_tensor(buf186, (s0, 4096), (4096, 1), 0); del buf186  # reuse
        buf196 = buf181; del buf181  # reuse
        buf198 = buf138; del buf138  # reuse
        # Topologically Sorted Source Nodes: [attn_output_27, triton_kernel_wrapper_mutation_13], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf191, arg82_1, buf168, buf176, buf196, buf198, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg82_1
        buf197 = buf177; del buf177  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_13], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf195, 4096, buf198, 4096, arg83_1, 1, buf197, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg83_1
        buf201 = buf198; del buf198  # reuse
        buf202 = reinterpret_tensor(buf175, (s0, 14336), (14336, 1), 0); del buf175  # reuse
        # Topologically Sorted Source Nodes: [linear_46], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf195, arg84_1, buf202, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg84_1
        buf204 = reinterpret_tensor(buf173, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf173  # reuse
        # Topologically Sorted Source Nodes: [silu_6, linear_47, mul_38], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf195, arg85_1, buf202, buf204, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg85_1
        buf205 = buf195; del buf195  # reuse
        buf207 = reinterpret_tensor(buf191, (s0, 4096), (4096, 1), 0); del buf191  # reuse
        # Topologically Sorted Source Nodes: [down_proj_6, triton_kernel_wrapper_mutation_14], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf204, arg86_1, buf168, buf176, buf196, buf205, buf207, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg86_1
        buf206 = buf197; del buf197  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_14], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf201, 4096, buf207, 4096, arg87_1, 1, buf206, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg87_1
        buf210 = buf207; del buf207  # reuse
        # Topologically Sorted Source Nodes: [linear_49], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf201, arg88_1, buf210, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg88_1
        buf211 = buf184; del buf184  # reuse
        # Topologically Sorted Source Nodes: [linear_50], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf201, arg89_1, buf211, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg89_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_41, cat_16, mul_42, k_embed_7, index_copy__14], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf211, buf7, arg91_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf213 = buf211; del buf211  # reuse
        # Topologically Sorted Source Nodes: [linear_51], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf201, arg90_1, buf213, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg90_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__15], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf213, arg92_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf215 = reinterpret_tensor(buf201, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf201  # reuse
        # Topologically Sorted Source Nodes: [mul_39, cat_15, mul_40, q_embed_7, query_7, attn_output_28], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf210, buf7, buf215, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf216 = buf188; del buf188  # reuse
        # Topologically Sorted Source Nodes: [mul_39, cat_15, mul_40, q_embed_7, query_7, attn_output_28], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg91_1, buf216, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg91_1
        _xnumel = 4096*s4
        buf217 = buf187; del buf187  # reuse
        # Topologically Sorted Source Nodes: [mul_39, cat_15, mul_40, q_embed_7, query_7, attn_output_28], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg92_1, buf217, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg92_1
        # Topologically Sorted Source Nodes: [mul_39, cat_15, mul_40, q_embed_7, query_7, attn_output_28], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf219 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf215, buf216, buf217, reinterpret_tensor(buf218, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf220 = buf219[0]
        assert_size_stride(buf220, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf219
        buf224 = reinterpret_tensor(buf215, (s0, 4096), (4096, 1), 0); del buf215  # reuse
        buf226 = reinterpret_tensor(buf210, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf210  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_23, hidden_states_26, hidden_states_27, attn_output_31, hidden_states_30], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_19.run(buf220, arg93_1, buf168, buf176, buf196, buf205, buf226, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg93_1
        del buf168
        del buf176
        buf227 = buf206; del buf206  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_15], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf224, 4096, reinterpret_tensor(buf226, (s0, 4096), (4096, 1), 0), 4096, arg94_1, 1, buf227, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg94_1
        buf230 = reinterpret_tensor(buf220, (s0, 4096), (4096, 1), 0); del buf220  # reuse
        buf231 = reinterpret_tensor(buf204, (s0, 14336), (14336, 1), 0); del buf204  # reuse
        # Topologically Sorted Source Nodes: [linear_53], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf224, arg95_1, buf231, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg95_1
        buf233 = reinterpret_tensor(buf202, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf202  # reuse
        # Topologically Sorted Source Nodes: [silu_7, linear_54, mul_43], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf224, arg96_1, buf231, buf233, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg96_1
        buf234 = buf224; del buf224  # reuse
        buf236 = buf205; del buf205  # reuse
        # Topologically Sorted Source Nodes: [down_proj_7, triton_kernel_wrapper_mutation_16], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_16.run(buf233, arg97_1, buf226, buf234, buf236, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg97_1
        buf235 = buf227; del buf227  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_16], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf230, 4096, buf236, 4096, arg98_1, 1, buf235, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg98_1
        buf239 = buf236; del buf236  # reuse
        # Topologically Sorted Source Nodes: [linear_56], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf230, arg99_1, buf239, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg99_1
        buf240 = buf213; del buf213  # reuse
        # Topologically Sorted Source Nodes: [linear_57], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf230, arg100_1, buf240, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg100_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_46, cat_18, mul_47, k_embed_8, index_copy__16], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf240, buf7, arg102_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf242 = buf240; del buf240  # reuse
        # Topologically Sorted Source Nodes: [linear_58], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf230, arg101_1, buf242, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg101_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__17], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf242, arg103_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf244 = reinterpret_tensor(buf230, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf230  # reuse
        # Topologically Sorted Source Nodes: [mul_44, cat_17, mul_45, q_embed_8, query_8, attn_output_32], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf239, buf7, buf244, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf245 = buf217; del buf217  # reuse
        # Topologically Sorted Source Nodes: [mul_44, cat_17, mul_45, q_embed_8, query_8, attn_output_32], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg102_1, buf245, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg102_1
        _xnumel = 4096*s4
        buf246 = buf216; del buf216  # reuse
        # Topologically Sorted Source Nodes: [mul_44, cat_17, mul_45, q_embed_8, query_8, attn_output_32], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg103_1, buf246, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg103_1
        # Topologically Sorted Source Nodes: [mul_44, cat_17, mul_45, q_embed_8, query_8, attn_output_32], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf248 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf244, buf245, buf246, reinterpret_tensor(buf247, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf249 = buf248[0]
        assert_size_stride(buf249, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf248
        buf253 = reinterpret_tensor(buf244, (s0, 4096), (4096, 1), 0); del buf244  # reuse
        buf254 = buf239; del buf239  # reuse
        buf256 = buf196; del buf196  # reuse
        # Topologically Sorted Source Nodes: [attn_output_35, triton_kernel_wrapper_mutation_17], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf249, arg104_1, buf226, buf234, buf254, buf256, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg104_1
        buf255 = buf235; del buf235  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_17], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf253, 4096, buf256, 4096, arg105_1, 1, buf255, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg105_1
        buf259 = buf256; del buf256  # reuse
        buf260 = reinterpret_tensor(buf233, (s0, 14336), (14336, 1), 0); del buf233  # reuse
        # Topologically Sorted Source Nodes: [linear_60], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf253, arg106_1, buf260, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg106_1
        buf262 = reinterpret_tensor(buf231, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf231  # reuse
        # Topologically Sorted Source Nodes: [silu_8, linear_61, mul_48], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf253, arg107_1, buf260, buf262, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg107_1
        buf263 = buf253; del buf253  # reuse
        buf265 = reinterpret_tensor(buf249, (s0, 4096), (4096, 1), 0); del buf249  # reuse
        # Topologically Sorted Source Nodes: [down_proj_8, triton_kernel_wrapper_mutation_18], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf262, arg108_1, buf226, buf234, buf254, buf263, buf265, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg108_1
        buf264 = buf255; del buf255  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_18], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf259, 4096, buf265, 4096, arg109_1, 1, buf264, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg109_1
        buf268 = buf265; del buf265  # reuse
        # Topologically Sorted Source Nodes: [linear_63], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf259, arg110_1, buf268, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg110_1
        buf269 = buf242; del buf242  # reuse
        # Topologically Sorted Source Nodes: [linear_64], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf259, arg111_1, buf269, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg111_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_51, cat_20, mul_52, k_embed_9, index_copy__18], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf269, buf7, arg113_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf271 = buf269; del buf269  # reuse
        # Topologically Sorted Source Nodes: [linear_65], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf259, arg112_1, buf271, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg112_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__19], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf271, arg114_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf273 = reinterpret_tensor(buf259, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf259  # reuse
        # Topologically Sorted Source Nodes: [mul_49, cat_19, mul_50, q_embed_9, query_9, attn_output_36], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf268, buf7, buf273, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf274 = buf246; del buf246  # reuse
        # Topologically Sorted Source Nodes: [mul_49, cat_19, mul_50, q_embed_9, query_9, attn_output_36], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg113_1, buf274, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg113_1
        _xnumel = 4096*s4
        buf275 = buf245; del buf245  # reuse
        # Topologically Sorted Source Nodes: [mul_49, cat_19, mul_50, q_embed_9, query_9, attn_output_36], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg114_1, buf275, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg114_1
        _xnumel = s0*s4
        buf276 = buf247; del buf247  # reuse
        buf305 = buf218; del buf218  # reuse
        buf334 = buf189; del buf189  # reuse
        # Topologically Sorted Source Nodes: [mul_49, cat_19, mul_50, q_embed_9, query_9, attn_output_36, mul_54, cat_21, mul_55, q_embed_10, query_10, attn_output_40, mul_59, cat_23, mul_60, q_embed_11, query_11, attn_output_44], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = s0*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(arg8_1, buf276, buf305, buf334, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_49, cat_19, mul_50, q_embed_9, query_9, attn_output_36], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf277 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf273, buf274, buf275, reinterpret_tensor(buf276, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf278 = buf277[0]
        assert_size_stride(buf278, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf277
        buf282 = reinterpret_tensor(buf273, (s0, 4096), (4096, 1), 0); del buf273  # reuse
        buf284 = reinterpret_tensor(buf268, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf268  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_31, hidden_states_34, hidden_states_35, attn_output_39, hidden_states_38], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_19.run(buf278, arg115_1, buf226, buf234, buf254, buf263, buf284, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg115_1
        del buf226
        del buf234
        buf285 = buf264; del buf264  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_19], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf282, 4096, reinterpret_tensor(buf284, (s0, 4096), (4096, 1), 0), 4096, arg116_1, 1, buf285, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg116_1
        buf288 = reinterpret_tensor(buf278, (s0, 4096), (4096, 1), 0); del buf278  # reuse
        buf289 = reinterpret_tensor(buf262, (s0, 14336), (14336, 1), 0); del buf262  # reuse
        # Topologically Sorted Source Nodes: [linear_67], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf282, arg117_1, buf289, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg117_1
        buf291 = reinterpret_tensor(buf260, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf260  # reuse
        # Topologically Sorted Source Nodes: [silu_9, linear_68, mul_53], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf282, arg118_1, buf289, buf291, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg118_1
        buf292 = buf282; del buf282  # reuse
        buf294 = buf263; del buf263  # reuse
        # Topologically Sorted Source Nodes: [down_proj_9, triton_kernel_wrapper_mutation_20], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_16.run(buf291, arg119_1, buf284, buf292, buf294, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg119_1
        buf293 = buf285; del buf285  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_20], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf288, 4096, buf294, 4096, arg120_1, 1, buf293, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg120_1
        buf297 = buf294; del buf294  # reuse
        # Topologically Sorted Source Nodes: [linear_70], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf288, arg121_1, buf297, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg121_1
        buf298 = buf271; del buf271  # reuse
        # Topologically Sorted Source Nodes: [linear_71], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf288, arg122_1, buf298, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg122_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_56, cat_22, mul_57, k_embed_10, index_copy__20], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf298, buf7, arg124_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf300 = buf298; del buf298  # reuse
        # Topologically Sorted Source Nodes: [linear_72], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf288, arg123_1, buf300, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg123_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__21], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf300, arg125_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf302 = reinterpret_tensor(buf288, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf288  # reuse
        # Topologically Sorted Source Nodes: [mul_54, cat_21, mul_55, q_embed_10, query_10, attn_output_40], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf297, buf7, buf302, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf303 = buf275; del buf275  # reuse
        # Topologically Sorted Source Nodes: [mul_54, cat_21, mul_55, q_embed_10, query_10, attn_output_40], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg124_1, buf303, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg124_1
        _xnumel = 4096*s4
        buf304 = buf274; del buf274  # reuse
        # Topologically Sorted Source Nodes: [mul_54, cat_21, mul_55, q_embed_10, query_10, attn_output_40], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg125_1, buf304, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg125_1
        # Topologically Sorted Source Nodes: [mul_54, cat_21, mul_55, q_embed_10, query_10, attn_output_40], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf306 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf302, buf303, buf304, reinterpret_tensor(buf305, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf307 = buf306[0]
        assert_size_stride(buf307, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf306
        buf311 = reinterpret_tensor(buf302, (s0, 4096), (4096, 1), 0); del buf302  # reuse
        buf312 = buf297; del buf297  # reuse
        buf314 = buf254; del buf254  # reuse
        # Topologically Sorted Source Nodes: [attn_output_43, triton_kernel_wrapper_mutation_21], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf307, arg126_1, buf284, buf292, buf312, buf314, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg126_1
        buf313 = buf293; del buf293  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_21], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf311, 4096, buf314, 4096, arg127_1, 1, buf313, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg127_1
        buf317 = buf314; del buf314  # reuse
        buf318 = reinterpret_tensor(buf291, (s0, 14336), (14336, 1), 0); del buf291  # reuse
        # Topologically Sorted Source Nodes: [linear_74], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf311, arg128_1, buf318, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg128_1
        buf320 = reinterpret_tensor(buf289, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf289  # reuse
        # Topologically Sorted Source Nodes: [silu_10, linear_75, mul_58], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf311, arg129_1, buf318, buf320, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg129_1
        buf321 = buf311; del buf311  # reuse
        buf323 = reinterpret_tensor(buf307, (s0, 4096), (4096, 1), 0); del buf307  # reuse
        # Topologically Sorted Source Nodes: [down_proj_10, triton_kernel_wrapper_mutation_22], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf320, arg130_1, buf284, buf292, buf312, buf321, buf323, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg130_1
        buf322 = buf313; del buf313  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_22], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf317, 4096, buf323, 4096, arg131_1, 1, buf322, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg131_1
        buf326 = buf323; del buf323  # reuse
        # Topologically Sorted Source Nodes: [linear_77], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf317, arg132_1, buf326, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg132_1
        buf327 = buf300; del buf300  # reuse
        # Topologically Sorted Source Nodes: [linear_78], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf317, arg133_1, buf327, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg133_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_61, cat_24, mul_62, k_embed_11, index_copy__22], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf327, buf7, arg135_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf329 = buf327; del buf327  # reuse
        # Topologically Sorted Source Nodes: [linear_79], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf317, arg134_1, buf329, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg134_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__23], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf329, arg136_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf331 = reinterpret_tensor(buf317, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf317  # reuse
        # Topologically Sorted Source Nodes: [mul_59, cat_23, mul_60, q_embed_11, query_11, attn_output_44], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf326, buf7, buf331, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf332 = buf304; del buf304  # reuse
        # Topologically Sorted Source Nodes: [mul_59, cat_23, mul_60, q_embed_11, query_11, attn_output_44], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg135_1, buf332, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg135_1
        _xnumel = 4096*s4
        buf333 = buf303; del buf303  # reuse
        # Topologically Sorted Source Nodes: [mul_59, cat_23, mul_60, q_embed_11, query_11, attn_output_44], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg136_1, buf333, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg136_1
        # Topologically Sorted Source Nodes: [mul_59, cat_23, mul_60, q_embed_11, query_11, attn_output_44], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf335 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf331, buf332, buf333, reinterpret_tensor(buf334, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf336 = buf335[0]
        assert_size_stride(buf336, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf335
        buf340 = reinterpret_tensor(buf331, (s0, 4096), (4096, 1), 0); del buf331  # reuse
        buf342 = reinterpret_tensor(buf326, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf326  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_39, hidden_states_42, hidden_states_43, attn_output_47, hidden_states_46], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_19.run(buf336, arg137_1, buf284, buf292, buf312, buf321, buf342, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg137_1
        del buf284
        del buf292
        buf343 = buf322; del buf322  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_23], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf340, 4096, reinterpret_tensor(buf342, (s0, 4096), (4096, 1), 0), 4096, arg138_1, 1, buf343, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg138_1
        buf346 = reinterpret_tensor(buf336, (s0, 4096), (4096, 1), 0); del buf336  # reuse
        buf347 = reinterpret_tensor(buf320, (s0, 14336), (14336, 1), 0); del buf320  # reuse
        # Topologically Sorted Source Nodes: [linear_81], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf340, arg139_1, buf347, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg139_1
        buf349 = reinterpret_tensor(buf318, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf318  # reuse
        # Topologically Sorted Source Nodes: [silu_11, linear_82, mul_63], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf340, arg140_1, buf347, buf349, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg140_1
        buf350 = buf340; del buf340  # reuse
        buf352 = buf321; del buf321  # reuse
        # Topologically Sorted Source Nodes: [down_proj_11, triton_kernel_wrapper_mutation_24], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_16.run(buf349, arg141_1, buf342, buf350, buf352, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg141_1
        buf351 = buf343; del buf343  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_24], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf346, 4096, buf352, 4096, arg142_1, 1, buf351, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg142_1
        buf355 = buf352; del buf352  # reuse
        # Topologically Sorted Source Nodes: [linear_84], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf346, arg143_1, buf355, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg143_1
        buf356 = buf329; del buf329  # reuse
        # Topologically Sorted Source Nodes: [linear_85], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf346, arg144_1, buf356, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg144_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_66, cat_26, mul_67, k_embed_12, index_copy__24], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf356, buf7, arg146_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf358 = buf356; del buf356  # reuse
        # Topologically Sorted Source Nodes: [linear_86], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf346, arg145_1, buf358, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg145_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__25], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf358, arg147_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf360 = reinterpret_tensor(buf346, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf346  # reuse
        # Topologically Sorted Source Nodes: [mul_64, cat_25, mul_65, q_embed_12, query_12, attn_output_48], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf355, buf7, buf360, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf361 = buf333; del buf333  # reuse
        # Topologically Sorted Source Nodes: [mul_64, cat_25, mul_65, q_embed_12, query_12, attn_output_48], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg146_1, buf361, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg146_1
        _xnumel = 4096*s4
        buf362 = buf332; del buf332  # reuse
        # Topologically Sorted Source Nodes: [mul_64, cat_25, mul_65, q_embed_12, query_12, attn_output_48], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg147_1, buf362, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg147_1
        _xnumel = s0*s4
        buf363 = buf334; del buf334  # reuse
        buf392 = buf305; del buf305  # reuse
        buf421 = buf276; del buf276  # reuse
        # Topologically Sorted Source Nodes: [mul_64, cat_25, mul_65, q_embed_12, query_12, attn_output_48, mul_69, cat_27, mul_70, q_embed_13, query_13, attn_output_52, mul_74, cat_29, mul_75, q_embed_14, query_14, attn_output_56], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = s0*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(arg8_1, buf363, buf392, buf421, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_64, cat_25, mul_65, q_embed_12, query_12, attn_output_48], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf364 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf360, buf361, buf362, reinterpret_tensor(buf363, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf365 = buf364[0]
        assert_size_stride(buf365, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf364
        buf369 = reinterpret_tensor(buf360, (s0, 4096), (4096, 1), 0); del buf360  # reuse
        buf370 = buf355; del buf355  # reuse
        buf372 = buf312; del buf312  # reuse
        # Topologically Sorted Source Nodes: [attn_output_51, triton_kernel_wrapper_mutation_25], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf365, arg148_1, buf342, buf350, buf370, buf372, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg148_1
        buf371 = buf351; del buf351  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_25], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf369, 4096, buf372, 4096, arg149_1, 1, buf371, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg149_1
        buf375 = buf372; del buf372  # reuse
        buf376 = reinterpret_tensor(buf349, (s0, 14336), (14336, 1), 0); del buf349  # reuse
        # Topologically Sorted Source Nodes: [linear_88], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf369, arg150_1, buf376, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg150_1
        buf378 = reinterpret_tensor(buf347, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf347  # reuse
        # Topologically Sorted Source Nodes: [silu_12, linear_89, mul_68], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf369, arg151_1, buf376, buf378, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg151_1
        buf379 = buf369; del buf369  # reuse
        buf381 = reinterpret_tensor(buf365, (s0, 4096), (4096, 1), 0); del buf365  # reuse
        # Topologically Sorted Source Nodes: [down_proj_12, triton_kernel_wrapper_mutation_26], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf378, arg152_1, buf342, buf350, buf370, buf379, buf381, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg152_1
        buf380 = buf371; del buf371  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_26], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf375, 4096, buf381, 4096, arg153_1, 1, buf380, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg153_1
        buf384 = buf381; del buf381  # reuse
        # Topologically Sorted Source Nodes: [linear_91], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf375, arg154_1, buf384, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg154_1
        buf385 = buf358; del buf358  # reuse
        # Topologically Sorted Source Nodes: [linear_92], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf375, arg155_1, buf385, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg155_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_71, cat_28, mul_72, k_embed_13, index_copy__26], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf385, buf7, arg157_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf387 = buf385; del buf385  # reuse
        # Topologically Sorted Source Nodes: [linear_93], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf375, arg156_1, buf387, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg156_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__27], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf387, arg158_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf389 = reinterpret_tensor(buf375, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf375  # reuse
        # Topologically Sorted Source Nodes: [mul_69, cat_27, mul_70, q_embed_13, query_13, attn_output_52], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf384, buf7, buf389, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf390 = buf362; del buf362  # reuse
        # Topologically Sorted Source Nodes: [mul_69, cat_27, mul_70, q_embed_13, query_13, attn_output_52], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg157_1, buf390, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg157_1
        _xnumel = 4096*s4
        buf391 = buf361; del buf361  # reuse
        # Topologically Sorted Source Nodes: [mul_69, cat_27, mul_70, q_embed_13, query_13, attn_output_52], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg158_1, buf391, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg158_1
        # Topologically Sorted Source Nodes: [mul_69, cat_27, mul_70, q_embed_13, query_13, attn_output_52], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf393 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf389, buf390, buf391, reinterpret_tensor(buf392, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf394 = buf393[0]
        assert_size_stride(buf394, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf393
        buf398 = reinterpret_tensor(buf389, (s0, 4096), (4096, 1), 0); del buf389  # reuse
        buf400 = reinterpret_tensor(buf384, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf384  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_47, hidden_states_50, hidden_states_51, attn_output_55, hidden_states_54], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_19.run(buf394, arg159_1, buf342, buf350, buf370, buf379, buf400, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg159_1
        del buf342
        del buf350
        buf401 = buf380; del buf380  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_27], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf398, 4096, reinterpret_tensor(buf400, (s0, 4096), (4096, 1), 0), 4096, arg160_1, 1, buf401, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg160_1
        buf404 = reinterpret_tensor(buf394, (s0, 4096), (4096, 1), 0); del buf394  # reuse
        buf405 = reinterpret_tensor(buf378, (s0, 14336), (14336, 1), 0); del buf378  # reuse
        # Topologically Sorted Source Nodes: [linear_95], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf398, arg161_1, buf405, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg161_1
        buf407 = reinterpret_tensor(buf376, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf376  # reuse
        # Topologically Sorted Source Nodes: [silu_13, linear_96, mul_73], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf398, arg162_1, buf405, buf407, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg162_1
        buf408 = buf398; del buf398  # reuse
        buf410 = buf379; del buf379  # reuse
        # Topologically Sorted Source Nodes: [down_proj_13, triton_kernel_wrapper_mutation_28], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_16.run(buf407, arg163_1, buf400, buf408, buf410, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg163_1
        buf409 = buf401; del buf401  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_28], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf404, 4096, buf410, 4096, arg164_1, 1, buf409, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg164_1
        buf413 = buf410; del buf410  # reuse
        # Topologically Sorted Source Nodes: [linear_98], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf404, arg165_1, buf413, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg165_1
        buf414 = buf387; del buf387  # reuse
        # Topologically Sorted Source Nodes: [linear_99], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf404, arg166_1, buf414, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg166_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_76, cat_30, mul_77, k_embed_14, index_copy__28], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf414, buf7, arg168_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf416 = buf414; del buf414  # reuse
        # Topologically Sorted Source Nodes: [linear_100], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf404, arg167_1, buf416, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg167_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__29], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf416, arg169_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf418 = reinterpret_tensor(buf404, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf404  # reuse
        # Topologically Sorted Source Nodes: [mul_74, cat_29, mul_75, q_embed_14, query_14, attn_output_56], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf413, buf7, buf418, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf419 = buf391; del buf391  # reuse
        # Topologically Sorted Source Nodes: [mul_74, cat_29, mul_75, q_embed_14, query_14, attn_output_56], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg168_1, buf419, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg168_1
        _xnumel = 4096*s4
        buf420 = buf390; del buf390  # reuse
        # Topologically Sorted Source Nodes: [mul_74, cat_29, mul_75, q_embed_14, query_14, attn_output_56], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg169_1, buf420, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg169_1
        # Topologically Sorted Source Nodes: [mul_74, cat_29, mul_75, q_embed_14, query_14, attn_output_56], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf422 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf418, buf419, buf420, reinterpret_tensor(buf421, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf423 = buf422[0]
        assert_size_stride(buf423, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf422
        buf427 = reinterpret_tensor(buf418, (s0, 4096), (4096, 1), 0); del buf418  # reuse
        buf428 = buf413; del buf413  # reuse
        buf430 = buf370; del buf370  # reuse
        # Topologically Sorted Source Nodes: [attn_output_59, triton_kernel_wrapper_mutation_29], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf423, arg170_1, buf400, buf408, buf428, buf430, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg170_1
        buf429 = buf409; del buf409  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_29], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf427, 4096, buf430, 4096, arg171_1, 1, buf429, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg171_1
        buf433 = buf430; del buf430  # reuse
        buf434 = reinterpret_tensor(buf407, (s0, 14336), (14336, 1), 0); del buf407  # reuse
        # Topologically Sorted Source Nodes: [linear_102], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf427, arg172_1, buf434, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg172_1
        buf436 = reinterpret_tensor(buf405, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf405  # reuse
        # Topologically Sorted Source Nodes: [silu_14, linear_103, mul_78], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf427, arg173_1, buf434, buf436, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg173_1
        buf437 = buf427; del buf427  # reuse
        buf439 = reinterpret_tensor(buf423, (s0, 4096), (4096, 1), 0); del buf423  # reuse
        # Topologically Sorted Source Nodes: [down_proj_14, triton_kernel_wrapper_mutation_30], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf436, arg174_1, buf400, buf408, buf428, buf437, buf439, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg174_1
        buf438 = buf429; del buf429  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_30], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf433, 4096, buf439, 4096, arg175_1, 1, buf438, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg175_1
        buf442 = buf439; del buf439  # reuse
        # Topologically Sorted Source Nodes: [linear_105], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf433, arg176_1, buf442, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg176_1
        buf443 = buf416; del buf416  # reuse
        # Topologically Sorted Source Nodes: [linear_106], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf433, arg177_1, buf443, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg177_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_81, cat_32, mul_82, k_embed_15, index_copy__30], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf443, buf7, arg179_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf445 = buf443; del buf443  # reuse
        # Topologically Sorted Source Nodes: [linear_107], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf433, arg178_1, buf445, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg178_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__31], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf445, arg180_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf447 = reinterpret_tensor(buf433, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf433  # reuse
        # Topologically Sorted Source Nodes: [mul_79, cat_31, mul_80, q_embed_15, query_15, attn_output_60], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf442, buf7, buf447, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf448 = buf420; del buf420  # reuse
        # Topologically Sorted Source Nodes: [mul_79, cat_31, mul_80, q_embed_15, query_15, attn_output_60], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg179_1, buf448, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg179_1
        _xnumel = 4096*s4
        buf449 = buf419; del buf419  # reuse
        # Topologically Sorted Source Nodes: [mul_79, cat_31, mul_80, q_embed_15, query_15, attn_output_60], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg180_1, buf449, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg180_1
        _xnumel = s0*s4
        buf450 = buf421; del buf421  # reuse
        buf479 = buf392; del buf392  # reuse
        buf508 = buf363; del buf363  # reuse
        # Topologically Sorted Source Nodes: [mul_79, cat_31, mul_80, q_embed_15, query_15, attn_output_60, mul_84, cat_33, mul_85, q_embed_16, query_16, attn_output_64, mul_89, cat_35, mul_90, q_embed_17, query_17, attn_output_68], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = s0*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(arg8_1, buf450, buf479, buf508, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_79, cat_31, mul_80, q_embed_15, query_15, attn_output_60], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf451 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf447, buf448, buf449, reinterpret_tensor(buf450, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf452 = buf451[0]
        assert_size_stride(buf452, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf451
        buf456 = reinterpret_tensor(buf447, (s0, 4096), (4096, 1), 0); del buf447  # reuse
        buf458 = reinterpret_tensor(buf442, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf442  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_55, hidden_states_58, hidden_states_59, attn_output_63, hidden_states_62], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_19.run(buf452, arg181_1, buf400, buf408, buf428, buf437, buf458, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg181_1
        del buf400
        del buf408
        buf459 = buf438; del buf438  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_31], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf456, 4096, reinterpret_tensor(buf458, (s0, 4096), (4096, 1), 0), 4096, arg182_1, 1, buf459, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg182_1
        buf462 = reinterpret_tensor(buf452, (s0, 4096), (4096, 1), 0); del buf452  # reuse
        buf463 = reinterpret_tensor(buf436, (s0, 14336), (14336, 1), 0); del buf436  # reuse
        # Topologically Sorted Source Nodes: [linear_109], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf456, arg183_1, buf463, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg183_1
        buf465 = reinterpret_tensor(buf434, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf434  # reuse
        # Topologically Sorted Source Nodes: [silu_15, linear_110, mul_83], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf456, arg184_1, buf463, buf465, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg184_1
        buf466 = buf456; del buf456  # reuse
        buf468 = buf437; del buf437  # reuse
        # Topologically Sorted Source Nodes: [down_proj_15, triton_kernel_wrapper_mutation_32], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_16.run(buf465, arg185_1, buf458, buf466, buf468, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg185_1
        buf467 = buf459; del buf459  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_32], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf462, 4096, buf468, 4096, arg186_1, 1, buf467, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg186_1
        buf471 = buf468; del buf468  # reuse
        # Topologically Sorted Source Nodes: [linear_112], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf462, arg187_1, buf471, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg187_1
        buf472 = buf445; del buf445  # reuse
        # Topologically Sorted Source Nodes: [linear_113], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf462, arg188_1, buf472, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg188_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_86, cat_34, mul_87, k_embed_16, index_copy__32], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf472, buf7, arg190_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf474 = buf472; del buf472  # reuse
        # Topologically Sorted Source Nodes: [linear_114], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf462, arg189_1, buf474, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg189_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__33], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf474, arg191_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf476 = reinterpret_tensor(buf462, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf462  # reuse
        # Topologically Sorted Source Nodes: [mul_84, cat_33, mul_85, q_embed_16, query_16, attn_output_64], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf471, buf7, buf476, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf477 = buf449; del buf449  # reuse
        # Topologically Sorted Source Nodes: [mul_84, cat_33, mul_85, q_embed_16, query_16, attn_output_64], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg190_1, buf477, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg190_1
        _xnumel = 4096*s4
        buf478 = buf448; del buf448  # reuse
        # Topologically Sorted Source Nodes: [mul_84, cat_33, mul_85, q_embed_16, query_16, attn_output_64], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg191_1, buf478, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg191_1
        # Topologically Sorted Source Nodes: [mul_84, cat_33, mul_85, q_embed_16, query_16, attn_output_64], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf480 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf476, buf477, buf478, reinterpret_tensor(buf479, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf481 = buf480[0]
        assert_size_stride(buf481, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf480
        buf485 = reinterpret_tensor(buf476, (s0, 4096), (4096, 1), 0); del buf476  # reuse
        buf486 = buf471; del buf471  # reuse
        buf488 = buf428; del buf428  # reuse
        # Topologically Sorted Source Nodes: [attn_output_67, triton_kernel_wrapper_mutation_33], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf481, arg192_1, buf458, buf466, buf486, buf488, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg192_1
        buf487 = buf467; del buf467  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_33], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf485, 4096, buf488, 4096, arg193_1, 1, buf487, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg193_1
        buf491 = buf488; del buf488  # reuse
        buf492 = reinterpret_tensor(buf465, (s0, 14336), (14336, 1), 0); del buf465  # reuse
        # Topologically Sorted Source Nodes: [linear_116], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf485, arg194_1, buf492, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg194_1
        buf494 = reinterpret_tensor(buf463, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf463  # reuse
        # Topologically Sorted Source Nodes: [silu_16, linear_117, mul_88], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf485, arg195_1, buf492, buf494, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg195_1
        buf495 = buf485; del buf485  # reuse
        buf497 = reinterpret_tensor(buf481, (s0, 4096), (4096, 1), 0); del buf481  # reuse
        # Topologically Sorted Source Nodes: [down_proj_16, triton_kernel_wrapper_mutation_34], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf494, arg196_1, buf458, buf466, buf486, buf495, buf497, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg196_1
        buf496 = buf487; del buf487  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_34], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf491, 4096, buf497, 4096, arg197_1, 1, buf496, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg197_1
        buf500 = buf497; del buf497  # reuse
        # Topologically Sorted Source Nodes: [linear_119], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf491, arg198_1, buf500, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg198_1
        buf501 = buf474; del buf474  # reuse
        # Topologically Sorted Source Nodes: [linear_120], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf491, arg199_1, buf501, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg199_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_91, cat_36, mul_92, k_embed_17, index_copy__34], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf501, buf7, arg201_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf503 = buf501; del buf501  # reuse
        # Topologically Sorted Source Nodes: [linear_121], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf491, arg200_1, buf503, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg200_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__35], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf503, arg202_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf505 = reinterpret_tensor(buf491, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf491  # reuse
        # Topologically Sorted Source Nodes: [mul_89, cat_35, mul_90, q_embed_17, query_17, attn_output_68], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf500, buf7, buf505, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf506 = buf478; del buf478  # reuse
        # Topologically Sorted Source Nodes: [mul_89, cat_35, mul_90, q_embed_17, query_17, attn_output_68], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg201_1, buf506, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg201_1
        _xnumel = 4096*s4
        buf507 = buf477; del buf477  # reuse
        # Topologically Sorted Source Nodes: [mul_89, cat_35, mul_90, q_embed_17, query_17, attn_output_68], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg202_1, buf507, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg202_1
        # Topologically Sorted Source Nodes: [mul_89, cat_35, mul_90, q_embed_17, query_17, attn_output_68], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf509 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf505, buf506, buf507, reinterpret_tensor(buf508, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf510 = buf509[0]
        assert_size_stride(buf510, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf509
        buf514 = reinterpret_tensor(buf505, (s0, 4096), (4096, 1), 0); del buf505  # reuse
        buf516 = reinterpret_tensor(buf500, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf500  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_63, hidden_states_66, hidden_states_67, attn_output_71, hidden_states_70], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_19.run(buf510, arg203_1, buf458, buf466, buf486, buf495, buf516, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg203_1
        del buf458
        del buf466
        buf517 = buf496; del buf496  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_35], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf514, 4096, reinterpret_tensor(buf516, (s0, 4096), (4096, 1), 0), 4096, arg204_1, 1, buf517, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg204_1
        buf520 = reinterpret_tensor(buf510, (s0, 4096), (4096, 1), 0); del buf510  # reuse
        buf521 = reinterpret_tensor(buf494, (s0, 14336), (14336, 1), 0); del buf494  # reuse
        # Topologically Sorted Source Nodes: [linear_123], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf514, arg205_1, buf521, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg205_1
        buf523 = reinterpret_tensor(buf492, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf492  # reuse
        # Topologically Sorted Source Nodes: [silu_17, linear_124, mul_93], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf514, arg206_1, buf521, buf523, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg206_1
        buf524 = buf514; del buf514  # reuse
        buf526 = buf495; del buf495  # reuse
        # Topologically Sorted Source Nodes: [down_proj_17, triton_kernel_wrapper_mutation_36], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_16.run(buf523, arg207_1, buf516, buf524, buf526, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg207_1
        buf525 = buf517; del buf517  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_36], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf520, 4096, buf526, 4096, arg208_1, 1, buf525, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg208_1
        buf529 = buf526; del buf526  # reuse
        # Topologically Sorted Source Nodes: [linear_126], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf520, arg209_1, buf529, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg209_1
        buf530 = buf503; del buf503  # reuse
        # Topologically Sorted Source Nodes: [linear_127], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf520, arg210_1, buf530, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg210_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_96, cat_38, mul_97, k_embed_18, index_copy__36], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf530, buf7, arg212_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf532 = buf530; del buf530  # reuse
        # Topologically Sorted Source Nodes: [linear_128], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf520, arg211_1, buf532, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg211_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__37], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf532, arg213_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf534 = reinterpret_tensor(buf520, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf520  # reuse
        # Topologically Sorted Source Nodes: [mul_94, cat_37, mul_95, q_embed_18, query_18, attn_output_72], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf529, buf7, buf534, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf535 = buf507; del buf507  # reuse
        # Topologically Sorted Source Nodes: [mul_94, cat_37, mul_95, q_embed_18, query_18, attn_output_72], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg212_1, buf535, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg212_1
        _xnumel = 4096*s4
        buf536 = buf506; del buf506  # reuse
        # Topologically Sorted Source Nodes: [mul_94, cat_37, mul_95, q_embed_18, query_18, attn_output_72], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg213_1, buf536, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg213_1
        _xnumel = s0*s4
        buf537 = buf508; del buf508  # reuse
        buf566 = buf479; del buf479  # reuse
        buf595 = buf450; del buf450  # reuse
        # Topologically Sorted Source Nodes: [mul_94, cat_37, mul_95, q_embed_18, query_18, attn_output_72, mul_99, cat_39, mul_100, q_embed_19, query_19, attn_output_76, mul_104, cat_41, mul_105, q_embed_20, query_20, attn_output_80], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = s0*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(arg8_1, buf537, buf566, buf595, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_94, cat_37, mul_95, q_embed_18, query_18, attn_output_72], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf538 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf534, buf535, buf536, reinterpret_tensor(buf537, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf539 = buf538[0]
        assert_size_stride(buf539, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf538
        buf543 = reinterpret_tensor(buf534, (s0, 4096), (4096, 1), 0); del buf534  # reuse
        buf544 = buf529; del buf529  # reuse
        buf546 = buf486; del buf486  # reuse
        # Topologically Sorted Source Nodes: [attn_output_75, triton_kernel_wrapper_mutation_37], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf539, arg214_1, buf516, buf524, buf544, buf546, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg214_1
        buf545 = buf525; del buf525  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_37], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf543, 4096, buf546, 4096, arg215_1, 1, buf545, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg215_1
        buf549 = buf546; del buf546  # reuse
        buf550 = reinterpret_tensor(buf523, (s0, 14336), (14336, 1), 0); del buf523  # reuse
        # Topologically Sorted Source Nodes: [linear_130], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf543, arg216_1, buf550, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg216_1
        buf552 = reinterpret_tensor(buf521, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf521  # reuse
        # Topologically Sorted Source Nodes: [silu_18, linear_131, mul_98], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf543, arg217_1, buf550, buf552, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg217_1
        buf553 = buf543; del buf543  # reuse
        buf555 = reinterpret_tensor(buf539, (s0, 4096), (4096, 1), 0); del buf539  # reuse
        # Topologically Sorted Source Nodes: [down_proj_18, triton_kernel_wrapper_mutation_38], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf552, arg218_1, buf516, buf524, buf544, buf553, buf555, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg218_1
        buf554 = buf545; del buf545  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_38], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf549, 4096, buf555, 4096, arg219_1, 1, buf554, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg219_1
        buf558 = buf555; del buf555  # reuse
        # Topologically Sorted Source Nodes: [linear_133], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf549, arg220_1, buf558, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg220_1
        buf559 = buf532; del buf532  # reuse
        # Topologically Sorted Source Nodes: [linear_134], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf549, arg221_1, buf559, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg221_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_101, cat_40, mul_102, k_embed_19, index_copy__38], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf559, buf7, arg223_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf561 = buf559; del buf559  # reuse
        # Topologically Sorted Source Nodes: [linear_135], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf549, arg222_1, buf561, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg222_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__39], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf561, arg224_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf563 = reinterpret_tensor(buf549, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf549  # reuse
        # Topologically Sorted Source Nodes: [mul_99, cat_39, mul_100, q_embed_19, query_19, attn_output_76], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf558, buf7, buf563, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf564 = buf536; del buf536  # reuse
        # Topologically Sorted Source Nodes: [mul_99, cat_39, mul_100, q_embed_19, query_19, attn_output_76], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg223_1, buf564, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg223_1
        _xnumel = 4096*s4
        buf565 = buf535; del buf535  # reuse
        # Topologically Sorted Source Nodes: [mul_99, cat_39, mul_100, q_embed_19, query_19, attn_output_76], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg224_1, buf565, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg224_1
        # Topologically Sorted Source Nodes: [mul_99, cat_39, mul_100, q_embed_19, query_19, attn_output_76], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf567 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf563, buf564, buf565, reinterpret_tensor(buf566, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf568 = buf567[0]
        assert_size_stride(buf568, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf567
        buf572 = reinterpret_tensor(buf563, (s0, 4096), (4096, 1), 0); del buf563  # reuse
        buf574 = reinterpret_tensor(buf558, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf558  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_71, hidden_states_74, hidden_states_75, attn_output_79, hidden_states_78], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_19.run(buf568, arg225_1, buf516, buf524, buf544, buf553, buf574, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg225_1
        del buf516
        del buf524
        buf575 = buf554; del buf554  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_39], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf572, 4096, reinterpret_tensor(buf574, (s0, 4096), (4096, 1), 0), 4096, arg226_1, 1, buf575, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg226_1
        buf578 = reinterpret_tensor(buf568, (s0, 4096), (4096, 1), 0); del buf568  # reuse
        buf579 = reinterpret_tensor(buf552, (s0, 14336), (14336, 1), 0); del buf552  # reuse
        # Topologically Sorted Source Nodes: [linear_137], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf572, arg227_1, buf579, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg227_1
        buf581 = reinterpret_tensor(buf550, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf550  # reuse
        # Topologically Sorted Source Nodes: [silu_19, linear_138, mul_103], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf572, arg228_1, buf579, buf581, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg228_1
        buf582 = buf572; del buf572  # reuse
        buf584 = buf553; del buf553  # reuse
        # Topologically Sorted Source Nodes: [down_proj_19, triton_kernel_wrapper_mutation_40], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_16.run(buf581, arg229_1, buf574, buf582, buf584, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg229_1
        buf583 = buf575; del buf575  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_40], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf578, 4096, buf584, 4096, arg230_1, 1, buf583, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg230_1
        buf587 = buf584; del buf584  # reuse
        # Topologically Sorted Source Nodes: [linear_140], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf578, arg231_1, buf587, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg231_1
        buf588 = buf561; del buf561  # reuse
        # Topologically Sorted Source Nodes: [linear_141], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf578, arg232_1, buf588, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg232_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_106, cat_42, mul_107, k_embed_20, index_copy__40], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf588, buf7, arg234_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf590 = buf588; del buf588  # reuse
        # Topologically Sorted Source Nodes: [linear_142], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf578, arg233_1, buf590, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg233_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__41], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf590, arg235_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf592 = reinterpret_tensor(buf578, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf578  # reuse
        # Topologically Sorted Source Nodes: [mul_104, cat_41, mul_105, q_embed_20, query_20, attn_output_80], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf587, buf7, buf592, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf593 = buf565; del buf565  # reuse
        # Topologically Sorted Source Nodes: [mul_104, cat_41, mul_105, q_embed_20, query_20, attn_output_80], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg234_1, buf593, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg234_1
        _xnumel = 4096*s4
        buf594 = buf564; del buf564  # reuse
        # Topologically Sorted Source Nodes: [mul_104, cat_41, mul_105, q_embed_20, query_20, attn_output_80], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg235_1, buf594, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg235_1
        # Topologically Sorted Source Nodes: [mul_104, cat_41, mul_105, q_embed_20, query_20, attn_output_80], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf596 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf592, buf593, buf594, reinterpret_tensor(buf595, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf597 = buf596[0]
        assert_size_stride(buf597, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf596
        buf601 = reinterpret_tensor(buf592, (s0, 4096), (4096, 1), 0); del buf592  # reuse
        buf602 = buf587; del buf587  # reuse
        buf604 = buf544; del buf544  # reuse
        # Topologically Sorted Source Nodes: [attn_output_83, triton_kernel_wrapper_mutation_41], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf597, arg236_1, buf574, buf582, buf602, buf604, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg236_1
        buf603 = buf583; del buf583  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_41], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf601, 4096, buf604, 4096, arg237_1, 1, buf603, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg237_1
        buf607 = buf604; del buf604  # reuse
        buf608 = reinterpret_tensor(buf581, (s0, 14336), (14336, 1), 0); del buf581  # reuse
        # Topologically Sorted Source Nodes: [linear_144], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf601, arg238_1, buf608, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg238_1
        buf610 = reinterpret_tensor(buf579, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf579  # reuse
        # Topologically Sorted Source Nodes: [silu_20, linear_145, mul_108], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf601, arg239_1, buf608, buf610, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg239_1
        buf611 = buf601; del buf601  # reuse
        buf613 = reinterpret_tensor(buf597, (s0, 4096), (4096, 1), 0); del buf597  # reuse
        # Topologically Sorted Source Nodes: [down_proj_20, triton_kernel_wrapper_mutation_42], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf610, arg240_1, buf574, buf582, buf602, buf611, buf613, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg240_1
        buf612 = buf603; del buf603  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_42], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf607, 4096, buf613, 4096, arg241_1, 1, buf612, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg241_1
        buf616 = buf613; del buf613  # reuse
        # Topologically Sorted Source Nodes: [linear_147], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf607, arg242_1, buf616, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg242_1
        buf617 = buf590; del buf590  # reuse
        # Topologically Sorted Source Nodes: [linear_148], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf607, arg243_1, buf617, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg243_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_111, cat_44, mul_112, k_embed_21, index_copy__42], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf617, buf7, arg245_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf619 = buf617; del buf617  # reuse
        # Topologically Sorted Source Nodes: [linear_149], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf607, arg244_1, buf619, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg244_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__43], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf619, arg246_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf621 = reinterpret_tensor(buf607, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf607  # reuse
        # Topologically Sorted Source Nodes: [mul_109, cat_43, mul_110, q_embed_21, query_21, attn_output_84], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf616, buf7, buf621, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf622 = buf594; del buf594  # reuse
        # Topologically Sorted Source Nodes: [mul_109, cat_43, mul_110, q_embed_21, query_21, attn_output_84], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg245_1, buf622, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg245_1
        _xnumel = 4096*s4
        buf623 = buf593; del buf593  # reuse
        # Topologically Sorted Source Nodes: [mul_109, cat_43, mul_110, q_embed_21, query_21, attn_output_84], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg246_1, buf623, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg246_1
        _xnumel = s0*s4
        buf624 = buf595; del buf595  # reuse
        buf653 = buf566; del buf566  # reuse
        buf682 = buf537; del buf537  # reuse
        # Topologically Sorted Source Nodes: [mul_109, cat_43, mul_110, q_embed_21, query_21, attn_output_84, mul_114, cat_45, mul_115, q_embed_22, query_22, attn_output_88, mul_119, cat_47, mul_120, q_embed_23, query_23, attn_output_92], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = s0*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(arg8_1, buf624, buf653, buf682, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_109, cat_43, mul_110, q_embed_21, query_21, attn_output_84], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf625 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf621, buf622, buf623, reinterpret_tensor(buf624, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf626 = buf625[0]
        assert_size_stride(buf626, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf625
        buf630 = reinterpret_tensor(buf621, (s0, 4096), (4096, 1), 0); del buf621  # reuse
        buf632 = reinterpret_tensor(buf616, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf616  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_79, hidden_states_82, hidden_states_83, attn_output_87, hidden_states_86], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_19.run(buf626, arg247_1, buf574, buf582, buf602, buf611, buf632, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg247_1
        del buf574
        del buf582
        buf633 = buf612; del buf612  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_43], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf630, 4096, reinterpret_tensor(buf632, (s0, 4096), (4096, 1), 0), 4096, arg248_1, 1, buf633, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg248_1
        buf636 = reinterpret_tensor(buf626, (s0, 4096), (4096, 1), 0); del buf626  # reuse
        buf637 = reinterpret_tensor(buf610, (s0, 14336), (14336, 1), 0); del buf610  # reuse
        # Topologically Sorted Source Nodes: [linear_151], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf630, arg249_1, buf637, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg249_1
        buf639 = reinterpret_tensor(buf608, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf608  # reuse
        # Topologically Sorted Source Nodes: [silu_21, linear_152, mul_113], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf630, arg250_1, buf637, buf639, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg250_1
        buf640 = buf630; del buf630  # reuse
        buf642 = buf611; del buf611  # reuse
        # Topologically Sorted Source Nodes: [down_proj_21, triton_kernel_wrapper_mutation_44], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_16.run(buf639, arg251_1, buf632, buf640, buf642, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg251_1
        buf641 = buf633; del buf633  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_44], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf636, 4096, buf642, 4096, arg252_1, 1, buf641, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg252_1
        buf645 = buf642; del buf642  # reuse
        # Topologically Sorted Source Nodes: [linear_154], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf636, arg253_1, buf645, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg253_1
        buf646 = buf619; del buf619  # reuse
        # Topologically Sorted Source Nodes: [linear_155], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf636, arg254_1, buf646, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg254_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_116, cat_46, mul_117, k_embed_22, index_copy__44], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf646, buf7, arg256_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf648 = buf646; del buf646  # reuse
        # Topologically Sorted Source Nodes: [linear_156], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf636, arg255_1, buf648, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg255_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__45], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf648, arg257_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf650 = reinterpret_tensor(buf636, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf636  # reuse
        # Topologically Sorted Source Nodes: [mul_114, cat_45, mul_115, q_embed_22, query_22, attn_output_88], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf645, buf7, buf650, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf651 = buf623; del buf623  # reuse
        # Topologically Sorted Source Nodes: [mul_114, cat_45, mul_115, q_embed_22, query_22, attn_output_88], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg256_1, buf651, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg256_1
        _xnumel = 4096*s4
        buf652 = buf622; del buf622  # reuse
        # Topologically Sorted Source Nodes: [mul_114, cat_45, mul_115, q_embed_22, query_22, attn_output_88], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg257_1, buf652, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg257_1
        # Topologically Sorted Source Nodes: [mul_114, cat_45, mul_115, q_embed_22, query_22, attn_output_88], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf654 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf650, buf651, buf652, reinterpret_tensor(buf653, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf655 = buf654[0]
        assert_size_stride(buf655, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf654
        buf659 = reinterpret_tensor(buf650, (s0, 4096), (4096, 1), 0); del buf650  # reuse
        buf660 = buf645; del buf645  # reuse
        buf662 = buf602; del buf602  # reuse
        # Topologically Sorted Source Nodes: [attn_output_91, triton_kernel_wrapper_mutation_45], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf655, arg258_1, buf632, buf640, buf660, buf662, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg258_1
        buf661 = buf641; del buf641  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_45], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf659, 4096, buf662, 4096, arg259_1, 1, buf661, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg259_1
        buf665 = buf662; del buf662  # reuse
        buf666 = reinterpret_tensor(buf639, (s0, 14336), (14336, 1), 0); del buf639  # reuse
        # Topologically Sorted Source Nodes: [linear_158], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf659, arg260_1, buf666, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg260_1
        buf668 = reinterpret_tensor(buf637, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf637  # reuse
        # Topologically Sorted Source Nodes: [silu_22, linear_159, mul_118], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf659, arg261_1, buf666, buf668, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg261_1
        buf669 = buf659; del buf659  # reuse
        buf671 = reinterpret_tensor(buf655, (s0, 4096), (4096, 1), 0); del buf655  # reuse
        # Topologically Sorted Source Nodes: [down_proj_22, triton_kernel_wrapper_mutation_46], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf668, arg262_1, buf632, buf640, buf660, buf669, buf671, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg262_1
        buf670 = buf661; del buf661  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_46], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf665, 4096, buf671, 4096, arg263_1, 1, buf670, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg263_1
        buf674 = buf671; del buf671  # reuse
        # Topologically Sorted Source Nodes: [linear_161], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf665, arg264_1, buf674, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg264_1
        buf675 = buf648; del buf648  # reuse
        # Topologically Sorted Source Nodes: [linear_162], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf665, arg265_1, buf675, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg265_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_121, cat_48, mul_122, k_embed_23, index_copy__46], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf675, buf7, arg267_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf677 = buf675; del buf675  # reuse
        # Topologically Sorted Source Nodes: [linear_163], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf665, arg266_1, buf677, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg266_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__47], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf677, arg268_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf679 = reinterpret_tensor(buf665, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf665  # reuse
        # Topologically Sorted Source Nodes: [mul_119, cat_47, mul_120, q_embed_23, query_23, attn_output_92], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf674, buf7, buf679, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf680 = buf652; del buf652  # reuse
        # Topologically Sorted Source Nodes: [mul_119, cat_47, mul_120, q_embed_23, query_23, attn_output_92], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg267_1, buf680, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg267_1
        _xnumel = 4096*s4
        buf681 = buf651; del buf651  # reuse
        # Topologically Sorted Source Nodes: [mul_119, cat_47, mul_120, q_embed_23, query_23, attn_output_92], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg268_1, buf681, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg268_1
        # Topologically Sorted Source Nodes: [mul_119, cat_47, mul_120, q_embed_23, query_23, attn_output_92], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf683 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf679, buf680, buf681, reinterpret_tensor(buf682, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf684 = buf683[0]
        assert_size_stride(buf684, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf683
        buf688 = reinterpret_tensor(buf679, (s0, 4096), (4096, 1), 0); del buf679  # reuse
        buf690 = reinterpret_tensor(buf674, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf674  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_87, hidden_states_90, hidden_states_91, attn_output_95, hidden_states_94], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_19.run(buf684, arg269_1, buf632, buf640, buf660, buf669, buf690, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg269_1
        del buf632
        del buf640
        buf691 = buf670; del buf670  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_47], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf688, 4096, reinterpret_tensor(buf690, (s0, 4096), (4096, 1), 0), 4096, arg270_1, 1, buf691, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg270_1
        buf694 = reinterpret_tensor(buf684, (s0, 4096), (4096, 1), 0); del buf684  # reuse
        buf695 = reinterpret_tensor(buf668, (s0, 14336), (14336, 1), 0); del buf668  # reuse
        # Topologically Sorted Source Nodes: [linear_165], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf688, arg271_1, buf695, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg271_1
        buf697 = reinterpret_tensor(buf666, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf666  # reuse
        # Topologically Sorted Source Nodes: [silu_23, linear_166, mul_123], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf688, arg272_1, buf695, buf697, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg272_1
        buf698 = buf688; del buf688  # reuse
        buf700 = buf669; del buf669  # reuse
        # Topologically Sorted Source Nodes: [down_proj_23, triton_kernel_wrapper_mutation_48], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_16.run(buf697, arg273_1, buf690, buf698, buf700, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg273_1
        buf699 = buf691; del buf691  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_48], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf694, 4096, buf700, 4096, arg274_1, 1, buf699, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg274_1
        buf703 = buf700; del buf700  # reuse
        # Topologically Sorted Source Nodes: [linear_168], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf694, arg275_1, buf703, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg275_1
        buf704 = buf677; del buf677  # reuse
        # Topologically Sorted Source Nodes: [linear_169], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf694, arg276_1, buf704, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg276_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_126, cat_50, mul_127, k_embed_24, index_copy__48], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf704, buf7, arg278_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf706 = buf704; del buf704  # reuse
        # Topologically Sorted Source Nodes: [linear_170], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf694, arg277_1, buf706, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg277_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__49], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf706, arg279_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf708 = reinterpret_tensor(buf694, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf694  # reuse
        # Topologically Sorted Source Nodes: [mul_124, cat_49, mul_125, q_embed_24, query_24, attn_output_96], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf703, buf7, buf708, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf709 = buf681; del buf681  # reuse
        # Topologically Sorted Source Nodes: [mul_124, cat_49, mul_125, q_embed_24, query_24, attn_output_96], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg278_1, buf709, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg278_1
        _xnumel = 4096*s4
        buf710 = buf680; del buf680  # reuse
        # Topologically Sorted Source Nodes: [mul_124, cat_49, mul_125, q_embed_24, query_24, attn_output_96], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg279_1, buf710, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg279_1
        _xnumel = s0*s4
        buf711 = buf682; del buf682  # reuse
        buf740 = buf653; del buf653  # reuse
        buf769 = buf624; del buf624  # reuse
        # Topologically Sorted Source Nodes: [mul_124, cat_49, mul_125, q_embed_24, query_24, attn_output_96, mul_129, cat_51, mul_130, q_embed_25, query_25, attn_output_100, mul_134, cat_53, mul_135, q_embed_26, query_26, attn_output_104], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = s0*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(arg8_1, buf711, buf740, buf769, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_124, cat_49, mul_125, q_embed_24, query_24, attn_output_96], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf712 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf708, buf709, buf710, reinterpret_tensor(buf711, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf713 = buf712[0]
        assert_size_stride(buf713, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf712
        buf717 = reinterpret_tensor(buf708, (s0, 4096), (4096, 1), 0); del buf708  # reuse
        buf718 = buf703; del buf703  # reuse
        buf720 = buf660; del buf660  # reuse
        # Topologically Sorted Source Nodes: [attn_output_99, triton_kernel_wrapper_mutation_49], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf713, arg280_1, buf690, buf698, buf718, buf720, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg280_1
        buf719 = buf699; del buf699  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_49], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf717, 4096, buf720, 4096, arg281_1, 1, buf719, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg281_1
        buf723 = buf720; del buf720  # reuse
        buf724 = reinterpret_tensor(buf697, (s0, 14336), (14336, 1), 0); del buf697  # reuse
        # Topologically Sorted Source Nodes: [linear_172], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf717, arg282_1, buf724, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg282_1
        buf726 = reinterpret_tensor(buf695, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf695  # reuse
        # Topologically Sorted Source Nodes: [silu_24, linear_173, mul_128], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf717, arg283_1, buf724, buf726, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg283_1
        buf727 = buf717; del buf717  # reuse
        buf729 = reinterpret_tensor(buf713, (s0, 4096), (4096, 1), 0); del buf713  # reuse
        # Topologically Sorted Source Nodes: [down_proj_24, triton_kernel_wrapper_mutation_50], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf726, arg284_1, buf690, buf698, buf718, buf727, buf729, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg284_1
        buf728 = buf719; del buf719  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_50], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf723, 4096, buf729, 4096, arg285_1, 1, buf728, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg285_1
        buf732 = buf729; del buf729  # reuse
        # Topologically Sorted Source Nodes: [linear_175], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf723, arg286_1, buf732, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg286_1
        buf733 = buf706; del buf706  # reuse
        # Topologically Sorted Source Nodes: [linear_176], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf723, arg287_1, buf733, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg287_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_131, cat_52, mul_132, k_embed_25, index_copy__50], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf733, buf7, arg289_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf735 = buf733; del buf733  # reuse
        # Topologically Sorted Source Nodes: [linear_177], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf723, arg288_1, buf735, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg288_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__51], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf735, arg290_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf737 = reinterpret_tensor(buf723, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf723  # reuse
        # Topologically Sorted Source Nodes: [mul_129, cat_51, mul_130, q_embed_25, query_25, attn_output_100], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf732, buf7, buf737, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf738 = buf710; del buf710  # reuse
        # Topologically Sorted Source Nodes: [mul_129, cat_51, mul_130, q_embed_25, query_25, attn_output_100], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg289_1, buf738, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg289_1
        _xnumel = 4096*s4
        buf739 = buf709; del buf709  # reuse
        # Topologically Sorted Source Nodes: [mul_129, cat_51, mul_130, q_embed_25, query_25, attn_output_100], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg290_1, buf739, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg290_1
        # Topologically Sorted Source Nodes: [mul_129, cat_51, mul_130, q_embed_25, query_25, attn_output_100], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf741 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf737, buf738, buf739, reinterpret_tensor(buf740, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf742 = buf741[0]
        assert_size_stride(buf742, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf741
        buf746 = reinterpret_tensor(buf737, (s0, 4096), (4096, 1), 0); del buf737  # reuse
        buf748 = reinterpret_tensor(buf732, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf732  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_95, hidden_states_98, hidden_states_99, attn_output_103, hidden_states_102], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_19.run(buf742, arg291_1, buf690, buf698, buf718, buf727, buf748, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg291_1
        del buf690
        del buf698
        buf749 = buf728; del buf728  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_51], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf746, 4096, reinterpret_tensor(buf748, (s0, 4096), (4096, 1), 0), 4096, arg292_1, 1, buf749, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg292_1
        buf752 = reinterpret_tensor(buf742, (s0, 4096), (4096, 1), 0); del buf742  # reuse
        buf753 = reinterpret_tensor(buf726, (s0, 14336), (14336, 1), 0); del buf726  # reuse
        # Topologically Sorted Source Nodes: [linear_179], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf746, arg293_1, buf753, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg293_1
        buf755 = reinterpret_tensor(buf724, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf724  # reuse
        # Topologically Sorted Source Nodes: [silu_25, linear_180, mul_133], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf746, arg294_1, buf753, buf755, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg294_1
        buf756 = buf746; del buf746  # reuse
        buf758 = buf727; del buf727  # reuse
        # Topologically Sorted Source Nodes: [down_proj_25, triton_kernel_wrapper_mutation_52], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_16.run(buf755, arg295_1, buf748, buf756, buf758, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg295_1
        buf757 = buf749; del buf749  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_52], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf752, 4096, buf758, 4096, arg296_1, 1, buf757, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg296_1
        buf761 = buf758; del buf758  # reuse
        # Topologically Sorted Source Nodes: [linear_182], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf752, arg297_1, buf761, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg297_1
        buf762 = buf735; del buf735  # reuse
        # Topologically Sorted Source Nodes: [linear_183], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf752, arg298_1, buf762, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg298_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_136, cat_54, mul_137, k_embed_26, index_copy__52], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf762, buf7, arg300_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf764 = buf762; del buf762  # reuse
        # Topologically Sorted Source Nodes: [linear_184], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf752, arg299_1, buf764, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg299_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__53], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf764, arg301_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf766 = reinterpret_tensor(buf752, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf752  # reuse
        # Topologically Sorted Source Nodes: [mul_134, cat_53, mul_135, q_embed_26, query_26, attn_output_104], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf761, buf7, buf766, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf767 = buf739; del buf739  # reuse
        # Topologically Sorted Source Nodes: [mul_134, cat_53, mul_135, q_embed_26, query_26, attn_output_104], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg300_1, buf767, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg300_1
        _xnumel = 4096*s4
        buf768 = buf738; del buf738  # reuse
        # Topologically Sorted Source Nodes: [mul_134, cat_53, mul_135, q_embed_26, query_26, attn_output_104], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg301_1, buf768, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg301_1
        # Topologically Sorted Source Nodes: [mul_134, cat_53, mul_135, q_embed_26, query_26, attn_output_104], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf770 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf766, buf767, buf768, reinterpret_tensor(buf769, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf771 = buf770[0]
        assert_size_stride(buf771, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf770
        buf775 = reinterpret_tensor(buf766, (s0, 4096), (4096, 1), 0); del buf766  # reuse
        buf776 = buf761; del buf761  # reuse
        buf778 = buf718; del buf718  # reuse
        # Topologically Sorted Source Nodes: [attn_output_107, triton_kernel_wrapper_mutation_53], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf771, arg302_1, buf748, buf756, buf776, buf778, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg302_1
        buf777 = buf757; del buf757  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_53], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf775, 4096, buf778, 4096, arg303_1, 1, buf777, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg303_1
        buf781 = buf778; del buf778  # reuse
        buf782 = reinterpret_tensor(buf755, (s0, 14336), (14336, 1), 0); del buf755  # reuse
        # Topologically Sorted Source Nodes: [linear_186], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf775, arg304_1, buf782, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg304_1
        buf784 = reinterpret_tensor(buf753, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf753  # reuse
        # Topologically Sorted Source Nodes: [silu_26, linear_187, mul_138], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf775, arg305_1, buf782, buf784, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg305_1
        buf785 = buf775; del buf775  # reuse
        buf787 = reinterpret_tensor(buf771, (s0, 4096), (4096, 1), 0); del buf771  # reuse
        # Topologically Sorted Source Nodes: [down_proj_26, triton_kernel_wrapper_mutation_54], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf784, arg306_1, buf748, buf756, buf776, buf785, buf787, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg306_1
        buf786 = buf777; del buf777  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_54], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf781, 4096, buf787, 4096, arg307_1, 1, buf786, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg307_1
        buf790 = buf787; del buf787  # reuse
        # Topologically Sorted Source Nodes: [linear_189], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf781, arg308_1, buf790, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg308_1
        buf791 = buf764; del buf764  # reuse
        # Topologically Sorted Source Nodes: [linear_190], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf781, arg309_1, buf791, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg309_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_141, cat_56, mul_142, k_embed_27, index_copy__54], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf791, buf7, arg311_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf793 = buf791; del buf791  # reuse
        # Topologically Sorted Source Nodes: [linear_191], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf781, arg310_1, buf793, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg310_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__55], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf793, arg312_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf795 = reinterpret_tensor(buf781, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf781  # reuse
        # Topologically Sorted Source Nodes: [mul_139, cat_55, mul_140, q_embed_27, query_27, attn_output_108], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf790, buf7, buf795, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf796 = buf768; del buf768  # reuse
        # Topologically Sorted Source Nodes: [mul_139, cat_55, mul_140, q_embed_27, query_27, attn_output_108], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg311_1, buf796, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg311_1
        _xnumel = 4096*s4
        buf797 = buf767; del buf767  # reuse
        # Topologically Sorted Source Nodes: [mul_139, cat_55, mul_140, q_embed_27, query_27, attn_output_108], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg312_1, buf797, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg312_1
        _xnumel = s0*s4
        buf798 = buf769; del buf769  # reuse
        buf827 = buf740; del buf740  # reuse
        buf856 = buf711; del buf711  # reuse
        # Topologically Sorted Source Nodes: [mul_139, cat_55, mul_140, q_embed_27, query_27, attn_output_108, mul_144, cat_57, mul_145, q_embed_28, query_28, attn_output_112, mul_149, cat_59, mul_150, q_embed_29, query_29, attn_output_116], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel = s0*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9.run(arg8_1, buf798, buf827, buf856, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_9_xnumel, stream=stream0)
        # Topologically Sorted Source Nodes: [mul_139, cat_55, mul_140, q_embed_27, query_27, attn_output_108], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf799 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf795, buf796, buf797, reinterpret_tensor(buf798, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        del buf798
        buf800 = buf799[0]
        assert_size_stride(buf800, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf799
        buf804 = reinterpret_tensor(buf795, (s0, 4096), (4096, 1), 0); del buf795  # reuse
        buf806 = reinterpret_tensor(buf790, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf790  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_103, hidden_states_106, hidden_states_107, attn_output_111, hidden_states_110], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_19.run(buf800, arg313_1, buf748, buf756, buf776, buf785, buf806, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg313_1
        del buf748
        del buf756
        buf807 = buf786; del buf786  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_55], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf804, 4096, reinterpret_tensor(buf806, (s0, 4096), (4096, 1), 0), 4096, arg314_1, 1, buf807, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg314_1
        buf810 = reinterpret_tensor(buf800, (s0, 4096), (4096, 1), 0); del buf800  # reuse
        buf811 = reinterpret_tensor(buf784, (s0, 14336), (14336, 1), 0); del buf784  # reuse
        # Topologically Sorted Source Nodes: [linear_193], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf804, arg315_1, buf811, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg315_1
        buf813 = reinterpret_tensor(buf782, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf782  # reuse
        # Topologically Sorted Source Nodes: [silu_27, linear_194, mul_143], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf804, arg316_1, buf811, buf813, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg316_1
        buf814 = buf804; del buf804  # reuse
        buf816 = buf785; del buf785  # reuse
        # Topologically Sorted Source Nodes: [down_proj_27, triton_kernel_wrapper_mutation_56], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_16.run(buf813, arg317_1, buf806, buf814, buf816, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg317_1
        buf815 = buf807; del buf807  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_56], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf810, 4096, buf816, 4096, arg318_1, 1, buf815, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg318_1
        buf819 = buf816; del buf816  # reuse
        # Topologically Sorted Source Nodes: [linear_196], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf810, arg319_1, buf819, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg319_1
        buf820 = buf793; del buf793  # reuse
        # Topologically Sorted Source Nodes: [linear_197], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf810, arg320_1, buf820, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg320_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_146, cat_58, mul_147, k_embed_28, index_copy__56], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf820, buf7, arg322_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf822 = buf820; del buf820  # reuse
        # Topologically Sorted Source Nodes: [linear_198], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf810, arg321_1, buf822, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg321_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__57], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf822, arg323_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf824 = reinterpret_tensor(buf810, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf810  # reuse
        # Topologically Sorted Source Nodes: [mul_144, cat_57, mul_145, q_embed_28, query_28, attn_output_112], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf819, buf7, buf824, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf825 = buf797; del buf797  # reuse
        # Topologically Sorted Source Nodes: [mul_144, cat_57, mul_145, q_embed_28, query_28, attn_output_112], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg322_1, buf825, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg322_1
        _xnumel = 4096*s4
        buf826 = buf796; del buf796  # reuse
        # Topologically Sorted Source Nodes: [mul_144, cat_57, mul_145, q_embed_28, query_28, attn_output_112], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg323_1, buf826, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg323_1
        # Topologically Sorted Source Nodes: [mul_144, cat_57, mul_145, q_embed_28, query_28, attn_output_112], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf828 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf824, buf825, buf826, reinterpret_tensor(buf827, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf829 = buf828[0]
        assert_size_stride(buf829, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf828
        buf833 = reinterpret_tensor(buf824, (s0, 4096), (4096, 1), 0); del buf824  # reuse
        buf834 = buf819; del buf819  # reuse
        buf836 = buf776; del buf776  # reuse
        # Topologically Sorted Source Nodes: [attn_output_115, triton_kernel_wrapper_mutation_57], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf829, arg324_1, buf806, buf814, buf834, buf836, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg324_1
        buf835 = buf815; del buf815  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_57], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf833, 4096, buf836, 4096, arg325_1, 1, buf835, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg325_1
        buf839 = buf836; del buf836  # reuse
        buf840 = reinterpret_tensor(buf813, (s0, 14336), (14336, 1), 0); del buf813  # reuse
        # Topologically Sorted Source Nodes: [linear_200], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf833, arg326_1, buf840, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg326_1
        buf842 = reinterpret_tensor(buf811, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf811  # reuse
        # Topologically Sorted Source Nodes: [silu_28, linear_201, mul_148], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf833, arg327_1, buf840, buf842, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg327_1
        buf843 = buf833; del buf833  # reuse
        buf845 = reinterpret_tensor(buf829, (s0, 4096), (4096, 1), 0); del buf829  # reuse
        # Topologically Sorted Source Nodes: [down_proj_28, triton_kernel_wrapper_mutation_58], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf842, arg328_1, buf806, buf814, buf834, buf843, buf845, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg328_1
        buf844 = buf835; del buf835  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_58], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf839, 4096, buf845, 4096, arg329_1, 1, buf844, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg329_1
        buf848 = buf845; del buf845  # reuse
        # Topologically Sorted Source Nodes: [linear_203], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf839, arg330_1, buf848, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg330_1
        buf849 = buf822; del buf822  # reuse
        # Topologically Sorted Source Nodes: [linear_204], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf839, arg331_1, buf849, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg331_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_151, cat_60, mul_152, k_embed_29, index_copy__58], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf849, buf7, arg333_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf851 = buf849; del buf849  # reuse
        # Topologically Sorted Source Nodes: [linear_205], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf839, arg332_1, buf851, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg332_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__59], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf851, arg334_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf853 = reinterpret_tensor(buf839, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf839  # reuse
        # Topologically Sorted Source Nodes: [mul_149, cat_59, mul_150, q_embed_29, query_29, attn_output_116], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf848, buf7, buf853, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf854 = buf826; del buf826  # reuse
        # Topologically Sorted Source Nodes: [mul_149, cat_59, mul_150, q_embed_29, query_29, attn_output_116], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg333_1, buf854, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg333_1
        _xnumel = 4096*s4
        buf855 = buf825; del buf825  # reuse
        # Topologically Sorted Source Nodes: [mul_149, cat_59, mul_150, q_embed_29, query_29, attn_output_116], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg334_1, buf855, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg334_1
        # Topologically Sorted Source Nodes: [mul_149, cat_59, mul_150, q_embed_29, query_29, attn_output_116], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf857 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf853, buf854, buf855, reinterpret_tensor(buf856, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        buf858 = buf857[0]
        assert_size_stride(buf858, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf857
        buf862 = reinterpret_tensor(buf853, (s0, 4096), (4096, 1), 0); del buf853  # reuse
        buf864 = reinterpret_tensor(buf848, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf848  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_111, hidden_states_114, hidden_states_115, attn_output_119, hidden_states_118], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_19.run(buf858, arg335_1, buf806, buf814, buf834, buf843, buf864, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg335_1
        del buf806
        del buf814
        buf865 = buf844; del buf844  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_59], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf862, 4096, reinterpret_tensor(buf864, (s0, 4096), (4096, 1), 0), 4096, arg336_1, 1, buf865, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg336_1
        buf868 = reinterpret_tensor(buf858, (s0, 4096), (4096, 1), 0); del buf858  # reuse
        buf869 = reinterpret_tensor(buf842, (s0, 14336), (14336, 1), 0); del buf842  # reuse
        # Topologically Sorted Source Nodes: [linear_207], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf862, arg337_1, buf869, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg337_1
        buf871 = reinterpret_tensor(buf840, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf840  # reuse
        # Topologically Sorted Source Nodes: [silu_29, linear_208, mul_153], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf862, arg338_1, buf869, buf871, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg338_1
        buf872 = buf862; del buf862  # reuse
        buf874 = buf843; del buf843  # reuse
        # Topologically Sorted Source Nodes: [down_proj_29, triton_kernel_wrapper_mutation_60], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_16.run(buf871, arg339_1, buf864, buf872, buf874, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg339_1
        buf873 = buf865; del buf865  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_60], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf868, 4096, buf874, 4096, arg340_1, 1, buf873, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg340_1
        buf877 = buf874; del buf874  # reuse
        # Topologically Sorted Source Nodes: [linear_210], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf868, arg341_1, buf877, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg341_1
        buf878 = buf851; del buf851  # reuse
        # Topologically Sorted Source Nodes: [linear_211], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf868, arg342_1, buf878, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg342_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_156, cat_62, mul_157, k_embed_30, index_copy__60], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf878, buf7, arg344_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf880 = buf878; del buf878  # reuse
        # Topologically Sorted Source Nodes: [linear_212], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf868, arg343_1, buf880, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg343_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__61], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf880, arg345_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        _xnumel = 4096*s0
        buf882 = reinterpret_tensor(buf868, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf868  # reuse
        # Topologically Sorted Source Nodes: [mul_154, cat_61, mul_155, q_embed_30, query_30, attn_output_120], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf877, buf7, buf882, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        _xnumel = 4096*s4
        buf883 = buf855; del buf855  # reuse
        # Topologically Sorted Source Nodes: [mul_154, cat_61, mul_155, q_embed_30, query_30, attn_output_120], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg344_1, buf883, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg344_1
        _xnumel = 4096*s4
        buf884 = buf854; del buf854  # reuse
        # Topologically Sorted Source Nodes: [mul_154, cat_61, mul_155, q_embed_30, query_30, attn_output_120], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg345_1, buf884, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg345_1
        _xnumel = s0*s4
        buf885 = buf856; del buf856  # reuse
        buf914 = buf827; del buf827  # reuse
        # Topologically Sorted Source Nodes: [mul_154, cat_61, mul_155, q_embed_30, query_30, attn_output_120, mul_159, cat_63, mul_160, q_embed_31, query_31, attn_output_124], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_20_xnumel = s0*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_20.run(arg8_1, buf885, buf914, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_20_xnumel, stream=stream0)
        del arg8_1
        # Topologically Sorted Source Nodes: [mul_154, cat_61, mul_155, q_embed_30, query_30, attn_output_120], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf886 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf882, buf883, buf884, reinterpret_tensor(buf885, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        del buf885
        buf887 = buf886[0]
        assert_size_stride(buf887, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf886
        buf891 = reinterpret_tensor(buf882, (s0, 4096), (4096, 1), 0); del buf882  # reuse
        buf892 = buf877; del buf877  # reuse
        buf894 = buf834; del buf834  # reuse
        # Topologically Sorted Source Nodes: [attn_output_123, triton_kernel_wrapper_mutation_61], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_17.run(buf887, arg346_1, buf864, buf872, buf892, buf894, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg346_1
        buf893 = buf873; del buf873  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_61], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf891, 4096, buf894, 4096, arg347_1, 1, buf893, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg347_1
        buf897 = buf894; del buf894  # reuse
        buf898 = reinterpret_tensor(buf871, (s0, 14336), (14336, 1), 0); del buf871  # reuse
        # Topologically Sorted Source Nodes: [linear_214], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf891, arg348_1, buf898, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg348_1
        buf900 = reinterpret_tensor(buf869, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf869  # reuse
        # Topologically Sorted Source Nodes: [silu_30, linear_215, mul_158], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf891, arg349_1, buf898, buf900, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg349_1
        buf901 = buf891; del buf891  # reuse
        buf903 = reinterpret_tensor(buf887, (s0, 4096), (4096, 1), 0); del buf887  # reuse
        # Topologically Sorted Source Nodes: [down_proj_30, triton_kernel_wrapper_mutation_62], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_18.run(buf900, arg350_1, buf864, buf872, buf892, buf901, buf903, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg350_1
        buf902 = buf893; del buf893  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_62], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf897, 4096, buf903, 4096, arg351_1, 1, buf902, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg351_1
        buf906 = buf903; del buf903  # reuse
        # Topologically Sorted Source Nodes: [linear_217], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_1.run(buf897, arg352_1, buf906, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg352_1
        buf907 = buf880; del buf880  # reuse
        # Topologically Sorted Source Nodes: [linear_218], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf897, arg353_1, buf907, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg353_1
        _xnumel = 8*s1
        # Topologically Sorted Source Nodes: [mul_161, cat_64, mul_162, k_embed_31, index_copy__62], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        triton_poi_fused_add_cat_index_copy_mul_5_xnumel = 8*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_5.run(arg4_1, buf907, buf7, arg355_1, s4, s0, 128, triton_poi_fused_add_cat_index_copy_mul_5_xnumel, stream=stream0)
        buf909 = buf907; del buf907  # reuse
        # Topologically Sorted Source Nodes: [linear_219], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_4.run(buf897, arg354_1, buf909, s0, 16*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg354_1
        _xnumel = 1024*s1
        # Topologically Sorted Source Nodes: [index_copy__63], Original ATen: [aten.index_copy]
        triton_poi_fused_index_copy_6_xnumel = 1024*s1
        stream0 = get_raw_stream(0)
        triton_poi_fused_index_copy_6.run(arg4_1, buf909, arg356_1, s4, triton_poi_fused_index_copy_6_xnumel, stream=stream0)
        del arg4_1
        del buf909
        _xnumel = 4096*s0
        buf911 = reinterpret_tensor(buf897, (1, 32, s0, 128), (4096*s0, 128*s0, 128, 1), 0); del buf897  # reuse
        # Topologically Sorted Source Nodes: [mul_159, cat_63, mul_160, q_embed_31, query_31, attn_output_124], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel = 4096*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7.run(buf906, buf7, buf911, s0, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_7_xnumel, stream=stream0)
        del buf7
        _xnumel = 4096*s4
        buf912 = buf884; del buf884  # reuse
        # Topologically Sorted Source Nodes: [mul_159, cat_63, mul_160, q_embed_31, query_31, attn_output_124], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg355_1, buf912, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg355_1
        _xnumel = 4096*s4
        buf913 = buf883; del buf883  # reuse
        # Topologically Sorted Source Nodes: [mul_159, cat_63, mul_160, q_embed_31, query_31, attn_output_124], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel = 4096*s4
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8.run(arg356_1, buf913, ps0, s4, triton_poi_fused__scaled_dot_product_efficient_attention_add_cat_clone_mul_8_xnumel, stream=stream0)
        del arg356_1
        # Topologically Sorted Source Nodes: [mul_159, cat_63, mul_160, q_embed_31, query_31, attn_output_124], Original ATen: [aten.mul, aten.cat, aten.add, aten.clone, aten._scaled_dot_product_efficient_attention]
        buf915 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf911, buf912, buf913, reinterpret_tensor(buf914, (1, 32, s0, s4), (8*s0*((7 + s4) // 8), 0, 8*((7 + s4) // 8), 1), 0), False, scale=0.08838834764831845)
        del buf912
        del buf913
        del buf914
        buf916 = buf915[0]
        assert_size_stride(buf916, (1, 32, s0, 128), (4096*s0, 128, 4096, 1))
        del buf915
        buf920 = reinterpret_tensor(buf911, (s0, 4096), (4096, 1), 0); del buf911  # reuse
        buf922 = reinterpret_tensor(buf906, (1, s0, 4096), (4096*s0, 4096, 1), 0); del buf906  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_119, hidden_states_122, hidden_states_123, attn_output_127, hidden_states_126], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_add_mm_19.run(buf916, arg357_1, buf864, buf872, buf892, buf901, buf922, s0, 128*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg357_1
        del buf864
        del buf872
        del buf892
        del buf901
        buf923 = buf902; del buf902  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_63], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf920, 4096, reinterpret_tensor(buf922, (s0, 4096), (4096, 1), 0), 4096, arg358_1, 1, buf923, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg358_1
        buf926 = reinterpret_tensor(buf916, (s0, 4096), (4096, 1), 0); del buf916  # reuse
        buf927 = reinterpret_tensor(buf900, (s0, 14336), (14336, 1), 0); del buf900  # reuse
        # Topologically Sorted Source Nodes: [linear_221], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_11.run(buf920, arg359_1, buf927, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg359_1
        buf929 = reinterpret_tensor(buf898, (1, s0, 14336), (14336*s0, 14336, 1), 0); del buf898  # reuse
        # Topologically Sorted Source Nodes: [silu_31, linear_222, mul_163], Original ATen: [aten.silu, aten.mm, aten.mul]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_mul_silu_12.run(buf920, arg360_1, buf927, buf929, s0, 448*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg360_1
        del buf927
        buf932 = buf920; del buf920  # reuse
        # Topologically Sorted Source Nodes: [down_proj_31, triton_kernel_wrapper_mutation_64], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_tem_fused_mm_21.run(buf929, arg361_1, buf922, buf932, s0, 64*((15 + s0) // 16), 1, 1, stream=stream0)
        del arg361_1
        del buf922
        del buf929
        buf931 = buf923; del buf923  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_64], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf926, 4096, buf932, 4096, arg362_1, 1, buf931, 1, 4096, 1e-05, 0.0, 0, 4096, s0, 1, 1, stream=stream0)
        del arg362_1
        del buf931
        del buf932
        buf936 = empty_strided_cuda((1, 128256), (128256, 1), torch.float16)
        # Topologically Sorted Source Nodes: [logits], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_22.run(buf926, arg364_1, buf936, s0, 128256, 4096, stream=stream0)
        del arg364_1
        del buf926
    return (reinterpret_tensor(buf936, (1, 1, 128256), (128256, 128256, 1), 0), )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = 9
    arg1_1 = rand_strided((1, 9), (9, 1), device='cuda:0', dtype=torch.int64)
    arg2_1 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg3_1 = 9
    arg4_1 = rand_strided((9, ), (1, ), device='cuda:0', dtype=torch.int64)
    arg5_1 = 9
    arg6_1 = rand_strided((1, 9), (9, 1), device='cuda:0', dtype=torch.int64)
    arg7_1 = 2048
    arg8_1 = rand_strided((1, 1, 9, 2048), (18432, 18432, 2048, 1), device='cuda:0', dtype=torch.bool)
    arg9_1 = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float32)
    arg10_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg11_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg12_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg13_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg14_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg15_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg16_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg17_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg18_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg19_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg20_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg21_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg22_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg23_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg24_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg25_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg26_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg27_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg28_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg29_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg30_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg31_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg32_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg33_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg34_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg35_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg36_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg37_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg38_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg39_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg40_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg41_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg42_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg43_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg44_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg45_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg46_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg47_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg48_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg49_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg50_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg51_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg52_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg53_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg54_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg55_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg56_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg57_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg58_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg59_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg60_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg61_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg62_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg63_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg64_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg65_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg66_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg67_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg68_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg69_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg70_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg71_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg72_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg73_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg74_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg75_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg76_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg77_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg78_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg79_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg80_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg81_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg82_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg83_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg84_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg85_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg86_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg87_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg88_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg89_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg90_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg91_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg92_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg93_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg94_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg95_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg96_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg97_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg98_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg99_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg100_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg101_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg102_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg103_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg104_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg105_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg106_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg107_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg108_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg109_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg110_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg111_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg112_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg113_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg114_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg115_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg116_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg117_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg118_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg119_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg120_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg121_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg122_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg123_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg124_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg125_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg126_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg127_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg128_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg129_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg130_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg131_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg132_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg133_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg134_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg135_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg136_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg137_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg138_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg139_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg140_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg141_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg142_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg143_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg144_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg145_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg146_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg147_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg148_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg149_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg150_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg151_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg152_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg153_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg154_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg155_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg156_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg157_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg158_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg159_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg160_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg161_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg162_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg163_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg164_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg165_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg166_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg167_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg168_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg169_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg170_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg171_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg172_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg173_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg174_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg175_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg176_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg177_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg178_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg179_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg180_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg181_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg182_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg183_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg184_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg185_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg186_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg187_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg188_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg189_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg190_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg191_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg192_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg193_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg194_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg195_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg196_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg197_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg198_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg199_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg200_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg201_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg202_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg203_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg204_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg205_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg206_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg207_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg208_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg209_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg210_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg211_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg212_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg213_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg214_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg215_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg216_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg217_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg218_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg219_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg220_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg221_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg222_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg223_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg224_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg225_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg226_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg227_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg228_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg229_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg230_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg231_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg232_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg233_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg234_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg235_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg236_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg237_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg238_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg239_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg240_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg241_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg242_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg243_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg244_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg245_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg246_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg247_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg248_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg249_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg250_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg251_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg252_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg253_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg254_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg255_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg256_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg257_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg258_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg259_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg260_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg261_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg262_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg263_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg264_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg265_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg266_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg267_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg268_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg269_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg270_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg271_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg272_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg273_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg274_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg275_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg276_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg277_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg278_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg279_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg280_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg281_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg282_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg283_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg284_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg285_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg286_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg287_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg288_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg289_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg290_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg291_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg292_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg293_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg294_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg295_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg296_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg297_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg298_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg299_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg300_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg301_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg302_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg303_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg304_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg305_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg306_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg307_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg308_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg309_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg310_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg311_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg312_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg313_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg314_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg315_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg316_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg317_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg318_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg319_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg320_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg321_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg322_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg323_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg324_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg325_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg326_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg327_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg328_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg329_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg330_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg331_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg332_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg333_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg334_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg335_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg336_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg337_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg338_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg339_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg340_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg341_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg342_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg343_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg344_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg345_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg346_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg347_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg348_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg349_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg350_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg351_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg352_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg353_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg354_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg355_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg356_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg357_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg358_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg359_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg360_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg361_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg362_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg363_1 = 1
    arg364_1 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    fn = lambda: call([arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1, arg27_1, arg28_1, arg29_1, arg30_1, arg31_1, arg32_1, arg33_1, arg34_1, arg35_1, arg36_1, arg37_1, arg38_1, arg39_1, arg40_1, arg41_1, arg42_1, arg43_1, arg44_1, arg45_1, arg46_1, arg47_1, arg48_1, arg49_1, arg50_1, arg51_1, arg52_1, arg53_1, arg54_1, arg55_1, arg56_1, arg57_1, arg58_1, arg59_1, arg60_1, arg61_1, arg62_1, arg63_1, arg64_1, arg65_1, arg66_1, arg67_1, arg68_1, arg69_1, arg70_1, arg71_1, arg72_1, arg73_1, arg74_1, arg75_1, arg76_1, arg77_1, arg78_1, arg79_1, arg80_1, arg81_1, arg82_1, arg83_1, arg84_1, arg85_1, arg86_1, arg87_1, arg88_1, arg89_1, arg90_1, arg91_1, arg92_1, arg93_1, arg94_1, arg95_1, arg96_1, arg97_1, arg98_1, arg99_1, arg100_1, arg101_1, arg102_1, arg103_1, arg104_1, arg105_1, arg106_1, arg107_1, arg108_1, arg109_1, arg110_1, arg111_1, arg112_1, arg113_1, arg114_1, arg115_1, arg116_1, arg117_1, arg118_1, arg119_1, arg120_1, arg121_1, arg122_1, arg123_1, arg124_1, arg125_1, arg126_1, arg127_1, arg128_1, arg129_1, arg130_1, arg131_1, arg132_1, arg133_1, arg134_1, arg135_1, arg136_1, arg137_1, arg138_1, arg139_1, arg140_1, arg141_1, arg142_1, arg143_1, arg144_1, arg145_1, arg146_1, arg147_1, arg148_1, arg149_1, arg150_1, arg151_1, arg152_1, arg153_1, arg154_1, arg155_1, arg156_1, arg157_1, arg158_1, arg159_1, arg160_1, arg161_1, arg162_1, arg163_1, arg164_1, arg165_1, arg166_1, arg167_1, arg168_1, arg169_1, arg170_1, arg171_1, arg172_1, arg173_1, arg174_1, arg175_1, arg176_1, arg177_1, arg178_1, arg179_1, arg180_1, arg181_1, arg182_1, arg183_1, arg184_1, arg185_1, arg186_1, arg187_1, arg188_1, arg189_1, arg190_1, arg191_1, arg192_1, arg193_1, arg194_1, arg195_1, arg196_1, arg197_1, arg198_1, arg199_1, arg200_1, arg201_1, arg202_1, arg203_1, arg204_1, arg205_1, arg206_1, arg207_1, arg208_1, arg209_1, arg210_1, arg211_1, arg212_1, arg213_1, arg214_1, arg215_1, arg216_1, arg217_1, arg218_1, arg219_1, arg220_1, arg221_1, arg222_1, arg223_1, arg224_1, arg225_1, arg226_1, arg227_1, arg228_1, arg229_1, arg230_1, arg231_1, arg232_1, arg233_1, arg234_1, arg235_1, arg236_1, arg237_1, arg238_1, arg239_1, arg240_1, arg241_1, arg242_1, arg243_1, arg244_1, arg245_1, arg246_1, arg247_1, arg248_1, arg249_1, arg250_1, arg251_1, arg252_1, arg253_1, arg254_1, arg255_1, arg256_1, arg257_1, arg258_1, arg259_1, arg260_1, arg261_1, arg262_1, arg263_1, arg264_1, arg265_1, arg266_1, arg267_1, arg268_1, arg269_1, arg270_1, arg271_1, arg272_1, arg273_1, arg274_1, arg275_1, arg276_1, arg277_1, arg278_1, arg279_1, arg280_1, arg281_1, arg282_1, arg283_1, arg284_1, arg285_1, arg286_1, arg287_1, arg288_1, arg289_1, arg290_1, arg291_1, arg292_1, arg293_1, arg294_1, arg295_1, arg296_1, arg297_1, arg298_1, arg299_1, arg300_1, arg301_1, arg302_1, arg303_1, arg304_1, arg305_1, arg306_1, arg307_1, arg308_1, arg309_1, arg310_1, arg311_1, arg312_1, arg313_1, arg314_1, arg315_1, arg316_1, arg317_1, arg318_1, arg319_1, arg320_1, arg321_1, arg322_1, arg323_1, arg324_1, arg325_1, arg326_1, arg327_1, arg328_1, arg329_1, arg330_1, arg331_1, arg332_1, arg333_1, arg334_1, arg335_1, arg336_1, arg337_1, arg338_1, arg339_1, arg340_1, arg341_1, arg342_1, arg343_1, arg344_1, arg345_1, arg346_1, arg347_1, arg348_1, arg349_1, arg350_1, arg351_1, arg352_1, arg353_1, arg354_1, arg355_1, arg356_1, arg357_1, arg358_1, arg359_1, arg360_1, arg361_1, arg362_1, arg363_1, arg364_1])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
