# AOT ID: ['5_inference']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from cmath import nanj
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()
empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p


# kernel path: /tmp/torchinductor_root/b7/cb7n2thbjavy5lzryi2ewgxyrsjful76jtmjjvm26xo25yxpkqoi.py
# Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation], Original ATen: []
# Source node to ATen node mapping:
#   triton_kernel_wrapper_mutation => triton_kernel_wrapper_mutation_64
# Graph fragment:
#   %triton_kernel_wrapper_mutation_64 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 326, constant_args_idx: 325, grid: [(1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty, X_ptr: %view_4, W_ptr: %arg6_1, RSTD_ptr: %empty_1}})
triton_poi_fused_0 = async_compile.triton('triton_poi_fused_0', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*i64', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 1.6392e-05},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4096
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (0))
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK])
    tmp2 = tl.full([XBLOCK], 128256, tl.int32)
    tmp3 = tmp1 + tmp2
    tmp4 = tmp1 < 0
    tmp5 = tl.where(tmp4, tmp3, tmp1)
    tl.device_assert((0 <= tmp5) & (tmp5 < 128256), "index out of bounds: 0 <= tmp5 < 128256")
    tmp7 = tl.load(in_ptr1 + (x0 + 4096*tmp5), None).to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp7, None)


def get_args():
    arg_0 = rand_strided((1, 1), (1, 1), device='cuda:0', dtype=torch.int64)
    arg_1 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 4096,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_0.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_0.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.6392e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# Original path: /app/low_bit_inference/optims/rms_norm_kernels/liger_rms_norm.py:102
_rms_norm_forward_kernel_0 = async_compile.triton('_rms_norm_forward_kernel', '''

import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.user_autotune(
    configs=[{'num_warps': 8, 'num_stages': 3}],
    inductor_meta={'grid_type': 'FixedGrid', 'fixed_grid': ['_grid_0', '_grid_1', '_grid_2'], 'extra_launcher_args': ['_grid_0', '_grid_1', '_grid_2'], 'kernel_name': '_rms_norm_forward_kernel_0', 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False},
    triton_meta={'signature': {'Y_ptr': '*fp16', 'Y_row_stride': 'i32', 'X_ptr': '*fp16', 'X_row_stride': 'i32', 'W_ptr': '*fp16', 'W_row_stride': 'constexpr', 'RSTD_ptr': '*fp32', 'RSTD_row_stride': 'constexpr', 'n_cols': 'i32', 'eps': 'fp32', 'offset': 'fp32', 'casting_mode': 'constexpr', 'BLOCK_SIZE': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {'W_row_stride': 1, 'RSTD_row_stride': 1, 'casting_mode': 0, 'BLOCK_SIZE': 4096}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    filename=__file__,
    custom_kernel=True,
)
@triton.jit
def _rms_norm_forward_kernel(
    Y_ptr,
    Y_row_stride,
    X_ptr,
    X_row_stride,
    W_ptr,
    W_row_stride,
    RSTD_ptr,
    RSTD_row_stride,
    n_cols,
    eps,
    offset,
    casting_mode: tl.constexpr,  # constexpr so the `if` blocks can be optimized out
    BLOCK_SIZE: tl.constexpr,
):
    """
    y_i = (x_i / (RMS)) * (offset + wi), RMS = sqrt(sum(x_i^2) / N)

    Reference:
    1. https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html
    2. https://github.com/unslothai/unsloth/blob/fd753fed99ed5f10ef8a9b7139588d9de9ddecfb/unsloth/kernels/rms_layernorm.py#L22
    3. https://arxiv.org/pdf/1910.07467
    """

    row_idx = tl.program_id(0).to(tl.int64)
    col_offsets = tl.arange(0, BLOCK_SIZE)
    mask = col_offsets < n_cols

    Y_ptr += row_idx * Y_row_stride
    X_ptr += row_idx * X_row_stride
    RSTD_ptr += row_idx * RSTD_row_stride

    X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0)
    X_row_dtype = X_row.dtype
    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0)

    # On Llama, only rstd is computed on fp32
    if casting_mode == _CASTING_MODE_LLAMA:
        X_row = X_row.to(tl.float32)

    # Gemma computes everything on fp32, and then casts back the output to the original dtype
    if casting_mode == _CASTING_MODE_GEMMA:
        W_row = W_row.to(tl.float32)
        X_row = X_row.to(tl.float32)

    if casting_mode == _CASTING_MODE_NONE:
        eps = eps.to(X_row_dtype)
        offset = offset.to(X_row_dtype)

    mean_square = tl.sum(X_row * X_row, axis=0) / n_cols
    rstd = rsqrt(mean_square + eps)

    # We can save time by caching rms with minimal memory overhead
    # because rms is much smaller compared to X_row, as rms is for each row.
    # However, on the computation side, it can save 4 operations (*, sum, /, sqrt).
    tl.store(RSTD_ptr, rstd)

    X_row = X_row * rstd

    # On Llama, the multiplication with the weight is done on the original dtype
    if casting_mode == _CASTING_MODE_LLAMA:
        X_row = X_row.to(X_row_dtype)

    Y_row = X_row * (offset + W_row)

    if casting_mode == _CASTING_MODE_GEMMA:
        Y_row = Y_row.to(X_row_dtype)

    tl.store(Y_ptr + col_offsets, Y_row, mask=mask)

_CASTING_MODE_LLAMA: triton.language.core.constexpr = tl.constexpr(0)

_CASTING_MODE_GEMMA: triton.language.core.constexpr = tl.constexpr(1)

_CASTING_MODE_NONE: triton.language.core.constexpr = tl.constexpr(-1)
from triton.language.extra.libdevice import rsqrt as rsqrt
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/2u/c2utyfviglrb7q53dfjxp65tgdmdomouoghkd7rt6pwu2fxyfogc.py
# Topologically Sorted Source Nodes: [linear], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   linear => mul_3, sum_2
# Graph fragment:
#   %mul_3 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%unsqueeze_6, %unsqueeze_7), kwargs = {})
#   %sum_2 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_3, [1]), kwargs = {})
triton_red_fused_mm_1 = async_compile.triton('triton_red_fused_mm_1', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 4096, 'r0_': 4096},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mm_1', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.033570816}
)
@triton.jit
def triton_red_fused_mm_1(in_ptr0, in_ptr1, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 4096
    r0_numel = 4096
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp6 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (r0_1), r0_mask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tl.load(in_ptr1 + (r0_1 + 4096*x0), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tmp1 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, R0_BLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(r0_mask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp6, None)


def get_args():
    arg_0 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, 4096, 4096,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mm_1.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.033570816
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/ea/ceafi7rknjpagaauta6zd7mnzqz63ewiozduz52i3wq4qxkrxfi2.py
# Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed], Original ATen: [aten.mul, aten.cat, aten.add]
# Source node to ATen node mapping:
#   cat_1 => cat
#   mul_2 => mul_6
#   mul_3 => mul_7
#   q_embed => add
# Graph fragment:
#   %mul_6 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_2, %unsqueeze_12), kwargs = {})
#   %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg, %slice_4], -1), kwargs = {})
#   %mul_7 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat, %unsqueeze_13), kwargs = {})
#   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_6, %mul_7), kwargs = {})
triton_poi_fused_add_cat_mul_2 = async_compile.triton('triton_poi_fused_add_cat_mul_2', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*i64', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_cat_mul_2', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 2.484e-05},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_cat_mul_2(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4096
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 128)
    x1 = xindex // 128
    tmp0 = tl.load(in_ptr0 + (x2), None)
    tmp2 = tl.load(in_ptr1 + ((x2 % 64)), None, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr2 + (0))
    tmp4 = tl.broadcast_to(tmp3, [XBLOCK])
    tmp1 = tmp0.to(tl.float32)
    tmp5 = tmp4.to(tl.float32)
    tmp6 = tmp2 * tmp5
    tmp7 = tl_math.cos(tmp6)
    tmp8 = 1.0
    tmp9 = tmp7 * tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp1 * tmp10
    tmp12 = x0
    tmp13 = tl.full([1], 0, tl.int64)
    tmp14 = tmp12 >= tmp13
    tmp15 = tl.full([1], 64, tl.int64)
    tmp16 = tmp12 < tmp15
    tmp17 = tl.load(in_ptr0 + (64 + 128*x1 + (x0)), tmp16, eviction_policy='evict_last', other=0.0)
    tmp18 = tmp17.to(tl.float32)
    tmp19 = -tmp18
    tmp20 = tl.full(tmp19.shape, 0.0, tmp19.dtype)
    tmp21 = tl.where(tmp16, tmp19, tmp20)
    tmp22 = tmp12 >= tmp15
    tmp23 = tl.full([1], 128, tl.int64)
    tmp24 = tmp12 < tmp23
    tmp25 = tl.load(in_ptr0 + (128*x1 + ((-64) + x0)), tmp22, eviction_policy='evict_last', other=0.0)
    tmp26 = tmp25.to(tl.float32)
    tmp27 = tl.full(tmp26.shape, 0.0, tmp26.dtype)
    tmp28 = tl.where(tmp22, tmp26, tmp27)
    tmp29 = tl.where(tmp16, tmp21, tmp28)
    tmp30 = tl_math.sin(tmp6)
    tmp31 = tmp30 * tmp8
    tmp32 = tmp31.to(tl.float32)
    tmp33 = tmp29 * tmp32
    tmp34 = tmp11 + tmp33
    tl.store(out_ptr0 + (x2), tmp34, None)


def get_args():
    arg_0 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((64,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 1), (1, 1), device='cuda:0', dtype=torch.int64)
    arg_3 = rand_strided((1, 32, 1, 128), (4096, 128, 4096, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 4096,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_cat_mul_2.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.484e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/6c/c6cwbqvz7olqwz3nopxuspezste3sngyigodxwgj3vg2unn3iuem.py
# Topologically Sorted Source Nodes: [linear_1, linear_2, index_copy__1], Original ATen: [aten.mm, aten.index_copy]
# Source node to ATen node mapping:
#   index_copy__1 => index_put_1
#   linear_1 => mul_4, sum_3
#   linear_2 => mul_5, sum_4
# Graph fragment:
#   %mul_4 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%unsqueeze_8, %unsqueeze_9), kwargs = {})
#   %sum_3 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_4, [1]), kwargs = {})
#   %mul_5 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%unsqueeze_10, %unsqueeze_11), kwargs = {})
#   %sum_4 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_5, [1]), kwargs = {})
#   %index_put_1 : [num_users=1] = call_function[target=torch.ops.aten.index_put_.default](args = (%arg11_1, [None, None, %arg2_1], %permute_6), kwargs = {})
triton_red_fused_index_copy_mm_3 = async_compile.triton('triton_red_fused_index_copy_mm_3', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 1024, 'r0_': 4096},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*i64', 'out_ptr0': '*fp32', 'out_ptr2': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_index_copy_mm_3', 'mutated_arg_names': ['out_ptr2'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 2, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.020975624}
)
@triton.jit
def triton_red_fused_index_copy_mm_3(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 1024
    r0_numel = 4096
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp6 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    _tmp12 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (r0_1), r0_mask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tl.load(in_ptr1 + (r0_1 + 4096*x0), xmask & r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp8 = tl.load(in_ptr2 + (r0_1 + 4096*x0), xmask & r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tmp1 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, R0_BLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(r0_mask & xmask, tmp7, _tmp6)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp1 * tmp9
        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, R0_BLOCK])
        tmp13 = _tmp12 + tmp11
        _tmp12 = tl.where(r0_mask & xmask, tmp13, _tmp12)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tmp12 = tl.sum(_tmp12, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp6, xmask)
    x2 = (xindex % 128)
    x3 = xindex // 128
    tmp14 = tl.load(in_ptr3 + (0))
    tmp15 = tl.broadcast_to(tmp14, [XBLOCK, 1])
    tmp16 = tl.full([XBLOCK, 1], 2048, tl.int32)
    tmp17 = tmp15 + tmp16
    tmp18 = tmp15 < 0
    tmp19 = tl.where(tmp18, tmp17, tmp15)
    tl.device_assert((0 <= tmp19) & (tmp19 < 2048), "index out of bounds: 0 <= tmp19 < 2048")
    tmp21 = tmp12.to(tl.float32)
    tl.store(out_ptr2 + (x2 + 128*tmp19 + 262144*x3), tmp21, xmask)


def get_args():
    arg_0 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1,), (1,), device='cuda:0', dtype=torch.int64)
    arg_4 = rand_strided((1, 1024), (1024, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 1024, 4096,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_index_copy_mm_3.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.020975624
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/gt/cgt5ozjzydistpdytukvwkgjjaiobigzmm4k5o45i4qqoc6u6dto.py
# Topologically Sorted Source Nodes: [mul_4, cat_2, mul_5, k_embed, index_copy_], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
# Source node to ATen node mapping:
#   cat_2 => cat_1
#   index_copy_ => index_put
#   k_embed => add_1
#   mul_4 => mul_8
#   mul_5 => mul_9
# Graph fragment:
#   %mul_8 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute_4, %unsqueeze_12), kwargs = {})
#   %cat_1 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%neg_1, %slice_6], -1), kwargs = {})
#   %mul_9 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_1, %unsqueeze_13), kwargs = {})
#   %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_8, %mul_9), kwargs = {})
#   %index_put : [num_users=1] = call_function[target=torch.ops.aten.index_put_.default](args = (%arg10_1, [None, None, %arg2_1], %add_1), kwargs = {})
triton_poi_fused_add_cat_index_copy_mul_4 = async_compile.triton('triton_poi_fused_add_cat_index_copy_mul_4', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 1024}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*i64', 'in_ptr3': '*i64', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_cat_index_copy_mul_4', 'mutated_arg_names': ['out_ptr1'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 6.416e-06},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_cat_index_copy_mul_4(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr1, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 128)
    x1 = xindex // 128
    tmp0 = tl.load(in_ptr0 + (x2), xmask)
    tmp2 = tl.load(in_ptr1 + ((x2 % 64)), xmask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr2 + (0))
    tmp4 = tl.broadcast_to(tmp3, [XBLOCK])
    tmp35 = tl.load(in_ptr3 + (0))
    tmp36 = tl.broadcast_to(tmp35, [XBLOCK])
    tmp1 = tmp0.to(tl.float32)
    tmp5 = tmp4.to(tl.float32)
    tmp6 = tmp2 * tmp5
    tmp7 = tl_math.cos(tmp6)
    tmp8 = 1.0
    tmp9 = tmp7 * tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp1 * tmp10
    tmp12 = x0
    tmp13 = tl.full([1], 0, tl.int64)
    tmp14 = tmp12 >= tmp13
    tmp15 = tl.full([1], 64, tl.int64)
    tmp16 = tmp12 < tmp15
    tmp17 = tl.load(in_ptr0 + (64 + 128*x1 + (x0)), xmask & tmp16, eviction_policy='evict_last', other=0.0)
    tmp18 = tmp17.to(tl.float32)
    tmp19 = -tmp18
    tmp20 = tl.full(tmp19.shape, 0.0, tmp19.dtype)
    tmp21 = tl.where(tmp16, tmp19, tmp20)
    tmp22 = tmp12 >= tmp15
    tmp23 = tl.full([1], 128, tl.int64)
    tmp24 = tmp12 < tmp23
    tmp25 = tl.load(in_ptr0 + (128*x1 + ((-64) + x0)), xmask & tmp22, eviction_policy='evict_last', other=0.0)
    tmp26 = tmp25.to(tl.float32)
    tmp27 = tl.full(tmp26.shape, 0.0, tmp26.dtype)
    tmp28 = tl.where(tmp22, tmp26, tmp27)
    tmp29 = tl.where(tmp16, tmp21, tmp28)
    tmp30 = tl_math.sin(tmp6)
    tmp31 = tmp30 * tmp8
    tmp32 = tmp31.to(tl.float32)
    tmp33 = tmp29 * tmp32
    tmp34 = tmp11 + tmp33
    tmp37 = tl.full([XBLOCK], 2048, tl.int32)
    tmp38 = tmp36 + tmp37
    tmp39 = tmp36 < 0
    tmp40 = tl.where(tmp39, tmp38, tmp36)
    tl.device_assert((0 <= tmp40) & (tmp40 < 2048), "index out of bounds: 0 <= tmp40 < 2048")
    tl.store(out_ptr1 + (x0 + 128*tmp40 + 262144*x1), tmp34, xmask)


def get_args():
    arg_0 = rand_strided((1, 1024), (1024, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((64,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 1), (1, 1), device='cuda:0', dtype=torch.int64)
    arg_3 = rand_strided((1,), (1,), device='cuda:0', dtype=torch.int64)
    arg_4 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 1024,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_cat_index_copy_mul_4.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 6.416e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/33/c33mm5f5xxb6yz5upkoihyltiuexli7b7kfppm4cjpm5b5gd3nlb.py
# Topologically Sorted Source Nodes: [attn_output], Original ATen: [aten._scaled_dot_product_efficient_attention]
# Source node to ATen node mapping:
#   attn_output => _scaled_dot_product_efficient_attention
# Graph fragment:
#   %_scaled_dot_product_efficient_attention : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%add, %view_21, %view_22, %expand_8, False), kwargs = {scale: 0.08838834764831845})
triton_poi_fused__scaled_dot_product_efficient_attention_5 = async_compile.triton('triton_poi_fused__scaled_dot_product_efficient_attention_5', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__scaled_dot_product_efficient_attention_5', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.016777216},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__scaled_dot_product_efficient_attention_5(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 8388608
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 262144)
    x1 = xindex // 262144
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 262144*(x1 // 4)), None).to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp0, None)


def get_args():
    arg_0 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 32, 2048, 128), (8388608, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 8388608,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__scaled_dot_product_efficient_attention_5.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.016777216
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/mj/cmjpcc3smrsgp2hyadzvdskf4qhmomohj77tknfcrncp76ae7r3l.py
# Topologically Sorted Source Nodes: [attn_output, attn_output_4, attn_output_8], Original ATen: [aten._scaled_dot_product_efficient_attention]
# Source node to ATen node mapping:
#   attn_output => _scaled_dot_product_efficient_attention
#   attn_output_4 => _scaled_dot_product_efficient_attention_1
#   attn_output_8 => _scaled_dot_product_efficient_attention_2
# Graph fragment:
#   %_scaled_dot_product_efficient_attention : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%add, %view_21, %view_22, %expand_8, False), kwargs = {scale: 0.08838834764831845})
#   %_scaled_dot_product_efficient_attention_1 : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%add_4, %view_55, %view_56, %expand_13, False), kwargs = {scale: 0.08838834764831845})
#   %_scaled_dot_product_efficient_attention_2 : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%add_8, %view_89, %view_90, %expand_18, False), kwargs = {scale: 0.08838834764831845})
triton_poi_fused__scaled_dot_product_efficient_attention_6 = async_compile.triton('triton_poi_fused__scaled_dot_product_efficient_attention_6', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*i1', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'out_ptr2': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__scaled_dot_product_efficient_attention_6', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 1.4336e-05},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__scaled_dot_product_efficient_attention_6(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2048
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.int1)
    tmp1 = 0.0
    tmp2 = float("-inf")
    tmp3 = tl.where(tmp0, tmp1, tmp2)
    tl.store(out_ptr0 + (x0), tmp3, xmask)
    tl.store(out_ptr1 + (x0), tmp3, xmask)
    tl.store(out_ptr2 + (x0), tmp3, xmask)


def get_args():
    arg_0 = rand_strided((1, 1, 1, 2048), (2048, 2048, 2048, 1), device='cuda:0', dtype=torch.bool)
    arg_1 = rand_strided((1, 1, 1, 2048), (2048, 0, 2048, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 1, 1, 2048), (2048, 0, 2048, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 1, 1, 2048), (2048, 0, 2048, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 2048,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_6.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__scaled_dot_product_efficient_attention_6.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.4336e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/vk/cvkygytwff2poiqs46lbrsqa76vlgqe6nspt22lggbhrxtya5ssn.py
# Topologically Sorted Source Nodes: [attn_output_3, triton_kernel_wrapper_mutation_1], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   attn_output_3 => mul_10, sum_5
#   triton_kernel_wrapper_mutation_1 => triton_kernel_wrapper_mutation_63
# Graph fragment:
#   %mul_10 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%unsqueeze_18, %unsqueeze_19), kwargs = {})
#   %sum_5 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_10, [1]), kwargs = {})
#   %triton_kernel_wrapper_mutation_63 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 327, constant_args_idx: 326, grid: [(1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_2, X_ptr: %view_26, W_ptr: %arg13_1, RSTD_ptr: %empty_3}})
triton_red_fused_mm_7 = async_compile.triton('triton_red_fused_mm_7', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 4096, 'r0_': 4096},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*i64', 'in_ptr3': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mm_7', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 1, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.06714164}
)
@triton.jit
def triton_red_fused_mm_7(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 4096
    r0_numel = 4096
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp6 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (r0_1), r0_mask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tl.load(in_ptr1 + (r0_1 + 4096*x0), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tmp1 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, R0_BLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(r0_mask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp6, None)
    tmp8 = tl.load(in_ptr2 + (0))
    tmp9 = tl.broadcast_to(tmp8, [XBLOCK, 1])
    tmp10 = tl.full([XBLOCK, 1], 128256, tl.int32)
    tmp11 = tmp9 + tmp10
    tmp12 = tmp9 < 0
    tmp13 = tl.where(tmp12, tmp11, tmp9)
    tl.device_assert((0 <= tmp13) & (tmp13 < 128256), "index out of bounds: 0 <= tmp13 < 128256")
    tmp15 = tl.load(in_ptr3 + (x0 + 4096*tmp13), None).to(tl.float32)
    tmp16 = tmp6.to(tl.float32)
    tmp17 = tmp15 + tmp16
    tl.store(out_ptr1 + (x0), tmp17, None)


def get_args():
    arg_0 = rand_strided((1, 32, 1, 128), (4096, 128, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 1), (1, 1), device='cuda:0', dtype=torch.int64)
    arg_3 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 4096, 4096,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_7.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mm_7.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.06714164
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/px/cpxnqg6aidgkkhskur27ze44ckbkjo42kr3temdckjiml3izgwi6.py
# Topologically Sorted Source Nodes: [linear_4], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   linear_4 => mul_11, sum_6
# Graph fragment:
#   %mul_11 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%unsqueeze_20, %unsqueeze_21), kwargs = {})
#   %sum_6 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_11, [1]), kwargs = {})
triton_red_fused_mm_8 = async_compile.triton('triton_red_fused_mm_8', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 16384, 'r0_': 4096},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mm_8', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.117497856}
)
@triton.jit
def triton_red_fused_mm_8(in_ptr0, in_ptr1, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 14336
    r0_numel = 4096
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp6 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (r0_1), r0_mask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tl.load(in_ptr1 + (r0_1 + 4096*x0), xmask & r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tmp1 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, R0_BLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(r0_mask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp6, xmask)


def get_args():
    arg_0 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 14336), (14336, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, 14336, 4096,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mm_8.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.117497856
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/3e/c3eupw2uasc22zyqx27cde5xyluwuqcbqouez5uqppxo5ocgizjo.py
# Topologically Sorted Source Nodes: [down_proj, triton_kernel_wrapper_mutation_2], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   down_proj => mul_15, sum_8
#   triton_kernel_wrapper_mutation_2 => triton_kernel_wrapper_mutation_62
# Graph fragment:
#   %mul_15 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%unsqueeze_24, %unsqueeze_25), kwargs = {})
#   %sum_8 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_15, [1]), kwargs = {})
#   %triton_kernel_wrapper_mutation_62 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 328, constant_args_idx: 327, grid: [(1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_4, X_ptr: %view_38, W_ptr: %arg17_1, RSTD_ptr: %empty_5}})
triton_red_fused_mm_9 = async_compile.triton('triton_red_fused_mm_9', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 4096, 'r0_': 16384},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp16', 'in_ptr3': '*i64', 'in_ptr4': '*fp16', 'in_ptr5': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mm_9', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 1, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.23503668}
)
@triton.jit
def triton_red_fused_mm_9(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 4096
    r0_numel = 14336
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp14 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (r0_1), r0_mask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr1 + (r0_1), r0_mask, eviction_policy='evict_last', other=0.0)
        tmp10 = tl.load(in_ptr2 + (r0_1 + 14336*x0), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tmp1.to(tl.float32)
        tmp3 = tl.sigmoid(tmp2)
        tmp4 = tmp2 * tmp3
        tmp5 = tmp4.to(tl.float32)
        tmp7 = tmp6.to(tl.float32)
        tmp8 = tmp5 * tmp7
        tmp9 = tmp8.to(tl.float32)
        tmp11 = tmp10.to(tl.float32)
        tmp12 = tmp9 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [XBLOCK, R0_BLOCK])
        tmp15 = _tmp14 + tmp13
        _tmp14 = tl.where(r0_mask, tmp15, _tmp14)
    tmp14 = tl.sum(_tmp14, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp14, None)
    tmp16 = tl.load(in_ptr3 + (0))
    tmp17 = tl.broadcast_to(tmp16, [XBLOCK, 1])
    tmp24 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp18 = tl.full([XBLOCK, 1], 128256, tl.int32)
    tmp19 = tmp17 + tmp18
    tmp20 = tmp17 < 0
    tmp21 = tl.where(tmp20, tmp19, tmp17)
    tl.device_assert((0 <= tmp21) & (tmp21 < 128256), "index out of bounds: 0 <= tmp21 < 128256")
    tmp23 = tl.load(in_ptr4 + (x0 + 4096*tmp21), None).to(tl.float32)
    tmp25 = tmp24.to(tl.float32)
    tmp26 = tmp23 + tmp25
    tmp27 = tmp14.to(tl.float32)
    tmp28 = tmp26 + tmp27
    tl.store(out_ptr1 + (x0), tmp28, None)


def get_args():
    arg_0 = rand_strided((1, 14336), (14336, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 14336), (14336, 1), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 1), (1, 1), device='cuda:0', dtype=torch.int64)
    arg_4 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 4096, 14336,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_9.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mm_9.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.23503668
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/e7/ce7qngssxhrsixwex5cxtv6n6p6efn534myd7krt5xtc3kio4jza.py
# Topologically Sorted Source Nodes: [inputs_embeds, hidden_states_2, hidden_states_3, attn_output_7, hidden_states_6], Original ATen: [aten.embedding, aten.add, aten.mm]
# Source node to ATen node mapping:
#   attn_output_7 => mul_23, sum_12
#   hidden_states_2 => add_2
#   hidden_states_3 => add_3
#   hidden_states_6 => add_6
#   inputs_embeds => embedding
# Graph fragment:
#   %embedding : [num_users=2] = call_function[target=torch.ops.aten.embedding.default](args = (%arg1_1, %arg0_1, 128004), kwargs = {})
#   %add_2 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%embedding, %view_25), kwargs = {})
#   %add_3 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_2, %view_37), kwargs = {})
#   %mul_23 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%unsqueeze_38, %unsqueeze_39), kwargs = {})
#   %sum_12 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_23, [1]), kwargs = {})
#   %add_6 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_3, %view_59), kwargs = {})
triton_red_fused_add_embedding_mm_10 = async_compile.triton('triton_red_fused_add_embedding_mm_10', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 4096, 'r0_': 4096},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*i64', 'in_ptr3': '*fp16', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_embedding_mm_10', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 1, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.067158024}
)
@triton.jit
def triton_red_fused_add_embedding_mm_10(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 4096
    r0_numel = 4096
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp6 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (r0_1), r0_mask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tl.load(in_ptr1 + (r0_1 + 4096*x0), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tmp1 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, R0_BLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(r0_mask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tmp8 = tl.load(in_ptr2 + (0))
    tmp9 = tl.broadcast_to(tmp8, [XBLOCK, 1])
    tmp16 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp19 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp10 = tl.full([XBLOCK, 1], 128256, tl.int32)
    tmp11 = tmp9 + tmp10
    tmp12 = tmp9 < 0
    tmp13 = tl.where(tmp12, tmp11, tmp9)
    tl.device_assert((0 <= tmp13) & (tmp13 < 128256), "index out of bounds: 0 <= tmp13 < 128256")
    tmp15 = tl.load(in_ptr3 + (x0 + 4096*tmp13), None).to(tl.float32)
    tmp17 = tmp16.to(tl.float32)
    tmp18 = tmp15 + tmp17
    tmp20 = tmp19.to(tl.float32)
    tmp21 = tmp18 + tmp20
    tmp22 = tmp6.to(tl.float32)
    tmp23 = tmp21 + tmp22
    tl.store(out_ptr1 + (x0), tmp23, None)


def get_args():
    arg_0 = rand_strided((1, 32, 1, 128), (4096, 128, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 1), (1, 1), device='cuda:0', dtype=torch.int64)
    arg_3 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 1, 4096), (4096, 4096, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 4096, 4096,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_add_embedding_mm_10.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_add_embedding_mm_10.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.067158024
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/pr/cprfv66tfbms3d5l63cpdilheueilewaanfaeeooraedh3rnyhug.py
# Topologically Sorted Source Nodes: [down_proj_1, triton_kernel_wrapper_mutation_4], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   down_proj_1 => mul_28, sum_15
#   triton_kernel_wrapper_mutation_4 => triton_kernel_wrapper_mutation_60
# Graph fragment:
#   %mul_28 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%unsqueeze_44, %unsqueeze_45), kwargs = {})
#   %sum_15 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_28, [1]), kwargs = {})
#   %triton_kernel_wrapper_mutation_60 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 330, constant_args_idx: 329, grid: [(1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_8, X_ptr: %view_72, W_ptr: %arg28_1, RSTD_ptr: %empty_9}})
triton_red_fused_mm_11 = async_compile.triton('triton_red_fused_mm_11', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 4096, 'r0_': 16384},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mm_11', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 1, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.117587968}
)
@triton.jit
def triton_red_fused_mm_11(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 4096
    r0_numel = 14336
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp14 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (r0_1), r0_mask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr1 + (r0_1), r0_mask, eviction_policy='evict_last', other=0.0)
        tmp10 = tl.load(in_ptr2 + (r0_1 + 14336*x0), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tmp1.to(tl.float32)
        tmp3 = tl.sigmoid(tmp2)
        tmp4 = tmp2 * tmp3
        tmp5 = tmp4.to(tl.float32)
        tmp7 = tmp6.to(tl.float32)
        tmp8 = tmp5 * tmp7
        tmp9 = tmp8.to(tl.float32)
        tmp11 = tmp10.to(tl.float32)
        tmp12 = tmp9 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [XBLOCK, R0_BLOCK])
        tmp15 = _tmp14 + tmp13
        _tmp14 = tl.where(r0_mask, tmp15, _tmp14)
    tmp14 = tl.sum(_tmp14, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp14, None)
    tmp16 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last').to(tl.float32)
    tmp17 = tmp14.to(tl.float32)
    tmp18 = tmp16 + tmp17
    tl.store(out_ptr1 + (x0), tmp18, None)


def get_args():
    arg_0 = rand_strided((1, 14336), (14336, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 14336), (14336, 1), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 1, 4096), (4096, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 4096, 14336,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_11.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mm_11.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.117587968
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/s7/cs74xsaya5l6mktvdofyxt7ypqu2i7zm3dcudc2aamf2zpyrwmn4.py
# Topologically Sorted Source Nodes: [attn_output_11, triton_kernel_wrapper_mutation_5], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   attn_output_11 => mul_36, sum_19
#   triton_kernel_wrapper_mutation_5 => triton_kernel_wrapper_mutation_59
# Graph fragment:
#   %mul_36 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%unsqueeze_58, %unsqueeze_59), kwargs = {})
#   %sum_19 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_36, [1]), kwargs = {})
#   %triton_kernel_wrapper_mutation_59 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 331, constant_args_idx: 330, grid: [(1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_10, X_ptr: %view_94, W_ptr: %arg35_1, RSTD_ptr: %empty_11}})
triton_red_fused_mm_12 = async_compile.triton('triton_red_fused_mm_12', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 4096, 'r0_': 4096},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mm_12', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 1, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.033611776}
)
@triton.jit
def triton_red_fused_mm_12(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 4096
    r0_numel = 4096
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp6 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (r0_1), r0_mask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tl.load(in_ptr1 + (r0_1 + 4096*x0), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tmp1 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, R0_BLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(r0_mask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp6, None)
    tmp8 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last').to(tl.float32)
    tmp9 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp8 + tmp10
    tmp12 = tmp6.to(tl.float32)
    tmp13 = tmp11 + tmp12
    tl.store(out_ptr1 + (x0), tmp13, None)


def get_args():
    arg_0 = rand_strided((1, 32, 1, 128), (4096, 128, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 1, 4096), (4096, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 4096, 4096,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_12.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mm_12.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.033611776
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/ec/cecdt2spjm5q654j7zzcwaoikuyuyeypsm476htgudb4lpqrk3nf.py
# Topologically Sorted Source Nodes: [down_proj_2, triton_kernel_wrapper_mutation_6], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   down_proj_2 => mul_41, sum_22
#   triton_kernel_wrapper_mutation_6 => triton_kernel_wrapper_mutation_58
# Graph fragment:
#   %mul_41 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%unsqueeze_64, %unsqueeze_65), kwargs = {})
#   %sum_22 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_41, [1]), kwargs = {})
#   %triton_kernel_wrapper_mutation_58 : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 332, constant_args_idx: 331, grid: [(1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_12, X_ptr: %view_106, W_ptr: %arg39_1, RSTD_ptr: %empty_13}})
triton_red_fused_mm_13 = async_compile.triton('triton_red_fused_mm_13', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 4096, 'r0_': 16384},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mm_13', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 1, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.117620736}
)
@triton.jit
def triton_red_fused_mm_13(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 4096
    r0_numel = 14336
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp14 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (r0_1), r0_mask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr1 + (r0_1), r0_mask, eviction_policy='evict_last', other=0.0)
        tmp10 = tl.load(in_ptr2 + (r0_1 + 14336*x0), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tmp1.to(tl.float32)
        tmp3 = tl.sigmoid(tmp2)
        tmp4 = tmp2 * tmp3
        tmp5 = tmp4.to(tl.float32)
        tmp7 = tmp6.to(tl.float32)
        tmp8 = tmp5 * tmp7
        tmp9 = tmp8.to(tl.float32)
        tmp11 = tmp10.to(tl.float32)
        tmp12 = tmp9 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [XBLOCK, R0_BLOCK])
        tmp15 = _tmp14 + tmp13
        _tmp14 = tl.where(r0_mask, tmp15, _tmp14)
    tmp14 = tl.sum(_tmp14, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp14, None)
    tmp16 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last').to(tl.float32)
    tmp17 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp18 = tmp17.to(tl.float32)
    tmp19 = tmp16 + tmp18
    tmp21 = tmp20.to(tl.float32)
    tmp22 = tmp19 + tmp21
    tmp23 = tmp14.to(tl.float32)
    tmp24 = tmp22 + tmp23
    tl.store(out_ptr1 + (x0), tmp24, None)


def get_args():
    arg_0 = rand_strided((1, 14336), (14336, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 14336), (14336, 1), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 1, 4096), (4096, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 4096, 14336,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_13.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mm_13.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.117620736
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/4j/c4juq4lf3rh3hv5hf2mn5xaqb7dsmbllydoo4xhjalqattigfhet.py
# Topologically Sorted Source Nodes: [hidden_states_7, hidden_states_10, hidden_states_11, attn_output_15, hidden_states_14], Original ATen: [aten.add, aten.mm]
# Source node to ATen node mapping:
#   attn_output_15 => mul_49, sum_26
#   hidden_states_10 => add_10
#   hidden_states_11 => add_11
#   hidden_states_14 => add_14
#   hidden_states_7 => add_7
# Graph fragment:
#   %add_7 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_6, %view_71), kwargs = {})
#   %add_10 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_7, %view_93), kwargs = {})
#   %add_11 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_10, %view_105), kwargs = {})
#   %mul_49 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%unsqueeze_78, %unsqueeze_79), kwargs = {})
#   %sum_26 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_49, [1]), kwargs = {})
#   %add_14 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_11, %view_127), kwargs = {})
triton_red_fused_add_mm_14 = async_compile.triton('triton_red_fused_add_mm_14', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 4096, 'r0_': 4096},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_mm_14', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 1, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.03362816}
)
@triton.jit
def triton_red_fused_add_mm_14(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 4096
    r0_numel = 4096
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp6 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (r0_1), r0_mask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tl.load(in_ptr1 + (r0_1 + 4096*x0), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tmp1 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, R0_BLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(r0_mask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tmp8 = tl.load(in_out_ptr0 + (x0), None, eviction_policy='evict_last').to(tl.float32)
    tmp9 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp8 + tmp10
    tmp13 = tmp12.to(tl.float32)
    tmp14 = tmp11 + tmp13
    tmp16 = tmp15.to(tl.float32)
    tmp17 = tmp14 + tmp16
    tmp18 = tmp6.to(tl.float32)
    tmp19 = tmp17 + tmp18
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x0), tmp19, None)


def get_args():
    arg_0 = rand_strided((1, 1, 4096), (4096, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 32, 1, 128), (4096, 128, 4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 4096, 4096,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_add_mm_14.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_add_mm_14.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.03362816
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/ci/cciadpcgkzspz2vldaren2twmct2pkpjxhdfpce4bhachnbjoblw.py
# Topologically Sorted Source Nodes: [attn_output_120, attn_output_124], Original ATen: [aten._scaled_dot_product_efficient_attention]
# Source node to ATen node mapping:
#   attn_output_120 => _scaled_dot_product_efficient_attention_30
#   attn_output_124 => _scaled_dot_product_efficient_attention_31
# Graph fragment:
#   %_scaled_dot_product_efficient_attention_30 : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%add_120, %view_1041, %view_1042, %expand_158, False), kwargs = {scale: 0.08838834764831845})
#   %_scaled_dot_product_efficient_attention_31 : [num_users=1] = call_function[target=torch.ops.aten._scaled_dot_product_efficient_attention.default](args = (%add_124, %view_1075, %view_1076, %expand_163, False), kwargs = {scale: 0.08838834764831845})
triton_poi_fused__scaled_dot_product_efficient_attention_15 = async_compile.triton('triton_poi_fused__scaled_dot_product_efficient_attention_15', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*i1', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__scaled_dot_product_efficient_attention_15', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 1.024e-05},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__scaled_dot_product_efficient_attention_15(in_ptr0, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2048
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.int1)
    tmp1 = 0.0
    tmp2 = float("-inf")
    tmp3 = tl.where(tmp0, tmp1, tmp2)
    tl.store(out_ptr0 + (x0), tmp3, xmask)
    tl.store(out_ptr1 + (x0), tmp3, xmask)


def get_args():
    arg_0 = rand_strided((1, 1, 1, 2048), (2048, 2048, 2048, 1), device='cuda:0', dtype=torch.bool)
    arg_1 = rand_strided((1, 1, 1, 2048), (2048, 0, 2048, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 1, 1, 2048), (2048, 0, 2048, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 2048,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_15.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__scaled_dot_product_efficient_attention_15.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.024e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/ex/cexpyuz2zbut34vmtgu7axfe6sql475a2th6tt352zwufibpvsr5.py
# Topologically Sorted Source Nodes: [down_proj_31, triton_kernel_wrapper_mutation_64], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   down_proj_31 => mul_418, sum_225
#   triton_kernel_wrapper_mutation_64 => triton_kernel_wrapper_mutation
# Graph fragment:
#   %mul_418 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%unsqueeze_644, %unsqueeze_645), kwargs = {})
#   %sum_225 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_418, [1]), kwargs = {})
#   %triton_kernel_wrapper_mutation : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 390, constant_args_idx: 389, grid: [(1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {Y_ptr: %empty_128, X_ptr: %view_1092, W_ptr: %arg358_1, RSTD_ptr: %empty_129}})
triton_red_fused_mm_16 = async_compile.triton('triton_red_fused_mm_16', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 4096, 'r0_': 16384},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mm_16', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 1, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 0.117571584}
)
@triton.jit
def triton_red_fused_mm_16(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 4096
    r0_numel = 14336
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp14 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (r0_1), r0_mask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr1 + (r0_1), r0_mask, eviction_policy='evict_last', other=0.0)
        tmp10 = tl.load(in_ptr2 + (r0_1 + 14336*x0), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tmp1.to(tl.float32)
        tmp3 = tl.sigmoid(tmp2)
        tmp4 = tmp2 * tmp3
        tmp5 = tmp4.to(tl.float32)
        tmp7 = tmp6.to(tl.float32)
        tmp8 = tmp5 * tmp7
        tmp9 = tmp8.to(tl.float32)
        tmp11 = tmp10.to(tl.float32)
        tmp12 = tmp9 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [XBLOCK, R0_BLOCK])
        tmp15 = _tmp14 + tmp13
        _tmp14 = tl.where(r0_mask, tmp15, _tmp14)
    tmp14 = tl.sum(_tmp14, 1)[:, None]
    tmp16 = tl.load(in_out_ptr0 + (x0), None, eviction_policy='evict_last').to(tl.float32)
    tmp17 = tmp14.to(tl.float32)
    tmp18 = tmp16 + tmp17
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x0), tmp18, None)


def get_args():
    arg_0 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 14336), (14336, 1), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 14336), (14336, 1), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 4096, 14336,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_16.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mm_16.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.117571584
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_root/ti/cti4ooukwhnjwjyrgro6i6dvgykqtcw7aqlfiem34hncmha7yuah.py
# Topologically Sorted Source Nodes: [logits], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   logits => convert_element_type_741, mul_419, sum_226
# Graph fragment:
#   %mul_419 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%unsqueeze_646, %unsqueeze_647), kwargs = {})
#   %sum_226 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_419, [1]), kwargs = {})
#   %convert_element_type_741 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_226, torch.float16), kwargs = {})
triton_red_fused_mm_17 = async_compile.triton('triton_red_fused_mm_17', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 4096},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=84, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mm_17', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'D9EAAD375A358F762F8856BDA56233E665DBC92995A007C016E1AC315333760D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'coordinate_descent_tuning': True, 'coordinate_descent_search_radius': 1, 'coordinate_descent_check_all_directions': False, 'kernel_num_gb': 1.050929664}
)
@triton.jit
def triton_red_fused_mm_17(in_ptr0, in_ptr1, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 128256
    r0_numel = 4096
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp6 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (r0_1), r0_mask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tl.load(in_ptr1 + (r0_1 + 4096*x0), xmask & r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tmp1 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, R0_BLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(r0_mask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tmp8 = tmp6.to(tl.float32)
    tl.store(out_ptr1 + (x0), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((1, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 128256), (128256, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 128256, 4096,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_17.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mm_17.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.050929664
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1, arg27_1, arg28_1, arg29_1, arg30_1, arg31_1, arg32_1, arg33_1, arg34_1, arg35_1, arg36_1, arg37_1, arg38_1, arg39_1, arg40_1, arg41_1, arg42_1, arg43_1, arg44_1, arg45_1, arg46_1, arg47_1, arg48_1, arg49_1, arg50_1, arg51_1, arg52_1, arg53_1, arg54_1, arg55_1, arg56_1, arg57_1, arg58_1, arg59_1, arg60_1, arg61_1, arg62_1, arg63_1, arg64_1, arg65_1, arg66_1, arg67_1, arg68_1, arg69_1, arg70_1, arg71_1, arg72_1, arg73_1, arg74_1, arg75_1, arg76_1, arg77_1, arg78_1, arg79_1, arg80_1, arg81_1, arg82_1, arg83_1, arg84_1, arg85_1, arg86_1, arg87_1, arg88_1, arg89_1, arg90_1, arg91_1, arg92_1, arg93_1, arg94_1, arg95_1, arg96_1, arg97_1, arg98_1, arg99_1, arg100_1, arg101_1, arg102_1, arg103_1, arg104_1, arg105_1, arg106_1, arg107_1, arg108_1, arg109_1, arg110_1, arg111_1, arg112_1, arg113_1, arg114_1, arg115_1, arg116_1, arg117_1, arg118_1, arg119_1, arg120_1, arg121_1, arg122_1, arg123_1, arg124_1, arg125_1, arg126_1, arg127_1, arg128_1, arg129_1, arg130_1, arg131_1, arg132_1, arg133_1, arg134_1, arg135_1, arg136_1, arg137_1, arg138_1, arg139_1, arg140_1, arg141_1, arg142_1, arg143_1, arg144_1, arg145_1, arg146_1, arg147_1, arg148_1, arg149_1, arg150_1, arg151_1, arg152_1, arg153_1, arg154_1, arg155_1, arg156_1, arg157_1, arg158_1, arg159_1, arg160_1, arg161_1, arg162_1, arg163_1, arg164_1, arg165_1, arg166_1, arg167_1, arg168_1, arg169_1, arg170_1, arg171_1, arg172_1, arg173_1, arg174_1, arg175_1, arg176_1, arg177_1, arg178_1, arg179_1, arg180_1, arg181_1, arg182_1, arg183_1, arg184_1, arg185_1, arg186_1, arg187_1, arg188_1, arg189_1, arg190_1, arg191_1, arg192_1, arg193_1, arg194_1, arg195_1, arg196_1, arg197_1, arg198_1, arg199_1, arg200_1, arg201_1, arg202_1, arg203_1, arg204_1, arg205_1, arg206_1, arg207_1, arg208_1, arg209_1, arg210_1, arg211_1, arg212_1, arg213_1, arg214_1, arg215_1, arg216_1, arg217_1, arg218_1, arg219_1, arg220_1, arg221_1, arg222_1, arg223_1, arg224_1, arg225_1, arg226_1, arg227_1, arg228_1, arg229_1, arg230_1, arg231_1, arg232_1, arg233_1, arg234_1, arg235_1, arg236_1, arg237_1, arg238_1, arg239_1, arg240_1, arg241_1, arg242_1, arg243_1, arg244_1, arg245_1, arg246_1, arg247_1, arg248_1, arg249_1, arg250_1, arg251_1, arg252_1, arg253_1, arg254_1, arg255_1, arg256_1, arg257_1, arg258_1, arg259_1, arg260_1, arg261_1, arg262_1, arg263_1, arg264_1, arg265_1, arg266_1, arg267_1, arg268_1, arg269_1, arg270_1, arg271_1, arg272_1, arg273_1, arg274_1, arg275_1, arg276_1, arg277_1, arg278_1, arg279_1, arg280_1, arg281_1, arg282_1, arg283_1, arg284_1, arg285_1, arg286_1, arg287_1, arg288_1, arg289_1, arg290_1, arg291_1, arg292_1, arg293_1, arg294_1, arg295_1, arg296_1, arg297_1, arg298_1, arg299_1, arg300_1, arg301_1, arg302_1, arg303_1, arg304_1, arg305_1, arg306_1, arg307_1, arg308_1, arg309_1, arg310_1, arg311_1, arg312_1, arg313_1, arg314_1, arg315_1, arg316_1, arg317_1, arg318_1, arg319_1, arg320_1, arg321_1, arg322_1, arg323_1, arg324_1, arg325_1, arg326_1, arg327_1, arg328_1, arg329_1, arg330_1, arg331_1, arg332_1, arg333_1, arg334_1, arg335_1, arg336_1, arg337_1, arg338_1, arg339_1, arg340_1, arg341_1, arg342_1, arg343_1, arg344_1, arg345_1, arg346_1, arg347_1, arg348_1, arg349_1, arg350_1, arg351_1, arg352_1, arg353_1, arg354_1, arg355_1, arg356_1, arg357_1, arg358_1, arg359_1 = args
    args.clear()
    assert_size_stride(arg0_1, (1, 1), (1, 1))
    assert_size_stride(arg1_1, (128256, 4096), (4096, 1))
    assert_size_stride(arg2_1, (1, ), (1, ))
    assert_size_stride(arg3_1, (1, 1), (1, 1))
    assert_size_stride(arg4_1, (1, 1, 1, 2048), (2048, 2048, 2048, 1))
    assert_size_stride(arg5_1, (64, ), (1, ))
    assert_size_stride(arg6_1, (4096, ), (1, ))
    assert_size_stride(arg7_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg8_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg9_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg10_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg11_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg12_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg13_1, (4096, ), (1, ))
    assert_size_stride(arg14_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg15_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg16_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg17_1, (4096, ), (1, ))
    assert_size_stride(arg18_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg19_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg20_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg21_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg22_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg23_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg24_1, (4096, ), (1, ))
    assert_size_stride(arg25_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg26_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg27_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg28_1, (4096, ), (1, ))
    assert_size_stride(arg29_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg30_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg31_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg32_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg33_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg34_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg35_1, (4096, ), (1, ))
    assert_size_stride(arg36_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg37_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg38_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg39_1, (4096, ), (1, ))
    assert_size_stride(arg40_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg41_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg42_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg43_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg44_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg45_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg46_1, (4096, ), (1, ))
    assert_size_stride(arg47_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg48_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg49_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg50_1, (4096, ), (1, ))
    assert_size_stride(arg51_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg52_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg53_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg54_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg55_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg56_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg57_1, (4096, ), (1, ))
    assert_size_stride(arg58_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg59_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg60_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg61_1, (4096, ), (1, ))
    assert_size_stride(arg62_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg63_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg64_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg65_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg66_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg67_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg68_1, (4096, ), (1, ))
    assert_size_stride(arg69_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg70_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg71_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg72_1, (4096, ), (1, ))
    assert_size_stride(arg73_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg74_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg75_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg76_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg77_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg78_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg79_1, (4096, ), (1, ))
    assert_size_stride(arg80_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg81_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg82_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg83_1, (4096, ), (1, ))
    assert_size_stride(arg84_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg85_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg86_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg87_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg88_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg89_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg90_1, (4096, ), (1, ))
    assert_size_stride(arg91_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg92_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg93_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg94_1, (4096, ), (1, ))
    assert_size_stride(arg95_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg96_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg97_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg98_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg99_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg100_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg101_1, (4096, ), (1, ))
    assert_size_stride(arg102_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg103_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg104_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg105_1, (4096, ), (1, ))
    assert_size_stride(arg106_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg107_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg108_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg109_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg110_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg111_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg112_1, (4096, ), (1, ))
    assert_size_stride(arg113_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg114_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg115_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg116_1, (4096, ), (1, ))
    assert_size_stride(arg117_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg118_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg119_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg120_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg121_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg122_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg123_1, (4096, ), (1, ))
    assert_size_stride(arg124_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg125_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg126_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg127_1, (4096, ), (1, ))
    assert_size_stride(arg128_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg129_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg130_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg131_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg132_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg133_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg134_1, (4096, ), (1, ))
    assert_size_stride(arg135_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg136_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg137_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg138_1, (4096, ), (1, ))
    assert_size_stride(arg139_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg140_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg141_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg142_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg143_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg144_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg145_1, (4096, ), (1, ))
    assert_size_stride(arg146_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg147_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg148_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg149_1, (4096, ), (1, ))
    assert_size_stride(arg150_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg151_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg152_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg153_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg154_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg155_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg156_1, (4096, ), (1, ))
    assert_size_stride(arg157_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg158_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg159_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg160_1, (4096, ), (1, ))
    assert_size_stride(arg161_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg162_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg163_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg164_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg165_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg166_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg167_1, (4096, ), (1, ))
    assert_size_stride(arg168_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg169_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg170_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg171_1, (4096, ), (1, ))
    assert_size_stride(arg172_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg173_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg174_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg175_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg176_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg177_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg178_1, (4096, ), (1, ))
    assert_size_stride(arg179_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg180_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg181_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg182_1, (4096, ), (1, ))
    assert_size_stride(arg183_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg184_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg185_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg186_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg187_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg188_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg189_1, (4096, ), (1, ))
    assert_size_stride(arg190_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg191_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg192_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg193_1, (4096, ), (1, ))
    assert_size_stride(arg194_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg195_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg196_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg197_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg198_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg199_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg200_1, (4096, ), (1, ))
    assert_size_stride(arg201_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg202_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg203_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg204_1, (4096, ), (1, ))
    assert_size_stride(arg205_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg206_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg207_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg208_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg209_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg210_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg211_1, (4096, ), (1, ))
    assert_size_stride(arg212_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg213_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg214_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg215_1, (4096, ), (1, ))
    assert_size_stride(arg216_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg217_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg218_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg219_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg220_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg221_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg222_1, (4096, ), (1, ))
    assert_size_stride(arg223_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg224_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg225_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg226_1, (4096, ), (1, ))
    assert_size_stride(arg227_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg228_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg229_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg230_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg231_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg232_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg233_1, (4096, ), (1, ))
    assert_size_stride(arg234_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg235_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg236_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg237_1, (4096, ), (1, ))
    assert_size_stride(arg238_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg239_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg240_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg241_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg242_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg243_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg244_1, (4096, ), (1, ))
    assert_size_stride(arg245_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg246_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg247_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg248_1, (4096, ), (1, ))
    assert_size_stride(arg249_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg250_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg251_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg252_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg253_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg254_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg255_1, (4096, ), (1, ))
    assert_size_stride(arg256_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg257_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg258_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg259_1, (4096, ), (1, ))
    assert_size_stride(arg260_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg261_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg262_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg263_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg264_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg265_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg266_1, (4096, ), (1, ))
    assert_size_stride(arg267_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg268_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg269_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg270_1, (4096, ), (1, ))
    assert_size_stride(arg271_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg272_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg273_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg274_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg275_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg276_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg277_1, (4096, ), (1, ))
    assert_size_stride(arg278_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg279_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg280_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg281_1, (4096, ), (1, ))
    assert_size_stride(arg282_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg283_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg284_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg285_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg286_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg287_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg288_1, (4096, ), (1, ))
    assert_size_stride(arg289_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg290_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg291_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg292_1, (4096, ), (1, ))
    assert_size_stride(arg293_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg294_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg295_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg296_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg297_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg298_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg299_1, (4096, ), (1, ))
    assert_size_stride(arg300_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg301_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg302_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg303_1, (4096, ), (1, ))
    assert_size_stride(arg304_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg305_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg306_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg307_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg308_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg309_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg310_1, (4096, ), (1, ))
    assert_size_stride(arg311_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg312_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg313_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg314_1, (4096, ), (1, ))
    assert_size_stride(arg315_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg316_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg317_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg318_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg319_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg320_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg321_1, (4096, ), (1, ))
    assert_size_stride(arg322_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg323_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg324_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg325_1, (4096, ), (1, ))
    assert_size_stride(arg326_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg327_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg328_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg329_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg330_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg331_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg332_1, (4096, ), (1, ))
    assert_size_stride(arg333_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg334_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg335_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg336_1, (4096, ), (1, ))
    assert_size_stride(arg337_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg338_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg339_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg340_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg341_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg342_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg343_1, (4096, ), (1, ))
    assert_size_stride(arg344_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg345_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg346_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg347_1, (4096, ), (1, ))
    assert_size_stride(arg348_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg349_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg350_1, (1024, 4096), (4096, 1))
    assert_size_stride(arg351_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg352_1, (1, 8, 2048, 128), (2097152, 262144, 128, 1))
    assert_size_stride(arg353_1, (4096, 4096), (4096, 1))
    assert_size_stride(arg354_1, (4096, ), (1, ))
    assert_size_stride(arg355_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg356_1, (14336, 4096), (4096, 1))
    assert_size_stride(arg357_1, (4096, 14336), (14336, 1))
    assert_size_stride(arg358_1, (4096, ), (1, ))
    assert_size_stride(arg359_1, (128256, 4096), (4096, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((1, 4096), (4096, 1), torch.float16)
        buf1 = empty_strided_cuda((1, ), (1, ), torch.float32)
        buf2 = empty_strided_cuda((1, 4096), (4096, 1), torch.float16)
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation], Original ATen: []
        stream0 = get_raw_stream(0)
        triton_poi_fused_0.run(arg0_1, arg1_1, buf2, 4096, stream=stream0)
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf0, 4096, buf2, 4096, arg6_1, 1, buf1, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg6_1
        buf5 = empty_strided_cuda((1, 4096), (4096, 1), torch.float32)
        # Topologically Sorted Source Nodes: [linear], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf0, arg7_1, buf5, 4096, 4096, stream=stream0)
        del arg7_1
        buf6 = reinterpret_tensor(buf2, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf2  # reuse
        # Topologically Sorted Source Nodes: [mul_2, cat_1, mul_3, q_embed], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf5, arg5_1, arg3_1, buf6, 4096, stream=stream0)
        buf7 = empty_strided_cuda((1, 1024), (1024, 1), torch.float32)
        # Topologically Sorted Source Nodes: [linear_1, linear_2, index_copy__1], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf0, arg8_1, arg9_1, arg2_1, buf7, arg11_1, 1024, 4096, stream=stream0)
        del arg8_1
        del arg9_1
        # Topologically Sorted Source Nodes: [mul_4, cat_2, mul_5, k_embed, index_copy_], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf7, arg5_1, arg3_1, arg2_1, arg10_1, 1024, stream=stream0)
        buf12 = empty_strided_cuda((1, 32, 2048, 128), (8388608, 262144, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [attn_output], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg10_1, buf12, 8388608, stream=stream0)
        del arg10_1
        buf13 = empty_strided_cuda((1, 32, 2048, 128), (8388608, 262144, 128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [attn_output], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg11_1, buf13, 8388608, stream=stream0)
        del arg11_1
        buf14 = empty_strided_cuda((1, 1, 1, 2048), (2048, 0, 2048, 1), torch.float16)
        buf43 = empty_strided_cuda((1, 1, 1, 2048), (2048, 0, 2048, 1), torch.float16)
        buf72 = empty_strided_cuda((1, 1, 1, 2048), (2048, 0, 2048, 1), torch.float16)
        # Topologically Sorted Source Nodes: [attn_output, attn_output_4, attn_output_8], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_6.run(arg4_1, buf14, buf43, buf72, 2048, stream=stream0)
        # Topologically Sorted Source Nodes: [attn_output], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf15 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf6, buf12, buf13, reinterpret_tensor(buf14, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf16 = buf15[0]
        assert_size_stride(buf16, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf15
        buf20 = reinterpret_tensor(buf6, (1, 4096), (4096, 1), 0); del buf6  # reuse
        buf21 = buf5; del buf5  # reuse
        buf23 = buf0; del buf0  # reuse
        # Topologically Sorted Source Nodes: [attn_output_3, triton_kernel_wrapper_mutation_1], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_7.run(buf16, arg12_1, arg0_1, arg1_1, buf21, buf23, 4096, 4096, stream=stream0)
        del arg12_1
        del buf16
        buf22 = buf1; del buf1  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_1], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf20, 4096, buf23, 4096, arg13_1, 1, buf22, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg13_1
        buf26 = buf23; del buf23  # reuse
        buf27 = empty_strided_cuda((1, 14336), (14336, 1), torch.float32)
        # Topologically Sorted Source Nodes: [linear_4], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf20, arg14_1, buf27, 14336, 4096, stream=stream0)
        del arg14_1
        buf28 = empty_strided_cuda((1, 14336), (14336, 1), torch.float32)
        # Topologically Sorted Source Nodes: [linear_5], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf20, arg15_1, buf28, 14336, 4096, stream=stream0)
        del arg15_1
        buf29 = empty_strided_cuda((1, 4096), (4096, 1), torch.float32)
        buf31 = buf20; del buf20  # reuse
        # Topologically Sorted Source Nodes: [down_proj, triton_kernel_wrapper_mutation_2], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_9.run(buf27, buf28, arg16_1, arg0_1, arg1_1, buf21, buf29, buf31, 4096, 14336, stream=stream0)
        del arg16_1
        buf30 = buf22; del buf22  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_2], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf26, 4096, buf31, 4096, arg17_1, 1, buf30, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg17_1
        buf34 = empty_strided_cuda((1, 4096), (4096, 1), torch.float32)
        # Topologically Sorted Source Nodes: [linear_7], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf26, arg18_1, buf34, 4096, 4096, stream=stream0)
        del arg18_1
        buf35 = reinterpret_tensor(buf31, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf31  # reuse
        # Topologically Sorted Source Nodes: [mul_7, cat_3, mul_8, q_embed_1], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf34, arg5_1, arg3_1, buf35, 4096, stream=stream0)
        buf36 = buf7; del buf7  # reuse
        # Topologically Sorted Source Nodes: [linear_8, linear_9, index_copy__3], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf26, arg19_1, arg20_1, arg2_1, buf36, arg22_1, 1024, 4096, stream=stream0)
        del arg19_1
        del arg20_1
        # Topologically Sorted Source Nodes: [mul_9, cat_4, mul_10, k_embed_1, index_copy__2], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf36, arg5_1, arg3_1, arg2_1, arg21_1, 1024, stream=stream0)
        buf41 = buf13; del buf13  # reuse
        # Topologically Sorted Source Nodes: [attn_output_4], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg21_1, buf41, 8388608, stream=stream0)
        del arg21_1
        buf42 = buf12; del buf12  # reuse
        # Topologically Sorted Source Nodes: [attn_output_4], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg22_1, buf42, 8388608, stream=stream0)
        del arg22_1
        # Topologically Sorted Source Nodes: [attn_output_4], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf44 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf35, buf41, buf42, reinterpret_tensor(buf43, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf45 = buf44[0]
        assert_size_stride(buf45, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf44
        buf49 = reinterpret_tensor(buf35, (1, 4096), (4096, 1), 0); del buf35  # reuse
        buf51 = reinterpret_tensor(buf26, (1, 1, 4096), (4096, 4096, 1), 0); del buf26  # reuse
        # Topologically Sorted Source Nodes: [inputs_embeds, hidden_states_2, hidden_states_3, attn_output_7, hidden_states_6], Original ATen: [aten.embedding, aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_add_embedding_mm_10.run(buf45, arg23_1, arg0_1, arg1_1, buf21, buf29, buf51, 4096, 4096, stream=stream0)
        del arg0_1
        del arg1_1
        del arg23_1
        buf52 = buf30; del buf30  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_3], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf49, 4096, reinterpret_tensor(buf51, (1, 4096), (4096, 1), 0), 4096, arg24_1, 1, buf52, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg24_1
        buf55 = reinterpret_tensor(buf45, (1, 4096), (4096, 1), 0); del buf45  # reuse
        buf56 = buf28; del buf28  # reuse
        # Topologically Sorted Source Nodes: [linear_11], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf49, arg25_1, buf56, 14336, 4096, stream=stream0)
        del arg25_1
        buf57 = buf27; del buf27  # reuse
        # Topologically Sorted Source Nodes: [linear_12], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf49, arg26_1, buf57, 14336, 4096, stream=stream0)
        del arg26_1
        buf58 = buf29; del buf29  # reuse
        buf60 = buf49; del buf49  # reuse
        # Topologically Sorted Source Nodes: [down_proj_1, triton_kernel_wrapper_mutation_4], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_11.run(buf56, buf57, arg27_1, buf51, buf58, buf60, 4096, 14336, stream=stream0)
        del arg27_1
        buf59 = buf52; del buf52  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_4], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf55, 4096, buf60, 4096, arg28_1, 1, buf59, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg28_1
        buf63 = buf21; del buf21  # reuse
        # Topologically Sorted Source Nodes: [linear_14], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf55, arg29_1, buf63, 4096, 4096, stream=stream0)
        del arg29_1
        buf64 = reinterpret_tensor(buf60, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf60  # reuse
        # Topologically Sorted Source Nodes: [mul_12, cat_5, mul_13, q_embed_2], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf63, arg5_1, arg3_1, buf64, 4096, stream=stream0)
        buf65 = buf36; del buf36  # reuse
        # Topologically Sorted Source Nodes: [linear_15, linear_16, index_copy__5], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf55, arg30_1, arg31_1, arg2_1, buf65, arg33_1, 1024, 4096, stream=stream0)
        del arg30_1
        del arg31_1
        # Topologically Sorted Source Nodes: [mul_14, cat_6, mul_15, k_embed_2, index_copy__4], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf65, arg5_1, arg3_1, arg2_1, arg32_1, 1024, stream=stream0)
        buf70 = buf42; del buf42  # reuse
        # Topologically Sorted Source Nodes: [attn_output_8], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg32_1, buf70, 8388608, stream=stream0)
        del arg32_1
        buf71 = buf41; del buf41  # reuse
        # Topologically Sorted Source Nodes: [attn_output_8], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg33_1, buf71, 8388608, stream=stream0)
        del arg33_1
        # Topologically Sorted Source Nodes: [attn_output_8], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf73 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf64, buf70, buf71, reinterpret_tensor(buf72, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf74 = buf73[0]
        assert_size_stride(buf74, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf73
        buf78 = reinterpret_tensor(buf64, (1, 4096), (4096, 1), 0); del buf64  # reuse
        buf79 = buf63; del buf63  # reuse
        buf81 = buf55; del buf55  # reuse
        # Topologically Sorted Source Nodes: [attn_output_11, triton_kernel_wrapper_mutation_5], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_12.run(buf74, arg34_1, buf51, buf58, buf79, buf81, 4096, 4096, stream=stream0)
        del arg34_1
        del buf74
        buf80 = buf59; del buf59  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_5], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf78, 4096, buf81, 4096, arg35_1, 1, buf80, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg35_1
        buf84 = buf81; del buf81  # reuse
        buf85 = buf57; del buf57  # reuse
        # Topologically Sorted Source Nodes: [linear_18], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf78, arg36_1, buf85, 14336, 4096, stream=stream0)
        del arg36_1
        buf86 = buf56; del buf56  # reuse
        # Topologically Sorted Source Nodes: [linear_19], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf78, arg37_1, buf86, 14336, 4096, stream=stream0)
        del arg37_1
        buf87 = buf34; del buf34  # reuse
        buf89 = buf78; del buf78  # reuse
        # Topologically Sorted Source Nodes: [down_proj_2, triton_kernel_wrapper_mutation_6], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_13.run(buf85, buf86, arg38_1, buf51, buf58, buf79, buf87, buf89, 4096, 14336, stream=stream0)
        del arg38_1
        buf88 = buf80; del buf80  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_6], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf84, 4096, buf89, 4096, arg39_1, 1, buf88, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg39_1
        buf92 = empty_strided_cuda((1, 4096), (4096, 1), torch.float32)
        # Topologically Sorted Source Nodes: [linear_21], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf84, arg40_1, buf92, 4096, 4096, stream=stream0)
        del arg40_1
        buf93 = reinterpret_tensor(buf89, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf89  # reuse
        # Topologically Sorted Source Nodes: [mul_17, cat_7, mul_18, q_embed_3], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf92, arg5_1, arg3_1, buf93, 4096, stream=stream0)
        buf94 = buf65; del buf65  # reuse
        # Topologically Sorted Source Nodes: [linear_22, linear_23, index_copy__7], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf84, arg41_1, arg42_1, arg2_1, buf94, arg44_1, 1024, 4096, stream=stream0)
        del arg41_1
        del arg42_1
        del buf84
        # Topologically Sorted Source Nodes: [mul_19, cat_8, mul_20, k_embed_3, index_copy__6], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf94, arg5_1, arg3_1, arg2_1, arg43_1, 1024, stream=stream0)
        buf99 = buf71; del buf71  # reuse
        # Topologically Sorted Source Nodes: [attn_output_12], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg43_1, buf99, 8388608, stream=stream0)
        del arg43_1
        buf100 = buf70; del buf70  # reuse
        # Topologically Sorted Source Nodes: [attn_output_12], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg44_1, buf100, 8388608, stream=stream0)
        del arg44_1
        buf101 = buf72; del buf72  # reuse
        buf130 = buf43; del buf43  # reuse
        buf159 = buf14; del buf14  # reuse
        # Topologically Sorted Source Nodes: [attn_output_12, attn_output_16, attn_output_20], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_6.run(arg4_1, buf101, buf130, buf159, 2048, stream=stream0)
        # Topologically Sorted Source Nodes: [attn_output_12], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf102 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf93, buf99, buf100, reinterpret_tensor(buf101, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf103 = buf102[0]
        assert_size_stride(buf103, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf102
        buf107 = reinterpret_tensor(buf93, (1, 4096), (4096, 1), 0); del buf93  # reuse
        buf109 = buf51; del buf51  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_7, hidden_states_10, hidden_states_11, attn_output_15, hidden_states_14], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_add_mm_14.run(buf109, buf103, arg45_1, buf58, buf79, buf87, 4096, 4096, stream=stream0)
        del arg45_1
        buf110 = buf88; del buf88  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_7], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf107, 4096, reinterpret_tensor(buf109, (1, 4096), (4096, 1), 0), 4096, arg46_1, 1, buf110, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg46_1
        buf113 = reinterpret_tensor(buf103, (1, 4096), (4096, 1), 0); del buf103  # reuse
        buf114 = buf86; del buf86  # reuse
        # Topologically Sorted Source Nodes: [linear_25], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf107, arg47_1, buf114, 14336, 4096, stream=stream0)
        del arg47_1
        buf115 = buf85; del buf85  # reuse
        # Topologically Sorted Source Nodes: [linear_26], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf107, arg48_1, buf115, 14336, 4096, stream=stream0)
        del arg48_1
        buf116 = buf87; del buf87  # reuse
        buf118 = buf107; del buf107  # reuse
        # Topologically Sorted Source Nodes: [down_proj_3, triton_kernel_wrapper_mutation_8], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_11.run(buf114, buf115, arg49_1, buf109, buf116, buf118, 4096, 14336, stream=stream0)
        del arg49_1
        buf117 = buf110; del buf110  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_8], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf113, 4096, buf118, 4096, arg50_1, 1, buf117, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg50_1
        buf121 = buf79; del buf79  # reuse
        # Topologically Sorted Source Nodes: [linear_28], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf113, arg51_1, buf121, 4096, 4096, stream=stream0)
        del arg51_1
        buf122 = reinterpret_tensor(buf118, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf118  # reuse
        # Topologically Sorted Source Nodes: [mul_22, cat_9, mul_23, q_embed_4], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf121, arg5_1, arg3_1, buf122, 4096, stream=stream0)
        buf123 = buf94; del buf94  # reuse
        # Topologically Sorted Source Nodes: [linear_29, linear_30, index_copy__9], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf113, arg52_1, arg53_1, arg2_1, buf123, arg55_1, 1024, 4096, stream=stream0)
        del arg52_1
        del arg53_1
        # Topologically Sorted Source Nodes: [mul_24, cat_10, mul_25, k_embed_4, index_copy__8], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf123, arg5_1, arg3_1, arg2_1, arg54_1, 1024, stream=stream0)
        buf128 = buf99; del buf99  # reuse
        # Topologically Sorted Source Nodes: [attn_output_16], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg54_1, buf128, 8388608, stream=stream0)
        del arg54_1
        buf129 = buf100; del buf100  # reuse
        # Topologically Sorted Source Nodes: [attn_output_16], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg55_1, buf129, 8388608, stream=stream0)
        del arg55_1
        # Topologically Sorted Source Nodes: [attn_output_16], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf131 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf122, buf128, buf129, reinterpret_tensor(buf130, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf132 = buf131[0]
        assert_size_stride(buf132, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf131
        buf136 = reinterpret_tensor(buf122, (1, 4096), (4096, 1), 0); del buf122  # reuse
        buf137 = buf121; del buf121  # reuse
        buf139 = buf113; del buf113  # reuse
        # Topologically Sorted Source Nodes: [attn_output_19, triton_kernel_wrapper_mutation_9], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_12.run(buf132, arg56_1, buf109, buf116, buf137, buf139, 4096, 4096, stream=stream0)
        del arg56_1
        del buf132
        buf138 = buf117; del buf117  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_9], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf136, 4096, buf139, 4096, arg57_1, 1, buf138, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg57_1
        buf142 = buf139; del buf139  # reuse
        buf143 = buf115; del buf115  # reuse
        # Topologically Sorted Source Nodes: [linear_32], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf136, arg58_1, buf143, 14336, 4096, stream=stream0)
        del arg58_1
        buf144 = buf114; del buf114  # reuse
        # Topologically Sorted Source Nodes: [linear_33], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf136, arg59_1, buf144, 14336, 4096, stream=stream0)
        del arg59_1
        buf145 = buf58; del buf58  # reuse
        buf147 = buf136; del buf136  # reuse
        # Topologically Sorted Source Nodes: [down_proj_4, triton_kernel_wrapper_mutation_10], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_13.run(buf143, buf144, arg60_1, buf109, buf116, buf137, buf145, buf147, 4096, 14336, stream=stream0)
        del arg60_1
        buf146 = buf138; del buf138  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_10], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf142, 4096, buf147, 4096, arg61_1, 1, buf146, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg61_1
        buf150 = buf92; del buf92  # reuse
        # Topologically Sorted Source Nodes: [linear_35], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf142, arg62_1, buf150, 4096, 4096, stream=stream0)
        del arg62_1
        buf151 = reinterpret_tensor(buf147, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf147  # reuse
        # Topologically Sorted Source Nodes: [mul_27, cat_11, mul_28, q_embed_5], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf150, arg5_1, arg3_1, buf151, 4096, stream=stream0)
        buf152 = buf123; del buf123  # reuse
        # Topologically Sorted Source Nodes: [linear_36, linear_37, index_copy__11], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf142, arg63_1, arg64_1, arg2_1, buf152, arg66_1, 1024, 4096, stream=stream0)
        del arg63_1
        del arg64_1
        del buf142
        # Topologically Sorted Source Nodes: [mul_29, cat_12, mul_30, k_embed_5, index_copy__10], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf152, arg5_1, arg3_1, arg2_1, arg65_1, 1024, stream=stream0)
        buf157 = buf129; del buf129  # reuse
        # Topologically Sorted Source Nodes: [attn_output_20], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg65_1, buf157, 8388608, stream=stream0)
        del arg65_1
        buf158 = buf128; del buf128  # reuse
        # Topologically Sorted Source Nodes: [attn_output_20], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg66_1, buf158, 8388608, stream=stream0)
        del arg66_1
        # Topologically Sorted Source Nodes: [attn_output_20], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf160 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf151, buf157, buf158, reinterpret_tensor(buf159, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf161 = buf160[0]
        assert_size_stride(buf161, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf160
        buf165 = reinterpret_tensor(buf151, (1, 4096), (4096, 1), 0); del buf151  # reuse
        buf167 = buf109; del buf109  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_15, hidden_states_18, hidden_states_19, attn_output_23, hidden_states_22], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_add_mm_14.run(buf167, buf161, arg67_1, buf116, buf137, buf145, 4096, 4096, stream=stream0)
        del arg67_1
        buf168 = buf146; del buf146  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_11], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf165, 4096, reinterpret_tensor(buf167, (1, 4096), (4096, 1), 0), 4096, arg68_1, 1, buf168, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg68_1
        buf171 = reinterpret_tensor(buf161, (1, 4096), (4096, 1), 0); del buf161  # reuse
        buf172 = buf144; del buf144  # reuse
        # Topologically Sorted Source Nodes: [linear_39], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf165, arg69_1, buf172, 14336, 4096, stream=stream0)
        del arg69_1
        buf173 = buf143; del buf143  # reuse
        # Topologically Sorted Source Nodes: [linear_40], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf165, arg70_1, buf173, 14336, 4096, stream=stream0)
        del arg70_1
        buf174 = buf145; del buf145  # reuse
        buf176 = buf165; del buf165  # reuse
        # Topologically Sorted Source Nodes: [down_proj_5, triton_kernel_wrapper_mutation_12], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_11.run(buf172, buf173, arg71_1, buf167, buf174, buf176, 4096, 14336, stream=stream0)
        del arg71_1
        buf175 = buf168; del buf168  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_12], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf171, 4096, buf176, 4096, arg72_1, 1, buf175, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg72_1
        buf179 = buf137; del buf137  # reuse
        # Topologically Sorted Source Nodes: [linear_42], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf171, arg73_1, buf179, 4096, 4096, stream=stream0)
        del arg73_1
        buf180 = reinterpret_tensor(buf176, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf176  # reuse
        # Topologically Sorted Source Nodes: [mul_32, cat_13, mul_33, q_embed_6], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf179, arg5_1, arg3_1, buf180, 4096, stream=stream0)
        buf181 = buf152; del buf152  # reuse
        # Topologically Sorted Source Nodes: [linear_43, linear_44, index_copy__13], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf171, arg74_1, arg75_1, arg2_1, buf181, arg77_1, 1024, 4096, stream=stream0)
        del arg74_1
        del arg75_1
        # Topologically Sorted Source Nodes: [mul_34, cat_14, mul_35, k_embed_6, index_copy__12], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf181, arg5_1, arg3_1, arg2_1, arg76_1, 1024, stream=stream0)
        buf186 = buf158; del buf158  # reuse
        # Topologically Sorted Source Nodes: [attn_output_24], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg76_1, buf186, 8388608, stream=stream0)
        del arg76_1
        buf187 = buf157; del buf157  # reuse
        # Topologically Sorted Source Nodes: [attn_output_24], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg77_1, buf187, 8388608, stream=stream0)
        del arg77_1
        buf188 = buf159; del buf159  # reuse
        buf217 = buf130; del buf130  # reuse
        buf246 = buf101; del buf101  # reuse
        # Topologically Sorted Source Nodes: [attn_output_24, attn_output_28, attn_output_32], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_6.run(arg4_1, buf188, buf217, buf246, 2048, stream=stream0)
        # Topologically Sorted Source Nodes: [attn_output_24], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf189 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf180, buf186, buf187, reinterpret_tensor(buf188, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf190 = buf189[0]
        assert_size_stride(buf190, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf189
        buf194 = reinterpret_tensor(buf180, (1, 4096), (4096, 1), 0); del buf180  # reuse
        buf195 = buf179; del buf179  # reuse
        buf197 = buf171; del buf171  # reuse
        # Topologically Sorted Source Nodes: [attn_output_27, triton_kernel_wrapper_mutation_13], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_12.run(buf190, arg78_1, buf167, buf174, buf195, buf197, 4096, 4096, stream=stream0)
        del arg78_1
        del buf190
        buf196 = buf175; del buf175  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_13], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf194, 4096, buf197, 4096, arg79_1, 1, buf196, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg79_1
        buf200 = buf197; del buf197  # reuse
        buf201 = buf173; del buf173  # reuse
        # Topologically Sorted Source Nodes: [linear_46], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf194, arg80_1, buf201, 14336, 4096, stream=stream0)
        del arg80_1
        buf202 = buf172; del buf172  # reuse
        # Topologically Sorted Source Nodes: [linear_47], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf194, arg81_1, buf202, 14336, 4096, stream=stream0)
        del arg81_1
        buf203 = buf116; del buf116  # reuse
        buf205 = buf194; del buf194  # reuse
        # Topologically Sorted Source Nodes: [down_proj_6, triton_kernel_wrapper_mutation_14], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_13.run(buf201, buf202, arg82_1, buf167, buf174, buf195, buf203, buf205, 4096, 14336, stream=stream0)
        del arg82_1
        buf204 = buf196; del buf196  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_14], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf200, 4096, buf205, 4096, arg83_1, 1, buf204, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg83_1
        buf208 = buf150; del buf150  # reuse
        # Topologically Sorted Source Nodes: [linear_49], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf200, arg84_1, buf208, 4096, 4096, stream=stream0)
        del arg84_1
        buf209 = reinterpret_tensor(buf205, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf205  # reuse
        # Topologically Sorted Source Nodes: [mul_37, cat_15, mul_38, q_embed_7], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf208, arg5_1, arg3_1, buf209, 4096, stream=stream0)
        buf210 = buf181; del buf181  # reuse
        # Topologically Sorted Source Nodes: [linear_50, linear_51, index_copy__15], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf200, arg85_1, arg86_1, arg2_1, buf210, arg88_1, 1024, 4096, stream=stream0)
        del arg85_1
        del arg86_1
        del buf200
        # Topologically Sorted Source Nodes: [mul_39, cat_16, mul_40, k_embed_7, index_copy__14], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf210, arg5_1, arg3_1, arg2_1, arg87_1, 1024, stream=stream0)
        buf215 = buf187; del buf187  # reuse
        # Topologically Sorted Source Nodes: [attn_output_28], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg87_1, buf215, 8388608, stream=stream0)
        del arg87_1
        buf216 = buf186; del buf186  # reuse
        # Topologically Sorted Source Nodes: [attn_output_28], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg88_1, buf216, 8388608, stream=stream0)
        del arg88_1
        # Topologically Sorted Source Nodes: [attn_output_28], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf218 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf209, buf215, buf216, reinterpret_tensor(buf217, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf219 = buf218[0]
        assert_size_stride(buf219, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf218
        buf223 = reinterpret_tensor(buf209, (1, 4096), (4096, 1), 0); del buf209  # reuse
        buf225 = buf167; del buf167  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_23, hidden_states_26, hidden_states_27, attn_output_31, hidden_states_30], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_add_mm_14.run(buf225, buf219, arg89_1, buf174, buf195, buf203, 4096, 4096, stream=stream0)
        del arg89_1
        buf226 = buf204; del buf204  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_15], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf223, 4096, reinterpret_tensor(buf225, (1, 4096), (4096, 1), 0), 4096, arg90_1, 1, buf226, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg90_1
        buf229 = reinterpret_tensor(buf219, (1, 4096), (4096, 1), 0); del buf219  # reuse
        buf230 = buf202; del buf202  # reuse
        # Topologically Sorted Source Nodes: [linear_53], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf223, arg91_1, buf230, 14336, 4096, stream=stream0)
        del arg91_1
        buf231 = buf201; del buf201  # reuse
        # Topologically Sorted Source Nodes: [linear_54], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf223, arg92_1, buf231, 14336, 4096, stream=stream0)
        del arg92_1
        buf232 = buf203; del buf203  # reuse
        buf234 = buf223; del buf223  # reuse
        # Topologically Sorted Source Nodes: [down_proj_7, triton_kernel_wrapper_mutation_16], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_11.run(buf230, buf231, arg93_1, buf225, buf232, buf234, 4096, 14336, stream=stream0)
        del arg93_1
        buf233 = buf226; del buf226  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_16], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf229, 4096, buf234, 4096, arg94_1, 1, buf233, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg94_1
        buf237 = buf195; del buf195  # reuse
        # Topologically Sorted Source Nodes: [linear_56], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf229, arg95_1, buf237, 4096, 4096, stream=stream0)
        del arg95_1
        buf238 = reinterpret_tensor(buf234, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf234  # reuse
        # Topologically Sorted Source Nodes: [mul_42, cat_17, mul_43, q_embed_8], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf237, arg5_1, arg3_1, buf238, 4096, stream=stream0)
        buf239 = buf210; del buf210  # reuse
        # Topologically Sorted Source Nodes: [linear_57, linear_58, index_copy__17], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf229, arg96_1, arg97_1, arg2_1, buf239, arg99_1, 1024, 4096, stream=stream0)
        del arg96_1
        del arg97_1
        # Topologically Sorted Source Nodes: [mul_44, cat_18, mul_45, k_embed_8, index_copy__16], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf239, arg5_1, arg3_1, arg2_1, arg98_1, 1024, stream=stream0)
        buf244 = buf216; del buf216  # reuse
        # Topologically Sorted Source Nodes: [attn_output_32], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg98_1, buf244, 8388608, stream=stream0)
        del arg98_1
        buf245 = buf215; del buf215  # reuse
        # Topologically Sorted Source Nodes: [attn_output_32], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg99_1, buf245, 8388608, stream=stream0)
        del arg99_1
        # Topologically Sorted Source Nodes: [attn_output_32], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf247 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf238, buf244, buf245, reinterpret_tensor(buf246, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf248 = buf247[0]
        assert_size_stride(buf248, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf247
        buf252 = reinterpret_tensor(buf238, (1, 4096), (4096, 1), 0); del buf238  # reuse
        buf253 = buf237; del buf237  # reuse
        buf255 = buf229; del buf229  # reuse
        # Topologically Sorted Source Nodes: [attn_output_35, triton_kernel_wrapper_mutation_17], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_12.run(buf248, arg100_1, buf225, buf232, buf253, buf255, 4096, 4096, stream=stream0)
        del arg100_1
        del buf248
        buf254 = buf233; del buf233  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_17], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf252, 4096, buf255, 4096, arg101_1, 1, buf254, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg101_1
        buf258 = buf255; del buf255  # reuse
        buf259 = buf231; del buf231  # reuse
        # Topologically Sorted Source Nodes: [linear_60], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf252, arg102_1, buf259, 14336, 4096, stream=stream0)
        del arg102_1
        buf260 = buf230; del buf230  # reuse
        # Topologically Sorted Source Nodes: [linear_61], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf252, arg103_1, buf260, 14336, 4096, stream=stream0)
        del arg103_1
        buf261 = buf174; del buf174  # reuse
        buf263 = buf252; del buf252  # reuse
        # Topologically Sorted Source Nodes: [down_proj_8, triton_kernel_wrapper_mutation_18], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_13.run(buf259, buf260, arg104_1, buf225, buf232, buf253, buf261, buf263, 4096, 14336, stream=stream0)
        del arg104_1
        buf262 = buf254; del buf254  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_18], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf258, 4096, buf263, 4096, arg105_1, 1, buf262, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg105_1
        buf266 = buf208; del buf208  # reuse
        # Topologically Sorted Source Nodes: [linear_63], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf258, arg106_1, buf266, 4096, 4096, stream=stream0)
        del arg106_1
        buf267 = reinterpret_tensor(buf263, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf263  # reuse
        # Topologically Sorted Source Nodes: [mul_47, cat_19, mul_48, q_embed_9], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf266, arg5_1, arg3_1, buf267, 4096, stream=stream0)
        buf268 = buf239; del buf239  # reuse
        # Topologically Sorted Source Nodes: [linear_64, linear_65, index_copy__19], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf258, arg107_1, arg108_1, arg2_1, buf268, arg110_1, 1024, 4096, stream=stream0)
        del arg107_1
        del arg108_1
        del buf258
        # Topologically Sorted Source Nodes: [mul_49, cat_20, mul_50, k_embed_9, index_copy__18], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf268, arg5_1, arg3_1, arg2_1, arg109_1, 1024, stream=stream0)
        buf273 = buf245; del buf245  # reuse
        # Topologically Sorted Source Nodes: [attn_output_36], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg109_1, buf273, 8388608, stream=stream0)
        del arg109_1
        buf274 = buf244; del buf244  # reuse
        # Topologically Sorted Source Nodes: [attn_output_36], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg110_1, buf274, 8388608, stream=stream0)
        del arg110_1
        buf275 = buf246; del buf246  # reuse
        buf304 = buf217; del buf217  # reuse
        buf333 = buf188; del buf188  # reuse
        # Topologically Sorted Source Nodes: [attn_output_36, attn_output_40, attn_output_44], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_6.run(arg4_1, buf275, buf304, buf333, 2048, stream=stream0)
        # Topologically Sorted Source Nodes: [attn_output_36], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf276 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf267, buf273, buf274, reinterpret_tensor(buf275, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf277 = buf276[0]
        assert_size_stride(buf277, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf276
        buf281 = reinterpret_tensor(buf267, (1, 4096), (4096, 1), 0); del buf267  # reuse
        buf283 = buf225; del buf225  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_31, hidden_states_34, hidden_states_35, attn_output_39, hidden_states_38], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_add_mm_14.run(buf283, buf277, arg111_1, buf232, buf253, buf261, 4096, 4096, stream=stream0)
        del arg111_1
        buf284 = buf262; del buf262  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_19], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf281, 4096, reinterpret_tensor(buf283, (1, 4096), (4096, 1), 0), 4096, arg112_1, 1, buf284, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg112_1
        buf287 = reinterpret_tensor(buf277, (1, 4096), (4096, 1), 0); del buf277  # reuse
        buf288 = buf260; del buf260  # reuse
        # Topologically Sorted Source Nodes: [linear_67], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf281, arg113_1, buf288, 14336, 4096, stream=stream0)
        del arg113_1
        buf289 = buf259; del buf259  # reuse
        # Topologically Sorted Source Nodes: [linear_68], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf281, arg114_1, buf289, 14336, 4096, stream=stream0)
        del arg114_1
        buf290 = buf261; del buf261  # reuse
        buf292 = buf281; del buf281  # reuse
        # Topologically Sorted Source Nodes: [down_proj_9, triton_kernel_wrapper_mutation_20], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_11.run(buf288, buf289, arg115_1, buf283, buf290, buf292, 4096, 14336, stream=stream0)
        del arg115_1
        buf291 = buf284; del buf284  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_20], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf287, 4096, buf292, 4096, arg116_1, 1, buf291, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg116_1
        buf295 = buf253; del buf253  # reuse
        # Topologically Sorted Source Nodes: [linear_70], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf287, arg117_1, buf295, 4096, 4096, stream=stream0)
        del arg117_1
        buf296 = reinterpret_tensor(buf292, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf292  # reuse
        # Topologically Sorted Source Nodes: [mul_52, cat_21, mul_53, q_embed_10], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf295, arg5_1, arg3_1, buf296, 4096, stream=stream0)
        buf297 = buf268; del buf268  # reuse
        # Topologically Sorted Source Nodes: [linear_71, linear_72, index_copy__21], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf287, arg118_1, arg119_1, arg2_1, buf297, arg121_1, 1024, 4096, stream=stream0)
        del arg118_1
        del arg119_1
        # Topologically Sorted Source Nodes: [mul_54, cat_22, mul_55, k_embed_10, index_copy__20], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf297, arg5_1, arg3_1, arg2_1, arg120_1, 1024, stream=stream0)
        buf302 = buf274; del buf274  # reuse
        # Topologically Sorted Source Nodes: [attn_output_40], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg120_1, buf302, 8388608, stream=stream0)
        del arg120_1
        buf303 = buf273; del buf273  # reuse
        # Topologically Sorted Source Nodes: [attn_output_40], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg121_1, buf303, 8388608, stream=stream0)
        del arg121_1
        # Topologically Sorted Source Nodes: [attn_output_40], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf305 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf296, buf302, buf303, reinterpret_tensor(buf304, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf306 = buf305[0]
        assert_size_stride(buf306, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf305
        buf310 = reinterpret_tensor(buf296, (1, 4096), (4096, 1), 0); del buf296  # reuse
        buf311 = buf295; del buf295  # reuse
        buf313 = buf287; del buf287  # reuse
        # Topologically Sorted Source Nodes: [attn_output_43, triton_kernel_wrapper_mutation_21], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_12.run(buf306, arg122_1, buf283, buf290, buf311, buf313, 4096, 4096, stream=stream0)
        del arg122_1
        del buf306
        buf312 = buf291; del buf291  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_21], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf310, 4096, buf313, 4096, arg123_1, 1, buf312, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg123_1
        buf316 = buf313; del buf313  # reuse
        buf317 = buf289; del buf289  # reuse
        # Topologically Sorted Source Nodes: [linear_74], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf310, arg124_1, buf317, 14336, 4096, stream=stream0)
        del arg124_1
        buf318 = buf288; del buf288  # reuse
        # Topologically Sorted Source Nodes: [linear_75], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf310, arg125_1, buf318, 14336, 4096, stream=stream0)
        del arg125_1
        buf319 = buf232; del buf232  # reuse
        buf321 = buf310; del buf310  # reuse
        # Topologically Sorted Source Nodes: [down_proj_10, triton_kernel_wrapper_mutation_22], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_13.run(buf317, buf318, arg126_1, buf283, buf290, buf311, buf319, buf321, 4096, 14336, stream=stream0)
        del arg126_1
        buf320 = buf312; del buf312  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_22], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf316, 4096, buf321, 4096, arg127_1, 1, buf320, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg127_1
        buf324 = buf266; del buf266  # reuse
        # Topologically Sorted Source Nodes: [linear_77], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf316, arg128_1, buf324, 4096, 4096, stream=stream0)
        del arg128_1
        buf325 = reinterpret_tensor(buf321, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf321  # reuse
        # Topologically Sorted Source Nodes: [mul_57, cat_23, mul_58, q_embed_11], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf324, arg5_1, arg3_1, buf325, 4096, stream=stream0)
        buf326 = buf297; del buf297  # reuse
        # Topologically Sorted Source Nodes: [linear_78, linear_79, index_copy__23], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf316, arg129_1, arg130_1, arg2_1, buf326, arg132_1, 1024, 4096, stream=stream0)
        del arg129_1
        del arg130_1
        del buf316
        # Topologically Sorted Source Nodes: [mul_59, cat_24, mul_60, k_embed_11, index_copy__22], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf326, arg5_1, arg3_1, arg2_1, arg131_1, 1024, stream=stream0)
        buf331 = buf303; del buf303  # reuse
        # Topologically Sorted Source Nodes: [attn_output_44], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg131_1, buf331, 8388608, stream=stream0)
        del arg131_1
        buf332 = buf302; del buf302  # reuse
        # Topologically Sorted Source Nodes: [attn_output_44], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg132_1, buf332, 8388608, stream=stream0)
        del arg132_1
        # Topologically Sorted Source Nodes: [attn_output_44], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf334 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf325, buf331, buf332, reinterpret_tensor(buf333, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf335 = buf334[0]
        assert_size_stride(buf335, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf334
        buf339 = reinterpret_tensor(buf325, (1, 4096), (4096, 1), 0); del buf325  # reuse
        buf341 = buf283; del buf283  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_39, hidden_states_42, hidden_states_43, attn_output_47, hidden_states_46], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_add_mm_14.run(buf341, buf335, arg133_1, buf290, buf311, buf319, 4096, 4096, stream=stream0)
        del arg133_1
        buf342 = buf320; del buf320  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_23], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf339, 4096, reinterpret_tensor(buf341, (1, 4096), (4096, 1), 0), 4096, arg134_1, 1, buf342, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg134_1
        buf345 = reinterpret_tensor(buf335, (1, 4096), (4096, 1), 0); del buf335  # reuse
        buf346 = buf318; del buf318  # reuse
        # Topologically Sorted Source Nodes: [linear_81], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf339, arg135_1, buf346, 14336, 4096, stream=stream0)
        del arg135_1
        buf347 = buf317; del buf317  # reuse
        # Topologically Sorted Source Nodes: [linear_82], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf339, arg136_1, buf347, 14336, 4096, stream=stream0)
        del arg136_1
        buf348 = buf319; del buf319  # reuse
        buf350 = buf339; del buf339  # reuse
        # Topologically Sorted Source Nodes: [down_proj_11, triton_kernel_wrapper_mutation_24], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_11.run(buf346, buf347, arg137_1, buf341, buf348, buf350, 4096, 14336, stream=stream0)
        del arg137_1
        buf349 = buf342; del buf342  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_24], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf345, 4096, buf350, 4096, arg138_1, 1, buf349, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg138_1
        buf353 = buf311; del buf311  # reuse
        # Topologically Sorted Source Nodes: [linear_84], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf345, arg139_1, buf353, 4096, 4096, stream=stream0)
        del arg139_1
        buf354 = reinterpret_tensor(buf350, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf350  # reuse
        # Topologically Sorted Source Nodes: [mul_62, cat_25, mul_63, q_embed_12], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf353, arg5_1, arg3_1, buf354, 4096, stream=stream0)
        buf355 = buf326; del buf326  # reuse
        # Topologically Sorted Source Nodes: [linear_85, linear_86, index_copy__25], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf345, arg140_1, arg141_1, arg2_1, buf355, arg143_1, 1024, 4096, stream=stream0)
        del arg140_1
        del arg141_1
        # Topologically Sorted Source Nodes: [mul_64, cat_26, mul_65, k_embed_12, index_copy__24], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf355, arg5_1, arg3_1, arg2_1, arg142_1, 1024, stream=stream0)
        buf360 = buf332; del buf332  # reuse
        # Topologically Sorted Source Nodes: [attn_output_48], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg142_1, buf360, 8388608, stream=stream0)
        del arg142_1
        buf361 = buf331; del buf331  # reuse
        # Topologically Sorted Source Nodes: [attn_output_48], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg143_1, buf361, 8388608, stream=stream0)
        del arg143_1
        buf362 = buf333; del buf333  # reuse
        buf391 = buf304; del buf304  # reuse
        buf420 = buf275; del buf275  # reuse
        # Topologically Sorted Source Nodes: [attn_output_48, attn_output_52, attn_output_56], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_6.run(arg4_1, buf362, buf391, buf420, 2048, stream=stream0)
        # Topologically Sorted Source Nodes: [attn_output_48], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf363 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf354, buf360, buf361, reinterpret_tensor(buf362, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf364 = buf363[0]
        assert_size_stride(buf364, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf363
        buf368 = reinterpret_tensor(buf354, (1, 4096), (4096, 1), 0); del buf354  # reuse
        buf369 = buf353; del buf353  # reuse
        buf371 = buf345; del buf345  # reuse
        # Topologically Sorted Source Nodes: [attn_output_51, triton_kernel_wrapper_mutation_25], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_12.run(buf364, arg144_1, buf341, buf348, buf369, buf371, 4096, 4096, stream=stream0)
        del arg144_1
        del buf364
        buf370 = buf349; del buf349  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_25], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf368, 4096, buf371, 4096, arg145_1, 1, buf370, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg145_1
        buf374 = buf371; del buf371  # reuse
        buf375 = buf347; del buf347  # reuse
        # Topologically Sorted Source Nodes: [linear_88], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf368, arg146_1, buf375, 14336, 4096, stream=stream0)
        del arg146_1
        buf376 = buf346; del buf346  # reuse
        # Topologically Sorted Source Nodes: [linear_89], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf368, arg147_1, buf376, 14336, 4096, stream=stream0)
        del arg147_1
        buf377 = buf290; del buf290  # reuse
        buf379 = buf368; del buf368  # reuse
        # Topologically Sorted Source Nodes: [down_proj_12, triton_kernel_wrapper_mutation_26], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_13.run(buf375, buf376, arg148_1, buf341, buf348, buf369, buf377, buf379, 4096, 14336, stream=stream0)
        del arg148_1
        buf378 = buf370; del buf370  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_26], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf374, 4096, buf379, 4096, arg149_1, 1, buf378, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg149_1
        buf382 = buf324; del buf324  # reuse
        # Topologically Sorted Source Nodes: [linear_91], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf374, arg150_1, buf382, 4096, 4096, stream=stream0)
        del arg150_1
        buf383 = reinterpret_tensor(buf379, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf379  # reuse
        # Topologically Sorted Source Nodes: [mul_67, cat_27, mul_68, q_embed_13], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf382, arg5_1, arg3_1, buf383, 4096, stream=stream0)
        buf384 = buf355; del buf355  # reuse
        # Topologically Sorted Source Nodes: [linear_92, linear_93, index_copy__27], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf374, arg151_1, arg152_1, arg2_1, buf384, arg154_1, 1024, 4096, stream=stream0)
        del arg151_1
        del arg152_1
        del buf374
        # Topologically Sorted Source Nodes: [mul_69, cat_28, mul_70, k_embed_13, index_copy__26], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf384, arg5_1, arg3_1, arg2_1, arg153_1, 1024, stream=stream0)
        buf389 = buf361; del buf361  # reuse
        # Topologically Sorted Source Nodes: [attn_output_52], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg153_1, buf389, 8388608, stream=stream0)
        del arg153_1
        buf390 = buf360; del buf360  # reuse
        # Topologically Sorted Source Nodes: [attn_output_52], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg154_1, buf390, 8388608, stream=stream0)
        del arg154_1
        # Topologically Sorted Source Nodes: [attn_output_52], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf392 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf383, buf389, buf390, reinterpret_tensor(buf391, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf393 = buf392[0]
        assert_size_stride(buf393, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf392
        buf397 = reinterpret_tensor(buf383, (1, 4096), (4096, 1), 0); del buf383  # reuse
        buf399 = buf341; del buf341  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_47, hidden_states_50, hidden_states_51, attn_output_55, hidden_states_54], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_add_mm_14.run(buf399, buf393, arg155_1, buf348, buf369, buf377, 4096, 4096, stream=stream0)
        del arg155_1
        buf400 = buf378; del buf378  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_27], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf397, 4096, reinterpret_tensor(buf399, (1, 4096), (4096, 1), 0), 4096, arg156_1, 1, buf400, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg156_1
        buf403 = reinterpret_tensor(buf393, (1, 4096), (4096, 1), 0); del buf393  # reuse
        buf404 = buf376; del buf376  # reuse
        # Topologically Sorted Source Nodes: [linear_95], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf397, arg157_1, buf404, 14336, 4096, stream=stream0)
        del arg157_1
        buf405 = buf375; del buf375  # reuse
        # Topologically Sorted Source Nodes: [linear_96], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf397, arg158_1, buf405, 14336, 4096, stream=stream0)
        del arg158_1
        buf406 = buf377; del buf377  # reuse
        buf408 = buf397; del buf397  # reuse
        # Topologically Sorted Source Nodes: [down_proj_13, triton_kernel_wrapper_mutation_28], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_11.run(buf404, buf405, arg159_1, buf399, buf406, buf408, 4096, 14336, stream=stream0)
        del arg159_1
        buf407 = buf400; del buf400  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_28], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf403, 4096, buf408, 4096, arg160_1, 1, buf407, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg160_1
        buf411 = buf369; del buf369  # reuse
        # Topologically Sorted Source Nodes: [linear_98], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf403, arg161_1, buf411, 4096, 4096, stream=stream0)
        del arg161_1
        buf412 = reinterpret_tensor(buf408, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf408  # reuse
        # Topologically Sorted Source Nodes: [mul_72, cat_29, mul_73, q_embed_14], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf411, arg5_1, arg3_1, buf412, 4096, stream=stream0)
        buf413 = buf384; del buf384  # reuse
        # Topologically Sorted Source Nodes: [linear_99, linear_100, index_copy__29], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf403, arg162_1, arg163_1, arg2_1, buf413, arg165_1, 1024, 4096, stream=stream0)
        del arg162_1
        del arg163_1
        # Topologically Sorted Source Nodes: [mul_74, cat_30, mul_75, k_embed_14, index_copy__28], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf413, arg5_1, arg3_1, arg2_1, arg164_1, 1024, stream=stream0)
        buf418 = buf390; del buf390  # reuse
        # Topologically Sorted Source Nodes: [attn_output_56], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg164_1, buf418, 8388608, stream=stream0)
        del arg164_1
        buf419 = buf389; del buf389  # reuse
        # Topologically Sorted Source Nodes: [attn_output_56], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg165_1, buf419, 8388608, stream=stream0)
        del arg165_1
        # Topologically Sorted Source Nodes: [attn_output_56], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf421 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf412, buf418, buf419, reinterpret_tensor(buf420, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf422 = buf421[0]
        assert_size_stride(buf422, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf421
        buf426 = reinterpret_tensor(buf412, (1, 4096), (4096, 1), 0); del buf412  # reuse
        buf427 = buf411; del buf411  # reuse
        buf429 = buf403; del buf403  # reuse
        # Topologically Sorted Source Nodes: [attn_output_59, triton_kernel_wrapper_mutation_29], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_12.run(buf422, arg166_1, buf399, buf406, buf427, buf429, 4096, 4096, stream=stream0)
        del arg166_1
        del buf422
        buf428 = buf407; del buf407  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_29], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf426, 4096, buf429, 4096, arg167_1, 1, buf428, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg167_1
        buf432 = buf429; del buf429  # reuse
        buf433 = buf405; del buf405  # reuse
        # Topologically Sorted Source Nodes: [linear_102], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf426, arg168_1, buf433, 14336, 4096, stream=stream0)
        del arg168_1
        buf434 = buf404; del buf404  # reuse
        # Topologically Sorted Source Nodes: [linear_103], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf426, arg169_1, buf434, 14336, 4096, stream=stream0)
        del arg169_1
        buf435 = buf348; del buf348  # reuse
        buf437 = buf426; del buf426  # reuse
        # Topologically Sorted Source Nodes: [down_proj_14, triton_kernel_wrapper_mutation_30], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_13.run(buf433, buf434, arg170_1, buf399, buf406, buf427, buf435, buf437, 4096, 14336, stream=stream0)
        del arg170_1
        buf436 = buf428; del buf428  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_30], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf432, 4096, buf437, 4096, arg171_1, 1, buf436, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg171_1
        buf440 = buf382; del buf382  # reuse
        # Topologically Sorted Source Nodes: [linear_105], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf432, arg172_1, buf440, 4096, 4096, stream=stream0)
        del arg172_1
        buf441 = reinterpret_tensor(buf437, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf437  # reuse
        # Topologically Sorted Source Nodes: [mul_77, cat_31, mul_78, q_embed_15], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf440, arg5_1, arg3_1, buf441, 4096, stream=stream0)
        buf442 = buf413; del buf413  # reuse
        # Topologically Sorted Source Nodes: [linear_106, linear_107, index_copy__31], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf432, arg173_1, arg174_1, arg2_1, buf442, arg176_1, 1024, 4096, stream=stream0)
        del arg173_1
        del arg174_1
        del buf432
        # Topologically Sorted Source Nodes: [mul_79, cat_32, mul_80, k_embed_15, index_copy__30], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf442, arg5_1, arg3_1, arg2_1, arg175_1, 1024, stream=stream0)
        buf447 = buf419; del buf419  # reuse
        # Topologically Sorted Source Nodes: [attn_output_60], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg175_1, buf447, 8388608, stream=stream0)
        del arg175_1
        buf448 = buf418; del buf418  # reuse
        # Topologically Sorted Source Nodes: [attn_output_60], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg176_1, buf448, 8388608, stream=stream0)
        del arg176_1
        buf449 = buf420; del buf420  # reuse
        buf478 = buf391; del buf391  # reuse
        buf507 = buf362; del buf362  # reuse
        # Topologically Sorted Source Nodes: [attn_output_60, attn_output_64, attn_output_68], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_6.run(arg4_1, buf449, buf478, buf507, 2048, stream=stream0)
        # Topologically Sorted Source Nodes: [attn_output_60], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf450 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf441, buf447, buf448, reinterpret_tensor(buf449, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf451 = buf450[0]
        assert_size_stride(buf451, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf450
        buf455 = reinterpret_tensor(buf441, (1, 4096), (4096, 1), 0); del buf441  # reuse
        buf457 = buf399; del buf399  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_55, hidden_states_58, hidden_states_59, attn_output_63, hidden_states_62], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_add_mm_14.run(buf457, buf451, arg177_1, buf406, buf427, buf435, 4096, 4096, stream=stream0)
        del arg177_1
        buf458 = buf436; del buf436  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_31], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf455, 4096, reinterpret_tensor(buf457, (1, 4096), (4096, 1), 0), 4096, arg178_1, 1, buf458, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg178_1
        buf461 = reinterpret_tensor(buf451, (1, 4096), (4096, 1), 0); del buf451  # reuse
        buf462 = buf434; del buf434  # reuse
        # Topologically Sorted Source Nodes: [linear_109], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf455, arg179_1, buf462, 14336, 4096, stream=stream0)
        del arg179_1
        buf463 = buf433; del buf433  # reuse
        # Topologically Sorted Source Nodes: [linear_110], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf455, arg180_1, buf463, 14336, 4096, stream=stream0)
        del arg180_1
        buf464 = buf435; del buf435  # reuse
        buf466 = buf455; del buf455  # reuse
        # Topologically Sorted Source Nodes: [down_proj_15, triton_kernel_wrapper_mutation_32], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_11.run(buf462, buf463, arg181_1, buf457, buf464, buf466, 4096, 14336, stream=stream0)
        del arg181_1
        buf465 = buf458; del buf458  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_32], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf461, 4096, buf466, 4096, arg182_1, 1, buf465, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg182_1
        buf469 = buf427; del buf427  # reuse
        # Topologically Sorted Source Nodes: [linear_112], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf461, arg183_1, buf469, 4096, 4096, stream=stream0)
        del arg183_1
        buf470 = reinterpret_tensor(buf466, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf466  # reuse
        # Topologically Sorted Source Nodes: [mul_82, cat_33, mul_83, q_embed_16], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf469, arg5_1, arg3_1, buf470, 4096, stream=stream0)
        buf471 = buf442; del buf442  # reuse
        # Topologically Sorted Source Nodes: [linear_113, linear_114, index_copy__33], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf461, arg184_1, arg185_1, arg2_1, buf471, arg187_1, 1024, 4096, stream=stream0)
        del arg184_1
        del arg185_1
        # Topologically Sorted Source Nodes: [mul_84, cat_34, mul_85, k_embed_16, index_copy__32], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf471, arg5_1, arg3_1, arg2_1, arg186_1, 1024, stream=stream0)
        buf476 = buf448; del buf448  # reuse
        # Topologically Sorted Source Nodes: [attn_output_64], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg186_1, buf476, 8388608, stream=stream0)
        del arg186_1
        buf477 = buf447; del buf447  # reuse
        # Topologically Sorted Source Nodes: [attn_output_64], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg187_1, buf477, 8388608, stream=stream0)
        del arg187_1
        # Topologically Sorted Source Nodes: [attn_output_64], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf479 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf470, buf476, buf477, reinterpret_tensor(buf478, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf480 = buf479[0]
        assert_size_stride(buf480, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf479
        buf484 = reinterpret_tensor(buf470, (1, 4096), (4096, 1), 0); del buf470  # reuse
        buf485 = buf469; del buf469  # reuse
        buf487 = buf461; del buf461  # reuse
        # Topologically Sorted Source Nodes: [attn_output_67, triton_kernel_wrapper_mutation_33], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_12.run(buf480, arg188_1, buf457, buf464, buf485, buf487, 4096, 4096, stream=stream0)
        del arg188_1
        del buf480
        buf486 = buf465; del buf465  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_33], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf484, 4096, buf487, 4096, arg189_1, 1, buf486, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg189_1
        buf490 = buf487; del buf487  # reuse
        buf491 = buf463; del buf463  # reuse
        # Topologically Sorted Source Nodes: [linear_116], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf484, arg190_1, buf491, 14336, 4096, stream=stream0)
        del arg190_1
        buf492 = buf462; del buf462  # reuse
        # Topologically Sorted Source Nodes: [linear_117], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf484, arg191_1, buf492, 14336, 4096, stream=stream0)
        del arg191_1
        buf493 = buf406; del buf406  # reuse
        buf495 = buf484; del buf484  # reuse
        # Topologically Sorted Source Nodes: [down_proj_16, triton_kernel_wrapper_mutation_34], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_13.run(buf491, buf492, arg192_1, buf457, buf464, buf485, buf493, buf495, 4096, 14336, stream=stream0)
        del arg192_1
        buf494 = buf486; del buf486  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_34], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf490, 4096, buf495, 4096, arg193_1, 1, buf494, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg193_1
        buf498 = buf440; del buf440  # reuse
        # Topologically Sorted Source Nodes: [linear_119], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf490, arg194_1, buf498, 4096, 4096, stream=stream0)
        del arg194_1
        buf499 = reinterpret_tensor(buf495, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf495  # reuse
        # Topologically Sorted Source Nodes: [mul_87, cat_35, mul_88, q_embed_17], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf498, arg5_1, arg3_1, buf499, 4096, stream=stream0)
        buf500 = buf471; del buf471  # reuse
        # Topologically Sorted Source Nodes: [linear_120, linear_121, index_copy__35], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf490, arg195_1, arg196_1, arg2_1, buf500, arg198_1, 1024, 4096, stream=stream0)
        del arg195_1
        del arg196_1
        del buf490
        # Topologically Sorted Source Nodes: [mul_89, cat_36, mul_90, k_embed_17, index_copy__34], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf500, arg5_1, arg3_1, arg2_1, arg197_1, 1024, stream=stream0)
        buf505 = buf477; del buf477  # reuse
        # Topologically Sorted Source Nodes: [attn_output_68], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg197_1, buf505, 8388608, stream=stream0)
        del arg197_1
        buf506 = buf476; del buf476  # reuse
        # Topologically Sorted Source Nodes: [attn_output_68], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg198_1, buf506, 8388608, stream=stream0)
        del arg198_1
        # Topologically Sorted Source Nodes: [attn_output_68], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf508 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf499, buf505, buf506, reinterpret_tensor(buf507, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf509 = buf508[0]
        assert_size_stride(buf509, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf508
        buf513 = reinterpret_tensor(buf499, (1, 4096), (4096, 1), 0); del buf499  # reuse
        buf515 = buf457; del buf457  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_63, hidden_states_66, hidden_states_67, attn_output_71, hidden_states_70], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_add_mm_14.run(buf515, buf509, arg199_1, buf464, buf485, buf493, 4096, 4096, stream=stream0)
        del arg199_1
        buf516 = buf494; del buf494  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_35], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf513, 4096, reinterpret_tensor(buf515, (1, 4096), (4096, 1), 0), 4096, arg200_1, 1, buf516, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg200_1
        buf519 = reinterpret_tensor(buf509, (1, 4096), (4096, 1), 0); del buf509  # reuse
        buf520 = buf492; del buf492  # reuse
        # Topologically Sorted Source Nodes: [linear_123], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf513, arg201_1, buf520, 14336, 4096, stream=stream0)
        del arg201_1
        buf521 = buf491; del buf491  # reuse
        # Topologically Sorted Source Nodes: [linear_124], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf513, arg202_1, buf521, 14336, 4096, stream=stream0)
        del arg202_1
        buf522 = buf493; del buf493  # reuse
        buf524 = buf513; del buf513  # reuse
        # Topologically Sorted Source Nodes: [down_proj_17, triton_kernel_wrapper_mutation_36], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_11.run(buf520, buf521, arg203_1, buf515, buf522, buf524, 4096, 14336, stream=stream0)
        del arg203_1
        buf523 = buf516; del buf516  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_36], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf519, 4096, buf524, 4096, arg204_1, 1, buf523, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg204_1
        buf527 = buf485; del buf485  # reuse
        # Topologically Sorted Source Nodes: [linear_126], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf519, arg205_1, buf527, 4096, 4096, stream=stream0)
        del arg205_1
        buf528 = reinterpret_tensor(buf524, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf524  # reuse
        # Topologically Sorted Source Nodes: [mul_92, cat_37, mul_93, q_embed_18], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf527, arg5_1, arg3_1, buf528, 4096, stream=stream0)
        buf529 = buf500; del buf500  # reuse
        # Topologically Sorted Source Nodes: [linear_127, linear_128, index_copy__37], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf519, arg206_1, arg207_1, arg2_1, buf529, arg209_1, 1024, 4096, stream=stream0)
        del arg206_1
        del arg207_1
        # Topologically Sorted Source Nodes: [mul_94, cat_38, mul_95, k_embed_18, index_copy__36], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf529, arg5_1, arg3_1, arg2_1, arg208_1, 1024, stream=stream0)
        buf534 = buf506; del buf506  # reuse
        # Topologically Sorted Source Nodes: [attn_output_72], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg208_1, buf534, 8388608, stream=stream0)
        del arg208_1
        buf535 = buf505; del buf505  # reuse
        # Topologically Sorted Source Nodes: [attn_output_72], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg209_1, buf535, 8388608, stream=stream0)
        del arg209_1
        buf536 = buf507; del buf507  # reuse
        buf565 = buf478; del buf478  # reuse
        buf594 = buf449; del buf449  # reuse
        # Topologically Sorted Source Nodes: [attn_output_72, attn_output_76, attn_output_80], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_6.run(arg4_1, buf536, buf565, buf594, 2048, stream=stream0)
        # Topologically Sorted Source Nodes: [attn_output_72], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf537 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf528, buf534, buf535, reinterpret_tensor(buf536, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf538 = buf537[0]
        assert_size_stride(buf538, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf537
        buf542 = reinterpret_tensor(buf528, (1, 4096), (4096, 1), 0); del buf528  # reuse
        buf543 = buf527; del buf527  # reuse
        buf545 = buf519; del buf519  # reuse
        # Topologically Sorted Source Nodes: [attn_output_75, triton_kernel_wrapper_mutation_37], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_12.run(buf538, arg210_1, buf515, buf522, buf543, buf545, 4096, 4096, stream=stream0)
        del arg210_1
        del buf538
        buf544 = buf523; del buf523  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_37], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf542, 4096, buf545, 4096, arg211_1, 1, buf544, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg211_1
        buf548 = buf545; del buf545  # reuse
        buf549 = buf521; del buf521  # reuse
        # Topologically Sorted Source Nodes: [linear_130], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf542, arg212_1, buf549, 14336, 4096, stream=stream0)
        del arg212_1
        buf550 = buf520; del buf520  # reuse
        # Topologically Sorted Source Nodes: [linear_131], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf542, arg213_1, buf550, 14336, 4096, stream=stream0)
        del arg213_1
        buf551 = buf464; del buf464  # reuse
        buf553 = buf542; del buf542  # reuse
        # Topologically Sorted Source Nodes: [down_proj_18, triton_kernel_wrapper_mutation_38], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_13.run(buf549, buf550, arg214_1, buf515, buf522, buf543, buf551, buf553, 4096, 14336, stream=stream0)
        del arg214_1
        buf552 = buf544; del buf544  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_38], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf548, 4096, buf553, 4096, arg215_1, 1, buf552, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg215_1
        buf556 = buf498; del buf498  # reuse
        # Topologically Sorted Source Nodes: [linear_133], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf548, arg216_1, buf556, 4096, 4096, stream=stream0)
        del arg216_1
        buf557 = reinterpret_tensor(buf553, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf553  # reuse
        # Topologically Sorted Source Nodes: [mul_97, cat_39, mul_98, q_embed_19], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf556, arg5_1, arg3_1, buf557, 4096, stream=stream0)
        buf558 = buf529; del buf529  # reuse
        # Topologically Sorted Source Nodes: [linear_134, linear_135, index_copy__39], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf548, arg217_1, arg218_1, arg2_1, buf558, arg220_1, 1024, 4096, stream=stream0)
        del arg217_1
        del arg218_1
        del buf548
        # Topologically Sorted Source Nodes: [mul_99, cat_40, mul_100, k_embed_19, index_copy__38], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf558, arg5_1, arg3_1, arg2_1, arg219_1, 1024, stream=stream0)
        buf563 = buf535; del buf535  # reuse
        # Topologically Sorted Source Nodes: [attn_output_76], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg219_1, buf563, 8388608, stream=stream0)
        del arg219_1
        buf564 = buf534; del buf534  # reuse
        # Topologically Sorted Source Nodes: [attn_output_76], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg220_1, buf564, 8388608, stream=stream0)
        del arg220_1
        # Topologically Sorted Source Nodes: [attn_output_76], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf566 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf557, buf563, buf564, reinterpret_tensor(buf565, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf567 = buf566[0]
        assert_size_stride(buf567, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf566
        buf571 = reinterpret_tensor(buf557, (1, 4096), (4096, 1), 0); del buf557  # reuse
        buf573 = buf515; del buf515  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_71, hidden_states_74, hidden_states_75, attn_output_79, hidden_states_78], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_add_mm_14.run(buf573, buf567, arg221_1, buf522, buf543, buf551, 4096, 4096, stream=stream0)
        del arg221_1
        buf574 = buf552; del buf552  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_39], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf571, 4096, reinterpret_tensor(buf573, (1, 4096), (4096, 1), 0), 4096, arg222_1, 1, buf574, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg222_1
        buf577 = reinterpret_tensor(buf567, (1, 4096), (4096, 1), 0); del buf567  # reuse
        buf578 = buf550; del buf550  # reuse
        # Topologically Sorted Source Nodes: [linear_137], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf571, arg223_1, buf578, 14336, 4096, stream=stream0)
        del arg223_1
        buf579 = buf549; del buf549  # reuse
        # Topologically Sorted Source Nodes: [linear_138], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf571, arg224_1, buf579, 14336, 4096, stream=stream0)
        del arg224_1
        buf580 = buf551; del buf551  # reuse
        buf582 = buf571; del buf571  # reuse
        # Topologically Sorted Source Nodes: [down_proj_19, triton_kernel_wrapper_mutation_40], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_11.run(buf578, buf579, arg225_1, buf573, buf580, buf582, 4096, 14336, stream=stream0)
        del arg225_1
        buf581 = buf574; del buf574  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_40], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf577, 4096, buf582, 4096, arg226_1, 1, buf581, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg226_1
        buf585 = buf543; del buf543  # reuse
        # Topologically Sorted Source Nodes: [linear_140], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf577, arg227_1, buf585, 4096, 4096, stream=stream0)
        del arg227_1
        buf586 = reinterpret_tensor(buf582, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf582  # reuse
        # Topologically Sorted Source Nodes: [mul_102, cat_41, mul_103, q_embed_20], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf585, arg5_1, arg3_1, buf586, 4096, stream=stream0)
        buf587 = buf558; del buf558  # reuse
        # Topologically Sorted Source Nodes: [linear_141, linear_142, index_copy__41], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf577, arg228_1, arg229_1, arg2_1, buf587, arg231_1, 1024, 4096, stream=stream0)
        del arg228_1
        del arg229_1
        # Topologically Sorted Source Nodes: [mul_104, cat_42, mul_105, k_embed_20, index_copy__40], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf587, arg5_1, arg3_1, arg2_1, arg230_1, 1024, stream=stream0)
        buf592 = buf564; del buf564  # reuse
        # Topologically Sorted Source Nodes: [attn_output_80], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg230_1, buf592, 8388608, stream=stream0)
        del arg230_1
        buf593 = buf563; del buf563  # reuse
        # Topologically Sorted Source Nodes: [attn_output_80], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg231_1, buf593, 8388608, stream=stream0)
        del arg231_1
        # Topologically Sorted Source Nodes: [attn_output_80], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf595 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf586, buf592, buf593, reinterpret_tensor(buf594, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf596 = buf595[0]
        assert_size_stride(buf596, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf595
        buf600 = reinterpret_tensor(buf586, (1, 4096), (4096, 1), 0); del buf586  # reuse
        buf601 = buf585; del buf585  # reuse
        buf603 = buf577; del buf577  # reuse
        # Topologically Sorted Source Nodes: [attn_output_83, triton_kernel_wrapper_mutation_41], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_12.run(buf596, arg232_1, buf573, buf580, buf601, buf603, 4096, 4096, stream=stream0)
        del arg232_1
        del buf596
        buf602 = buf581; del buf581  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_41], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf600, 4096, buf603, 4096, arg233_1, 1, buf602, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg233_1
        buf606 = buf603; del buf603  # reuse
        buf607 = buf579; del buf579  # reuse
        # Topologically Sorted Source Nodes: [linear_144], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf600, arg234_1, buf607, 14336, 4096, stream=stream0)
        del arg234_1
        buf608 = buf578; del buf578  # reuse
        # Topologically Sorted Source Nodes: [linear_145], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf600, arg235_1, buf608, 14336, 4096, stream=stream0)
        del arg235_1
        buf609 = buf522; del buf522  # reuse
        buf611 = buf600; del buf600  # reuse
        # Topologically Sorted Source Nodes: [down_proj_20, triton_kernel_wrapper_mutation_42], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_13.run(buf607, buf608, arg236_1, buf573, buf580, buf601, buf609, buf611, 4096, 14336, stream=stream0)
        del arg236_1
        buf610 = buf602; del buf602  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_42], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf606, 4096, buf611, 4096, arg237_1, 1, buf610, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg237_1
        buf614 = buf556; del buf556  # reuse
        # Topologically Sorted Source Nodes: [linear_147], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf606, arg238_1, buf614, 4096, 4096, stream=stream0)
        del arg238_1
        buf615 = reinterpret_tensor(buf611, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf611  # reuse
        # Topologically Sorted Source Nodes: [mul_107, cat_43, mul_108, q_embed_21], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf614, arg5_1, arg3_1, buf615, 4096, stream=stream0)
        buf616 = buf587; del buf587  # reuse
        # Topologically Sorted Source Nodes: [linear_148, linear_149, index_copy__43], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf606, arg239_1, arg240_1, arg2_1, buf616, arg242_1, 1024, 4096, stream=stream0)
        del arg239_1
        del arg240_1
        del buf606
        # Topologically Sorted Source Nodes: [mul_109, cat_44, mul_110, k_embed_21, index_copy__42], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf616, arg5_1, arg3_1, arg2_1, arg241_1, 1024, stream=stream0)
        buf621 = buf593; del buf593  # reuse
        # Topologically Sorted Source Nodes: [attn_output_84], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg241_1, buf621, 8388608, stream=stream0)
        del arg241_1
        buf622 = buf592; del buf592  # reuse
        # Topologically Sorted Source Nodes: [attn_output_84], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg242_1, buf622, 8388608, stream=stream0)
        del arg242_1
        buf623 = buf594; del buf594  # reuse
        buf652 = buf565; del buf565  # reuse
        buf681 = buf536; del buf536  # reuse
        # Topologically Sorted Source Nodes: [attn_output_84, attn_output_88, attn_output_92], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_6.run(arg4_1, buf623, buf652, buf681, 2048, stream=stream0)
        # Topologically Sorted Source Nodes: [attn_output_84], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf624 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf615, buf621, buf622, reinterpret_tensor(buf623, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf625 = buf624[0]
        assert_size_stride(buf625, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf624
        buf629 = reinterpret_tensor(buf615, (1, 4096), (4096, 1), 0); del buf615  # reuse
        buf631 = buf573; del buf573  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_79, hidden_states_82, hidden_states_83, attn_output_87, hidden_states_86], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_add_mm_14.run(buf631, buf625, arg243_1, buf580, buf601, buf609, 4096, 4096, stream=stream0)
        del arg243_1
        buf632 = buf610; del buf610  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_43], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf629, 4096, reinterpret_tensor(buf631, (1, 4096), (4096, 1), 0), 4096, arg244_1, 1, buf632, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg244_1
        buf635 = reinterpret_tensor(buf625, (1, 4096), (4096, 1), 0); del buf625  # reuse
        buf636 = buf608; del buf608  # reuse
        # Topologically Sorted Source Nodes: [linear_151], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf629, arg245_1, buf636, 14336, 4096, stream=stream0)
        del arg245_1
        buf637 = buf607; del buf607  # reuse
        # Topologically Sorted Source Nodes: [linear_152], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf629, arg246_1, buf637, 14336, 4096, stream=stream0)
        del arg246_1
        buf638 = buf609; del buf609  # reuse
        buf640 = buf629; del buf629  # reuse
        # Topologically Sorted Source Nodes: [down_proj_21, triton_kernel_wrapper_mutation_44], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_11.run(buf636, buf637, arg247_1, buf631, buf638, buf640, 4096, 14336, stream=stream0)
        del arg247_1
        buf639 = buf632; del buf632  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_44], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf635, 4096, buf640, 4096, arg248_1, 1, buf639, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg248_1
        buf643 = buf601; del buf601  # reuse
        # Topologically Sorted Source Nodes: [linear_154], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf635, arg249_1, buf643, 4096, 4096, stream=stream0)
        del arg249_1
        buf644 = reinterpret_tensor(buf640, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf640  # reuse
        # Topologically Sorted Source Nodes: [mul_112, cat_45, mul_113, q_embed_22], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf643, arg5_1, arg3_1, buf644, 4096, stream=stream0)
        buf645 = buf616; del buf616  # reuse
        # Topologically Sorted Source Nodes: [linear_155, linear_156, index_copy__45], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf635, arg250_1, arg251_1, arg2_1, buf645, arg253_1, 1024, 4096, stream=stream0)
        del arg250_1
        del arg251_1
        # Topologically Sorted Source Nodes: [mul_114, cat_46, mul_115, k_embed_22, index_copy__44], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf645, arg5_1, arg3_1, arg2_1, arg252_1, 1024, stream=stream0)
        buf650 = buf622; del buf622  # reuse
        # Topologically Sorted Source Nodes: [attn_output_88], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg252_1, buf650, 8388608, stream=stream0)
        del arg252_1
        buf651 = buf621; del buf621  # reuse
        # Topologically Sorted Source Nodes: [attn_output_88], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg253_1, buf651, 8388608, stream=stream0)
        del arg253_1
        # Topologically Sorted Source Nodes: [attn_output_88], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf653 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf644, buf650, buf651, reinterpret_tensor(buf652, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf654 = buf653[0]
        assert_size_stride(buf654, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf653
        buf658 = reinterpret_tensor(buf644, (1, 4096), (4096, 1), 0); del buf644  # reuse
        buf659 = buf643; del buf643  # reuse
        buf661 = buf635; del buf635  # reuse
        # Topologically Sorted Source Nodes: [attn_output_91, triton_kernel_wrapper_mutation_45], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_12.run(buf654, arg254_1, buf631, buf638, buf659, buf661, 4096, 4096, stream=stream0)
        del arg254_1
        del buf654
        buf660 = buf639; del buf639  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_45], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf658, 4096, buf661, 4096, arg255_1, 1, buf660, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg255_1
        buf664 = buf661; del buf661  # reuse
        buf665 = buf637; del buf637  # reuse
        # Topologically Sorted Source Nodes: [linear_158], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf658, arg256_1, buf665, 14336, 4096, stream=stream0)
        del arg256_1
        buf666 = buf636; del buf636  # reuse
        # Topologically Sorted Source Nodes: [linear_159], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf658, arg257_1, buf666, 14336, 4096, stream=stream0)
        del arg257_1
        buf667 = buf580; del buf580  # reuse
        buf669 = buf658; del buf658  # reuse
        # Topologically Sorted Source Nodes: [down_proj_22, triton_kernel_wrapper_mutation_46], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_13.run(buf665, buf666, arg258_1, buf631, buf638, buf659, buf667, buf669, 4096, 14336, stream=stream0)
        del arg258_1
        buf668 = buf660; del buf660  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_46], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf664, 4096, buf669, 4096, arg259_1, 1, buf668, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg259_1
        buf672 = buf614; del buf614  # reuse
        # Topologically Sorted Source Nodes: [linear_161], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf664, arg260_1, buf672, 4096, 4096, stream=stream0)
        del arg260_1
        buf673 = reinterpret_tensor(buf669, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf669  # reuse
        # Topologically Sorted Source Nodes: [mul_117, cat_47, mul_118, q_embed_23], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf672, arg5_1, arg3_1, buf673, 4096, stream=stream0)
        buf674 = buf645; del buf645  # reuse
        # Topologically Sorted Source Nodes: [linear_162, linear_163, index_copy__47], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf664, arg261_1, arg262_1, arg2_1, buf674, arg264_1, 1024, 4096, stream=stream0)
        del arg261_1
        del arg262_1
        del buf664
        # Topologically Sorted Source Nodes: [mul_119, cat_48, mul_120, k_embed_23, index_copy__46], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf674, arg5_1, arg3_1, arg2_1, arg263_1, 1024, stream=stream0)
        buf679 = buf651; del buf651  # reuse
        # Topologically Sorted Source Nodes: [attn_output_92], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg263_1, buf679, 8388608, stream=stream0)
        del arg263_1
        buf680 = buf650; del buf650  # reuse
        # Topologically Sorted Source Nodes: [attn_output_92], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg264_1, buf680, 8388608, stream=stream0)
        del arg264_1
        # Topologically Sorted Source Nodes: [attn_output_92], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf682 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf673, buf679, buf680, reinterpret_tensor(buf681, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf683 = buf682[0]
        assert_size_stride(buf683, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf682
        buf687 = reinterpret_tensor(buf673, (1, 4096), (4096, 1), 0); del buf673  # reuse
        buf689 = buf631; del buf631  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_87, hidden_states_90, hidden_states_91, attn_output_95, hidden_states_94], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_add_mm_14.run(buf689, buf683, arg265_1, buf638, buf659, buf667, 4096, 4096, stream=stream0)
        del arg265_1
        buf690 = buf668; del buf668  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_47], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf687, 4096, reinterpret_tensor(buf689, (1, 4096), (4096, 1), 0), 4096, arg266_1, 1, buf690, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg266_1
        buf693 = reinterpret_tensor(buf683, (1, 4096), (4096, 1), 0); del buf683  # reuse
        buf694 = buf666; del buf666  # reuse
        # Topologically Sorted Source Nodes: [linear_165], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf687, arg267_1, buf694, 14336, 4096, stream=stream0)
        del arg267_1
        buf695 = buf665; del buf665  # reuse
        # Topologically Sorted Source Nodes: [linear_166], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf687, arg268_1, buf695, 14336, 4096, stream=stream0)
        del arg268_1
        buf696 = buf667; del buf667  # reuse
        buf698 = buf687; del buf687  # reuse
        # Topologically Sorted Source Nodes: [down_proj_23, triton_kernel_wrapper_mutation_48], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_11.run(buf694, buf695, arg269_1, buf689, buf696, buf698, 4096, 14336, stream=stream0)
        del arg269_1
        buf697 = buf690; del buf690  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_48], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf693, 4096, buf698, 4096, arg270_1, 1, buf697, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg270_1
        buf701 = buf659; del buf659  # reuse
        # Topologically Sorted Source Nodes: [linear_168], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf693, arg271_1, buf701, 4096, 4096, stream=stream0)
        del arg271_1
        buf702 = reinterpret_tensor(buf698, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf698  # reuse
        # Topologically Sorted Source Nodes: [mul_122, cat_49, mul_123, q_embed_24], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf701, arg5_1, arg3_1, buf702, 4096, stream=stream0)
        buf703 = buf674; del buf674  # reuse
        # Topologically Sorted Source Nodes: [linear_169, linear_170, index_copy__49], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf693, arg272_1, arg273_1, arg2_1, buf703, arg275_1, 1024, 4096, stream=stream0)
        del arg272_1
        del arg273_1
        # Topologically Sorted Source Nodes: [mul_124, cat_50, mul_125, k_embed_24, index_copy__48], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf703, arg5_1, arg3_1, arg2_1, arg274_1, 1024, stream=stream0)
        buf708 = buf680; del buf680  # reuse
        # Topologically Sorted Source Nodes: [attn_output_96], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg274_1, buf708, 8388608, stream=stream0)
        del arg274_1
        buf709 = buf679; del buf679  # reuse
        # Topologically Sorted Source Nodes: [attn_output_96], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg275_1, buf709, 8388608, stream=stream0)
        del arg275_1
        buf710 = buf681; del buf681  # reuse
        buf739 = buf652; del buf652  # reuse
        buf768 = buf623; del buf623  # reuse
        # Topologically Sorted Source Nodes: [attn_output_96, attn_output_100, attn_output_104], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_6.run(arg4_1, buf710, buf739, buf768, 2048, stream=stream0)
        # Topologically Sorted Source Nodes: [attn_output_96], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf711 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf702, buf708, buf709, reinterpret_tensor(buf710, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf712 = buf711[0]
        assert_size_stride(buf712, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf711
        buf716 = reinterpret_tensor(buf702, (1, 4096), (4096, 1), 0); del buf702  # reuse
        buf717 = buf701; del buf701  # reuse
        buf719 = buf693; del buf693  # reuse
        # Topologically Sorted Source Nodes: [attn_output_99, triton_kernel_wrapper_mutation_49], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_12.run(buf712, arg276_1, buf689, buf696, buf717, buf719, 4096, 4096, stream=stream0)
        del arg276_1
        del buf712
        buf718 = buf697; del buf697  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_49], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf716, 4096, buf719, 4096, arg277_1, 1, buf718, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg277_1
        buf722 = buf719; del buf719  # reuse
        buf723 = buf695; del buf695  # reuse
        # Topologically Sorted Source Nodes: [linear_172], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf716, arg278_1, buf723, 14336, 4096, stream=stream0)
        del arg278_1
        buf724 = buf694; del buf694  # reuse
        # Topologically Sorted Source Nodes: [linear_173], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf716, arg279_1, buf724, 14336, 4096, stream=stream0)
        del arg279_1
        buf725 = buf638; del buf638  # reuse
        buf727 = buf716; del buf716  # reuse
        # Topologically Sorted Source Nodes: [down_proj_24, triton_kernel_wrapper_mutation_50], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_13.run(buf723, buf724, arg280_1, buf689, buf696, buf717, buf725, buf727, 4096, 14336, stream=stream0)
        del arg280_1
        buf726 = buf718; del buf718  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_50], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf722, 4096, buf727, 4096, arg281_1, 1, buf726, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg281_1
        buf730 = buf672; del buf672  # reuse
        # Topologically Sorted Source Nodes: [linear_175], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf722, arg282_1, buf730, 4096, 4096, stream=stream0)
        del arg282_1
        buf731 = reinterpret_tensor(buf727, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf727  # reuse
        # Topologically Sorted Source Nodes: [mul_127, cat_51, mul_128, q_embed_25], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf730, arg5_1, arg3_1, buf731, 4096, stream=stream0)
        buf732 = buf703; del buf703  # reuse
        # Topologically Sorted Source Nodes: [linear_176, linear_177, index_copy__51], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf722, arg283_1, arg284_1, arg2_1, buf732, arg286_1, 1024, 4096, stream=stream0)
        del arg283_1
        del arg284_1
        del buf722
        # Topologically Sorted Source Nodes: [mul_129, cat_52, mul_130, k_embed_25, index_copy__50], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf732, arg5_1, arg3_1, arg2_1, arg285_1, 1024, stream=stream0)
        buf737 = buf709; del buf709  # reuse
        # Topologically Sorted Source Nodes: [attn_output_100], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg285_1, buf737, 8388608, stream=stream0)
        del arg285_1
        buf738 = buf708; del buf708  # reuse
        # Topologically Sorted Source Nodes: [attn_output_100], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg286_1, buf738, 8388608, stream=stream0)
        del arg286_1
        # Topologically Sorted Source Nodes: [attn_output_100], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf740 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf731, buf737, buf738, reinterpret_tensor(buf739, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf741 = buf740[0]
        assert_size_stride(buf741, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf740
        buf745 = reinterpret_tensor(buf731, (1, 4096), (4096, 1), 0); del buf731  # reuse
        buf747 = buf689; del buf689  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_95, hidden_states_98, hidden_states_99, attn_output_103, hidden_states_102], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_add_mm_14.run(buf747, buf741, arg287_1, buf696, buf717, buf725, 4096, 4096, stream=stream0)
        del arg287_1
        buf748 = buf726; del buf726  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_51], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf745, 4096, reinterpret_tensor(buf747, (1, 4096), (4096, 1), 0), 4096, arg288_1, 1, buf748, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg288_1
        buf751 = reinterpret_tensor(buf741, (1, 4096), (4096, 1), 0); del buf741  # reuse
        buf752 = buf724; del buf724  # reuse
        # Topologically Sorted Source Nodes: [linear_179], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf745, arg289_1, buf752, 14336, 4096, stream=stream0)
        del arg289_1
        buf753 = buf723; del buf723  # reuse
        # Topologically Sorted Source Nodes: [linear_180], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf745, arg290_1, buf753, 14336, 4096, stream=stream0)
        del arg290_1
        buf754 = buf725; del buf725  # reuse
        buf756 = buf745; del buf745  # reuse
        # Topologically Sorted Source Nodes: [down_proj_25, triton_kernel_wrapper_mutation_52], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_11.run(buf752, buf753, arg291_1, buf747, buf754, buf756, 4096, 14336, stream=stream0)
        del arg291_1
        buf755 = buf748; del buf748  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_52], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf751, 4096, buf756, 4096, arg292_1, 1, buf755, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg292_1
        buf759 = buf717; del buf717  # reuse
        # Topologically Sorted Source Nodes: [linear_182], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf751, arg293_1, buf759, 4096, 4096, stream=stream0)
        del arg293_1
        buf760 = reinterpret_tensor(buf756, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf756  # reuse
        # Topologically Sorted Source Nodes: [mul_132, cat_53, mul_133, q_embed_26], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf759, arg5_1, arg3_1, buf760, 4096, stream=stream0)
        buf761 = buf732; del buf732  # reuse
        # Topologically Sorted Source Nodes: [linear_183, linear_184, index_copy__53], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf751, arg294_1, arg295_1, arg2_1, buf761, arg297_1, 1024, 4096, stream=stream0)
        del arg294_1
        del arg295_1
        # Topologically Sorted Source Nodes: [mul_134, cat_54, mul_135, k_embed_26, index_copy__52], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf761, arg5_1, arg3_1, arg2_1, arg296_1, 1024, stream=stream0)
        buf766 = buf738; del buf738  # reuse
        # Topologically Sorted Source Nodes: [attn_output_104], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg296_1, buf766, 8388608, stream=stream0)
        del arg296_1
        buf767 = buf737; del buf737  # reuse
        # Topologically Sorted Source Nodes: [attn_output_104], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg297_1, buf767, 8388608, stream=stream0)
        del arg297_1
        # Topologically Sorted Source Nodes: [attn_output_104], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf769 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf760, buf766, buf767, reinterpret_tensor(buf768, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf770 = buf769[0]
        assert_size_stride(buf770, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf769
        buf774 = reinterpret_tensor(buf760, (1, 4096), (4096, 1), 0); del buf760  # reuse
        buf775 = buf759; del buf759  # reuse
        buf777 = buf751; del buf751  # reuse
        # Topologically Sorted Source Nodes: [attn_output_107, triton_kernel_wrapper_mutation_53], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_12.run(buf770, arg298_1, buf747, buf754, buf775, buf777, 4096, 4096, stream=stream0)
        del arg298_1
        del buf770
        buf776 = buf755; del buf755  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_53], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf774, 4096, buf777, 4096, arg299_1, 1, buf776, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg299_1
        buf780 = buf777; del buf777  # reuse
        buf781 = buf753; del buf753  # reuse
        # Topologically Sorted Source Nodes: [linear_186], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf774, arg300_1, buf781, 14336, 4096, stream=stream0)
        del arg300_1
        buf782 = buf752; del buf752  # reuse
        # Topologically Sorted Source Nodes: [linear_187], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf774, arg301_1, buf782, 14336, 4096, stream=stream0)
        del arg301_1
        buf783 = buf696; del buf696  # reuse
        buf785 = buf774; del buf774  # reuse
        # Topologically Sorted Source Nodes: [down_proj_26, triton_kernel_wrapper_mutation_54], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_13.run(buf781, buf782, arg302_1, buf747, buf754, buf775, buf783, buf785, 4096, 14336, stream=stream0)
        del arg302_1
        buf784 = buf776; del buf776  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_54], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf780, 4096, buf785, 4096, arg303_1, 1, buf784, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg303_1
        buf788 = buf730; del buf730  # reuse
        # Topologically Sorted Source Nodes: [linear_189], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf780, arg304_1, buf788, 4096, 4096, stream=stream0)
        del arg304_1
        buf789 = reinterpret_tensor(buf785, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf785  # reuse
        # Topologically Sorted Source Nodes: [mul_137, cat_55, mul_138, q_embed_27], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf788, arg5_1, arg3_1, buf789, 4096, stream=stream0)
        buf790 = buf761; del buf761  # reuse
        # Topologically Sorted Source Nodes: [linear_190, linear_191, index_copy__55], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf780, arg305_1, arg306_1, arg2_1, buf790, arg308_1, 1024, 4096, stream=stream0)
        del arg305_1
        del arg306_1
        del buf780
        # Topologically Sorted Source Nodes: [mul_139, cat_56, mul_140, k_embed_27, index_copy__54], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf790, arg5_1, arg3_1, arg2_1, arg307_1, 1024, stream=stream0)
        buf795 = buf767; del buf767  # reuse
        # Topologically Sorted Source Nodes: [attn_output_108], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg307_1, buf795, 8388608, stream=stream0)
        del arg307_1
        buf796 = buf766; del buf766  # reuse
        # Topologically Sorted Source Nodes: [attn_output_108], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg308_1, buf796, 8388608, stream=stream0)
        del arg308_1
        buf797 = buf768; del buf768  # reuse
        buf826 = buf739; del buf739  # reuse
        buf855 = buf710; del buf710  # reuse
        # Topologically Sorted Source Nodes: [attn_output_108, attn_output_112, attn_output_116], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_6.run(arg4_1, buf797, buf826, buf855, 2048, stream=stream0)
        # Topologically Sorted Source Nodes: [attn_output_108], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf798 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf789, buf795, buf796, reinterpret_tensor(buf797, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        del buf797
        buf799 = buf798[0]
        assert_size_stride(buf799, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf798
        buf803 = reinterpret_tensor(buf789, (1, 4096), (4096, 1), 0); del buf789  # reuse
        buf805 = buf747; del buf747  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_103, hidden_states_106, hidden_states_107, attn_output_111, hidden_states_110], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_add_mm_14.run(buf805, buf799, arg309_1, buf754, buf775, buf783, 4096, 4096, stream=stream0)
        del arg309_1
        buf806 = buf784; del buf784  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_55], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf803, 4096, reinterpret_tensor(buf805, (1, 4096), (4096, 1), 0), 4096, arg310_1, 1, buf806, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg310_1
        buf809 = reinterpret_tensor(buf799, (1, 4096), (4096, 1), 0); del buf799  # reuse
        buf810 = buf782; del buf782  # reuse
        # Topologically Sorted Source Nodes: [linear_193], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf803, arg311_1, buf810, 14336, 4096, stream=stream0)
        del arg311_1
        buf811 = buf781; del buf781  # reuse
        # Topologically Sorted Source Nodes: [linear_194], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf803, arg312_1, buf811, 14336, 4096, stream=stream0)
        del arg312_1
        buf812 = buf783; del buf783  # reuse
        buf814 = buf803; del buf803  # reuse
        # Topologically Sorted Source Nodes: [down_proj_27, triton_kernel_wrapper_mutation_56], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_11.run(buf810, buf811, arg313_1, buf805, buf812, buf814, 4096, 14336, stream=stream0)
        del arg313_1
        buf813 = buf806; del buf806  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_56], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf809, 4096, buf814, 4096, arg314_1, 1, buf813, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg314_1
        buf817 = buf775; del buf775  # reuse
        # Topologically Sorted Source Nodes: [linear_196], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf809, arg315_1, buf817, 4096, 4096, stream=stream0)
        del arg315_1
        buf818 = reinterpret_tensor(buf814, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf814  # reuse
        # Topologically Sorted Source Nodes: [mul_142, cat_57, mul_143, q_embed_28], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf817, arg5_1, arg3_1, buf818, 4096, stream=stream0)
        buf819 = buf790; del buf790  # reuse
        # Topologically Sorted Source Nodes: [linear_197, linear_198, index_copy__57], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf809, arg316_1, arg317_1, arg2_1, buf819, arg319_1, 1024, 4096, stream=stream0)
        del arg316_1
        del arg317_1
        # Topologically Sorted Source Nodes: [mul_144, cat_58, mul_145, k_embed_28, index_copy__56], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf819, arg5_1, arg3_1, arg2_1, arg318_1, 1024, stream=stream0)
        buf824 = buf796; del buf796  # reuse
        # Topologically Sorted Source Nodes: [attn_output_112], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg318_1, buf824, 8388608, stream=stream0)
        del arg318_1
        buf825 = buf795; del buf795  # reuse
        # Topologically Sorted Source Nodes: [attn_output_112], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg319_1, buf825, 8388608, stream=stream0)
        del arg319_1
        # Topologically Sorted Source Nodes: [attn_output_112], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf827 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf818, buf824, buf825, reinterpret_tensor(buf826, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf828 = buf827[0]
        assert_size_stride(buf828, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf827
        buf832 = reinterpret_tensor(buf818, (1, 4096), (4096, 1), 0); del buf818  # reuse
        buf833 = buf817; del buf817  # reuse
        buf835 = buf809; del buf809  # reuse
        # Topologically Sorted Source Nodes: [attn_output_115, triton_kernel_wrapper_mutation_57], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_12.run(buf828, arg320_1, buf805, buf812, buf833, buf835, 4096, 4096, stream=stream0)
        del arg320_1
        del buf828
        buf834 = buf813; del buf813  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_57], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf832, 4096, buf835, 4096, arg321_1, 1, buf834, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg321_1
        buf838 = buf835; del buf835  # reuse
        buf839 = buf811; del buf811  # reuse
        # Topologically Sorted Source Nodes: [linear_200], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf832, arg322_1, buf839, 14336, 4096, stream=stream0)
        del arg322_1
        buf840 = buf810; del buf810  # reuse
        # Topologically Sorted Source Nodes: [linear_201], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf832, arg323_1, buf840, 14336, 4096, stream=stream0)
        del arg323_1
        buf841 = buf754; del buf754  # reuse
        buf843 = buf832; del buf832  # reuse
        # Topologically Sorted Source Nodes: [down_proj_28, triton_kernel_wrapper_mutation_58], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_13.run(buf839, buf840, arg324_1, buf805, buf812, buf833, buf841, buf843, 4096, 14336, stream=stream0)
        del arg324_1
        buf842 = buf834; del buf834  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_58], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf838, 4096, buf843, 4096, arg325_1, 1, buf842, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg325_1
        buf846 = buf788; del buf788  # reuse
        # Topologically Sorted Source Nodes: [linear_203], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf838, arg326_1, buf846, 4096, 4096, stream=stream0)
        del arg326_1
        buf847 = reinterpret_tensor(buf843, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf843  # reuse
        # Topologically Sorted Source Nodes: [mul_147, cat_59, mul_148, q_embed_29], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf846, arg5_1, arg3_1, buf847, 4096, stream=stream0)
        buf848 = buf819; del buf819  # reuse
        # Topologically Sorted Source Nodes: [linear_204, linear_205, index_copy__59], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf838, arg327_1, arg328_1, arg2_1, buf848, arg330_1, 1024, 4096, stream=stream0)
        del arg327_1
        del arg328_1
        del buf838
        # Topologically Sorted Source Nodes: [mul_149, cat_60, mul_150, k_embed_29, index_copy__58], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf848, arg5_1, arg3_1, arg2_1, arg329_1, 1024, stream=stream0)
        buf853 = buf825; del buf825  # reuse
        # Topologically Sorted Source Nodes: [attn_output_116], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg329_1, buf853, 8388608, stream=stream0)
        del arg329_1
        buf854 = buf824; del buf824  # reuse
        # Topologically Sorted Source Nodes: [attn_output_116], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg330_1, buf854, 8388608, stream=stream0)
        del arg330_1
        # Topologically Sorted Source Nodes: [attn_output_116], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf856 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf847, buf853, buf854, reinterpret_tensor(buf855, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        buf857 = buf856[0]
        assert_size_stride(buf857, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf856
        buf861 = reinterpret_tensor(buf847, (1, 4096), (4096, 1), 0); del buf847  # reuse
        buf863 = buf805; del buf805  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_111, hidden_states_114, hidden_states_115, attn_output_119, hidden_states_118], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_add_mm_14.run(buf863, buf857, arg331_1, buf812, buf833, buf841, 4096, 4096, stream=stream0)
        del arg331_1
        buf864 = buf842; del buf842  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_59], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf861, 4096, reinterpret_tensor(buf863, (1, 4096), (4096, 1), 0), 4096, arg332_1, 1, buf864, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg332_1
        buf867 = reinterpret_tensor(buf857, (1, 4096), (4096, 1), 0); del buf857  # reuse
        buf868 = buf840; del buf840  # reuse
        # Topologically Sorted Source Nodes: [linear_207], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf861, arg333_1, buf868, 14336, 4096, stream=stream0)
        del arg333_1
        buf869 = buf839; del buf839  # reuse
        # Topologically Sorted Source Nodes: [linear_208], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf861, arg334_1, buf869, 14336, 4096, stream=stream0)
        del arg334_1
        buf870 = buf841; del buf841  # reuse
        buf872 = buf861; del buf861  # reuse
        # Topologically Sorted Source Nodes: [down_proj_29, triton_kernel_wrapper_mutation_60], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_11.run(buf868, buf869, arg335_1, buf863, buf870, buf872, 4096, 14336, stream=stream0)
        del arg335_1
        buf871 = buf864; del buf864  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_60], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf867, 4096, buf872, 4096, arg336_1, 1, buf871, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg336_1
        buf875 = buf833; del buf833  # reuse
        # Topologically Sorted Source Nodes: [linear_210], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf867, arg337_1, buf875, 4096, 4096, stream=stream0)
        del arg337_1
        buf876 = reinterpret_tensor(buf872, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf872  # reuse
        # Topologically Sorted Source Nodes: [mul_152, cat_61, mul_153, q_embed_30], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf875, arg5_1, arg3_1, buf876, 4096, stream=stream0)
        buf877 = buf848; del buf848  # reuse
        # Topologically Sorted Source Nodes: [linear_211, linear_212, index_copy__61], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf867, arg338_1, arg339_1, arg2_1, buf877, arg341_1, 1024, 4096, stream=stream0)
        del arg338_1
        del arg339_1
        # Topologically Sorted Source Nodes: [mul_154, cat_62, mul_155, k_embed_30, index_copy__60], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf877, arg5_1, arg3_1, arg2_1, arg340_1, 1024, stream=stream0)
        buf882 = buf854; del buf854  # reuse
        # Topologically Sorted Source Nodes: [attn_output_120], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg340_1, buf882, 8388608, stream=stream0)
        del arg340_1
        buf883 = buf853; del buf853  # reuse
        # Topologically Sorted Source Nodes: [attn_output_120], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg341_1, buf883, 8388608, stream=stream0)
        del arg341_1
        buf884 = buf855; del buf855  # reuse
        buf913 = buf826; del buf826  # reuse
        # Topologically Sorted Source Nodes: [attn_output_120, attn_output_124], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_15.run(arg4_1, buf884, buf913, 2048, stream=stream0)
        del arg4_1
        # Topologically Sorted Source Nodes: [attn_output_120], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf885 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf876, buf882, buf883, reinterpret_tensor(buf884, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        del buf884
        buf886 = buf885[0]
        assert_size_stride(buf886, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf885
        buf890 = reinterpret_tensor(buf876, (1, 4096), (4096, 1), 0); del buf876  # reuse
        buf891 = buf875; del buf875  # reuse
        buf893 = buf867; del buf867  # reuse
        # Topologically Sorted Source Nodes: [attn_output_123, triton_kernel_wrapper_mutation_61], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_12.run(buf886, arg342_1, buf863, buf870, buf891, buf893, 4096, 4096, stream=stream0)
        del arg342_1
        del buf886
        buf892 = buf871; del buf871  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_61], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf890, 4096, buf893, 4096, arg343_1, 1, buf892, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg343_1
        buf896 = buf893; del buf893  # reuse
        buf897 = buf869; del buf869  # reuse
        # Topologically Sorted Source Nodes: [linear_214], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf890, arg344_1, buf897, 14336, 4096, stream=stream0)
        del arg344_1
        buf898 = buf868; del buf868  # reuse
        # Topologically Sorted Source Nodes: [linear_215], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf890, arg345_1, buf898, 14336, 4096, stream=stream0)
        del arg345_1
        buf899 = buf812; del buf812  # reuse
        buf901 = buf890; del buf890  # reuse
        # Topologically Sorted Source Nodes: [down_proj_30, triton_kernel_wrapper_mutation_62], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_13.run(buf897, buf898, arg346_1, buf863, buf870, buf891, buf899, buf901, 4096, 14336, stream=stream0)
        del arg346_1
        buf900 = buf892; del buf892  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_62], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf896, 4096, buf901, 4096, arg347_1, 1, buf900, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg347_1
        buf904 = buf846; del buf846  # reuse
        # Topologically Sorted Source Nodes: [linear_217], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_1.run(buf896, arg348_1, buf904, 4096, 4096, stream=stream0)
        del arg348_1
        buf905 = reinterpret_tensor(buf901, (1, 32, 1, 128), (4096, 128, 4096, 1), 0); del buf901  # reuse
        # Topologically Sorted Source Nodes: [mul_157, cat_63, mul_158, q_embed_31], Original ATen: [aten.mul, aten.cat, aten.add]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_mul_2.run(buf904, arg5_1, arg3_1, buf905, 4096, stream=stream0)
        del buf904
        buf906 = buf877; del buf877  # reuse
        # Topologically Sorted Source Nodes: [linear_218, linear_219, index_copy__63], Original ATen: [aten.mm, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_red_fused_index_copy_mm_3.run(buf896, arg349_1, arg350_1, arg2_1, buf906, arg352_1, 1024, 4096, stream=stream0)
        del arg349_1
        del arg350_1
        del buf896
        # Topologically Sorted Source Nodes: [mul_159, cat_64, mul_160, k_embed_31, index_copy__62], Original ATen: [aten.mul, aten.cat, aten.add, aten.index_copy]
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_index_copy_mul_4.run(buf906, arg5_1, arg3_1, arg2_1, arg351_1, 1024, stream=stream0)
        del arg2_1
        del arg3_1
        del arg5_1
        del buf906
        buf911 = buf883; del buf883  # reuse
        # Topologically Sorted Source Nodes: [attn_output_124], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg351_1, buf911, 8388608, stream=stream0)
        del arg351_1
        buf912 = buf882; del buf882  # reuse
        # Topologically Sorted Source Nodes: [attn_output_124], Original ATen: [aten._scaled_dot_product_efficient_attention]
        stream0 = get_raw_stream(0)
        triton_poi_fused__scaled_dot_product_efficient_attention_5.run(arg352_1, buf912, 8388608, stream=stream0)
        del arg352_1
        # Topologically Sorted Source Nodes: [attn_output_124], Original ATen: [aten._scaled_dot_product_efficient_attention]
        buf914 = torch.ops.aten._scaled_dot_product_efficient_attention.default(buf905, buf911, buf912, reinterpret_tensor(buf913, (1, 32, 1, 2048), (2048, 0, 2048, 1), 0), False, scale=0.08838834764831845)
        del buf911
        del buf912
        del buf913
        buf915 = buf914[0]
        assert_size_stride(buf915, (1, 32, 1, 128), (4096, 128, 4096, 1))
        del buf914
        buf919 = reinterpret_tensor(buf905, (1, 4096), (4096, 1), 0); del buf905  # reuse
        buf921 = buf863; del buf863  # reuse
        # Topologically Sorted Source Nodes: [hidden_states_119, hidden_states_122, hidden_states_123, attn_output_127, hidden_states_126], Original ATen: [aten.add, aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_add_mm_14.run(buf921, buf915, arg353_1, buf870, buf891, buf899, 4096, 4096, stream=stream0)
        del arg353_1
        del buf870
        del buf891
        del buf899
        buf922 = buf900; del buf900  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_63], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf919, 4096, reinterpret_tensor(buf921, (1, 4096), (4096, 1), 0), 4096, arg354_1, 1, buf922, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg354_1
        buf925 = reinterpret_tensor(buf915, (1, 4096), (4096, 1), 0); del buf915  # reuse
        buf926 = buf898; del buf898  # reuse
        # Topologically Sorted Source Nodes: [linear_221], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf919, arg355_1, buf926, 14336, 4096, stream=stream0)
        del arg355_1
        buf927 = buf897; del buf897  # reuse
        # Topologically Sorted Source Nodes: [linear_222], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_8.run(buf919, arg356_1, buf927, 14336, 4096, stream=stream0)
        del arg356_1
        del buf919
        buf930 = reinterpret_tensor(buf921, (1, 4096), (4096, 1), 0); del buf921  # reuse
        # Topologically Sorted Source Nodes: [down_proj_31, triton_kernel_wrapper_mutation_64], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_16.run(buf930, buf926, buf927, arg357_1, 4096, 14336, stream=stream0)
        del arg357_1
        del buf926
        del buf927
        buf929 = buf922; del buf922  # reuse
        # Topologically Sorted Source Nodes: [triton_kernel_wrapper_mutation_64], Original ATen: []
        stream0 = get_raw_stream(0)
        _rms_norm_forward_kernel_0.run(buf925, 4096, buf930, 4096, arg358_1, 1, buf929, 1, 4096, 1e-05, 0.0, 0, 4096, 1, 1, 1, stream=stream0)
        del arg358_1
        del buf929
        del buf930
        buf934 = empty_strided_cuda((1, 128256), (128256, 1), torch.float16)
        # Topologically Sorted Source Nodes: [logits], Original ATen: [aten.mm]
        stream0 = get_raw_stream(0)
        triton_red_fused_mm_17.run(buf925, arg359_1, buf934, 128256, 4096, stream=stream0)
        del arg359_1
        del buf925
    return (reinterpret_tensor(buf934, (1, 1, 128256), (128256, 128256, 1), 0), )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = rand_strided((1, 1), (1, 1), device='cuda:0', dtype=torch.int64)
    arg1_1 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg2_1 = rand_strided((1, ), (1, ), device='cuda:0', dtype=torch.int64)
    arg3_1 = rand_strided((1, 1), (1, 1), device='cuda:0', dtype=torch.int64)
    arg4_1 = rand_strided((1, 1, 1, 2048), (2048, 2048, 2048, 1), device='cuda:0', dtype=torch.bool)
    arg5_1 = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float32)
    arg6_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg7_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg8_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg9_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg10_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg11_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg12_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg13_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg14_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg15_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg16_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg17_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg18_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg19_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg20_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg21_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg22_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg23_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg24_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg25_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg26_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg27_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg28_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg29_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg30_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg31_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg32_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg33_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg34_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg35_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg36_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg37_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg38_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg39_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg40_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg41_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg42_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg43_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg44_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg45_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg46_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg47_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg48_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg49_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg50_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg51_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg52_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg53_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg54_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg55_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg56_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg57_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg58_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg59_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg60_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg61_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg62_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg63_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg64_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg65_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg66_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg67_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg68_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg69_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg70_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg71_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg72_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg73_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg74_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg75_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg76_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg77_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg78_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg79_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg80_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg81_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg82_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg83_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg84_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg85_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg86_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg87_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg88_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg89_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg90_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg91_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg92_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg93_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg94_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg95_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg96_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg97_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg98_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg99_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg100_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg101_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg102_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg103_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg104_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg105_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg106_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg107_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg108_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg109_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg110_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg111_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg112_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg113_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg114_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg115_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg116_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg117_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg118_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg119_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg120_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg121_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg122_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg123_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg124_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg125_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg126_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg127_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg128_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg129_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg130_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg131_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg132_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg133_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg134_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg135_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg136_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg137_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg138_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg139_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg140_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg141_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg142_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg143_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg144_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg145_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg146_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg147_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg148_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg149_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg150_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg151_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg152_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg153_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg154_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg155_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg156_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg157_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg158_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg159_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg160_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg161_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg162_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg163_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg164_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg165_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg166_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg167_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg168_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg169_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg170_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg171_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg172_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg173_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg174_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg175_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg176_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg177_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg178_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg179_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg180_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg181_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg182_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg183_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg184_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg185_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg186_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg187_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg188_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg189_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg190_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg191_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg192_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg193_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg194_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg195_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg196_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg197_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg198_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg199_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg200_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg201_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg202_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg203_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg204_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg205_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg206_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg207_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg208_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg209_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg210_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg211_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg212_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg213_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg214_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg215_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg216_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg217_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg218_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg219_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg220_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg221_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg222_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg223_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg224_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg225_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg226_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg227_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg228_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg229_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg230_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg231_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg232_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg233_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg234_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg235_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg236_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg237_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg238_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg239_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg240_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg241_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg242_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg243_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg244_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg245_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg246_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg247_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg248_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg249_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg250_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg251_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg252_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg253_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg254_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg255_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg256_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg257_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg258_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg259_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg260_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg261_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg262_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg263_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg264_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg265_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg266_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg267_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg268_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg269_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg270_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg271_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg272_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg273_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg274_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg275_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg276_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg277_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg278_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg279_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg280_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg281_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg282_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg283_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg284_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg285_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg286_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg287_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg288_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg289_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg290_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg291_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg292_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg293_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg294_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg295_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg296_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg297_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg298_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg299_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg300_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg301_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg302_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg303_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg304_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg305_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg306_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg307_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg308_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg309_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg310_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg311_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg312_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg313_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg314_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg315_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg316_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg317_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg318_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg319_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg320_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg321_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg322_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg323_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg324_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg325_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg326_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg327_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg328_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg329_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg330_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg331_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg332_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg333_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg334_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg335_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg336_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg337_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg338_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg339_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg340_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg341_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg342_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg343_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg344_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg345_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg346_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg347_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg348_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg349_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg350_1 = rand_strided((1024, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg351_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg352_1 = rand_strided((1, 8, 2048, 128), (2097152, 262144, 128, 1), device='cuda:0', dtype=torch.float16)
    arg353_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg354_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg355_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg356_1 = rand_strided((14336, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    arg357_1 = rand_strided((4096, 14336), (14336, 1), device='cuda:0', dtype=torch.float16)
    arg358_1 = rand_strided((4096, ), (1, ), device='cuda:0', dtype=torch.float16)
    arg359_1 = rand_strided((128256, 4096), (4096, 1), device='cuda:0', dtype=torch.float16)
    fn = lambda: call([arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1, arg27_1, arg28_1, arg29_1, arg30_1, arg31_1, arg32_1, arg33_1, arg34_1, arg35_1, arg36_1, arg37_1, arg38_1, arg39_1, arg40_1, arg41_1, arg42_1, arg43_1, arg44_1, arg45_1, arg46_1, arg47_1, arg48_1, arg49_1, arg50_1, arg51_1, arg52_1, arg53_1, arg54_1, arg55_1, arg56_1, arg57_1, arg58_1, arg59_1, arg60_1, arg61_1, arg62_1, arg63_1, arg64_1, arg65_1, arg66_1, arg67_1, arg68_1, arg69_1, arg70_1, arg71_1, arg72_1, arg73_1, arg74_1, arg75_1, arg76_1, arg77_1, arg78_1, arg79_1, arg80_1, arg81_1, arg82_1, arg83_1, arg84_1, arg85_1, arg86_1, arg87_1, arg88_1, arg89_1, arg90_1, arg91_1, arg92_1, arg93_1, arg94_1, arg95_1, arg96_1, arg97_1, arg98_1, arg99_1, arg100_1, arg101_1, arg102_1, arg103_1, arg104_1, arg105_1, arg106_1, arg107_1, arg108_1, arg109_1, arg110_1, arg111_1, arg112_1, arg113_1, arg114_1, arg115_1, arg116_1, arg117_1, arg118_1, arg119_1, arg120_1, arg121_1, arg122_1, arg123_1, arg124_1, arg125_1, arg126_1, arg127_1, arg128_1, arg129_1, arg130_1, arg131_1, arg132_1, arg133_1, arg134_1, arg135_1, arg136_1, arg137_1, arg138_1, arg139_1, arg140_1, arg141_1, arg142_1, arg143_1, arg144_1, arg145_1, arg146_1, arg147_1, arg148_1, arg149_1, arg150_1, arg151_1, arg152_1, arg153_1, arg154_1, arg155_1, arg156_1, arg157_1, arg158_1, arg159_1, arg160_1, arg161_1, arg162_1, arg163_1, arg164_1, arg165_1, arg166_1, arg167_1, arg168_1, arg169_1, arg170_1, arg171_1, arg172_1, arg173_1, arg174_1, arg175_1, arg176_1, arg177_1, arg178_1, arg179_1, arg180_1, arg181_1, arg182_1, arg183_1, arg184_1, arg185_1, arg186_1, arg187_1, arg188_1, arg189_1, arg190_1, arg191_1, arg192_1, arg193_1, arg194_1, arg195_1, arg196_1, arg197_1, arg198_1, arg199_1, arg200_1, arg201_1, arg202_1, arg203_1, arg204_1, arg205_1, arg206_1, arg207_1, arg208_1, arg209_1, arg210_1, arg211_1, arg212_1, arg213_1, arg214_1, arg215_1, arg216_1, arg217_1, arg218_1, arg219_1, arg220_1, arg221_1, arg222_1, arg223_1, arg224_1, arg225_1, arg226_1, arg227_1, arg228_1, arg229_1, arg230_1, arg231_1, arg232_1, arg233_1, arg234_1, arg235_1, arg236_1, arg237_1, arg238_1, arg239_1, arg240_1, arg241_1, arg242_1, arg243_1, arg244_1, arg245_1, arg246_1, arg247_1, arg248_1, arg249_1, arg250_1, arg251_1, arg252_1, arg253_1, arg254_1, arg255_1, arg256_1, arg257_1, arg258_1, arg259_1, arg260_1, arg261_1, arg262_1, arg263_1, arg264_1, arg265_1, arg266_1, arg267_1, arg268_1, arg269_1, arg270_1, arg271_1, arg272_1, arg273_1, arg274_1, arg275_1, arg276_1, arg277_1, arg278_1, arg279_1, arg280_1, arg281_1, arg282_1, arg283_1, arg284_1, arg285_1, arg286_1, arg287_1, arg288_1, arg289_1, arg290_1, arg291_1, arg292_1, arg293_1, arg294_1, arg295_1, arg296_1, arg297_1, arg298_1, arg299_1, arg300_1, arg301_1, arg302_1, arg303_1, arg304_1, arg305_1, arg306_1, arg307_1, arg308_1, arg309_1, arg310_1, arg311_1, arg312_1, arg313_1, arg314_1, arg315_1, arg316_1, arg317_1, arg318_1, arg319_1, arg320_1, arg321_1, arg322_1, arg323_1, arg324_1, arg325_1, arg326_1, arg327_1, arg328_1, arg329_1, arg330_1, arg331_1, arg332_1, arg333_1, arg334_1, arg335_1, arg336_1, arg337_1, arg338_1, arg339_1, arg340_1, arg341_1, arg342_1, arg343_1, arg344_1, arg345_1, arg346_1, arg347_1, arg348_1, arg349_1, arg350_1, arg351_1, arg352_1, arg353_1, arg354_1, arg355_1, arg356_1, arg357_1, arg358_1, arg359_1])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
