# hf model args
device: "cuda:0"
compute_dtype: fp16
model_id: "unsloth/Meta-Llama-3.1-8B-Instruct"
cache_dir: "/root/.cache/huggingface/hub"
attn_implementation: "sdpa"  # flex_attention, sdpa, flash_attention_2, eager

# profiling args
profiling_dir: "log/baseline"
skip_first: 0
wait: 0
warmup: 1
active: 1
repeat: 1

# prompt args
prompt: "Write an essay about large language models."
max_new_tokens: 1024
chat_template: ""
do_sample: False
top_k: null
top_p: null
temperature: null
use_cache: True
cache_implementation: "static"
padding_side: "left"