# hf model args
device: "cuda:0"
dtype: fp16
model_id: unsloth/gemma-3-270m
cache_dir: "/root/.cache/huggingface/hub"
attn_implementation: "sdpa"  # flex_attention, sdpa, flash_attention_2, eager

# profiling args
profiling_dir: "log/sanity_check"
skip_first: 0
wait: 0
warmup: 0
active: 1
repeat: 1

# prompt args
prompt: "Write an essay about large language models."
max_new_tokens: 1024
chat_template: ""
do_sample: False
top_k: null
temperature: null
use_cache: True
cache_implementation: "static"
padding_side: "left"