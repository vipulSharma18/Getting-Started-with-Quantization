[project]
name = "low_bit_inference"
version = "0.1.0"
description = "Low bit inference with inference using different quantization and optimization tools."
readme = "README.md"
requires-python = ">=3.12"
authors = [
    { name = "Vipul Sharma", email = "vipuls181999@gmail.com" }
]

dependencies = [
    "torch>=2.7.0",
    "torchvision",
    "HolisticTraceAnalysis",
    "ipython",
    "jupyterlab",
    "notebook",
    "ipykernel",
    "huggingface_hub[hf_transfer,cli]",
    "tensorboard",
    "tensorboard-data-server",
    "torch-tb-profiler",
    "omegaconf",
    "tlparse",
    "graphviz",
    "snakeviz",
]

[dependency-groups]
big_deps = [
    "nvidia-modelopt[all]",
    "torchao",
    "transformers",
    "gemlite",
    "hqq",
    "optimum-quanto",
    # "tensorrt-llm", -> makes uv lock get stuck, figure out installation later
]

[tool.uv.sources]
torch = { index = "pytorch-cu128" }
torchvision = { index = "pytorch-cu128" }
torchao = { index = "pytorch-cu128" }
hqq = { git = "git+https://github.com/mobiusml/hqq/" }
gemlite = { git = "git+https://github.com/mobiusml/gemlite/" }

[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true