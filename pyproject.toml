[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "low_bit_inference"
version = "0.1.0"
description = "Low bit inference using different quantization and optimization tools."
readme = "README.md"
requires-python = ">=3.12"
authors = [
    { name = "Vipul Sharma", email = "vipuls181999@gmail.com" }
]

dependencies = [
    "torch>=2.8.0",
    "ipython",
    "huggingface_hub[hf_transfer,cli]",
    "omegaconf",
    "tlparse",
]

[dependency-groups]
big_deps = [
    "torchao>=0.14.0",
    "transformers",
    "gemlite",
    "hqq",
    "nvidia-ml-py",
    "fbgemm_gpu>=1.3",
]

[tool.uv.sources]
torch = { index = "pytorch-cu128" }
torchvision = { index = "pytorch-cu128" }
torchao = { index = "pytorch-cu128" }
hqq = { git = "git+https://github.com/dropbox/hqq/" }
gemlite = { git = "git+https://github.com/dropbox/gemlite/" }

[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true