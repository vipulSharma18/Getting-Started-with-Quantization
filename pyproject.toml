[project]
name = "low_bit_vllm"
version = "0.1.0"
description = "Low bit inference with vLLM using different quantization and optimization tools."
readme = "README.md"
requires-python = ">=3.12"
authors = [
    { name = "Vipul Sharma", email = "vipuls181999@gmail.com" }
]

dependencies = [
    "torch>=2.7.0",
    "torchvision",
    "HolisticTraceAnalysis",
    "ipython",
    "jupyterlab",
    "notebook",
    "ipykernel",
    "huggingface_hub[hf_transfer,cli]",
    "tensorboard",
    "tensorboard-data-server",
    "torch-tb-profiler",
    "omegaconf",
]

[dependency-groups]
gemlite = [
    "transformers==4.53.2",
    "gemlite",
    "hqq",
]
tensorRT = [
    "tensorrt-llm>=0.21.0",
]
torchao = [
    "torchao>=0.12.0",
]

[tool.uv]
conflicts = [
    [
        { group = "gemlite" },
        { group = "tensorRT" },
        { group = "torchao" },
    ]
]

[tool.uv.sources]
torch = { index = "pytorch-cu128" }
torchvision = { index = "pytorch-cu128" }
torchao = { index = "pytorch-cu128" }
hqq = { git = "git+https://github.com/mobiusml/hqq/" }
gemlite = { git = "git+https://github.com/mobiusml/gemlite/" }

[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true
