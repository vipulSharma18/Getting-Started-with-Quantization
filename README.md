# low-bit-vllm

[![Docker](https://github.com/vipulSharma18/low-bit-vllm/actions/workflows/docker-publish.yml/badge.svg)](https://github.com/vipulSharma18/low-bit-vllm/actions/workflows/docker-publish.yml) [![Run on VastAI](https://img.shields.io/badge/Run_on-VastAI-blue)](https://cloud.vast.ai?ref_id=288801&template_id=bc0609fee288cad6d15b1262dbc83214)

Experimenting with different methods for low bit inference of Llama-3.1 with vLLM support.

GitHub setup within the container:
```
gh auth login
git config --global user.email "vipuls181999@gmail.com"
git config --global user.name "Vipul Sharma"
```