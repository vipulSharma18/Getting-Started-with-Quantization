# low-bit-vllm
Experimenting with different methods for low bit inference of Llama-3.1 with vLLM support.
